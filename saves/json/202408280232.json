[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.14434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14434v1",
                "updated": "2024-08-26T17:21:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:21:19Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "title": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena"
                },
                "summary": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing."
                },
                "authors": [
                    {
                        "name": "Logan Ward"
                    },
                    {
                        "name": "J. Gregory Pauloski"
                    },
                    {
                        "name": "Valerie Hayot-Sasson"
                    },
                    {
                        "name": "Yadu Babuji"
                    },
                    {
                        "name": "Alexander Brace"
                    },
                    {
                        "name": "Ryan Chard"
                    },
                    {
                        "name": "Kyle Chard"
                    },
                    {
                        "name": "Rajeev Thakur"
                    },
                    {
                        "name": "Ian Foster"
                    }
                ],
                "author_detail": {
                    "name": "Ian Foster"
                },
                "author": "Ian Foster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06876v2",
                "updated": "2024-08-26T11:29:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    11,
                    29,
                    7,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-13T13:14:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "Decision-Focused Learning to Predict Action Costs for Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Focused Learning to Predict Action Costs for Planning"
                },
                "summary": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements."
                },
                "authors": [
                    {
                        "name": "Jayanta Mandi"
                    },
                    {
                        "name": "Marco Foschini"
                    },
                    {
                        "name": "Daniel Holler"
                    },
                    {
                        "name": "Sylvie Thiebaux"
                    },
                    {
                        "name": "Jorg Hoffmann"
                    },
                    {
                        "name": "Tias Guns"
                    }
                ],
                "author_detail": {
                    "name": "Tias Guns"
                },
                "author": "Tias Guns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16343v2",
                "updated": "2024-08-26T07:26:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    7,
                    26,
                    27,
                    0,
                    239,
                    0
                ],
                "published": "2024-02-26T06:55:36Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    6,
                    55,
                    36,
                    0,
                    57,
                    0
                ],
                "title": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems"
                },
                "summary": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures."
                },
                "authors": [
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "Accepted by PACT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08795v2",
                "updated": "2024-08-26T04:32:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    32,
                    56,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-16T15:11:12Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "title": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks"
                },
                "summary": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding"
                },
                "authors": [
                    {
                        "name": "Divya Ojha"
                    },
                    {
                        "name": "Sandhya Dwarkadas"
                    }
                ],
                "author_detail": {
                    "name": "Sandhya Dwarkadas"
                },
                "author": "Sandhya Dwarkadas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14001v1",
                "updated": "2024-08-26T03:58:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T03:58:20Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "title": "Decentralized Federated Learning with Model Caching on Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Model Caching on Mobile Agents"
                },
                "summary": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Houwei Cao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13605v1",
                "updated": "2024-08-24T15:23:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "published": "2024-08-24T15:23:32Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "title": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning"
                },
                "summary": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods."
                },
                "authors": [
                    {
                        "name": "Yuhan Yi"
                    },
                    {
                        "name": "Guanglin Zhang"
                    },
                    {
                        "name": "Hai Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hai Jiang"
                },
                "author": "Hai Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v3",
                "updated": "2024-08-23T17:54:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    54,
                    34,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/."
                },
                "authors": [
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13165v1",
                "updated": "2024-08-23T15:39:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T15:39:20Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "title": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches"
                },
                "summary": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme."
                },
                "authors": [
                    {
                        "name": "Dhruv Pratap Singh"
                    },
                    {
                        "name": "Anjana A. Mahesh"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "15 pages, 5 figures and one table. Some overlap of introductory and\n  background materials with our earlier submission arXiv:2407.00677v1 dated 30\n  June 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.05332v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.05332v5",
                "updated": "2024-08-23T13:25:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    25,
                    7,
                    4,
                    236,
                    0
                ],
                "published": "2023-05-09T10:41:36Z",
                "published_parsed": [
                    2023,
                    5,
                    9,
                    10,
                    41,
                    36,
                    1,
                    129,
                    0
                ],
                "title": "Fundamental Limits of Multi-Message Private Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limits of Multi-Message Private Computation"
                },
                "summary": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$."
                },
                "authors": [
                    {
                        "name": "Ali Gholami"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Tayyebeh Jahani-Nezhad"
                    },
                    {
                        "name": "Hua Sun"
                    },
                    {
                        "name": "Mingyue Ji"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "A version of this paper is submitted to IEEE Transactions on\n  Communications. A short version was accepted and presented at ISIT 2024 in\n  Athens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.05332v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.05332v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12947v1",
                "updated": "2024-08-23T09:54:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T09:54:22Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "title": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis"
                },
                "summary": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness."
                },
                "authors": [
                    {
                        "name": "Vini Kanvar"
                    },
                    {
                        "name": "Uday P. Khedker"
                    }
                ],
                "author_detail": {
                    "name": "Uday P. Khedker"
                },
                "author": "Uday P. Khedker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v1",
                "updated": "2024-08-22T17:56:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "13 pages, 16 figures, Submitted to ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14533v2",
                "updated": "2024-08-22T17:47:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    47,
                    49,
                    3,
                    235,
                    0
                ],
                "published": "2023-09-25T21:17:17Z",
                "published_parsed": [
                    2023,
                    9,
                    25,
                    21,
                    17,
                    17,
                    0,
                    268,
                    0
                ],
                "title": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties"
                },
                "summary": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry."
                },
                "authors": [
                    {
                        "name": "Simon Hettler"
                    },
                    {
                        "name": "Kankona Singha Roy"
                    },
                    {
                        "name": "Raul Arenal"
                    },
                    {
                        "name": "Leela S. Panchakarla"
                    }
                ],
                "author_detail": {
                    "name": "Leela S. Panchakarla"
                },
                "author": "Leela S. Panchakarla",
                "arxiv_doi": "10.1002/admi.202400317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/admi.202400317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.14533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Adv. Mater. Interfaces 2024, 2400317",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11506v1",
                "updated": "2024-08-21T10:26:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:26:26Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "title": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage"
                },
                "summary": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration."
                },
                "authors": [
                    {
                        "name": "Pedro C Rijo"
                    },
                    {
                        "name": "Francisco J. Galindo-Rosales"
                    }
                ],
                "author_detail": {
                    "name": "Francisco J. Galindo-Rosales"
                },
                "author": "Francisco J. Galindo-Rosales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10685v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10685v2",
                "updated": "2024-08-21T06:10:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    10,
                    2,
                    2,
                    234,
                    0
                ],
                "published": "2024-01-19T13:32:55Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    13,
                    32,
                    55,
                    4,
                    19,
                    0
                ],
                "title": "Towards End-to-End GPS Localization with Neural Pseudorange Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards End-to-End GPS Localization with Neural Pseudorange Correction"
                },
                "summary": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet."
                },
                "authors": [
                    {
                        "name": "Xu Weng"
                    },
                    {
                        "name": "KV Ling"
                    },
                    {
                        "name": "Haochen Liu"
                    },
                    {
                        "name": "Kun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Kun Cao"
                },
                "author": "Kun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10685v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10685v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11325v1",
                "updated": "2024-08-21T04:16:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T04:16:49Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "title": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory"
                },
                "summary": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads."
                },
                "authors": [
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Ehsan Hajyjasini"
                    },
                    {
                        "name": "Seungjin Lee"
                    },
                    {
                        "name": "Zifeng Zhang"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Steven Swanson"
                    }
                ],
                "author_detail": {
                    "name": "Steven Swanson"
                },
                "author": "Steven Swanson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v3",
                "updated": "2024-08-21T02:32:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    2,
                    32,
                    43,
                    2,
                    234,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "Matrix quantization compresses matrix elements into a more compact form to\nreduce storage requirements, with dequantization enabling reconstruction for\nuse. We define the Quantization Error Minimization (QEM) problem as minimizing\nthe difference between the original and quantized matrices while ensuring the\nquantized matrix remains within fixed memory constraints. This technique is\ncrucial in applications like Large Language Model (LLM) weight compression and\nKV cache compression, where large matrix sizes demand efficient storage\nsolutions.\n  As modern LLMs like GPT-4 and BERT continue to grow, effective matrix\ncompression is increasingly important. These models contain billions of\nparameters in matrix form, making efficient weight quantization essential for\nboth storage and computational efficiency. Similarly, KV caches, storing\nintermediate inference results, are matrix-based and benefit significantly from\noptimized compression techniques.\n  To address the QEM problem in the context of LLM weight and KV cache\ncompression, we propose Quantum Entanglement Trees (QET). QET leverages the\nlocal structure of matrix elements by iteratively swapping elements to create a\nlocally ordered matrix, which is then grouped and quantized column by column.\nTo enhance QET, we introduce two optimizations: residual quantization to\nfurther reduce Mean Squared Error (MSE) and masking with batch processing to\naccelerate the algorithm.\n  Our experiments demonstrate that QET can reduce MSE to 12.3% of its original\nvalue at the same compression ratio, outperforming leading baseline methods.\nOur contributions include framing the QEM problem specifically for LLM and KV\ncache compression, developing the QET algorithm, and implementing optimizations\nthat improve accuracy and processing speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix quantization compresses matrix elements into a more compact form to\nreduce storage requirements, with dequantization enabling reconstruction for\nuse. We define the Quantization Error Minimization (QEM) problem as minimizing\nthe difference between the original and quantized matrices while ensuring the\nquantized matrix remains within fixed memory constraints. This technique is\ncrucial in applications like Large Language Model (LLM) weight compression and\nKV cache compression, where large matrix sizes demand efficient storage\nsolutions.\n  As modern LLMs like GPT-4 and BERT continue to grow, effective matrix\ncompression is increasingly important. These models contain billions of\nparameters in matrix form, making efficient weight quantization essential for\nboth storage and computational efficiency. Similarly, KV caches, storing\nintermediate inference results, are matrix-based and benefit significantly from\noptimized compression techniques.\n  To address the QEM problem in the context of LLM weight and KV cache\ncompression, we propose Quantum Entanglement Trees (QET). QET leverages the\nlocal structure of matrix elements by iteratively swapping elements to create a\nlocally ordered matrix, which is then grouped and quantized column by column.\nTo enhance QET, we introduce two optimizations: residual quantization to\nfurther reduce Mean Squared Error (MSE) and masking with batch processing to\naccelerate the algorithm.\n  Our experiments demonstrate that QET can reduce MSE to 12.3% of its original\nvalue at the same compression ratio, outperforming leading baseline methods.\nOur contributions include framing the QEM problem specifically for LLM and KV\ncache compression, developing the QET algorithm, and implementing optimizations\nthat improve accuracy and processing speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10970v1",
                "updated": "2024-08-20T16:02:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T16:02:54Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "title": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "4 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10746v1",
                "updated": "2024-08-20T11:30:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T11:30:12Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "title": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint."
                },
                "authors": [
                    {
                        "name": "Bei Ouyang"
                    },
                    {
                        "name": "Shengyuan Ye"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Tianyi Qian"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "arxiv_comment": "Accepted by The 53rd International Conference on Parallel Processing\n  (ICPP'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09697v2",
                "updated": "2024-08-20T04:46:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    4,
                    46,
                    18,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T04:43:56Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    43,
                    56,
                    0,
                    232,
                    0
                ],
                "title": "Heta: Distributed Training of Heterogeneous Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heta: Distributed Training of Heterogeneous Graph Neural Networks"
                },
                "summary": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhong"
                    },
                    {
                        "name": "Junwei Su"
                    },
                    {
                        "name": "Chuan Wu"
                    },
                    {
                        "name": "Minjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Minjie Wang"
                },
                "author": "Minjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10104v1",
                "updated": "2024-08-19T15:47:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T15:47:17Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "title": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory"
                },
                "summary": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging."
                },
                "authors": [
                    {
                        "name": "Olena Tkach"
                    },
                    {
                        "name": "Gerd Schoenhense"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Schoenhense"
                },
                "author": "Gerd Schoenhense",
                "arxiv_comment": "17 pages, 4 figures, 44 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09848v1",
                "updated": "2024-08-19T09:50:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:50:35Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "title": "Abstract Environment Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract Environment Trimming"
                },
                "summary": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times."
                },
                "authors": [
                    {
                        "name": "Daniel Jurjo-Rivas"
                    },
                    {
                        "name": "Jose F. Morales"
                    },
                    {
                        "name": "Pedro López-García"
                    },
                    {
                        "name": "Manuel V. Hermenegildo"
                    }
                ],
                "author_detail": {
                    "name": "Manuel V. Hermenegildo"
                },
                "author": "Manuel V. Hermenegildo",
                "arxiv_comment": "61 pages, 10 figures, 7 tables, submitted to ICLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10284v1",
                "updated": "2024-08-19T03:27:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T03:27:15Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "title": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_doi": "10.1145/3676536.3676741",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676741",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.10284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07092v2",
                "updated": "2024-08-18T17:27:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    17,
                    27,
                    17,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-11T18:40:36Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    40,
                    36,
                    6,
                    224,
                    0
                ],
                "title": "Post-Training Sparse Attention with Double Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Sparse Attention with Double Sparsity"
                },
                "summary": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Lianmin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Lianmin Zheng"
                },
                "author": "Lianmin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09483v1",
                "updated": "2024-08-18T13:54:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-18T13:54:46Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMD: A Cache-assisted GPU Memory Deduplication Architecture"
                },
                "summary": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Dan Feng"
                    },
                    {
                        "name": "Wei Tong"
                    },
                    {
                        "name": "Xueliang Wei"
                    },
                    {
                        "name": "Bing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Bing Wu"
                },
                "author": "Bing Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v3",
                "updated": "2024-08-16T08:46:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    46,
                    33,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v1",
                "updated": "2024-08-16T06:11:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v2",
                "updated": "2024-08-16T04:12:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    12,
                    25,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v3",
                "updated": "2024-08-15T05:24:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    24,
                    19,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07853v1",
                "updated": "2024-08-14T23:42:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T23:42:46Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "title": "A Case for Enabling Delegation of 5G Core Decisions to the RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case for Enabling Delegation of 5G Core Decisions to the RAN"
                },
                "summary": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation."
                },
                "authors": [
                    {
                        "name": "Lucas Vancina"
                    },
                    {
                        "name": "Geoffrey Xie"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Xie"
                },
                "author": "Geoffrey Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v2",
                "updated": "2024-08-14T09:18:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    18,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07304v1",
                "updated": "2024-08-14T05:42:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T05:42:35Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "title": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption"
                },
                "summary": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS."
                },
                "authors": [
                    {
                        "name": "Jonathan Ly"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Ly"
                },
                "author": "Jonathan Ly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15743v2",
                "updated": "2024-08-13T13:56:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    56,
                    14,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization"
                },
                "summary": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04043v3",
                "updated": "2024-08-13T13:31:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    31,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-07T18:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    18,
                    51,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "Ownership in low-level intermediate representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ownership in low-level intermediate representation"
                },
                "summary": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving."
                },
                "authors": [
                    {
                        "name": "Siddharth Priya"
                    },
                    {
                        "name": "Arie Gurfinkel"
                    }
                ],
                "author_detail": {
                    "name": "Arie Gurfinkel"
                },
                "author": "Arie Gurfinkel",
                "arxiv_comment": "FMCAD 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v3",
                "updated": "2024-08-13T09:55:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    55,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00167v2",
                "updated": "2024-08-13T09:08:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    8,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-31T21:33:56Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    33,
                    56,
                    2,
                    213,
                    0
                ],
                "title": "Finch: Prompt-guided Key-Value Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finch: Prompt-guided Key-Value Cache Compression"
                },
                "summary": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "arxiv_comment": "Accepted for publication at TACL - pre-MIT Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v1",
                "updated": "2024-08-12T08:46:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles"
                },
                "summary": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19895v2",
                "updated": "2024-08-12T07:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    47,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-29T11:17:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    11,
                    17,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor"
                },
                "summary": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Luca Valente"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Massimiliano Giacometti"
                    },
                    {
                        "name": "Abdul Basit Sajjad"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "4 pages, 4 figures, DSD2024 and SEAA2024 Works in Progress Session\n  AUG 2024; Updated the acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05912v1",
                "updated": "2024-08-12T03:53:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T03:53:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Correct Wrong Path",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correct Wrong Path"
                },
                "summary": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP."
                },
                "authors": [
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Sankara Prasad Ramesh"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Svilen Kanev"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "5 pages, 7 Figures, Submited to Computer Architecture Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v2",
                "updated": "2024-08-11T16:35:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    16,
                    35,
                    10,
                    6,
                    224,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "Added Section IV - (performance analysis of proposed HPDA\n  construction). The term 'coding delay' is formally defined (page no. 5). 14\n  pages, 10 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19410v2",
                "updated": "2024-08-11T08:07:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    8,
                    7,
                    28,
                    6,
                    224,
                    0
                ],
                "published": "2024-02-29T18:07:58Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    18,
                    7,
                    58,
                    3,
                    60,
                    0
                ],
                "title": "Genie: Smart ROS-based Caching for Connected Autonomous Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genie: Smart ROS-based Caching for Connected Autonomous Robots"
                },
                "summary": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time."
                },
                "authors": [
                    {
                        "name": "Zexin Li"
                    },
                    {
                        "name": "Soroush Bateni"
                    },
                    {
                        "name": "Cong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cong Liu"
                },
                "author": "Cong Liu",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v1",
                "updated": "2024-08-10T22:47:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05614v1",
                "updated": "2024-08-10T19:17:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T19:17:46Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "title": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model"
                },
                "summary": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources."
                },
                "authors": [
                    {
                        "name": "Hanqiu Chen"
                    },
                    {
                        "name": "Yitu Wang"
                    },
                    {
                        "name": "Luis Vitorio Cargnini"
                    },
                    {
                        "name": "Mohammadreza Soltaniyeh"
                    },
                    {
                        "name": "Dongyang Li"
                    },
                    {
                        "name": "Gongjin Sun"
                    },
                    {
                        "name": "Pradeep Subedi"
                    },
                    {
                        "name": "Andrew Chang"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Cong Hao"
                    }
                ],
                "author_detail": {
                    "name": "Cong Hao"
                },
                "author": "Cong Hao",
                "arxiv_comment": "This paper is accepted by DAC2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05171v1",
                "updated": "2024-08-09T16:48:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:48:01Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "title": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch"
                },
                "summary": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin."
                },
                "authors": [
                    {
                        "name": "R. A. Ryan"
                    },
                    {
                        "name": "P. E. Tsai"
                    },
                    {
                        "name": "A. R. Johansen"
                    },
                    {
                        "name": "A. Youmans"
                    },
                    {
                        "name": "D. P. Higginson"
                    },
                    {
                        "name": "J. M. Mitrani"
                    },
                    {
                        "name": "C. S. Adams"
                    },
                    {
                        "name": "D. A. Sutherland"
                    },
                    {
                        "name": "B. Levitt"
                    },
                    {
                        "name": "U. Shumlak"
                    }
                ],
                "author_detail": {
                    "name": "U. Shumlak"
                },
                "author": "U. Shumlak",
                "arxiv_comment": "16 pages, 11 figures, submitted to Journal of Nuclear Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03675v2",
                "updated": "2024-08-08T01:20:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    8,
                    1,
                    20,
                    13,
                    3,
                    221,
                    0
                ],
                "published": "2024-08-07T10:31:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    10,
                    31,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time"
                },
                "summary": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Guoxia Wang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "arxiv_comment": "Accepted by ACL 2024 (main conference, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.10978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.10978v2",
                "updated": "2024-08-07T23:48:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    23,
                    48,
                    59,
                    2,
                    220,
                    0
                ],
                "published": "2022-10-20T02:58:36Z",
                "published_parsed": [
                    2022,
                    10,
                    20,
                    2,
                    58,
                    36,
                    3,
                    293,
                    0
                ],
                "title": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends"
                },
                "summary": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem."
                },
                "authors": [
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Youyang Qu"
                    },
                    {
                        "name": "Yong Xiang"
                    },
                    {
                        "name": "Md Palash Uddin"
                    },
                    {
                        "name": "Dezhong Peng"
                    },
                    {
                        "name": "Longxiang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Longxiang Gao"
                },
                "author": "Longxiang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.10978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.10978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v1",
                "updated": "2024-08-07T22:10:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v2",
                "updated": "2024-08-07T20:43:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    20,
                    43,
                    10,
                    2,
                    220,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration..",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03652v1",
                "updated": "2024-08-07T09:34:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T09:34:55Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "title": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search"
                },
                "summary": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdou"
                    },
                    {
                        "name": "Tasneem Mohsen"
                    }
                ],
                "author_detail": {
                    "name": "Tasneem Mohsen"
                },
                "author": "Tasneem Mohsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v1",
                "updated": "2024-08-06T17:16:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02999v1",
                "updated": "2024-08-06T07:12:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T07:12:09Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "title": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning"
                },
                "summary": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop."
                },
                "authors": [
                    {
                        "name": "Lekai Chen"
                    },
                    {
                        "name": "Ashutosh Trivedi"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    }
                ],
                "author_detail": {
                    "name": "Alvaro Velasquez"
                },
                "author": "Alvaro Velasquez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02911v1",
                "updated": "2024-08-06T02:51:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T02:51:22Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "title": "NVPC: A Transparent NVM Page Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVPC: A Transparent NVM Page Cache"
                },
                "summary": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases."
                },
                "authors": [
                    {
                        "name": "Guoyu Wang"
                    },
                    {
                        "name": "Xilong Che"
                    },
                    {
                        "name": "Haoyang Wei"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Puyi He"
                    },
                    {
                        "name": "Juncheng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Hu"
                },
                "author": "Juncheng Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02409v1",
                "updated": "2024-08-05T12:09:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T12:09:50Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "title": "Electron-beam-induced modification of gold microparticles in an SEM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced modification of gold microparticles in an SEM"
                },
                "summary": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings."
                },
                "authors": [
                    {
                        "name": "Kristina Weinel"
                    },
                    {
                        "name": "Marc Benjamin Hahn"
                    },
                    {
                        "name": "Axel Lubk"
                    },
                    {
                        "name": "Wen Feng"
                    },
                    {
                        "name": "Ignacio Gonzalez Martinez"
                    },
                    {
                        "name": "Bernd Büchner"
                    },
                    {
                        "name": "Leonardo Agudo Jácome"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo Agudo Jácome"
                },
                "author": "Leonardo Agudo Jácome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05235v1",
                "updated": "2024-08-05T09:07:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T09:07:06Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "title": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving"
                },
                "summary": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server."
                },
                "authors": [
                    {
                        "name": "Andreas Kosmas Kakolyris"
                    },
                    {
                        "name": "Dimosthenis Masouros"
                    },
                    {
                        "name": "Petros Vavaroutsos"
                    },
                    {
                        "name": "Sotirios Xydis"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11912v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11912v3",
                "updated": "2024-08-04T00:58:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    58,
                    4,
                    6,
                    217,
                    0
                ],
                "published": "2024-04-18T05:25:54Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    5,
                    25,
                    54,
                    3,
                    109,
                    0
                ],
                "title": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding"
                },
                "summary": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11912v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11912v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v1",
                "updated": "2024-08-04T00:38:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Large Language Models"
                },
                "summary": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Working in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01519v1",
                "updated": "2024-08-02T18:25:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-02T18:25:57Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "title": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling"
                },
                "summary": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition."
                },
                "authors": [
                    {
                        "name": "Xiao Jiang"
                    },
                    {
                        "name": "Grace J. Gang"
                    },
                    {
                        "name": "J. Webster Stayman"
                    }
                ],
                "author_detail": {
                    "name": "J. Webster Stayman"
                },
                "author": "J. Webster Stayman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00327v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00327v2",
                "updated": "2024-08-02T07:37:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    7,
                    37,
                    51,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-01T07:00:18Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    0,
                    18,
                    3,
                    214,
                    0
                ],
                "title": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration"
                },
                "summary": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Yuan-Hao Chang"
                    },
                    {
                        "name": "Tei-Wei Kuo"
                    }
                ],
                "author_detail": {
                    "name": "Tei-Wei Kuo"
                },
                "author": "Tei-Wei Kuo",
                "arxiv_comment": "This paper has been accepted for presentation at the The\n  International Conference on Hardware/Software Codesign and System Synthesis\n  (CODES+ISSS) in September, 2024. An extended abstract of this paper was\n  presented in Design, Automation & Test in Europe Conference & Exhibition\n  (DATE), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00327v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00327v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00957v1",
                "updated": "2024-08-01T23:52:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T23:52:43Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "title": "Caching Aided Multi-Tenant Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Aided Multi-Tenant Serverless Computing"
                },
                "summary": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead."
                },
                "authors": [
                    {
                        "name": "Chu Qiao"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Zhenkai Zhang"
                    },
                    {
                        "name": "Yuede Ji"
                    },
                    {
                        "name": "Xing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xing Gao"
                },
                "author": "Xing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00859v2",
                "updated": "2024-08-01T21:21:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    21,
                    21,
                    28,
                    3,
                    214,
                    0
                ],
                "published": "2024-04-01T02:01:28Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    2,
                    1,
                    28,
                    0,
                    92,
                    0
                ],
                "title": "Do language models plan ahead for future tokens?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do language models plan ahead for future tokens?"
                },
                "summary": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale."
                },
                "authors": [
                    {
                        "name": "Wilson Wu"
                    },
                    {
                        "name": "John X. Morris"
                    },
                    {
                        "name": "Lionel Levine"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Levine"
                },
                "author": "Lionel Levine",
                "arxiv_comment": "24 pages, 11 figures. Camera-ready for COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00539v1",
                "updated": "2024-08-01T13:22:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T13:22:01Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs"
                },
                "summary": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance."
                },
                "authors": [
                    {
                        "name": "Mingcong Lu"
                    },
                    {
                        "name": "Jiangcai Zhu"
                    },
                    {
                        "name": "Wang Hao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Shusheng Zhang"
                    },
                    {
                        "name": "Kailai Shao"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Xin Lu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Lu"
                },
                "author": "Xin Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v2",
                "updated": "2024-08-01T13:21:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    21,
                    24,
                    3,
                    214,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Offloading-Efficient MoE Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Offloading-Efficient MoE Model Serving"
                },
                "summary": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15220v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15220v4",
                "updated": "2024-08-01T07:51:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    51,
                    25,
                    3,
                    214,
                    0
                ],
                "published": "2024-02-23T09:29:19Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    9,
                    29,
                    19,
                    4,
                    54,
                    0
                ],
                "title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition"
                },
                "summary": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096."
                },
                "authors": [
                    {
                        "name": "Lu Ye"
                    },
                    {
                        "name": "Ze Tao"
                    },
                    {
                        "name": "Yong Huang"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15220v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15220v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00232v1",
                "updated": "2024-08-01T01:57:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T01:57:09Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "title": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction"
                },
                "summary": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks."
                },
                "authors": [
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Zite Jiang"
                    },
                    {
                        "name": "Haihang You"
                    }
                ],
                "author_detail": {
                    "name": "Haihang You"
                },
                "author": "Haihang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v2",
                "updated": "2024-08-01T00:41:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    0,
                    41,
                    52,
                    3,
                    214,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Towards Variable-Length In-Network Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Variable-Length In-Network Caching"
                },
                "summary": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20485v2",
                "updated": "2024-07-31T02:02:40Z",
                "updated_parsed": [
                    2024,
                    7,
                    31,
                    2,
                    2,
                    40,
                    2,
                    213,
                    0
                ],
                "published": "2024-07-30T01:13:42Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    1,
                    13,
                    42,
                    1,
                    212,
                    0
                ],
                "title": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder"
                },
                "summary": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot."
                },
                "authors": [
                    {
                        "name": "Hyun-rae Jo"
                    },
                    {
                        "name": "Dongkun Shin"
                    }
                ],
                "author_detail": {
                    "name": "Dongkun Shin"
                },
                "author": "Dongkun Shin",
                "arxiv_comment": "11 pages(9 pages + reference 2 pages), 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21201v1",
                "updated": "2024-07-30T21:27:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T21:27:00Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "title": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite"
                },
                "summary": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE"
                },
                "authors": [
                    {
                        "name": "Abdulkarim A. Amirov"
                    },
                    {
                        "name": "Maksim A. Koliushenkov"
                    },
                    {
                        "name": "Abdula A. Mukhuchev"
                    },
                    {
                        "name": "Dibir M. Yusupov"
                    },
                    {
                        "name": "Valeriya V. Govorina"
                    },
                    {
                        "name": "Dmitriy S. Neznakhin"
                    },
                    {
                        "name": "Gennady A. Govor"
                    },
                    {
                        "name": "Akhmed M. Aliev"
                    }
                ],
                "author_detail": {
                    "name": "Akhmed M. Aliev"
                },
                "author": "Akhmed M. Aliev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v1",
                "updated": "2024-07-30T18:19:38Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v1",
                "updated": "2024-07-30T17:59:08Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.06944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.06944v2",
                "updated": "2024-07-30T13:06:36Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    13,
                    6,
                    36,
                    1,
                    212,
                    0
                ],
                "published": "2023-04-14T06:21:57Z",
                "published_parsed": [
                    2023,
                    4,
                    14,
                    6,
                    21,
                    57,
                    4,
                    104,
                    0
                ],
                "title": "SpChar: Characterizing the Sparse Puzzle via Decision Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpChar: Characterizing the Sparse Puzzle via Decision Trees"
                },
                "summary": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied."
                },
                "authors": [
                    {
                        "name": "Francesco Sgherzi"
                    },
                    {
                        "name": "Marco Siracusa"
                    },
                    {
                        "name": "Ivan Fernandez"
                    },
                    {
                        "name": "Adrià Armejach"
                    },
                    {
                        "name": "Miquel Moretó"
                    }
                ],
                "author_detail": {
                    "name": "Miquel Moretó"
                },
                "author": "Miquel Moretó",
                "arxiv_comment": "27 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.06944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.06944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20773v1",
                "updated": "2024-07-30T12:16:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T12:16:39Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "title": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications"
                },
                "summary": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability."
                },
                "authors": [
                    {
                        "name": "Andronicus Rajasukumar"
                    },
                    {
                        "name": "Jiya Su"
                    },
                    {
                        "name": "Yuqing"
                    },
                    {
                        "name": "Wang"
                    },
                    {
                        "name": "Tianshuo Su"
                    },
                    {
                        "name": "Marziyeh Nourian"
                    },
                    {
                        "name": "Jose M Monsalve Diaz"
                    },
                    {
                        "name": "Tianchi Zhang"
                    },
                    {
                        "name": "Jianru Ding"
                    },
                    {
                        "name": "Wenyi Wang"
                    },
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Moubarak Jeje"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Yanjing Li"
                    },
                    {
                        "name": "Andrew A. Chien"
                    }
                ],
                "author_detail": {
                    "name": "Andrew A. Chien"
                },
                "arxiv_affiliation": "Ivy",
                "author": "Andrew A. Chien",
                "arxiv_comment": "14 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14928v3",
                "updated": "2024-07-30T08:39:52Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    39,
                    52,
                    1,
                    212,
                    0
                ],
                "published": "2023-09-26T13:35:31Z",
                "published_parsed": [
                    2023,
                    9,
                    26,
                    13,
                    35,
                    31,
                    1,
                    269,
                    0
                ],
                "title": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models"
                },
                "summary": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Eman Ali"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Haris Khan"
                },
                "author": "Muhammad Haris Khan",
                "arxiv_comment": "Accepted at BMVC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.14928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03088v2",
                "updated": "2024-07-30T08:19:53Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    19,
                    53,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-03T22:03:28Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    22,
                    3,
                    28,
                    2,
                    94,
                    0
                ],
                "title": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation"
                },
                "summary": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation."
                },
                "authors": [
                    {
                        "name": "Zexin Fang"
                    },
                    {
                        "name": "Bin Han"
                    },
                    {
                        "name": "Hans D. Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D. Schotten"
                },
                "author": "Hans D. Schotten",
                "arxiv_comment": "Submitted to IEEE GLOBECOM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v3",
                "updated": "2024-07-30T04:01:25Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    4,
                    1,
                    25,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19637v1",
                "updated": "2024-07-29T01:43:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:43:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "STT-RAM-based Hierarchical In-Memory Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STT-RAM-based Hierarchical In-Memory Computing"
                },
                "summary": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kevin Antony Gomez"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/TPDS.2024.3430853",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TPDS.2024.3430853",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: IEEE Transactions on Parallel and Distributed Systems (\n  Volume: 35, Issue: 9, September 2024)",
                "arxiv_journal_ref": "IEEE Transactions on Parallel and Distributed Systems, vol. 35,\n  no. 9, pp. 1615-1629, Sept. 2024",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19627v1",
                "updated": "2024-07-29T01:17:54Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:17:54Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "title": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing"
                },
                "summary": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    },
                    {
                        "name": "Kevin Gomez"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Gomez"
                },
                "author": "Kevin Gomez",
                "arxiv_comment": "Accepted in 35th IEEE International Conference on\n  Application-specific Systems, Architectures and Processors (ASAP 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19612v1",
                "updated": "2024-07-28T23:43:59Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T23:43:59Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "title": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors"
                },
                "summary": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1145/3357526.3357553",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3357526.3357553",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the international symposium on memory systems, pp.\n  439-450. 2019",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19604v1",
                "updated": "2024-07-28T22:34:20Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T22:34:20Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "title": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning"
                },
                "summary": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kyle Kuan"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/IGSC48788.2019.8957182",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IGSC48788.2019.8957182",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: 2019 Tenth International Green and Sustainable\n  Computing Conference (IGSC)",
                "arxiv_journal_ref": "2019 Tenth International Green and Sustainable Computing\n  Conference (IGSC), Alexandria, VA, USA, 2019, pp. 1-7,",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19318v1",
                "updated": "2024-07-27T18:26:32Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T18:26:32Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "title": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review"
                },
                "summary": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture."
                },
                "authors": [
                    {
                        "name": "Anujkumarsinh Donvir"
                    },
                    {
                        "name": "Apeksha Jain"
                    },
                    {
                        "name": "Pradeep Kumar Saraswathi"
                    }
                ],
                "author_detail": {
                    "name": "Pradeep Kumar Saraswathi"
                },
                "author": "Pradeep Kumar Saraswathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13996v2",
                "updated": "2024-07-27T08:52:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    52,
                    39,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-19T03:01:32Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    3,
                    1,
                    32,
                    4,
                    201,
                    0
                ],
                "title": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference"
                },
                "summary": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance."
                },
                "authors": [
                    {
                        "name": "Yongkang Zhang"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Chenxia Han"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Huaicheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaicheng Li"
                },
                "author": "Huaicheng Li",
                "arxiv_comment": "18 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.9; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19205v1",
                "updated": "2024-07-27T08:21:14Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T08:21:14Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "title": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions"
                },
                "summary": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Ashkan Taghipour"
                    },
                    {
                        "name": "Morteza Ghahremani"
                    },
                    {
                        "name": "Mohammed Bennamoun"
                    },
                    {
                        "name": "Aref Miri Rekavandi"
                    },
                    {
                        "name": "Zinuo Li"
                    },
                    {
                        "name": "Hamid Laga"
                    },
                    {
                        "name": "Farid Boussaid"
                    }
                ],
                "author_detail": {
                    "name": "Farid Boussaid"
                },
                "author": "Farid Boussaid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19090v1",
                "updated": "2024-07-26T21:11:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "published": "2024-07-26T21:11:58Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "title": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores"
                },
                "summary": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance."
                },
                "authors": [
                    {
                        "name": "Alireza Heidari"
                    },
                    {
                        "name": "Amirhossein Ahmadi"
                    },
                    {
                        "name": "Zefeng Zhi"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "Cloud Databases",
                "arxiv_journal_ref": "VLDB 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18121v1",
                "updated": "2024-07-25T15:29:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T15:29:05Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "title": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache"
                },
                "summary": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache"
                },
                "authors": [
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Benlin Liu"
                    },
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Yuhao Dong"
                    },
                    {
                        "name": "Guangyi Chen"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02750v2",
                "updated": "2024-07-25T09:16:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    9,
                    16,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-02-05T06:06:47Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    6,
                    6,
                    47,
                    0,
                    36,
                    0
                ],
                "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"
                },
                "summary": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI."
                },
                "authors": [
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongye Jin"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Vladimir Braverman"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "arxiv_doi": "10.13140/RG.2.2.28167.37282",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.13140/RG.2.2.28167.37282",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.02750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ICML2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20272v1",
                "updated": "2024-07-25T07:50:17Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T07:50:17Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "title": "An Efficient Inference Framework for Early-exit Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Inference Framework for Early-exit Large Language Models"
                },
                "summary": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up."
                },
                "authors": [
                    {
                        "name": "Ruijie Miao"
                    },
                    {
                        "name": "Yihan Yan"
                    },
                    {
                        "name": "Xinshuo Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v1",
                "updated": "2024-07-25T00:27:07Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.08711v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.08711v3",
                "updated": "2024-07-24T13:36:03Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    13,
                    36,
                    3,
                    2,
                    206,
                    0
                ],
                "published": "2023-01-20T18:13:38Z",
                "published_parsed": [
                    2023,
                    1,
                    20,
                    18,
                    13,
                    38,
                    4,
                    20,
                    0
                ],
                "title": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers"
                },
                "summary": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers."
                },
                "authors": [
                    {
                        "name": "Qifa Yan"
                    },
                    {
                        "name": "Xiaohu Tang"
                    },
                    {
                        "name": "Zhengchun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zhengchun Zhou"
                },
                "author": "Zhengchun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.08711v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.08711v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15771v2",
                "updated": "2024-07-24T12:56:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    12,
                    56,
                    41,
                    2,
                    206,
                    0
                ],
                "published": "2024-03-13T17:47:39Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    17,
                    47,
                    39,
                    2,
                    73,
                    0
                ],
                "title": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations"
                },
                "summary": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations."
                },
                "authors": [
                    {
                        "name": "Craig Innes"
                    },
                    {
                        "name": "Subramanian Ramamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Subramanian Ramamoorthy"
                },
                "author": "Subramanian Ramamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15569v2",
                "updated": "2024-07-24T08:56:11Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    8,
                    56,
                    11,
                    2,
                    206,
                    0
                ],
                "published": "2024-01-28T05:12:09Z",
                "published_parsed": [
                    2024,
                    1,
                    28,
                    5,
                    12,
                    9,
                    6,
                    28,
                    0
                ],
                "title": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs"
                },
                "summary": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE"
                },
                "authors": [
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Yaoke Wang"
                    },
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Siliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Siliang Tang"
                },
                "author": "Siliang Tang",
                "arxiv_comment": "Accepted by IJCAI2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09636v2",
                "updated": "2024-07-23T17:55:30Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    55,
                    30,
                    1,
                    205,
                    0
                ],
                "published": "2024-03-14T17:59:26Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    17,
                    59,
                    26,
                    3,
                    74,
                    0
                ],
                "title": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference"
                },
                "summary": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget."
                },
                "authors": [
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Adrian Łańcucki"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "David Tarjan"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024) 37396-37412",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16672v1",
                "updated": "2024-07-23T17:42:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T17:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "title": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications"
                },
                "summary": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications."
                },
                "authors": [
                    {
                        "name": "Sahan Liyanaarachchi"
                    },
                    {
                        "name": "Stavros Mitrolaris"
                    },
                    {
                        "name": "Purbesh Mitra"
                    },
                    {
                        "name": "Sennur Ulukus"
                    }
                ],
                "author_detail": {
                    "name": "Sennur Ulukus"
                },
                "author": "Sennur Ulukus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16303v1",
                "updated": "2024-07-23T08:58:06Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:58:06Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "title": "Hidden Web Caches Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden Web Caches Discovery"
                },
                "summary": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers."
                },
                "authors": [
                    {
                        "name": "Matteo Golinelli"
                    },
                    {
                        "name": "Bruno Crispo"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Crispo"
                },
                "author": "Bruno Crispo",
                "arxiv_doi": "10.1145/3678890.3678931",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3678890.3678931",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.16303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The definitive Version of Record was published in The 27th\n  International Symposium on Research in Attacks, Intrusions and Defenses (RAID\n  2024), September 30-October 02, 2024, Padua, Italy,\n  https://doi.org/10.1145/3678890.3678931",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16300v1",
                "updated": "2024-07-23T08:55:10Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:55:10Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "title": "A Programming Model for Disaggregated Memory over CXL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Programming Model for Disaggregated Memory over CXL"
                },
                "summary": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores in a cacheline granularity. Alongside with unleashing unique\nopportunities for a wide range of applications, CXL introduces new challenges\nof data management and crash consistency. Alas, CXL lacks an adequate\nprogramming model, which makes reasoning about the correctness and expected\nbehaviors of algorithms and systems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. Using these transformations, every\nlinearizable algorithm can be easily transformed into its provably correct\nversion in the face of a full-system or sub-system crash. We believe that this\nwork will serve as the stepping stone for systems design and modelling on top\nof CXL, and support the development of future models as software and hardware\nevolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores in a cacheline granularity. Alongside with unleashing unique\nopportunities for a wide range of applications, CXL introduces new challenges\nof data management and crash consistency. Alas, CXL lacks an adequate\nprogramming model, which makes reasoning about the correctness and expected\nbehaviors of algorithms and systems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. Using these transformations, every\nlinearizable algorithm can be easily transformed into its provably correct\nversion in the face of a full-system or sub-system crash. We believe that this\nwork will serve as the stepping stone for systems design and modelling on top\nof CXL, and support the development of future models as software and hardware\nevolve."
                },
                "authors": [
                    {
                        "name": "Gal Assa"
                    },
                    {
                        "name": "Michal Friedman"
                    },
                    {
                        "name": "Ori Lahav"
                    }
                ],
                "author_detail": {
                    "name": "Ori Lahav"
                },
                "author": "Ori Lahav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16286v1",
                "updated": "2024-07-23T08:40:27Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    40,
                    27,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:40:27Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    40,
                    27,
                    1,
                    205,
                    0
                ],
                "title": "A deeper look at depth pruning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A deeper look at depth pruning of LLMs"
                },
                "summary": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique."
                },
                "authors": [
                    {
                        "name": "Shoaib Ahmed Siddiqui"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Greg Heinrich"
                    },
                    {
                        "name": "Thomas Breuel"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15309v1",
                "updated": "2024-07-22T14:37:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    14,
                    37,
                    58,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T14:37:58Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    14,
                    37,
                    58,
                    0,
                    204,
                    0
                ],
                "title": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving"
                },
                "summary": "Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads."
                },
                "authors": [
                    {
                        "name": "Jiale Xu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "arxiv_comment": "16 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15581v1",
                "updated": "2024-07-22T12:17:01Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    12,
                    17,
                    1,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T12:17:01Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    12,
                    17,
                    1,
                    0,
                    204,
                    0
                ],
                "title": "vLSM: Low tail latency and I/O amplification in LSM-based KV stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vLSM: Low tail latency and I/O amplification in LSM-based KV stores"
                },
                "summary": "LSM-based key-value (KV) stores are an important component in modern data\ninfrastructures. However, they suffer from high tail latency, in the order of\nseveral seconds, making them less attractive for user-facing applications. In\nthis paper, we introduce the notion of compaction chains and we analyse how\nthey affect tail latency. Then, we show that modern designs reduce tail\nlatency, by trading I/O amplification or require large amounts of memory. Based\non our analysis, we present vLSM, a new KV store design that improves tail\nlatency significantly without compromising on memory or I/O amplification. vLSM\nreduces (a) compaction chain width by using small SSTs and eliminating the\ntiering compaction required in L0 by modern systems and (b) compaction chain\nlength by using a larger than typical growth factor between L1 and L2 and\nintroducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate\nit using db_bench and YCSB. Our evaluation highlights the underlying trade-off\namong memory requirements, I/O amplification, and tail latency, as well as the\nadvantage of vLSM over current approaches. vLSM improves P99 tail latency by up\nto 4.8x for writes and by up to 12.5x for reads, reduces cumulative write\nstalls by up to 60% while also slightly improves I/O amplification at the same\nmemory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSM-based key-value (KV) stores are an important component in modern data\ninfrastructures. However, they suffer from high tail latency, in the order of\nseveral seconds, making them less attractive for user-facing applications. In\nthis paper, we introduce the notion of compaction chains and we analyse how\nthey affect tail latency. Then, we show that modern designs reduce tail\nlatency, by trading I/O amplification or require large amounts of memory. Based\non our analysis, we present vLSM, a new KV store design that improves tail\nlatency significantly without compromising on memory or I/O amplification. vLSM\nreduces (a) compaction chain width by using small SSTs and eliminating the\ntiering compaction required in L0 by modern systems and (b) compaction chain\nlength by using a larger than typical growth factor between L1 and L2 and\nintroducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate\nit using db_bench and YCSB. Our evaluation highlights the underlying trade-off\namong memory requirements, I/O amplification, and tail latency, as well as the\nadvantage of vLSM over current approaches. vLSM improves P99 tail latency by up\nto 4.8x for writes and by up to 12.5x for reads, reduces cumulative write\nstalls by up to 60% while also slightly improves I/O amplification at the same\nmemory budget."
                },
                "authors": [
                    {
                        "name": "Giorgos Xanthakis"
                    },
                    {
                        "name": "Antonios Katsarakis"
                    },
                    {
                        "name": "Giorgos Saloustros"
                    },
                    {
                        "name": "Angelos Bilas"
                    }
                ],
                "author_detail": {
                    "name": "Angelos Bilas"
                },
                "author": "Angelos Bilas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.11055v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.11055v5",
                "updated": "2024-07-22T10:02:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    10,
                    2,
                    57,
                    0,
                    204,
                    0
                ],
                "published": "2022-12-21T14:59:23Z",
                "published_parsed": [
                    2022,
                    12,
                    21,
                    14,
                    59,
                    23,
                    2,
                    355,
                    0
                ],
                "title": "Coalgebraic Satisfiability Checking for Arithmetic $μ$-Calculi",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coalgebraic Satisfiability Checking for Arithmetic $μ$-Calculi"
                },
                "summary": "The coalgebraic $\\mu$-calculus provides a generic semantic framework for\nfixpoint logics over systems whose branching type goes beyond the standard\nrelational setup, e.g. probabilistic, weighted, or game-based. Previous work on\nthe coalgebraic $\\mu$-calculus includes an exponential-time upper bound on\nsatisfiability checking, which however relies on the availability of tableau\nrules for the next-step modalities that are sufficiently well-behaved in a\nformally defined sense; in particular, rule matches need to be representable by\npolynomial-sized codes, and the sequent duals of the rules need to absorb cut.\nWhile such rule sets have been identified for some important cases, they are\nnot known to exist in all cases of interest, in particular ones involving\neither integer weights as in the graded $\\mu$-calculus, or real-valued weights\nin combination with non-linear arithmetic. In the present work, we prove the\nsame upper complexity bound under more general assumptions, specifically\nregarding the complexity of the (much simpler) satisfiability problem for the\nunderlying one-step logic, roughly described as the nesting-free next-step\nfragment of the logic. The bound is realized by a generic global caching\nalgorithm that supports on-the-fly satisfiability checking. Notably, our\napproach directly accommodates unguarded formulae, and thus avoids use of the\nguardedness transformation. Example applications include new exponential-time\nupper bounds for satisfiability checking in an extension of the graded\n$\\mu$-calculus with polynomial inequalities (including positive Presburger\narithmetic), as well as an extension of the (two-valued) probabilistic\n$\\mu$-calculus with polynomial inequalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coalgebraic $\\mu$-calculus provides a generic semantic framework for\nfixpoint logics over systems whose branching type goes beyond the standard\nrelational setup, e.g. probabilistic, weighted, or game-based. Previous work on\nthe coalgebraic $\\mu$-calculus includes an exponential-time upper bound on\nsatisfiability checking, which however relies on the availability of tableau\nrules for the next-step modalities that are sufficiently well-behaved in a\nformally defined sense; in particular, rule matches need to be representable by\npolynomial-sized codes, and the sequent duals of the rules need to absorb cut.\nWhile such rule sets have been identified for some important cases, they are\nnot known to exist in all cases of interest, in particular ones involving\neither integer weights as in the graded $\\mu$-calculus, or real-valued weights\nin combination with non-linear arithmetic. In the present work, we prove the\nsame upper complexity bound under more general assumptions, specifically\nregarding the complexity of the (much simpler) satisfiability problem for the\nunderlying one-step logic, roughly described as the nesting-free next-step\nfragment of the logic. The bound is realized by a generic global caching\nalgorithm that supports on-the-fly satisfiability checking. Notably, our\napproach directly accommodates unguarded formulae, and thus avoids use of the\nguardedness transformation. Example applications include new exponential-time\nupper bounds for satisfiability checking in an extension of the graded\n$\\mu$-calculus with polynomial inequalities (including positive Presburger\narithmetic), as well as an extension of the (two-valued) probabilistic\n$\\mu$-calculus with polynomial inequalities."
                },
                "authors": [
                    {
                        "name": "Daniel Hausmann"
                    },
                    {
                        "name": "Lutz Schröder"
                    }
                ],
                "author_detail": {
                    "name": "Lutz Schröder"
                },
                "author": "Lutz Schröder",
                "arxiv_doi": "10.46298/lmcs-20(3:9)2024",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.46298/lmcs-20(3:9)2024",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2212.11055v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.11055v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Logical Methods in Computer Science, Volume 20, Issue 3 (July 23,\n  2024) lmcs:10532",
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "03B70, 03B44",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15360v1",
                "updated": "2024-07-22T04:07:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    4,
                    7,
                    26,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T04:07:26Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    4,
                    7,
                    26,
                    0,
                    204,
                    0
                ],
                "title": "Dissecting Multiplication in Transformers: Insights into LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting Multiplication in Transformers: Insights into LLMs"
                },
                "summary": "Transformer-based large language models have achieved remarkable performance\nacross various natural language processing tasks. However, they often struggle\nwith seemingly easy tasks like arithmetic despite their vast capabilities. This\nstark disparity raise human's concerns about their safe and ethical use, hinder\ntheir widespread adoption.In this paper, we focus on a typical arithmetic task,\ninteger multiplication, to explore and explain the imperfection of transformers\nin this domain. We provide comprehensive analysis of a vanilla transformer\ntrained to perform n-digit integer multiplication. Our observations indicate\nthat the model decomposes multiplication task into multiple parallel subtasks,\nsequentially optimizing each subtask for each digit to complete the final\nmultiplication. Based on observation and analysis, we infer the reasons of\ntransformers deficiencies in multiplication tasks lies in their difficulty in\ncalculating successive carryovers and caching intermediate results, and\nconfirmed this inference through experiments. Guided by these findings, we\npropose improvements to enhance transformers performance on multiplication\ntasks. These enhancements are validated through rigorous testing and\nmathematical modeling, not only enhance transformer's interpretability, but\nalso improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit\ninteger multiplication with a tiny transformer, outperform LLMs GPT-4. Our\nmethod contributes to the broader fields of model understanding and\ninterpretability, paving the way for analyzing more complex tasks and\nTransformer models. This work underscores the importance of explainable AI,\nhelping to build trust in large language models and promoting their adoption in\ncritical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models have achieved remarkable performance\nacross various natural language processing tasks. However, they often struggle\nwith seemingly easy tasks like arithmetic despite their vast capabilities. This\nstark disparity raise human's concerns about their safe and ethical use, hinder\ntheir widespread adoption.In this paper, we focus on a typical arithmetic task,\ninteger multiplication, to explore and explain the imperfection of transformers\nin this domain. We provide comprehensive analysis of a vanilla transformer\ntrained to perform n-digit integer multiplication. Our observations indicate\nthat the model decomposes multiplication task into multiple parallel subtasks,\nsequentially optimizing each subtask for each digit to complete the final\nmultiplication. Based on observation and analysis, we infer the reasons of\ntransformers deficiencies in multiplication tasks lies in their difficulty in\ncalculating successive carryovers and caching intermediate results, and\nconfirmed this inference through experiments. Guided by these findings, we\npropose improvements to enhance transformers performance on multiplication\ntasks. These enhancements are validated through rigorous testing and\nmathematical modeling, not only enhance transformer's interpretability, but\nalso improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit\ninteger multiplication with a tiny transformer, outperform LLMs GPT-4. Our\nmethod contributes to the broader fields of model understanding and\ninterpretability, paving the way for analyzing more complex tasks and\nTransformer models. This work underscores the importance of explainable AI,\nhelping to build trust in large language models and promoting their adoption in\ncritical applications."
                },
                "authors": [
                    {
                        "name": "Luyu Qiu"
                    },
                    {
                        "name": "Jianing Li"
                    },
                    {
                        "name": "Chi Su"
                    },
                    {
                        "name": "Chen Jason Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15891v1",
                "updated": "2024-07-22T01:12:23Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    1,
                    12,
                    23,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T01:12:23Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    1,
                    12,
                    23,
                    0,
                    204,
                    0
                ],
                "title": "RazorAttention: Efficient KV Cache Compression Through Retrieval Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RazorAttention: Efficient KV Cache Compression Through Retrieval Heads"
                },
                "summary": "The memory and computational demands of Key-Value (KV) cache present\nsignificant challenges for deploying long-context language models. Previous\napproaches attempt to mitigate this issue by selectively dropping tokens, which\nirreversibly erases critical information that might be needed for future\nqueries. In this paper, we propose a novel compression technique for KV cache\nthat preserves all token information. Our investigation reveals that: i) Most\nattention heads primarily focus on the local context; ii) Only a few heads,\ndenoted as retrieval heads, can essentially pay attention to all input tokens.\nThese key observations motivate us to use separate caching strategy for\nattention heads. Therefore, we propose RazorAttention, a training-free KV cache\ncompression algorithm, which maintains a full cache for these crucial retrieval\nheads and discards the remote tokens in non-retrieval heads. Furthermore, we\nintroduce a novel mechanism involving a \"compensation token\" to further recover\nthe information in the dropped tokens. Extensive evaluations across a diverse\nset of large language models (LLMs) demonstrate that RazorAttention achieves a\nreduction in KV cache size by over 70% without noticeable impacts on\nperformance. Additionally, RazorAttention is compatible with FlashAttention,\nrendering it an efficient and plug-and-play solution that enhances LLM\ninference efficiency without overhead or retraining of the original model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The memory and computational demands of Key-Value (KV) cache present\nsignificant challenges for deploying long-context language models. Previous\napproaches attempt to mitigate this issue by selectively dropping tokens, which\nirreversibly erases critical information that might be needed for future\nqueries. In this paper, we propose a novel compression technique for KV cache\nthat preserves all token information. Our investigation reveals that: i) Most\nattention heads primarily focus on the local context; ii) Only a few heads,\ndenoted as retrieval heads, can essentially pay attention to all input tokens.\nThese key observations motivate us to use separate caching strategy for\nattention heads. Therefore, we propose RazorAttention, a training-free KV cache\ncompression algorithm, which maintains a full cache for these crucial retrieval\nheads and discards the remote tokens in non-retrieval heads. Furthermore, we\nintroduce a novel mechanism involving a \"compensation token\" to further recover\nthe information in the dropped tokens. Extensive evaluations across a diverse\nset of large language models (LLMs) demonstrate that RazorAttention achieves a\nreduction in KV cache size by over 70% without noticeable impacts on\nperformance. Additionally, RazorAttention is compatible with FlashAttention,\nrendering it an efficient and plug-and-play solution that enhances LLM\ninference efficiency without overhead or retraining of the original model."
                },
                "authors": [
                    {
                        "name": "Hanlin Tang"
                    },
                    {
                        "name": "Yang Lin"
                    },
                    {
                        "name": "Jing Lin"
                    },
                    {
                        "name": "Qingsen Han"
                    },
                    {
                        "name": "Shikuan Hong"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Gongyi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gongyi Wang"
                },
                "author": "Gongyi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15264v1",
                "updated": "2024-07-21T20:41:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    20,
                    41,
                    39,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-21T20:41:39Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    20,
                    41,
                    39,
                    6,
                    203,
                    0
                ],
                "title": "LSM-GNN: Large-scale Storage-based Multi-GPU GNN Training by Optimizing\n  Data Transfer Scheme",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSM-GNN: Large-scale Storage-based Multi-GPU GNN Training by Optimizing\n  Data Transfer Scheme"
                },
                "summary": "Graph Neural Networks (GNNs) are widely used today in recommendation systems,\nfraud detection, and node/link classification tasks. Real world GNNs continue\nto scale in size and require a large memory footprint for storing graphs and\nembeddings that often exceed the memory capacities of the target GPUs used for\ntraining. To address limited memory capacities, traditional GNN training\napproaches use graph partitioning and sharding techniques to scale up across\nmultiple GPUs within a node and/or scale out across multiple nodes. However,\nthis approach suffers from the high computational costs of graph partitioning\nalgorithms and inefficient communication across GPUs.\n  To address these overheads, we propose Large-scale Storage-based Multi-GPU\nGNN framework (LSM-GNN), a storagebased approach to train GNN models that\nutilizes a novel communication layer enabling GPU software caches to function\nas a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid\neviction policy that intelligently manages cache space by using both static and\ndynamic node information to significantly enhance cache performance.\nFurthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a\nmechanism for prefetching node feature data from a Victim Buffer located in CPU\npinned-memory to further reduce the pressure on the storage devices.\nExperimental results show that despite the lower compute capabilities and\nmemory capacities, LSM-GNN in a single node with two GPUs offers superior\nperformance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x\nspeed up on end-to-end epoch time while running large-scale GNN training",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are widely used today in recommendation systems,\nfraud detection, and node/link classification tasks. Real world GNNs continue\nto scale in size and require a large memory footprint for storing graphs and\nembeddings that often exceed the memory capacities of the target GPUs used for\ntraining. To address limited memory capacities, traditional GNN training\napproaches use graph partitioning and sharding techniques to scale up across\nmultiple GPUs within a node and/or scale out across multiple nodes. However,\nthis approach suffers from the high computational costs of graph partitioning\nalgorithms and inefficient communication across GPUs.\n  To address these overheads, we propose Large-scale Storage-based Multi-GPU\nGNN framework (LSM-GNN), a storagebased approach to train GNN models that\nutilizes a novel communication layer enabling GPU software caches to function\nas a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid\neviction policy that intelligently manages cache space by using both static and\ndynamic node information to significantly enhance cache performance.\nFurthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a\nmechanism for prefetching node feature data from a Victim Buffer located in CPU\npinned-memory to further reduce the pressure on the storage devices.\nExperimental results show that despite the lower compute capabilities and\nmemory capacities, LSM-GNN in a single node with two GPUs offers superior\nperformance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x\nspeed up on end-to-end epoch time while running large-scale GNN training"
                },
                "authors": [
                    {
                        "name": "Jeongmin Brian Park"
                    },
                    {
                        "name": "Kun Wu"
                    },
                    {
                        "name": "Vikram Sharma Mailthody"
                    },
                    {
                        "name": "Zaid Quresh"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Wen-mei Hwu"
                    }
                ],
                "author_detail": {
                    "name": "Wen-mei Hwu"
                },
                "author": "Wen-mei Hwu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15176v1",
                "updated": "2024-07-21T14:23:37Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    23,
                    37,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-21T14:23:37Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    23,
                    37,
                    6,
                    203,
                    0
                ],
                "title": "Farewell to Length Extrapolation, a Training-Free Infinite Context with\n  Finite Attention Scope",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Farewell to Length Extrapolation, a Training-Free Infinite Context with\n  Finite Attention Scope"
                },
                "summary": "The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon."
                },
                "authors": [
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Kai Lv"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.14470v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14470v2",
                "updated": "2024-08-27T03:56:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    3,
                    56,
                    11,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-26T17:58:53Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    58,
                    53,
                    0,
                    239,
                    0
                ],
                "title": "Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large\n  Language Models"
                },
                "summary": "Fine-tuning large language models (LLMs) on downstream tasks requires\nsubstantial computational resources. A class of parameter-efficient fine-tuning\n(PEFT) aims to mitigate these computational challenges by selectively\nfine-tuning only a small fraction of the model parameters. Although\ncomputationally efficient, these techniques often fail to match the performance\nof fully fine-tuned models, primarily due to inherent biases introduced during\nparameter selection. Traditional selective PEFT techniques use a fixed set of\nparameters based on a predefined budget (a process also known as unmasking),\nfailing to capture parameter importance dynamically and often ending up\nexceeding the budget. We introduce $\\text{ID}^3$, a novel selective PEFT method\nthat calculates parameter importance continually and dynamically unmasks\nparameters by balancing exploration and exploitation in parameter selection.\nOur empirical study on 15 tasks spanning natural language understanding and\ngenerative tasks demonstrates the effectiveness of our method compared to\nfixed-masking-based PEFT techniques. We analytically show that $\\text{ID}^3$\nreduces the number of gradient updates by a factor of two, enhancing\ncomputational efficiency. $\\text{ID}^3$ is robust to random initialization of\nneurons and, therefore, can be seamlessly integrated into existing additive and\nreparametrization-based PEFT modules such as adapters and LoRA for dynamic\nsparsification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) on downstream tasks requires\nsubstantial computational resources. A class of parameter-efficient fine-tuning\n(PEFT) aims to mitigate these computational challenges by selectively\nfine-tuning only a small fraction of the model parameters. Although\ncomputationally efficient, these techniques often fail to match the performance\nof fully fine-tuned models, primarily due to inherent biases introduced during\nparameter selection. Traditional selective PEFT techniques use a fixed set of\nparameters based on a predefined budget (a process also known as unmasking),\nfailing to capture parameter importance dynamically and often ending up\nexceeding the budget. We introduce $\\text{ID}^3$, a novel selective PEFT method\nthat calculates parameter importance continually and dynamically unmasks\nparameters by balancing exploration and exploitation in parameter selection.\nOur empirical study on 15 tasks spanning natural language understanding and\ngenerative tasks demonstrates the effectiveness of our method compared to\nfixed-masking-based PEFT techniques. We analytically show that $\\text{ID}^3$\nreduces the number of gradient updates by a factor of two, enhancing\ncomputational efficiency. $\\text{ID}^3$ is robust to random initialization of\nneurons and, therefore, can be seamlessly integrated into existing additive and\nreparametrization-based PEFT modules such as adapters and LoRA for dynamic\nsparsification."
                },
                "authors": [
                    {
                        "name": "Aradhye Agarwal"
                    },
                    {
                        "name": "Suhas K Ramesh"
                    },
                    {
                        "name": "Ayan Sengupta"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "arxiv_comment": "15 pages, 7 tables, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14470v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14470v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14467v1",
                "updated": "2024-08-26T17:58:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    58,
                    17,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:58:17Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    58,
                    17,
                    0,
                    239,
                    0
                ],
                "title": "Explicit Inductive Inference using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explicit Inductive Inference using Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are reported to hold undesirable attestation\nbias on inference tasks: when asked to predict if a premise P entails a\nhypothesis H, instead of considering H's conditional truthfulness entailed by\nP, LLMs tend to use the out-of-context truth label of H as a fragile proxy. In\nthis paper, we propose a pipeline that exploits this bias to do explicit\ninductive inference. Our pipeline uses an LLM to transform a premise into a set\nof attested alternatives, and then aggregate answers of the derived new\nentailment inquiries to support the original inference prediction. On a\ndirectional predicate entailment benchmark, we demonstrate that by applying\nthis simple pipeline, we can improve the overall performance of LLMs on\ninference and substantially alleviate the impact of their attestation bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are reported to hold undesirable attestation\nbias on inference tasks: when asked to predict if a premise P entails a\nhypothesis H, instead of considering H's conditional truthfulness entailed by\nP, LLMs tend to use the out-of-context truth label of H as a fragile proxy. In\nthis paper, we propose a pipeline that exploits this bias to do explicit\ninductive inference. Our pipeline uses an LLM to transform a premise into a set\nof attested alternatives, and then aggregate answers of the derived new\nentailment inquiries to support the original inference prediction. On a\ndirectional predicate entailment benchmark, we demonstrate that by applying\nthis simple pipeline, we can improve the overall performance of LLMs on\ninference and substantially alleviate the impact of their attestation bias."
                },
                "authors": [
                    {
                        "name": "Tianyang Liu"
                    },
                    {
                        "name": "Tianyi Li"
                    },
                    {
                        "name": "Liang Cheng"
                    },
                    {
                        "name": "Mark Steedman"
                    }
                ],
                "author_detail": {
                    "name": "Mark Steedman"
                },
                "author": "Mark Steedman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14466v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14466v1",
                "updated": "2024-08-26T17:55:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    55,
                    18,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:55:18Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    55,
                    18,
                    0,
                    239,
                    0
                ],
                "title": "Bayesian functional data analysis in astronomy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian functional data analysis in astronomy"
                },
                "summary": "Cosmic demographics -- the statistical study of populations of astrophysical\nobjects -- has long relied on *multivariate statistics*, providing methods for\nanalyzing data comprising fixed-length vectors of properties of objects, as\nmight be compiled in a tabular astronomical catalog (say, with sky coordinates,\nand brightness measurements in a fixed number of spectral passbands). But\nbeginning with the emergence of automated digital sky surveys, ca. ~2000,\nastronomers began producing large collections of data with more complex\nstructure: light curves (brightness time series) and spectra (brightness vs.\nwavelength). These comprise what statisticians call *functional data* --\nmeasurements of populations of functions. Upcoming automated sky surveys will\nsoon provide astronomers with a flood of functional data. New methods are\nneeded to accurately and optimally analyze large ensembles of light curves and\nspectra, accumulating information both along and across measured functions.\nFunctional data analysis (FDA) provides tools for statistical modeling of\nfunctional data. Astronomical data presents several challenges for FDA\nmethodology, e.g., sparse, irregular, and asynchronous sampling, and\nheteroscedastic measurement error. Bayesian FDA uses hierarchical Bayesian\nmodels for function populations, and is well suited to addressing these\nchallenges. We provide an overview of astronomical functional data, and of some\nkey Bayesian FDA modeling approaches, including functional mixed effects\nmodels, and stochastic process models. We briefly describe a Bayesian FDA\nframework combining FDA and machine learning methods to build low-dimensional\nparametric models for galaxy spectra.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmic demographics -- the statistical study of populations of astrophysical\nobjects -- has long relied on *multivariate statistics*, providing methods for\nanalyzing data comprising fixed-length vectors of properties of objects, as\nmight be compiled in a tabular astronomical catalog (say, with sky coordinates,\nand brightness measurements in a fixed number of spectral passbands). But\nbeginning with the emergence of automated digital sky surveys, ca. ~2000,\nastronomers began producing large collections of data with more complex\nstructure: light curves (brightness time series) and spectra (brightness vs.\nwavelength). These comprise what statisticians call *functional data* --\nmeasurements of populations of functions. Upcoming automated sky surveys will\nsoon provide astronomers with a flood of functional data. New methods are\nneeded to accurately and optimally analyze large ensembles of light curves and\nspectra, accumulating information both along and across measured functions.\nFunctional data analysis (FDA) provides tools for statistical modeling of\nfunctional data. Astronomical data presents several challenges for FDA\nmethodology, e.g., sparse, irregular, and asynchronous sampling, and\nheteroscedastic measurement error. Bayesian FDA uses hierarchical Bayesian\nmodels for function populations, and is well suited to addressing these\nchallenges. We provide an overview of astronomical functional data, and of some\nkey Bayesian FDA modeling approaches, including functional mixed effects\nmodels, and stochastic process models. We briefly describe a Bayesian FDA\nframework combining FDA and machine learning methods to build low-dimensional\nparametric models for galaxy spectra."
                },
                "authors": [
                    {
                        "name": "Thomas Loredo"
                    },
                    {
                        "name": "Tamas Budavari"
                    },
                    {
                        "name": "David Kent"
                    },
                    {
                        "name": "David Ruppert"
                    }
                ],
                "author_detail": {
                    "name": "David Ruppert"
                },
                "author": "David Ruppert",
                "arxiv_comment": "9 pages, 2 figures; for the Proceedings of the 43rd International\n  Workshop on Bayesian Inference and Maximum Entropy Methods in Science and\n  Engineering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14466v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14466v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14464v1",
                "updated": "2024-08-26T17:54:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    54,
                    46,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:54:46Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    54,
                    46,
                    0,
                    239,
                    0
                ],
                "title": "Eclipse mapping study of the eclipsing binary KIC~3858884 with hybrid\n  $δ$~Sct/$γ$~Dor component",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eclipse mapping study of the eclipsing binary KIC~3858884 with hybrid\n  $δ$~Sct/$γ$~Dor component"
                },
                "summary": "Pulsating stars in eclipsing binary systems offer a unique possibility to\nempirically identify pulsation modes using the geometric effect of the eclipses\non the pulsation signals. Here we explore the $\\delta$ Scuti type pulsations in\nthe eclipsing binary system KIC~3858884 with the aim of identifying the\ndominant modes using various photometric methods. We used the \\textit{Kepler}\nshort cadence photometry data. Refined binary model and pulsation parameters\nwere determined using an iterative separation of the eclipsing binary and\npulsation signals. We used various methods to identify the host stars of the\ndominant pulsations. \\'Echelle diagram diagnostics were employed to locate the\nfrequencies with most influence from the eclipses. Direct Fitting methods\nassuming spherical harmonic surface patterns were explored to determine an\norientation for the symmetry axis and to infer surface mode numbers $\\ell$ and\n$m$. General surface patterns were reconstructed using dynamic eclipse mapping,\nand provided ancillary mode number estimates. We established the secondary star\nas the main source of the pulsations. Seven peaks, including the two strongest\nmodes, were found to show modulations during the secondary eclipses. For the\nfirst time ever, we could detect two hidden modes with amplitude\nintensification during the eclipses. Only one frequency appears to originate\nfrom the primary. We successfully reconstructed surface patterns and determined\nmode numbers for most of the selected frequencies with both of our methods. One\nradial and three sectoral modes, among them hidden modes. The two hidden modes\nwere identified as (3,$\\pm$1) and (2,$\\pm$1). One additional radial mode turned\nout to be a combination frequency. Partial disagreement between EM and DF\nresults may indicate that the strongest modes deviate from strict spherical\nharmonics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pulsating stars in eclipsing binary systems offer a unique possibility to\nempirically identify pulsation modes using the geometric effect of the eclipses\non the pulsation signals. Here we explore the $\\delta$ Scuti type pulsations in\nthe eclipsing binary system KIC~3858884 with the aim of identifying the\ndominant modes using various photometric methods. We used the \\textit{Kepler}\nshort cadence photometry data. Refined binary model and pulsation parameters\nwere determined using an iterative separation of the eclipsing binary and\npulsation signals. We used various methods to identify the host stars of the\ndominant pulsations. \\'Echelle diagram diagnostics were employed to locate the\nfrequencies with most influence from the eclipses. Direct Fitting methods\nassuming spherical harmonic surface patterns were explored to determine an\norientation for the symmetry axis and to infer surface mode numbers $\\ell$ and\n$m$. General surface patterns were reconstructed using dynamic eclipse mapping,\nand provided ancillary mode number estimates. We established the secondary star\nas the main source of the pulsations. Seven peaks, including the two strongest\nmodes, were found to show modulations during the secondary eclipses. For the\nfirst time ever, we could detect two hidden modes with amplitude\nintensification during the eclipses. Only one frequency appears to originate\nfrom the primary. We successfully reconstructed surface patterns and determined\nmode numbers for most of the selected frequencies with both of our methods. One\nradial and three sectoral modes, among them hidden modes. The two hidden modes\nwere identified as (3,$\\pm$1) and (2,$\\pm$1). One additional radial mode turned\nout to be a combination frequency. Partial disagreement between EM and DF\nresults may indicate that the strongest modes deviate from strict spherical\nharmonics."
                },
                "authors": [
                    {
                        "name": "A. Bókon"
                    },
                    {
                        "name": "I. B. Bíró"
                    },
                    {
                        "name": "A. Derekas"
                    }
                ],
                "author_detail": {
                    "name": "A. Derekas"
                },
                "author": "A. Derekas",
                "arxiv_comment": "20 pages, 12 figures, 7 tables, submitted to A&A, comments are\n  welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11796v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11796v2",
                "updated": "2024-08-26T17:50:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    50,
                    46,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-21T17:38:48Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    38,
                    48,
                    2,
                    234,
                    0
                ],
                "title": "LLM Pruning and Distillation in Practice: The Minitron Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Pruning and Distillation in Practice: The Minitron Approach"
                },
                "summary": "We present a comprehensive report on compressing the Llama 3.1 8B and Mistral\nNeMo 12B models to 4B and 8B parameters, respectively, using pruning and\ndistillation. We explore two distinct pruning strategies: (1) depth pruning and\n(2) joint hidden/attention/MLP (width) pruning, and evaluate the results on\ncommon benchmarks from the LM Evaluation Harness. The models are then aligned\nwith NeMo Aligner and tested in instruct-tuned versions. This approach produces\na compelling 4B model from Llama 3.1 8B and a state-of-the-art\nMistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo\n12B. We found that with no access to the original data, it is beneficial to\nslightly fine-tune teacher models on the distillation dataset. We open-source\nour base model weights on Hugging Face with a permissive license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive report on compressing the Llama 3.1 8B and Mistral\nNeMo 12B models to 4B and 8B parameters, respectively, using pruning and\ndistillation. We explore two distinct pruning strategies: (1) depth pruning and\n(2) joint hidden/attention/MLP (width) pruning, and evaluate the results on\ncommon benchmarks from the LM Evaluation Harness. The models are then aligned\nwith NeMo Aligner and tested in instruct-tuned versions. This approach produces\na compelling 4B model from Llama 3.1 8B and a state-of-the-art\nMistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo\n12B. We found that with no access to the original data, it is beneficial to\nslightly fine-tune teacher models on the distillation dataset. We open-source\nour base model weights on Hugging Face with a permissive license."
                },
                "authors": [
                    {
                        "name": "Sharath Turuvekere Sreenivas"
                    },
                    {
                        "name": "Saurav Muralidharan"
                    },
                    {
                        "name": "Raviraj Joshi"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "Mostofa Patwary"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "arxiv_comment": "v2: Added missing references. Cleaned up runtime performance section",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11796v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11796v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14453v1",
                "updated": "2024-08-26T17:48:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    48,
                    42,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:48:42Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    48,
                    42,
                    0,
                    239,
                    0
                ],
                "title": "Reconstructing physiological signals from fMRI across the adult lifespan",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing physiological signals from fMRI across the adult lifespan"
                },
                "summary": "Interactions between the brain and body are of fundamental importance for\nhuman behavior and health. Functional magnetic resonance imaging (fMRI)\ncaptures whole-brain activity noninvasively, and modeling how fMRI signals\ninteract with physiological dynamics of the body can provide new insight into\nbrain function and offer potential biomarkers of disease. However,\nphysiological recordings are not always possible to acquire since they require\nextra equipment and setup, and even when they are, the recorded physiological\nsignals may contain substantial artifacts. To overcome this limitation, machine\nlearning models have been proposed to directly extract features of respiratory\nand cardiac activity from resting-state fMRI signals. To date, such work has\nbeen carried out only in healthy young adults and in a pediatric population,\nleaving open questions about the efficacy of these approaches on older adults.\nHere, we propose a novel framework that leverages Transformer-based\narchitectures for reconstructing two key physiological signals - low-frequency\nrespiratory volume (RV) and heart rate (HR) fluctuations - from fMRI data, and\ntest these models on a dataset of individuals aged 36-89 years old. Our\nframework outperforms previously proposed approaches (attaining median\ncorrelations between predicted and measured signals of r ~ .698 for RV and r ~\n.618 for HR), indicating the potential of leveraging attention mechanisms to\nmodel fMRI-physiological signal relationships. We also evaluate several model\ntraining and fine-tuning strategies, and find that incorporating young-adult\ndata during training improves the performance when predicting physiological\nsignals in the aging cohort. Overall, our approach successfully infers key\nphysiological variables directly from fMRI data from individuals across a wide\nrange of the adult lifespan.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactions between the brain and body are of fundamental importance for\nhuman behavior and health. Functional magnetic resonance imaging (fMRI)\ncaptures whole-brain activity noninvasively, and modeling how fMRI signals\ninteract with physiological dynamics of the body can provide new insight into\nbrain function and offer potential biomarkers of disease. However,\nphysiological recordings are not always possible to acquire since they require\nextra equipment and setup, and even when they are, the recorded physiological\nsignals may contain substantial artifacts. To overcome this limitation, machine\nlearning models have been proposed to directly extract features of respiratory\nand cardiac activity from resting-state fMRI signals. To date, such work has\nbeen carried out only in healthy young adults and in a pediatric population,\nleaving open questions about the efficacy of these approaches on older adults.\nHere, we propose a novel framework that leverages Transformer-based\narchitectures for reconstructing two key physiological signals - low-frequency\nrespiratory volume (RV) and heart rate (HR) fluctuations - from fMRI data, and\ntest these models on a dataset of individuals aged 36-89 years old. Our\nframework outperforms previously proposed approaches (attaining median\ncorrelations between predicted and measured signals of r ~ .698 for RV and r ~\n.618 for HR), indicating the potential of leveraging attention mechanisms to\nmodel fMRI-physiological signal relationships. We also evaluate several model\ntraining and fine-tuning strategies, and find that incorporating young-adult\ndata during training improves the performance when predicting physiological\nsignals in the aging cohort. Overall, our approach successfully infers key\nphysiological variables directly from fMRI data from individuals across a wide\nrange of the adult lifespan."
                },
                "authors": [
                    {
                        "name": "Shiyu Wang"
                    },
                    {
                        "name": "Ziyuan Xu"
                    },
                    {
                        "name": "Yamin Li"
                    },
                    {
                        "name": "Mara Mather"
                    },
                    {
                        "name": "Roza G. Bayrak"
                    },
                    {
                        "name": "Catie Chang"
                    }
                ],
                "author_detail": {
                    "name": "Catie Chang"
                },
                "author": "Catie Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.13840v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.13840v3",
                "updated": "2024-08-26T17:34:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    34,
                    44,
                    0,
                    239,
                    0
                ],
                "published": "2023-06-24T02:25:56Z",
                "published_parsed": [
                    2023,
                    6,
                    24,
                    2,
                    25,
                    56,
                    5,
                    175,
                    0
                ],
                "title": "Beyond Scale: The Diversity Coefficient as a Data Quality Metric for\n  Variability in Natural Language Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Scale: The Diversity Coefficient as a Data Quality Metric for\n  Variability in Natural Language Data"
                },
                "summary": "Current trends in pre-training Large Language Models (LLMs) primarily focus\non the scaling of model and dataset size. While the quality of pre-training\ndata is considered an important factor for training powerful LLMs, it remains a\nnebulous concept that has not been rigorously characterized. To this end, we\npropose a formalization of one key aspect of data quality -- measuring the\nvariability of natural language data -- specifically via a measure we call the\ndiversity coefficient. Our empirical analysis shows that the proposed diversity\ncoefficient aligns with the intuitive properties of diversity and variability,\ne.g., it increases as the number of latent concepts increases. Then, we measure\nthe diversity coefficient of publicly available pre-training datasets and\ndemonstrate that their formal diversity is high compared to theoretical lower\nand upper bounds. Finally, we conduct a comprehensive set of controlled\ninterventional experiments with GPT-2 and LLaMAv2 that demonstrate the\ndiversity coefficient of pre-training data characterizes useful aspects of\ndownstream model evaluation performance -- totaling 44 models of various sizes\n(51M to 7B parameters). We conclude that our formal notion of diversity is an\nimportant aspect of data quality that captures variability and causally leads\nto improved evaluation performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current trends in pre-training Large Language Models (LLMs) primarily focus\non the scaling of model and dataset size. While the quality of pre-training\ndata is considered an important factor for training powerful LLMs, it remains a\nnebulous concept that has not been rigorously characterized. To this end, we\npropose a formalization of one key aspect of data quality -- measuring the\nvariability of natural language data -- specifically via a measure we call the\ndiversity coefficient. Our empirical analysis shows that the proposed diversity\ncoefficient aligns with the intuitive properties of diversity and variability,\ne.g., it increases as the number of latent concepts increases. Then, we measure\nthe diversity coefficient of publicly available pre-training datasets and\ndemonstrate that their formal diversity is high compared to theoretical lower\nand upper bounds. Finally, we conduct a comprehensive set of controlled\ninterventional experiments with GPT-2 and LLaMAv2 that demonstrate the\ndiversity coefficient of pre-training data characterizes useful aspects of\ndownstream model evaluation performance -- totaling 44 models of various sizes\n(51M to 7B parameters). We conclude that our formal notion of diversity is an\nimportant aspect of data quality that captures variability and causally leads\nto improved evaluation performance."
                },
                "authors": [
                    {
                        "name": "Brando Miranda"
                    },
                    {
                        "name": "Alycia Lee"
                    },
                    {
                        "name": "Sudharsan Sundar"
                    },
                    {
                        "name": "Allison Casasola"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    }
                ],
                "author_detail": {
                    "name": "Sanmi Koyejo"
                },
                "author": "Sanmi Koyejo",
                "arxiv_journal_ref": "Published as workshop paper in the Data-centric Machine Learning\n  Research (DMLR) Workshop, ICLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.13840v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.13840v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10468v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10468v3",
                "updated": "2024-08-26T17:28:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    28,
                    23,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-20T00:40:49Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    0,
                    40,
                    49,
                    1,
                    233,
                    0
                ],
                "title": "Tracing Privacy Leakage of Language Models to Training Data via Adjusted\n  Influence Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracing Privacy Leakage of Language Models to Training Data via Adjusted\n  Influence Functions"
                },
                "summary": "The responses generated by Large Language Models (LLMs) can include sensitive\ninformation from individuals and organizations, leading to potential privacy\nleakage. This work implements Influence Functions (IFs) to trace privacy\nleakage back to the training data, thereby mitigating privacy concerns of\nLanguage Models (LMs). However, we notice that current IFs struggle to\naccurately estimate the influence of tokens with large gradient norms,\npotentially overestimating their influence. When tracing the most influential\nsamples, this leads to frequently tracing back to samples with large gradient\nnorm tokens, overshadowing the actual most influential samples even if their\ninfluences are well estimated. To address this issue, we propose Heuristically\nAdjusted IF (HAIF), which reduces the weight of tokens with large gradient\nnorms, thereby significantly improving the accuracy of tracing the most\ninfluential samples. To establish easily obtained groundtruth for tracing\nprivacy leakage, we construct two datasets, PII-E and PII-CR, representing two\ndistinct scenarios: one with identical text in the model outputs and\npre-training data, and the other where models leverage their reasoning\nabilities to generate text divergent from pre-training data. HAIF significantly\nimproves tracing accuracy, enhancing it by 20.96% to 73.71% on the PII-E\ndataset and 3.21% to 45.93% on the PII-CR dataset, compared to the best SOTA\nIFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs\non real-world pretraining data CLUECorpus2020, demonstrating strong robustness\nregardless prompt and response lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The responses generated by Large Language Models (LLMs) can include sensitive\ninformation from individuals and organizations, leading to potential privacy\nleakage. This work implements Influence Functions (IFs) to trace privacy\nleakage back to the training data, thereby mitigating privacy concerns of\nLanguage Models (LMs). However, we notice that current IFs struggle to\naccurately estimate the influence of tokens with large gradient norms,\npotentially overestimating their influence. When tracing the most influential\nsamples, this leads to frequently tracing back to samples with large gradient\nnorm tokens, overshadowing the actual most influential samples even if their\ninfluences are well estimated. To address this issue, we propose Heuristically\nAdjusted IF (HAIF), which reduces the weight of tokens with large gradient\nnorms, thereby significantly improving the accuracy of tracing the most\ninfluential samples. To establish easily obtained groundtruth for tracing\nprivacy leakage, we construct two datasets, PII-E and PII-CR, representing two\ndistinct scenarios: one with identical text in the model outputs and\npre-training data, and the other where models leverage their reasoning\nabilities to generate text divergent from pre-training data. HAIF significantly\nimproves tracing accuracy, enhancing it by 20.96% to 73.71% on the PII-E\ndataset and 3.21% to 45.93% on the PII-CR dataset, compared to the best SOTA\nIFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs\non real-world pretraining data CLUECorpus2020, demonstrating strong robustness\nregardless prompt and response lengths."
                },
                "authors": [
                    {
                        "name": "Jinxin Liu"
                    },
                    {
                        "name": "Zao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zao Yang"
                },
                "author": "Zao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10468v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10468v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14425v1",
                "updated": "2024-08-26T17:13:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    13,
                    43,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:13:43Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    13,
                    43,
                    0,
                    239,
                    0
                ],
                "title": "Radiance Cascades: A Novel High-Resolution Formal Solution for\n  Multidimensional Non-LTE Radiative Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radiance Cascades: A Novel High-Resolution Formal Solution for\n  Multidimensional Non-LTE Radiative Transfer"
                },
                "summary": "Non-LTE radiative transfer is a key tool for modern astrophysics: it is the\nmeans by which many key synthetic observables are produced, thus connecting\nsimulations and observations. Radiative transfer models also inform our\nunderstanding of the primary formation layers and parameters of different\nspectral lines, and serve as the basis of inversion tools used to infer the\nstructure of the solar atmosphere from observations. The default approach for\ncomputing the radiation field in multidimensional solar radiative transfer\nmodels has long remained the same: a short characteristics, discrete ordinates\nmethod, formal solver. In situations with complex atmospheric structure and\nmultiple transitions between optically-thick and -thin regimes these solvers\nrequire prohibitively high angular resolution to correctly resolve the\nradiation field. Here, we present the theory of radiance cascades, a technique\ndesigned to exploit structure inherent to the radiation field, allowing for\nefficient reuse of calculated samples, thus providing a very high-resolution\nresult at a fraction of the computational cost of existing methods. We\nadditionally describe our implementation of this method in the DexRT code, and\npresent initial results of the synthesis of a snapshot of a magnetohydrodynamic\nmodel of a solar prominence formed via levitation-condensation. The approach\npresented here provides a credible route for routinely performing\nmultidimensional radiative transfer calculations free from so-called ray\neffects, and scaling high-quality non-LTE models to next-generation\nhigh-performance computing systems with GPU accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-LTE radiative transfer is a key tool for modern astrophysics: it is the\nmeans by which many key synthetic observables are produced, thus connecting\nsimulations and observations. Radiative transfer models also inform our\nunderstanding of the primary formation layers and parameters of different\nspectral lines, and serve as the basis of inversion tools used to infer the\nstructure of the solar atmosphere from observations. The default approach for\ncomputing the radiation field in multidimensional solar radiative transfer\nmodels has long remained the same: a short characteristics, discrete ordinates\nmethod, formal solver. In situations with complex atmospheric structure and\nmultiple transitions between optically-thick and -thin regimes these solvers\nrequire prohibitively high angular resolution to correctly resolve the\nradiation field. Here, we present the theory of radiance cascades, a technique\ndesigned to exploit structure inherent to the radiation field, allowing for\nefficient reuse of calculated samples, thus providing a very high-resolution\nresult at a fraction of the computational cost of existing methods. We\nadditionally describe our implementation of this method in the DexRT code, and\npresent initial results of the synthesis of a snapshot of a magnetohydrodynamic\nmodel of a solar prominence formed via levitation-condensation. The approach\npresented here provides a credible route for routinely performing\nmultidimensional radiative transfer calculations free from so-called ray\neffects, and scaling high-quality non-LTE models to next-generation\nhigh-performance computing systems with GPU accelerators."
                },
                "authors": [
                    {
                        "name": "Christopher M. J. Osborne"
                    },
                    {
                        "name": "Alexander Sannikov"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Sannikov"
                },
                "author": "Alexander Sannikov",
                "arxiv_comment": "21 pages, 14 figures, ancillary videos. To be submitted to RASTI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14422v1",
                "updated": "2024-08-26T17:08:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    8,
                    40,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:08:40Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    8,
                    40,
                    0,
                    239,
                    0
                ],
                "title": "Using a high-fidelity numerical model to infer the shape of a few-hole\n  Ge quantum dot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using a high-fidelity numerical model to infer the shape of a few-hole\n  Ge quantum dot"
                },
                "summary": "The magnetic properties of hole quantum dots in Ge are sensitive to their\nshape due to the interplay between strong spin-orbit coupling and confinement.\nWe show that the split-off band, surrounding SiGe layers, and hole-hole\ninteractions have a strong influence on calculations of the effective $g$\nfactor of a lithographic quantum dot in a Ge/SiGe heterostructure. Comparing\npredictions from a model including these effects to raw magnetospectroscopy\ndata, we apply maximum-likelihood estimation to infer the shape of a quantum\ndot with up to four holes. We expect that methods like this will be useful in\nassessing qubit-to-qubit variability critical to further scaling quantum\ncomputing technologies based on spins in semiconductors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The magnetic properties of hole quantum dots in Ge are sensitive to their\nshape due to the interplay between strong spin-orbit coupling and confinement.\nWe show that the split-off band, surrounding SiGe layers, and hole-hole\ninteractions have a strong influence on calculations of the effective $g$\nfactor of a lithographic quantum dot in a Ge/SiGe heterostructure. Comparing\npredictions from a model including these effects to raw magnetospectroscopy\ndata, we apply maximum-likelihood estimation to infer the shape of a quantum\ndot with up to four holes. We expect that methods like this will be useful in\nassessing qubit-to-qubit variability critical to further scaling quantum\ncomputing technologies based on spins in semiconductors."
                },
                "authors": [
                    {
                        "name": "Mitchell Brickson"
                    },
                    {
                        "name": "N. Tobias Jacobson"
                    },
                    {
                        "name": "Andrew J. Miller"
                    },
                    {
                        "name": "Leon N. Maurer"
                    },
                    {
                        "name": "Tzu-Ming Lu"
                    },
                    {
                        "name": "Dwight R. Luhman"
                    },
                    {
                        "name": "Andrew D. Baczewski"
                    }
                ],
                "author_detail": {
                    "name": "Andrew D. Baczewski"
                },
                "arxiv_affiliation": "Center for Quantum Information and Control",
                "author": "Andrew D. Baczewski",
                "arxiv_comment": "9 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14418v1",
                "updated": "2024-08-26T17:04:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    4,
                    0,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:04:00Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    4,
                    0,
                    0,
                    239,
                    0
                ],
                "title": "MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR\n  Errors with LLM-generated Synthetic Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR\n  Errors with LLM-generated Synthetic Dialogues"
                },
                "summary": "Automatic Speech Recognition (ASR) systems are pivotal in transcribing speech\ninto text, yet the errors they introduce can significantly degrade the\nperformance of downstream tasks like summarization. This issue is particularly\npronounced in clinical dialogue summarization, a low-resource domain where\nsupervised data for fine-tuning is scarce, necessitating the use of ASR models\nas black-box solutions. Employing conventional data augmentation for enhancing\nthe noise robustness of summarization models is not feasible either due to the\nunavailability of sufficient medical dialogue audio recordings and\ncorresponding ASR transcripts. To address this challenge, we propose MEDSAGE,\nan approach for generating synthetic samples for data augmentation using Large\nLanguage Models (LLMs). Specifically, we leverage the in-context learning\ncapabilities of LLMs and instruct them to generate ASR-like errors based on a\nfew available medical dialogue examples with audio recordings. Experimental\nresults show that LLMs can effectively model ASR noise, and incorporating this\nnoisy data into the training process significantly improves the robustness and\naccuracy of medical dialogue summarization systems. This approach addresses the\nchallenges of noisy ASR outputs in critical applications, offering a robust\nsolution to enhance the reliability of clinical dialogue summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Speech Recognition (ASR) systems are pivotal in transcribing speech\ninto text, yet the errors they introduce can significantly degrade the\nperformance of downstream tasks like summarization. This issue is particularly\npronounced in clinical dialogue summarization, a low-resource domain where\nsupervised data for fine-tuning is scarce, necessitating the use of ASR models\nas black-box solutions. Employing conventional data augmentation for enhancing\nthe noise robustness of summarization models is not feasible either due to the\nunavailability of sufficient medical dialogue audio recordings and\ncorresponding ASR transcripts. To address this challenge, we propose MEDSAGE,\nan approach for generating synthetic samples for data augmentation using Large\nLanguage Models (LLMs). Specifically, we leverage the in-context learning\ncapabilities of LLMs and instruct them to generate ASR-like errors based on a\nfew available medical dialogue examples with audio recordings. Experimental\nresults show that LLMs can effectively model ASR noise, and incorporating this\nnoisy data into the training process significantly improves the robustness and\naccuracy of medical dialogue summarization systems. This approach addresses the\nchallenges of noisy ASR outputs in critical applications, offering a robust\nsolution to enhance the reliability of clinical dialogue summarization."
                },
                "authors": [
                    {
                        "name": "Kuluhan Binici"
                    },
                    {
                        "name": "Abhinav Ramesh Kashyap"
                    },
                    {
                        "name": "Viktor Schlegel"
                    },
                    {
                        "name": "Andy T. Liu"
                    },
                    {
                        "name": "Vijay Prakash Dwivedi"
                    },
                    {
                        "name": "Thanh-Tung Nguyen"
                    },
                    {
                        "name": "Xiaoxue Gao"
                    },
                    {
                        "name": "Nancy F. Chen"
                    },
                    {
                        "name": "Stefan Winkler"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Winkler"
                },
                "author": "Stefan Winkler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14410v1",
                "updated": "2024-08-26T16:58:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    58,
                    21,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T16:58:21Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    58,
                    21,
                    0,
                    239,
                    0
                ],
                "title": "Generalized Bayesian nonparametric clustering framework for\n  high-dimensional spatial omics data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized Bayesian nonparametric clustering framework for\n  high-dimensional spatial omics data"
                },
                "summary": "The advent of next-generation sequencing-based spatially resolved\ntranscriptomics (SRT) techniques has transformed genomic research by enabling\nhigh-throughput gene expression profiling while preserving spatial context.\nIdentifying spatial domains within SRT data is a critical task, with numerous\ncomputational approaches currently available. However, most existing methods\nrely on a multi-stage process that involves ad-hoc dimension reduction\ntechniques to manage the high dimensionality of SRT data. These low-dimensional\nembeddings are then subjected to model-based or distance-based clustering\nmethods. Additionally, many approaches depend on arbitrarily specifying the\nnumber of clusters (i.e., spatial domains), which can result in information\nloss and suboptimal downstream analysis. To address these limitations, we\npropose a novel Bayesian nonparametric mixture of factor analysis (BNPMFA)\nmodel, which incorporates a Markov random field-constrained Gibbs-type prior\nfor partitioning high-dimensional spatial omics data. This new prior\neffectively integrates the spatial constraints inherent in SRT data while\nsimultaneously inferring cluster membership and determining the optimal number\nof spatial domains. We have established the theoretical identifiability of\ncluster membership within this framework. The efficacy of our proposed approach\nis demonstrated through realistic simulations and applications to two SRT\ndatasets. Our results show that the BNPMFA model not only surpasses\nstate-of-the-art methods in clustering accuracy and estimating the number of\nclusters but also offers novel insights for identifying cellular regions within\ntissue samples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of next-generation sequencing-based spatially resolved\ntranscriptomics (SRT) techniques has transformed genomic research by enabling\nhigh-throughput gene expression profiling while preserving spatial context.\nIdentifying spatial domains within SRT data is a critical task, with numerous\ncomputational approaches currently available. However, most existing methods\nrely on a multi-stage process that involves ad-hoc dimension reduction\ntechniques to manage the high dimensionality of SRT data. These low-dimensional\nembeddings are then subjected to model-based or distance-based clustering\nmethods. Additionally, many approaches depend on arbitrarily specifying the\nnumber of clusters (i.e., spatial domains), which can result in information\nloss and suboptimal downstream analysis. To address these limitations, we\npropose a novel Bayesian nonparametric mixture of factor analysis (BNPMFA)\nmodel, which incorporates a Markov random field-constrained Gibbs-type prior\nfor partitioning high-dimensional spatial omics data. This new prior\neffectively integrates the spatial constraints inherent in SRT data while\nsimultaneously inferring cluster membership and determining the optimal number\nof spatial domains. We have established the theoretical identifiability of\ncluster membership within this framework. The efficacy of our proposed approach\nis demonstrated through realistic simulations and applications to two SRT\ndatasets. Our results show that the BNPMFA model not only surpasses\nstate-of-the-art methods in clustering accuracy and estimating the number of\nclusters but also offers novel insights for identifying cellular regions within\ntissue samples."
                },
                "authors": [
                    {
                        "name": "Bencong Zhu"
                    },
                    {
                        "name": "Guanyu Hu"
                    },
                    {
                        "name": "Xiaodan Fan"
                    },
                    {
                        "name": "Qiwei Li"
                    }
                ],
                "author_detail": {
                    "name": "Qiwei Li"
                },
                "author": "Qiwei Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05873v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05873v6",
                "updated": "2024-08-26T16:55:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    55,
                    2,
                    0,
                    239,
                    0
                ],
                "published": "2023-10-09T17:13:10Z",
                "published_parsed": [
                    2023,
                    10,
                    9,
                    17,
                    13,
                    10,
                    0,
                    282,
                    0
                ],
                "title": "Implicit Concept Removal of Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Concept Removal of Diffusion Models"
                },
                "summary": "Text-to-image (T2I) diffusion models often inadvertently generate unwanted\nconcepts such as watermarks and unsafe images. These concepts, termed as the\n\"implicit concepts\", could be unintentionally learned during training and then\nbe generated uncontrollably during inference. Existing removal methods still\nstruggle to eliminate implicit concepts primarily due to their dependency on\nthe model's ability to recognize concepts it actually can not discern. To\naddress this, we utilize the intrinsic geometric characteristics of implicit\nconcepts and present the Geom-Erasing, a novel concept removal method based on\nthe geometric-driven control. Specifically, once an unwanted implicit concept\nis identified, we integrate the existence and geometric information of the\nconcept into the text prompts with the help of an accessible classifier or\ndetector model. Subsequently, the model is optimized to identify and\ndisentangle this information, which is then adopted as negative prompts during\ngeneration. Moreover, we introduce the Implicit Concept Dataset (ICD), a novel\nimage-text dataset imbued with three typical implicit concepts (i.e., QR codes,\nwatermarks, and text), reflecting real-life situations where implicit concepts\nare easily injected. Geom-Erasing effectively mitigates the generation of\nimplicit concepts, achieving the state-of-the-art results on the Inappropriate\nImage Prompts (I2P) and our challenging Implicit Concept Dataset (ICD)\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) diffusion models often inadvertently generate unwanted\nconcepts such as watermarks and unsafe images. These concepts, termed as the\n\"implicit concepts\", could be unintentionally learned during training and then\nbe generated uncontrollably during inference. Existing removal methods still\nstruggle to eliminate implicit concepts primarily due to their dependency on\nthe model's ability to recognize concepts it actually can not discern. To\naddress this, we utilize the intrinsic geometric characteristics of implicit\nconcepts and present the Geom-Erasing, a novel concept removal method based on\nthe geometric-driven control. Specifically, once an unwanted implicit concept\nis identified, we integrate the existence and geometric information of the\nconcept into the text prompts with the help of an accessible classifier or\ndetector model. Subsequently, the model is optimized to identify and\ndisentangle this information, which is then adopted as negative prompts during\ngeneration. Moreover, we introduce the Implicit Concept Dataset (ICD), a novel\nimage-text dataset imbued with three typical implicit concepts (i.e., QR codes,\nwatermarks, and text), reflecting real-life situations where implicit concepts\nare easily injected. Geom-Erasing effectively mitigates the generation of\nimplicit concepts, achieving the state-of-the-art results on the Inappropriate\nImage Prompts (I2P) and our challenging Implicit Concept Dataset (ICD)\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Zhili Liu"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Jianhua Han"
                    },
                    {
                        "name": "Lanqing Hong"
                    },
                    {
                        "name": "Hang Xu"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Dit-Yan Yeung"
                    },
                    {
                        "name": "James Kwok"
                    }
                ],
                "author_detail": {
                    "name": "James Kwok"
                },
                "author": "James Kwok",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.05873v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05873v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05720v2",
                "updated": "2024-08-26T16:48:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    48,
                    8,
                    0,
                    239,
                    0
                ],
                "published": "2024-03-08T23:17:55Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    23,
                    17,
                    55,
                    4,
                    68,
                    0
                ],
                "title": "A Dataset and Benchmark for Hospital Course Summarization with Adapted\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Dataset and Benchmark for Hospital Course Summarization with Adapted\n  Large Language Models"
                },
                "summary": "Brief hospital course (BHC) summaries are clinical documents that summarize a\npatient's hospital stay. While large language models (LLMs) depict remarkable\ncapabilities in automating real-world tasks, their capabilities for healthcare\napplications such as synthesizing BHCs from clinical notes have not been shown.\nWe introduce a novel pre-processed dataset, the MIMIC-IV-BHC, encapsulating\nclinical note and brief hospital course (BHC) pairs to adapt LLMs for BHC\nsynthesis. Furthermore, we introduce a benchmark of the summarization\nperformance of two general-purpose LLMs and three healthcare-adapted LLMs.\n  Using clinical notes as input, we apply prompting-based (using in-context\nlearning) and fine-tuning-based adaptation strategies to three open-source LLMs\n(Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5,\nGPT-4). We evaluate these LLMs across multiple context-length inputs using\nnatural language similarity metrics. We further conduct a clinical study with\nfive clinicians, comparing clinician-written and LLM-generated BHCs across 30\nsamples, focusing on their potential to enhance clinical decision-making\nthrough improved summary quality. We observe that the Llama2-13B fine-tuned LLM\noutperforms other domain-adapted models given quantitative evaluation metrics\nof BLEU and BERT-Score. GPT-4 with in-context learning shows more robustness to\nincreasing context lengths of clinical note inputs than fine-tuned Llama2-13B.\nDespite comparable quantitative metrics, the reader study depicts a significant\npreference for summaries generated by GPT-4 with in-context learning compared\nto both Llama2-13B fine-tuned summaries and the original summaries,\nhighlighting the need for qualitative clinical evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brief hospital course (BHC) summaries are clinical documents that summarize a\npatient's hospital stay. While large language models (LLMs) depict remarkable\ncapabilities in automating real-world tasks, their capabilities for healthcare\napplications such as synthesizing BHCs from clinical notes have not been shown.\nWe introduce a novel pre-processed dataset, the MIMIC-IV-BHC, encapsulating\nclinical note and brief hospital course (BHC) pairs to adapt LLMs for BHC\nsynthesis. Furthermore, we introduce a benchmark of the summarization\nperformance of two general-purpose LLMs and three healthcare-adapted LLMs.\n  Using clinical notes as input, we apply prompting-based (using in-context\nlearning) and fine-tuning-based adaptation strategies to three open-source LLMs\n(Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5,\nGPT-4). We evaluate these LLMs across multiple context-length inputs using\nnatural language similarity metrics. We further conduct a clinical study with\nfive clinicians, comparing clinician-written and LLM-generated BHCs across 30\nsamples, focusing on their potential to enhance clinical decision-making\nthrough improved summary quality. We observe that the Llama2-13B fine-tuned LLM\noutperforms other domain-adapted models given quantitative evaluation metrics\nof BLEU and BERT-Score. GPT-4 with in-context learning shows more robustness to\nincreasing context lengths of clinical note inputs than fine-tuned Llama2-13B.\nDespite comparable quantitative metrics, the reader study depicts a significant\npreference for summaries generated by GPT-4 with in-context learning compared\nto both Llama2-13B fine-tuned summaries and the original summaries,\nhighlighting the need for qualitative clinical evaluation."
                },
                "authors": [
                    {
                        "name": "Asad Aali"
                    },
                    {
                        "name": "Dave Van Veen"
                    },
                    {
                        "name": "Yamin Ishraq Arefeen"
                    },
                    {
                        "name": "Jason Hom"
                    },
                    {
                        "name": "Christian Bluethgen"
                    },
                    {
                        "name": "Eduardo Pontes Reis"
                    },
                    {
                        "name": "Sergios Gatidis"
                    },
                    {
                        "name": "Namuun Clifford"
                    },
                    {
                        "name": "Joseph Daws"
                    },
                    {
                        "name": "Arash S. Tehrani"
                    },
                    {
                        "name": "Jangwon Kim"
                    },
                    {
                        "name": "Akshay S. Chaudhari"
                    }
                ],
                "author_detail": {
                    "name": "Akshay S. Chaudhari"
                },
                "author": "Akshay S. Chaudhari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12347v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12347v2",
                "updated": "2024-08-26T16:45:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    45,
                    34,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-22T12:43:14Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    43,
                    14,
                    3,
                    235,
                    0
                ],
                "title": "Preregistration does not improve the transparent evaluation of severity\n  in Popper's philosophy of science or when deviations are allowed",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preregistration does not improve the transparent evaluation of severity\n  in Popper's philosophy of science or when deviations are allowed"
                },
                "summary": "One justification for preregistering research hypotheses, methods, and\nanalyses is that it improves the transparent evaluation of the severity of\nhypothesis tests. In this article, I consider two cases in which\npreregistration does not improve this evaluation. First, I argue that, although\npreregistration can facilitate the transparent evaluation of severity in Mayo's\nerror statistical philosophy of science, it does not facilitate this evaluation\nin Popper's theory-centric approach. To illustrate, I show that associated\nconcerns about Type I error rate inflation are only relevant in the error\nstatistical approach and not in a theory-centric approach. Second, I argue that\na preregistered test procedure that allows deviations in its implementation\ndoes not provide a more transparent evaluation of Mayoian severity than a\nnon-preregistered procedure. In particular, I argue that sample-based\nvalidity-enhancing deviations cause an unknown inflation of the test\nprocedure's Type I (familywise) error rate and, consequently, an unknown\nreduction in its capability to license inferences severely. I conclude that\npreregistration does not improve the transparent evaluation of severity in\nPopper's philosophy of science or when deviations are allowed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One justification for preregistering research hypotheses, methods, and\nanalyses is that it improves the transparent evaluation of the severity of\nhypothesis tests. In this article, I consider two cases in which\npreregistration does not improve this evaluation. First, I argue that, although\npreregistration can facilitate the transparent evaluation of severity in Mayo's\nerror statistical philosophy of science, it does not facilitate this evaluation\nin Popper's theory-centric approach. To illustrate, I show that associated\nconcerns about Type I error rate inflation are only relevant in the error\nstatistical approach and not in a theory-centric approach. Second, I argue that\na preregistered test procedure that allows deviations in its implementation\ndoes not provide a more transparent evaluation of Mayoian severity than a\nnon-preregistered procedure. In particular, I argue that sample-based\nvalidity-enhancing deviations cause an unknown inflation of the test\nprocedure's Type I (familywise) error rate and, consequently, an unknown\nreduction in its capability to license inferences severely. I conclude that\npreregistration does not improve the transparent evaluation of severity in\nPopper's philosophy of science or when deviations are allowed."
                },
                "authors": [
                    {
                        "name": "Mark Rubin"
                    }
                ],
                "author_detail": {
                    "name": "Mark Rubin"
                },
                "author": "Mark Rubin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12347v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12347v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03661v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03661v2",
                "updated": "2024-08-26T16:38:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    38,
                    30,
                    0,
                    239,
                    0
                ],
                "published": "2024-07-04T06:02:52Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    6,
                    2,
                    52,
                    3,
                    186,
                    0
                ],
                "title": "Configurable DOA Estimation using Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Configurable DOA Estimation using Incremental Learning"
                },
                "summary": "This study introduces a progressive neural network (PNN) model for direction\nof arrival (DOA) estimation, DOA-PNN, addressing the challenge due to\ncatastrophic forgetting in adapting dynamic acoustic environments. While\ntraditional methods such as GCC, MUSIC, and SRP-PHAT are effective in static\nsettings, they perform worse in noisy, reverberant conditions. Deep learning\nmodels, particularly CNNs, offer improvements but struggle with a mismatch\nconfiguration between the training and inference phases. The proposed DOA-PNN\novercomes these limitations by incorporating task incremental learning of\ncontinual learning, allowing for adaptation across varying acoustic scenarios\nwith less forgetting of previously learned knowledge. Featuring task-specific\nsub-networks and a scaling mechanism, DOA-PNN efficiently manages parameter\ngrowth, ensuring high performance across incremental microphone configurations.\nWe study DOA-PNN on a simulated data under various mic distance based\nmicrophone settings. The studies reveal its capability to maintain performance\nwith minimal parameter increase, presenting an efficient solution for DOA\nestimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces a progressive neural network (PNN) model for direction\nof arrival (DOA) estimation, DOA-PNN, addressing the challenge due to\ncatastrophic forgetting in adapting dynamic acoustic environments. While\ntraditional methods such as GCC, MUSIC, and SRP-PHAT are effective in static\nsettings, they perform worse in noisy, reverberant conditions. Deep learning\nmodels, particularly CNNs, offer improvements but struggle with a mismatch\nconfiguration between the training and inference phases. The proposed DOA-PNN\novercomes these limitations by incorporating task incremental learning of\ncontinual learning, allowing for adaptation across varying acoustic scenarios\nwith less forgetting of previously learned knowledge. Featuring task-specific\nsub-networks and a scaling mechanism, DOA-PNN efficiently manages parameter\ngrowth, ensuring high performance across incremental microphone configurations.\nWe study DOA-PNN on a simulated data under various mic distance based\nmicrophone settings. The studies reveal its capability to maintain performance\nwith minimal parameter increase, presenting an efficient solution for DOA\nestimation."
                },
                "authors": [
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Rohan Kumar Das"
                    }
                ],
                "author_detail": {
                    "name": "Rohan Kumar Das"
                },
                "author": "Rohan Kumar Das",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03661v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03661v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14398v1",
                "updated": "2024-08-26T16:29:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    29,
                    13,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T16:29:13Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    29,
                    13,
                    0,
                    239,
                    0
                ],
                "title": "Language-specific Calibration for Pruning Multilingual Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-specific Calibration for Pruning Multilingual Language Models"
                },
                "summary": "Recent advances in large language model (LLM) pruning have shown\nstate-of-the-art compression results in post-training and retraining-free\nsettings while maintaining high predictive performance. However, such research\nmainly considers calibrating pruning using English text, despite the\nmultilingual nature of modern LLMs and their frequent uses in non-English\nlanguages. In this paper, we set out to explore effective strategies for\ncalibrating the pruning of multilingual language models. We present the first\ncomprehensive empirical study, comparing different calibration languages for\npruning multilingual models across diverse tasks, models, and state-of-the-art\npruning techniques. Our results present practical suggestions, for example,\ncalibrating in the target language can efficiently yield lower perplexity, but\ndoes not necessarily benefit downstream tasks. Our further analysis experiments\nunveil that calibration in the target language mainly contributes to preserving\nlanguage-specific features related to fluency and coherence, but might not\ncontribute to capturing language-agnostic features such as language\nunderstanding and reasoning. Last, we provide practical recommendations for\nfuture practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language model (LLM) pruning have shown\nstate-of-the-art compression results in post-training and retraining-free\nsettings while maintaining high predictive performance. However, such research\nmainly considers calibrating pruning using English text, despite the\nmultilingual nature of modern LLMs and their frequent uses in non-English\nlanguages. In this paper, we set out to explore effective strategies for\ncalibrating the pruning of multilingual language models. We present the first\ncomprehensive empirical study, comparing different calibration languages for\npruning multilingual models across diverse tasks, models, and state-of-the-art\npruning techniques. Our results present practical suggestions, for example,\ncalibrating in the target language can efficiently yield lower perplexity, but\ndoes not necessarily benefit downstream tasks. Our further analysis experiments\nunveil that calibration in the target language mainly contributes to preserving\nlanguage-specific features related to fluency and coherence, but might not\ncontribute to capturing language-agnostic features such as language\nunderstanding and reasoning. Last, we provide practical recommendations for\nfuture practitioners."
                },
                "authors": [
                    {
                        "name": "Simon Kurz"
                    },
                    {
                        "name": "Zhixue Zhao"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "Lucie Flek"
                    }
                ],
                "author_detail": {
                    "name": "Lucie Flek"
                },
                "author": "Lucie Flek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14387v1",
                "updated": "2024-08-26T16:11:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    11,
                    53,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T16:11:53Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    11,
                    53,
                    0,
                    239,
                    0
                ],
                "title": "Reprogramming Foundational Large Language Models(LLMs) for Enterprise\n  Adoption for Spatio-Temporal Forecasting Applications: Unveiling a New Era in\n  Copilot-Guided Cross-Modal Time Series Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reprogramming Foundational Large Language Models(LLMs) for Enterprise\n  Adoption for Spatio-Temporal Forecasting Applications: Unveiling a New Era in\n  Copilot-Guided Cross-Modal Time Series Representation Learning"
                },
                "summary": "Spatio-temporal forecasting plays a crucial role in various sectors such as\ntransportation systems, logistics, and supply chain management. However,\nexisting methods are limited by their ability to handle large, complex\ndatasets. To overcome this limitation, we introduce a hybrid approach that\ncombines the strengths of open-source large and small-scale language models\n(LLMs and LMs) with traditional forecasting methods. We augment traditional\nmethods with dynamic prompting and a grouped-query, multi-head attention\nmechanism to more effectively capture both intra-series and inter-series\ndependencies in evolving nonlinear time series data. In addition, we facilitate\non-premises customization by fine-tuning smaller open-source LMs for time\nseries trend analysis utilizing descriptions generated by open-source large LMs\non consumer-grade hardware using Low-Rank Adaptation with Activation Memory\nReduction (LoRA-AMR) technique to reduce computational overhead and activation\nstorage memory demands while preserving inference latency. We combine language\nmodel processing for time series trend analysis with traditional time series\nrepresentation learning method for cross-modal integration, achieving robust\nand accurate forecasts. The framework effectiveness is demonstrated through\nextensive experiments on various real-world datasets, outperforming existing\nmethods by significant margins in terms of forecast accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatio-temporal forecasting plays a crucial role in various sectors such as\ntransportation systems, logistics, and supply chain management. However,\nexisting methods are limited by their ability to handle large, complex\ndatasets. To overcome this limitation, we introduce a hybrid approach that\ncombines the strengths of open-source large and small-scale language models\n(LLMs and LMs) with traditional forecasting methods. We augment traditional\nmethods with dynamic prompting and a grouped-query, multi-head attention\nmechanism to more effectively capture both intra-series and inter-series\ndependencies in evolving nonlinear time series data. In addition, we facilitate\non-premises customization by fine-tuning smaller open-source LMs for time\nseries trend analysis utilizing descriptions generated by open-source large LMs\non consumer-grade hardware using Low-Rank Adaptation with Activation Memory\nReduction (LoRA-AMR) technique to reduce computational overhead and activation\nstorage memory demands while preserving inference latency. We combine language\nmodel processing for time series trend analysis with traditional time series\nrepresentation learning method for cross-modal integration, achieving robust\nand accurate forecasts. The framework effectiveness is demonstrated through\nextensive experiments on various real-world datasets, outperforming existing\nmethods by significant margins in terms of forecast accuracy."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Chidaksh Ravuru"
                    },
                    {
                        "name": "Geethan Sannidhi"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "arxiv_comment": "Paper published at the Deployable AI (DAI) workshop at AAAI-2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14380v1",
                "updated": "2024-08-26T16:00:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    0,
                    41,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T16:00:41Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    0,
                    41,
                    0,
                    239,
                    0
                ],
                "title": "Probing Causality Manipulation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing Causality Manipulation of Large Language Models"
                },
                "summary": "Large language models (LLMs) have shown various ability on natural language\nprocessing, including problems about causality. It is not intuitive for LLMs to\ncommand causality, since pretrained models usually work on statistical\nassociations, and do not focus on causes and effects in sentences. So that\nprobing internal manipulation of causality is necessary for LLMs. This paper\nproposes a novel approach to probe causality manipulation hierarchically, by\nproviding different shortcuts to models and observe behaviors. We exploit\nretrieval augmented generation (RAG) and in-context learning (ICL) for models\non a designed causality classification task. We conduct experiments on\nmainstream LLMs, including GPT-4 and some smaller and domain-specific models.\nOur results suggest that LLMs can detect entities related to causality and\nrecognize direct causal relationships. However, LLMs lack specialized cognition\nfor causality, merely treating them as part of the global semantic of the\nsentence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown various ability on natural language\nprocessing, including problems about causality. It is not intuitive for LLMs to\ncommand causality, since pretrained models usually work on statistical\nassociations, and do not focus on causes and effects in sentences. So that\nprobing internal manipulation of causality is necessary for LLMs. This paper\nproposes a novel approach to probe causality manipulation hierarchically, by\nproviding different shortcuts to models and observe behaviors. We exploit\nretrieval augmented generation (RAG) and in-context learning (ICL) for models\non a designed causality classification task. We conduct experiments on\nmainstream LLMs, including GPT-4 and some smaller and domain-specific models.\nOur results suggest that LLMs can detect entities related to causality and\nrecognize direct causal relationships. However, LLMs lack specialized cognition\nfor causality, merely treating them as part of the global semantic of the\nsentence."
                },
                "authors": [
                    {
                        "name": "Chenyang Zhang"
                    },
                    {
                        "name": "Haibo Tong"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Dongyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongyu Zhang"
                },
                "author": "Dongyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14379v1",
                "updated": "2024-08-26T16:00:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    0,
                    40,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T16:00:40Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    0,
                    40,
                    0,
                    239,
                    0
                ],
                "title": "Synergistic and Efficient Edge-Host Communication for Energy Harvesting\n  Wireless Sensor Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergistic and Efficient Edge-Host Communication for Energy Harvesting\n  Wireless Sensor Networks"
                },
                "summary": "There is an increasing demand for intelligent processing on ultra-low-power\ninternet of things (IoT) device. Recent works have shown substantial efficiency\nboosts by executing inferences directly on the IoT device (node) rather than\ntransmitting data. However, the computation and power demands of Deep Neural\nNetwork (DNN)-based inference pose significant challenges in an\nenergy-harvesting wireless sensor network (EH-WSN). Moreover, these tasks often\nrequire responses from multiple physically distributed EH sensor nodes, which\nimpose crucial system optimization challenges in addition to per-node\nconstraints. To address these challenges, we propose Seeker, a\nhardware-software co-design approach for increasing on-sensor computation,\nreducing communication volume, and maximizing inference completion, without\nviolating the quality of service, in EH-WSNs coordinated by a mobile device.\nSeeker uses a store-and-execute approach to complete a subset of inferences on\nthe EH sensor node, reducing communication with the mobile host. Further, for\nthose inferences unfinished because of the harvested energy constraints, it\nleverages task-aware coreset construction to efficiently communicate compact\nfeatures to the host device. We evaluate Seeker for human activity recognition,\nas well as predictive maintenance and show ~8.9x reduction in communication\ndata volume with 86.8% accuracy, surpassing the 81.2% accuracy of the\nstate-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is an increasing demand for intelligent processing on ultra-low-power\ninternet of things (IoT) device. Recent works have shown substantial efficiency\nboosts by executing inferences directly on the IoT device (node) rather than\ntransmitting data. However, the computation and power demands of Deep Neural\nNetwork (DNN)-based inference pose significant challenges in an\nenergy-harvesting wireless sensor network (EH-WSN). Moreover, these tasks often\nrequire responses from multiple physically distributed EH sensor nodes, which\nimpose crucial system optimization challenges in addition to per-node\nconstraints. To address these challenges, we propose Seeker, a\nhardware-software co-design approach for increasing on-sensor computation,\nreducing communication volume, and maximizing inference completion, without\nviolating the quality of service, in EH-WSNs coordinated by a mobile device.\nSeeker uses a store-and-execute approach to complete a subset of inferences on\nthe EH sensor node, reducing communication with the mobile host. Further, for\nthose inferences unfinished because of the harvested energy constraints, it\nleverages task-aware coreset construction to efficiently communicate compact\nfeatures to the host device. We evaluate Seeker for human activity recognition,\nas well as predictive maintenance and show ~8.9x reduction in communication\ndata volume with 86.8% accuracy, surpassing the 81.2% accuracy of the\nstate-of-the-art."
                },
                "authors": [
                    {
                        "name": "Cyan Subhra Mishra"
                    },
                    {
                        "name": "Jack Sampson"
                    },
                    {
                        "name": "Mahmut Taylan Kandmeir"
                    },
                    {
                        "name": "Vijaykrishnan Narayanan"
                    },
                    {
                        "name": "Chita R Das"
                    }
                ],
                "author_detail": {
                    "name": "Chita R Das"
                },
                "author": "Chita R Das",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2204.13106",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14368v1",
                "updated": "2024-08-26T15:46:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    46,
                    41,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T15:46:41Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    46,
                    41,
                    0,
                    239,
                    0
                ],
                "title": "GR-MG: Leveraging Partially Annotated Data via Multi-Modal Goal\n  Conditioned Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GR-MG: Leveraging Partially Annotated Data via Multi-Modal Goal\n  Conditioned Policy"
                },
                "summary": "The robotics community has consistently aimed to achieve generalizable robot\nmanipulation with flexible natural language instructions. One of the primary\nchallenges is that obtaining robot data fully annotated with both actions and\ntexts is time-consuming and labor-intensive. However, partially annotated data,\nsuch as human activity videos without action labels and robot play data without\nlanguage labels, is much easier to collect. Can we leverage these data to\nenhance the generalization capability of robots? In this paper, we propose\nGR-MG, a novel method which supports conditioning on both a language\ninstruction and a goal image. During training, GR-MG samples goal images from\ntrajectories and conditions on both the text and the goal image or solely on\nthe image when text is unavailable. During inference, where only the text is\nprovided, GR-MG generates the goal image via a diffusion-based image-editing\nmodel and condition on both the text and the generated image. This approach\nenables GR-MG to leverage large amounts of partially annotated data while still\nusing language to flexibly specify tasks. To generate accurate goal images, we\npropose a novel progress-guided goal image generation model which injects task\nprogress information into the generation process, significantly improving the\nfidelity and the performance. In simulation experiments, GR-MG improves the\naverage number of tasks completed in a row of 5 from 3.35 to 4.04. In\nreal-robot experiments, GR-MG is able to perform 47 different tasks and\nimproves the success rate from 62.5% to 75.0% and 42.4% to 57.6% in simple and\ngeneralization settings, respectively. Code and checkpoints will be available\nat the project page: https://gr-mg.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The robotics community has consistently aimed to achieve generalizable robot\nmanipulation with flexible natural language instructions. One of the primary\nchallenges is that obtaining robot data fully annotated with both actions and\ntexts is time-consuming and labor-intensive. However, partially annotated data,\nsuch as human activity videos without action labels and robot play data without\nlanguage labels, is much easier to collect. Can we leverage these data to\nenhance the generalization capability of robots? In this paper, we propose\nGR-MG, a novel method which supports conditioning on both a language\ninstruction and a goal image. During training, GR-MG samples goal images from\ntrajectories and conditions on both the text and the goal image or solely on\nthe image when text is unavailable. During inference, where only the text is\nprovided, GR-MG generates the goal image via a diffusion-based image-editing\nmodel and condition on both the text and the generated image. This approach\nenables GR-MG to leverage large amounts of partially annotated data while still\nusing language to flexibly specify tasks. To generate accurate goal images, we\npropose a novel progress-guided goal image generation model which injects task\nprogress information into the generation process, significantly improving the\nfidelity and the performance. In simulation experiments, GR-MG improves the\naverage number of tasks completed in a row of 5 from 3.35 to 4.04. In\nreal-robot experiments, GR-MG is able to perform 47 different tasks and\nimproves the success rate from 62.5% to 75.0% and 42.4% to 57.6% in simple and\ngeneralization settings, respectively. Code and checkpoints will be available\nat the project page: https://gr-mg.github.io/."
                },
                "authors": [
                    {
                        "name": "Peiyan Li"
                    },
                    {
                        "name": "Hongtao Wu"
                    },
                    {
                        "name": "Yan Huang"
                    },
                    {
                        "name": "Chilam Cheang"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Tao Kong"
                    }
                ],
                "author_detail": {
                    "name": "Tao Kong"
                },
                "author": "Tao Kong",
                "arxiv_comment": "9 pages, 7 figures, letter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14357v1",
                "updated": "2024-08-26T15:31:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    31,
                    58,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T15:31:58Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    31,
                    58,
                    0,
                    239,
                    0
                ],
                "title": "Exploring ChatGPT App Ecosystem: Distribution, Deployment and Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring ChatGPT App Ecosystem: Distribution, Deployment and Security"
                },
                "summary": "ChatGPT has enabled third-party developers to create plugins to expand\nChatGPT's capabilities.These plugins are distributed through OpenAI's plugin\nstore, making them easily accessible to users. With ChatGPT as the backbone,\nthis app ecosystem has illustrated great business potential by offering users\npersonalized services in a conversational manner. Nonetheless, many crucial\naspects regarding app development, deployment, and security of this ecosystem\nhave yet to be thoroughly studied in the research community, potentially\nhindering a broader adoption by both developers and users. In this work, we\nconduct the first comprehensive study of the ChatGPT app ecosystem, aiming to\nilluminate its landscape for our research community. Our study examines the\ndistribution and deployment models in the integration of LLMs and third-party\napps, and assesses their security and privacy implications. We uncover an\nuneven distribution of functionality among ChatGPT plugins, highlighting\nprevalent and emerging topics. We also identify severe flaws in the\nauthentication and user data protection for third-party app APIs integrated\nwithin LLMs, revealing a concerning status quo of security and privacy in this\napp ecosystem. Our work provides insights for the secure and sustainable\ndevelopment of this rapidly evolving ecosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatGPT has enabled third-party developers to create plugins to expand\nChatGPT's capabilities.These plugins are distributed through OpenAI's plugin\nstore, making them easily accessible to users. With ChatGPT as the backbone,\nthis app ecosystem has illustrated great business potential by offering users\npersonalized services in a conversational manner. Nonetheless, many crucial\naspects regarding app development, deployment, and security of this ecosystem\nhave yet to be thoroughly studied in the research community, potentially\nhindering a broader adoption by both developers and users. In this work, we\nconduct the first comprehensive study of the ChatGPT app ecosystem, aiming to\nilluminate its landscape for our research community. Our study examines the\ndistribution and deployment models in the integration of LLMs and third-party\napps, and assesses their security and privacy implications. We uncover an\nuneven distribution of functionality among ChatGPT plugins, highlighting\nprevalent and emerging topics. We also identify severe flaws in the\nauthentication and user data protection for third-party app APIs integrated\nwithin LLMs, revealing a concerning status quo of security and privacy in this\napp ecosystem. Our work provides insights for the secure and sustainable\ndevelopment of this rapidly evolving ecosystem."
                },
                "authors": [
                    {
                        "name": "Chuan Yan"
                    },
                    {
                        "name": "Ruomai Ren"
                    },
                    {
                        "name": "Mark Huasong Meng"
                    },
                    {
                        "name": "Liuhuo Wan"
                    },
                    {
                        "name": "Tian Yang Ooi"
                    },
                    {
                        "name": "Guangdong Bai"
                    }
                ],
                "author_detail": {
                    "name": "Guangdong Bai"
                },
                "author": "Guangdong Bai",
                "arxiv_comment": "Accepted by the 39th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14354v1",
                "updated": "2024-08-26T15:30:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    30,
                    5,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T15:30:05Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    30,
                    5,
                    0,
                    239,
                    0
                ],
                "title": "SWE-bench-java: A GitHub Issue Resolving Benchmark for Java",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-bench-java: A GitHub Issue Resolving Benchmark for Java"
                },
                "summary": "GitHub issue resolving is a critical task in software engineering, recently\ngaining significant attention in both industry and academia. Within this task,\nSWE-bench has been released to evaluate issue resolving capabilities of large\nlanguage models (LLMs), but has so far only focused on Python version. However,\nsupporting more programming languages is also important, as there is a strong\ndemand in industry. As a first step toward multilingual support, we have\ndeveloped a Java version of SWE-bench, called SWE-bench-java. We have publicly\nreleased the dataset, along with the corresponding Docker-based evaluation\nenvironment and leaderboard, which will be continuously maintained and updated\nin the coming months. To verify the reliability of SWE-bench-java, we implement\na classic method SWE-agent and test several powerful LLMs on it. As is well\nknown, developing a high-quality multi-lingual benchmark is time-consuming and\nlabor-intensive, so we welcome contributions through pull requests or\ncollaboration to accelerate its iteration and refinement, paving the way for\nfully automated programming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GitHub issue resolving is a critical task in software engineering, recently\ngaining significant attention in both industry and academia. Within this task,\nSWE-bench has been released to evaluate issue resolving capabilities of large\nlanguage models (LLMs), but has so far only focused on Python version. However,\nsupporting more programming languages is also important, as there is a strong\ndemand in industry. As a first step toward multilingual support, we have\ndeveloped a Java version of SWE-bench, called SWE-bench-java. We have publicly\nreleased the dataset, along with the corresponding Docker-based evaluation\nenvironment and leaderboard, which will be continuously maintained and updated\nin the coming months. To verify the reliability of SWE-bench-java, we implement\na classic method SWE-agent and test several powerful LLMs on it. As is well\nknown, developing a high-quality multi-lingual benchmark is time-consuming and\nlabor-intensive, so we welcome contributions through pull requests or\ncollaboration to accelerate its iteration and refinement, paving the way for\nfully automated programming."
                },
                "authors": [
                    {
                        "name": "Daoguang Zan"
                    },
                    {
                        "name": "Zhirong Huang"
                    },
                    {
                        "name": "Ailun Yu"
                    },
                    {
                        "name": "Shaoxin Lin"
                    },
                    {
                        "name": "Yifan Shi"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Dong Chen"
                    },
                    {
                        "name": "Zongshuai Qi"
                    },
                    {
                        "name": "Hao Yu"
                    },
                    {
                        "name": "Lei Yu"
                    },
                    {
                        "name": "Dezhi Ran"
                    },
                    {
                        "name": "Muhan Zeng"
                    },
                    {
                        "name": "Bo Shen"
                    },
                    {
                        "name": "Pan Bian"
                    },
                    {
                        "name": "Guangtai Liang"
                    },
                    {
                        "name": "Bei Guan"
                    },
                    {
                        "name": "Pengjie Huang"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Yongji Wang"
                    },
                    {
                        "name": "Qianxiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qianxiang Wang"
                },
                "author": "Qianxiang Wang",
                "arxiv_comment": "This work is in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14352v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14352v1",
                "updated": "2024-08-26T15:29:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    29,
                    34,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T15:29:34Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    29,
                    34,
                    0,
                    239,
                    0
                ],
                "title": "Assessing Contamination in Large Language Models: Introducing the\n  LogProber method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Contamination in Large Language Models: Introducing the\n  LogProber method"
                },
                "summary": "In machine learning, contamination refers to situations where testing data\nleak into the training set. The issue is particularly relevant for the\nevaluation of the performance of Large Language Models (LLMs), which are\ngenerally trained on gargantuan, and generally opaque, corpora of text scraped\nfrom the world wide web. Developing tools to detect contamination is therefore\ncrucial to be able to fairly and properly track the evolution of the\nperformance of LLMs. Most recent works in the field are not tailored to\nquantify contamination on short sequences of text like we find in psychology\nquestionnaires. In the present paper we introduce LogProber, a novel,\nefficient, algorithm that we show able to detect contamination using token\nprobability in given sentences. In the second part we investigate the\nlimitations of the method and discuss how different training methods can\ncontaminate models without leaving traces in the token probabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In machine learning, contamination refers to situations where testing data\nleak into the training set. The issue is particularly relevant for the\nevaluation of the performance of Large Language Models (LLMs), which are\ngenerally trained on gargantuan, and generally opaque, corpora of text scraped\nfrom the world wide web. Developing tools to detect contamination is therefore\ncrucial to be able to fairly and properly track the evolution of the\nperformance of LLMs. Most recent works in the field are not tailored to\nquantify contamination on short sequences of text like we find in psychology\nquestionnaires. In the present paper we introduce LogProber, a novel,\nefficient, algorithm that we show able to detect contamination using token\nprobability in given sentences. In the second part we investigate the\nlimitations of the method and discuss how different training methods can\ncontaminate models without leaving traces in the token probabilities."
                },
                "authors": [
                    {
                        "name": "Nicolas Yax"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    },
                    {
                        "name": "Stefano Palminteri"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Palminteri"
                },
                "author": "Stefano Palminteri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14352v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14352v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14351v1",
                "updated": "2024-08-26T15:29:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    29,
                    9,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T15:29:09Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    29,
                    9,
                    0,
                    239,
                    0
                ],
                "title": "Can supernova from runaway stars mimic the signs of absorbing\n  `super-virial' gas?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can supernova from runaway stars mimic the signs of absorbing\n  `super-virial' gas?"
                },
                "summary": "The recent detection of large column density absorption lines from highly\nionized gas in a few directions through the circumgalactic medium (CGM) of the\nMilky Way (MW) has been puzzling. The inferred temperature from these\nabsorption lines far exceeds the virial temperature of the MW, and the column\ndensities are also too large to be easily explained. In this paper, we propose\na novel idea to explain these observations and claim that they may not have\noriginated from the CGM, but from a totally different type of source, namely,\nstellar ejecta from supernovae (SNe) above the Galactic disk that happen to lie\nin the line of sight to the background quasars. About $\\sim 20\\%$ of massive OB\nstars (progenitors of core-collapse supernovae) are known to be runaway stars\nthat have high ejection velocities near the Galactic plane and can end up\nexploding as SNe above the Galactic disk. We show that the associated reverse\nshock in the supernova remnant in the early non-radiative phase can heat the\nejecta to temperatures of $\\gtrsim 10^7\\,{\\rm K}$ and can naturally explain the\nobserved high column density of ions in the observed `super-virial' phase along\nwith $\\alpha$-enriched super-solar abundance that is typical of core-collapse\nsupernovae. However, SNe from runaway stars has a covering fraction of\n$\\lesssim 0.7 \\%$ and thus can only explain the observations along limited\nsightlines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent detection of large column density absorption lines from highly\nionized gas in a few directions through the circumgalactic medium (CGM) of the\nMilky Way (MW) has been puzzling. The inferred temperature from these\nabsorption lines far exceeds the virial temperature of the MW, and the column\ndensities are also too large to be easily explained. In this paper, we propose\na novel idea to explain these observations and claim that they may not have\noriginated from the CGM, but from a totally different type of source, namely,\nstellar ejecta from supernovae (SNe) above the Galactic disk that happen to lie\nin the line of sight to the background quasars. About $\\sim 20\\%$ of massive OB\nstars (progenitors of core-collapse supernovae) are known to be runaway stars\nthat have high ejection velocities near the Galactic plane and can end up\nexploding as SNe above the Galactic disk. We show that the associated reverse\nshock in the supernova remnant in the early non-radiative phase can heat the\nejecta to temperatures of $\\gtrsim 10^7\\,{\\rm K}$ and can naturally explain the\nobserved high column density of ions in the observed `super-virial' phase along\nwith $\\alpha$-enriched super-solar abundance that is typical of core-collapse\nsupernovae. However, SNe from runaway stars has a covering fraction of\n$\\lesssim 0.7 \\%$ and thus can only explain the observations along limited\nsightlines."
                },
                "authors": [
                    {
                        "name": "Mukesh Singh Bisht"
                    },
                    {
                        "name": "Projjwal Banerjee"
                    },
                    {
                        "name": "Biman B. Nath"
                    },
                    {
                        "name": "Yuri Shchekinov"
                    }
                ],
                "author_detail": {
                    "name": "Yuri Shchekinov"
                },
                "arxiv_affiliation": "Raman Research Institute, Bengaluru - 560080, India",
                "author": "Yuri Shchekinov",
                "arxiv_comment": "14 pages, submitted to the Astrophysical Journal (ApJ)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14340v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14340v2",
                "updated": "2024-08-27T14:09:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    9,
                    44,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-26T15:13:14Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    13,
                    14,
                    0,
                    239,
                    0
                ],
                "title": "Foundation Models for Music: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Models for Music: A Survey"
                },
                "summary": "In recent years, foundation models (FMs) such as large language models (LLMs)\nand latent diffusion models (LDMs) have profoundly impacted diverse sectors,\nincluding music. This comprehensive review examines state-of-the-art (SOTA)\npre-trained models and foundation models in music, spanning from representation\nlearning, generative learning and multimodal learning. We first contextualise\nthe significance of music in various industries and trace the evolution of AI\nin music. By delineating the modalities targeted by foundation models, we\ndiscover many of the music representations are underexplored in FM development.\nThen, emphasis is placed on the lack of versatility of previous methods on\ndiverse music applications, along with the potential of FMs in music\nunderstanding, generation and medical application. By comprehensively exploring\nthe details of the model pre-training paradigm, architectural choices,\ntokenisation, finetuning methodologies and controllability, we emphasise the\nimportant topics that should have been well explored, like instruction tuning\nand in-context learning, scaling law and emergent ability, as well as\nlong-sequence modelling etc. A dedicated section presents insights into music\nagents, accompanied by a thorough analysis of datasets and evaluations\nessential for pre-training and downstream tasks. Finally, by underscoring the\nvital importance of ethical considerations, we advocate that following research\non FM for music should focus more on such issues as interpretability,\ntransparency, human responsibility, and copyright issues. The paper offers\ninsights into future challenges and trends on FMs for music, aiming to shape\nthe trajectory of human-AI collaboration in the music realm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, foundation models (FMs) such as large language models (LLMs)\nand latent diffusion models (LDMs) have profoundly impacted diverse sectors,\nincluding music. This comprehensive review examines state-of-the-art (SOTA)\npre-trained models and foundation models in music, spanning from representation\nlearning, generative learning and multimodal learning. We first contextualise\nthe significance of music in various industries and trace the evolution of AI\nin music. By delineating the modalities targeted by foundation models, we\ndiscover many of the music representations are underexplored in FM development.\nThen, emphasis is placed on the lack of versatility of previous methods on\ndiverse music applications, along with the potential of FMs in music\nunderstanding, generation and medical application. By comprehensively exploring\nthe details of the model pre-training paradigm, architectural choices,\ntokenisation, finetuning methodologies and controllability, we emphasise the\nimportant topics that should have been well explored, like instruction tuning\nand in-context learning, scaling law and emergent ability, as well as\nlong-sequence modelling etc. A dedicated section presents insights into music\nagents, accompanied by a thorough analysis of datasets and evaluations\nessential for pre-training and downstream tasks. Finally, by underscoring the\nvital importance of ethical considerations, we advocate that following research\non FM for music should focus more on such issues as interpretability,\ntransparency, human responsibility, and copyright issues. The paper offers\ninsights into future challenges and trends on FMs for music, aiming to shape\nthe trajectory of human-AI collaboration in the music realm."
                },
                "authors": [
                    {
                        "name": "Yinghao Ma"
                    },
                    {
                        "name": "Anders Øland"
                    },
                    {
                        "name": "Anton Ragni"
                    },
                    {
                        "name": "Bleiz MacSen Del Sette"
                    },
                    {
                        "name": "Charalampos Saitis"
                    },
                    {
                        "name": "Chris Donahue"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Christos Plachouras"
                    },
                    {
                        "name": "Emmanouil Benetos"
                    },
                    {
                        "name": "Elio Quinton"
                    },
                    {
                        "name": "Elona Shatri"
                    },
                    {
                        "name": "Fabio Morreale"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "György Fazekas"
                    },
                    {
                        "name": "Gus Xia"
                    },
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Ilaria Manco"
                    },
                    {
                        "name": "Jiawen Huang"
                    },
                    {
                        "name": "Julien Guinot"
                    },
                    {
                        "name": "Liwei Lin"
                    },
                    {
                        "name": "Luca Marinelli"
                    },
                    {
                        "name": "Max W. Y. Lam"
                    },
                    {
                        "name": "Megha Sharma"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Roger B. Dannenberg"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Shangda Wu"
                    },
                    {
                        "name": "Shih-Lun Wu"
                    },
                    {
                        "name": "Shuqi Dai"
                    },
                    {
                        "name": "Shun Lei"
                    },
                    {
                        "name": "Shiyin Kang"
                    },
                    {
                        "name": "Simon Dixon"
                    },
                    {
                        "name": "Wenhu Chen"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Xingjian Du"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Zeyue Tian"
                    },
                    {
                        "name": "Zhiyong Wu"
                    },
                    {
                        "name": "Zhizheng Wu"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Ziyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ziyu Wang"
                },
                "author": "Ziyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14340v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14340v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16528v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16528v2",
                "updated": "2024-08-26T14:59:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    59,
                    53,
                    0,
                    239,
                    0
                ],
                "published": "2024-05-26T11:29:57Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    11,
                    29,
                    57,
                    6,
                    147,
                    0
                ],
                "title": "LoQT: Low Rank Adapters for Quantized Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoQT: Low Rank Adapters for Quantized Training"
                },
                "summary": "Training of large neural networks requires significant computational\nresources. Despite advances using low-rank adapters and quantization,\npretraining of models such as LLMs on consumer hardware has not been possible\nwithout model sharding, offloading during training, or per-layer gradient\nupdates. To address these limitations, we propose LoQT, a method for\nefficiently training quantized models. LoQT uses gradient-based tensor\nfactorization to initialize low-rank trainable weight matrices that are\nperiodically merged into quantized full-rank weight matrices. Our approach is\nsuitable for both pretraining and fine-tuning of models, which we demonstrate\nexperimentally for language modeling and downstream task adaptation. We find\nthat LoQT enables efficient training of models up to 7B parameters on a\nconsumer-grade 24GB GPU. We also demonstrate the feasibility of training a 13B\nparameter model using per-layer gradient updates on the same hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training of large neural networks requires significant computational\nresources. Despite advances using low-rank adapters and quantization,\npretraining of models such as LLMs on consumer hardware has not been possible\nwithout model sharding, offloading during training, or per-layer gradient\nupdates. To address these limitations, we propose LoQT, a method for\nefficiently training quantized models. LoQT uses gradient-based tensor\nfactorization to initialize low-rank trainable weight matrices that are\nperiodically merged into quantized full-rank weight matrices. Our approach is\nsuitable for both pretraining and fine-tuning of models, which we demonstrate\nexperimentally for language modeling and downstream task adaptation. We find\nthat LoQT enables efficient training of models up to 7B parameters on a\nconsumer-grade 24GB GPU. We also demonstrate the feasibility of training a 13B\nparameter model using per-layer gradient updates on the same hardware."
                },
                "authors": [
                    {
                        "name": "Sebastian Loeschcke"
                    },
                    {
                        "name": "Mads Toftrup"
                    },
                    {
                        "name": "Michael J. Kastoryano"
                    },
                    {
                        "name": "Serge Belongie"
                    },
                    {
                        "name": "Vésteinn Snæbjarnarson"
                    }
                ],
                "author_detail": {
                    "name": "Vésteinn Snæbjarnarson"
                },
                "author": "Vésteinn Snæbjarnarson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16528v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16528v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14326v1",
                "updated": "2024-08-26T14:54:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    54,
                    14,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T14:54:14Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    54,
                    14,
                    0,
                    239,
                    0
                ],
                "title": "Streamline tractography of the fetal brain in utero with machine\n  learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streamline tractography of the fetal brain in utero with machine\n  learning"
                },
                "summary": "Diffusion-weighted magnetic resonance imaging (dMRI) is the only non-invasive\ntool for studying white matter tracts and structural connectivity of the brain.\nThese assessments rely heavily on tractography techniques, which reconstruct\nvirtual streamlines representing white matter fibers. Much effort has been\ndevoted to improving tractography methodology for adult brains, while\ntractography of the fetal brain has been largely neglected. Fetal tractography\nfaces unique difficulties due to low dMRI signal quality, immature and rapidly\ndeveloping brain structures, and paucity of reference data. This work presents\nthe first machine learning model for fetal tractography. The model input\nconsists of five sources of information: (1) Fiber orientation, inferred from a\ndiffusion tensor fit to the dMRI signal; (2) Directions of recent propagation\nsteps; (3) Global spatial information, encoded as distances to keypoints in the\nbrain cortex; (4) Tissue segmentation information; and (5) Prior information\nabout the expected local fiber orientations supplied with an atlas. In order to\nmitigate the local tensor estimation error, a large spatial context around the\ncurrent point in the diffusion tensor image is encoded using convolutional and\nattention neural network modules. Moreover, the diffusion tensor information at\na hypothetical next point is included in the model input. Filtering rules based\non anatomically constrained tractography are applied to prune implausible\nstreamlines. We trained the model on manually-refined whole-brain fetal\ntractograms and validated the trained model on an independent set of 11 test\nscans with gestational ages between 23 and 36 weeks. Results show that our\nproposed method achieves superior performance across all evaluated tracts. The\nnew method can significantly advance the capabilities of dMRI for studying\nnormal and abnormal brain development in utero.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-weighted magnetic resonance imaging (dMRI) is the only non-invasive\ntool for studying white matter tracts and structural connectivity of the brain.\nThese assessments rely heavily on tractography techniques, which reconstruct\nvirtual streamlines representing white matter fibers. Much effort has been\ndevoted to improving tractography methodology for adult brains, while\ntractography of the fetal brain has been largely neglected. Fetal tractography\nfaces unique difficulties due to low dMRI signal quality, immature and rapidly\ndeveloping brain structures, and paucity of reference data. This work presents\nthe first machine learning model for fetal tractography. The model input\nconsists of five sources of information: (1) Fiber orientation, inferred from a\ndiffusion tensor fit to the dMRI signal; (2) Directions of recent propagation\nsteps; (3) Global spatial information, encoded as distances to keypoints in the\nbrain cortex; (4) Tissue segmentation information; and (5) Prior information\nabout the expected local fiber orientations supplied with an atlas. In order to\nmitigate the local tensor estimation error, a large spatial context around the\ncurrent point in the diffusion tensor image is encoded using convolutional and\nattention neural network modules. Moreover, the diffusion tensor information at\na hypothetical next point is included in the model input. Filtering rules based\non anatomically constrained tractography are applied to prune implausible\nstreamlines. We trained the model on manually-refined whole-brain fetal\ntractograms and validated the trained model on an independent set of 11 test\nscans with gestational ages between 23 and 36 weeks. Results show that our\nproposed method achieves superior performance across all evaluated tracts. The\nnew method can significantly advance the capabilities of dMRI for studying\nnormal and abnormal brain development in utero."
                },
                "authors": [
                    {
                        "name": "Weide Liu"
                    },
                    {
                        "name": "Camilo Calixto"
                    },
                    {
                        "name": "Simon K. Warfield"
                    },
                    {
                        "name": "Davood Karimi"
                    }
                ],
                "author_detail": {
                    "name": "Davood Karimi"
                },
                "author": "Davood Karimi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14319v1",
                "updated": "2024-08-26T14:51:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    51,
                    26,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T14:51:26Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    51,
                    26,
                    0,
                    239,
                    0
                ],
                "title": "Rethinking Knowledge Transfer in Learning Using Privileged Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Knowledge Transfer in Learning Using Privileged Information"
                },
                "summary": "In supervised machine learning, privileged information (PI) is information\nthat is unavailable at inference, but is accessible during training time.\nResearch on learning using privileged information (LUPI) aims to transfer the\nknowledge captured in PI onto a model that can perform inference without PI. It\nseems that this extra bit of information ought to make the resulting model\nbetter. However, finding conclusive theoretical or empirical evidence that\nsupports the ability to transfer knowledge using PI has been challenging. In\nthis paper, we critically examine the assumptions underlying existing\ntheoretical analyses and argue that there is little theoretical justification\nfor when LUPI should work. We analyze LUPI methods and reveal that apparent\nimprovements in empirical risk of existing research may not directly result\nfrom PI. Instead, these improvements often stem from dataset anomalies or\nmodifications in model design misguidedly attributed to PI. Our experiments for\na wide variety of application domains further demonstrate that state-of-the-art\nLUPI approaches fail to effectively transfer knowledge from PI. Thus, we\nadvocate for practitioners to exercise caution when working with PI to avoid\nunintended inductive biases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In supervised machine learning, privileged information (PI) is information\nthat is unavailable at inference, but is accessible during training time.\nResearch on learning using privileged information (LUPI) aims to transfer the\nknowledge captured in PI onto a model that can perform inference without PI. It\nseems that this extra bit of information ought to make the resulting model\nbetter. However, finding conclusive theoretical or empirical evidence that\nsupports the ability to transfer knowledge using PI has been challenging. In\nthis paper, we critically examine the assumptions underlying existing\ntheoretical analyses and argue that there is little theoretical justification\nfor when LUPI should work. We analyze LUPI methods and reveal that apparent\nimprovements in empirical risk of existing research may not directly result\nfrom PI. Instead, these improvements often stem from dataset anomalies or\nmodifications in model design misguidedly attributed to PI. Our experiments for\na wide variety of application domains further demonstrate that state-of-the-art\nLUPI approaches fail to effectively transfer knowledge from PI. Thus, we\nadvocate for practitioners to exercise caution when working with PI to avoid\nunintended inductive biases."
                },
                "authors": [
                    {
                        "name": "Danil Provodin"
                    },
                    {
                        "name": "Bram van den Akker"
                    },
                    {
                        "name": "Christina Katsimerou"
                    },
                    {
                        "name": "Maurits Kaptein"
                    },
                    {
                        "name": "Mykola Pechenizkiy"
                    }
                ],
                "author_detail": {
                    "name": "Mykola Pechenizkiy"
                },
                "author": "Mykola Pechenizkiy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.07437v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.07437v3",
                "updated": "2024-08-26T14:46:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    46,
                    8,
                    0,
                    239,
                    0
                ],
                "published": "2023-02-15T02:58:09Z",
                "published_parsed": [
                    2023,
                    2,
                    15,
                    2,
                    58,
                    9,
                    2,
                    46,
                    0
                ],
                "title": "Bridging the Usability Gap: Theoretical and Methodological Advances for\n  Spectral Learning of Hidden Markov Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Usability Gap: Theoretical and Methodological Advances for\n  Spectral Learning of Hidden Markov Models"
                },
                "summary": "The Baum-Welch (B-W) algorithm is the most widely accepted method for\ninferring hidden Markov models (HMM). However, it is prone to getting stuck in\nlocal optima, and can be too slow for many real-time applications. Spectral\nlearning of HMMs (SHMM), based on the method of moments (MOM) has been proposed\nin the literature to overcome these obstacles. Despite its promises, asymptotic\ntheory for SHMM has been elusive, and the long-run performance of SHMM can\ndegrade due to unchecked propagation of error. In this paper, we (1) provide an\nasymptotic distribution for the approximate error of the likelihood estimated\nby SHMM, (2) propose a novel algorithm called projected SHMM (PSHMM) that\nmitigates the problem of error propagation, and (3) develop online learning\nvariants of both SHMM and PSHMM that accommodate potential nonstationarity. We\ncompare the performance of SHMM with PSHMM and estimation through the B-W\nalgorithm on both simulated data and data from real world applications, and\nfind that PSHMM not only retains the computational advantages of SHMM, but also\nprovides more robust estimation and forecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Baum-Welch (B-W) algorithm is the most widely accepted method for\ninferring hidden Markov models (HMM). However, it is prone to getting stuck in\nlocal optima, and can be too slow for many real-time applications. Spectral\nlearning of HMMs (SHMM), based on the method of moments (MOM) has been proposed\nin the literature to overcome these obstacles. Despite its promises, asymptotic\ntheory for SHMM has been elusive, and the long-run performance of SHMM can\ndegrade due to unchecked propagation of error. In this paper, we (1) provide an\nasymptotic distribution for the approximate error of the likelihood estimated\nby SHMM, (2) propose a novel algorithm called projected SHMM (PSHMM) that\nmitigates the problem of error propagation, and (3) develop online learning\nvariants of both SHMM and PSHMM that accommodate potential nonstationarity. We\ncompare the performance of SHMM with PSHMM and estimation through the B-W\nalgorithm on both simulated data and data from real world applications, and\nfind that PSHMM not only retains the computational advantages of SHMM, but also\nprovides more robust estimation and forecasting."
                },
                "authors": [
                    {
                        "name": "Xiaoyuan Ma"
                    },
                    {
                        "name": "Jordan Rodu"
                    }
                ],
                "author_detail": {
                    "name": "Jordan Rodu"
                },
                "author": "Jordan Rodu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.07437v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.07437v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14317v1",
                "updated": "2024-08-26T14:45:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    45,
                    3,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T14:45:03Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    45,
                    3,
                    0,
                    239,
                    0
                ],
                "title": "Claim Verification in the Age of Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Claim Verification in the Age of Large Language Models: A Survey"
                },
                "summary": "The large and ever-increasing amount of data available on the Internet\ncoupled with the laborious task of manual claim and fact verification has\nsparked the interest in the development of automated claim verification\nsystems. Several deep learning and transformer-based models have been proposed\nfor this task over the years. With the introduction of Large Language Models\n(LLMs) and their superior performance in several NLP tasks, we have seen a\nsurge of LLM-based approaches to claim verification along with the use of novel\nmethods such as Retrieval Augmented Generation (RAG). In this survey, we\npresent a comprehensive account of recent claim verification frameworks using\nLLMs. We describe the different components of the claim verification pipeline\nused in these frameworks in detail including common approaches to retrieval,\nprompting, and fine-tuning. Finally, we describe publicly available English\ndatasets created for this task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The large and ever-increasing amount of data available on the Internet\ncoupled with the laborious task of manual claim and fact verification has\nsparked the interest in the development of automated claim verification\nsystems. Several deep learning and transformer-based models have been proposed\nfor this task over the years. With the introduction of Large Language Models\n(LLMs) and their superior performance in several NLP tasks, we have seen a\nsurge of LLM-based approaches to claim verification along with the use of novel\nmethods such as Retrieval Augmented Generation (RAG). In this survey, we\npresent a comprehensive account of recent claim verification frameworks using\nLLMs. We describe the different components of the claim verification pipeline\nused in these frameworks in detail including common approaches to retrieval,\nprompting, and fine-tuning. Finally, we describe publicly available English\ndatasets created for this task."
                },
                "authors": [
                    {
                        "name": "Alphaeus Dmonte"
                    },
                    {
                        "name": "Roland Oruche"
                    },
                    {
                        "name": "Marcos Zampieri"
                    },
                    {
                        "name": "Prasad Calyam"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14307v1",
                "updated": "2024-08-26T14:38:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    38,
                    19,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T14:38:19Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    38,
                    19,
                    0,
                    239,
                    0
                ],
                "title": "LLM-3D Print: Large Language Models To Monitor and Control 3D Printing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-3D Print: Large Language Models To Monitor and Control 3D Printing"
                },
                "summary": "Industry 4.0 has revolutionized manufacturing by driving digitalization and\nshifting the paradigm toward additive manufacturing (AM). Fused Deposition\nModeling (FDM), a key AM technology, enables the creation of highly customized,\ncost-effective products with minimal material waste through layer-by-layer\nextrusion, posing a significant challenge to traditional subtractive methods.\nHowever, the susceptibility of material extrusion techniques to errors often\nrequires expert intervention to detect and mitigate defects that can severely\ncompromise product quality. While automated error detection and machine\nlearning models exist, their generalizability across diverse 3D printer setups,\nfirmware, and sensors is limited, and deep learning methods require extensive\nlabeled datasets, hindering scalability and adaptability. To address these\nchallenges, we present a process monitoring and control framework that\nleverages pre-trained Large Language Models (LLMs) alongside 3D printers to\ndetect and address printing defects. The LLM evaluates print quality by\nanalyzing images captured after each layer or print segment, identifying\nfailure modes and querying the printer for relevant parameters. It then\ngenerates and executes a corrective action plan. We validated the effectiveness\nof the proposed framework in identifying defects by comparing it against a\ncontrol group of engineers with diverse AM expertise. Our evaluation\ndemonstrated that LLM-based agents not only accurately identify common 3D\nprinting errors, such as inconsistent extrusion, stringing, warping, and layer\nadhesion, but also effectively determine the parameters causing these failures\nand autonomously correct them without any need for human intervention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industry 4.0 has revolutionized manufacturing by driving digitalization and\nshifting the paradigm toward additive manufacturing (AM). Fused Deposition\nModeling (FDM), a key AM technology, enables the creation of highly customized,\ncost-effective products with minimal material waste through layer-by-layer\nextrusion, posing a significant challenge to traditional subtractive methods.\nHowever, the susceptibility of material extrusion techniques to errors often\nrequires expert intervention to detect and mitigate defects that can severely\ncompromise product quality. While automated error detection and machine\nlearning models exist, their generalizability across diverse 3D printer setups,\nfirmware, and sensors is limited, and deep learning methods require extensive\nlabeled datasets, hindering scalability and adaptability. To address these\nchallenges, we present a process monitoring and control framework that\nleverages pre-trained Large Language Models (LLMs) alongside 3D printers to\ndetect and address printing defects. The LLM evaluates print quality by\nanalyzing images captured after each layer or print segment, identifying\nfailure modes and querying the printer for relevant parameters. It then\ngenerates and executes a corrective action plan. We validated the effectiveness\nof the proposed framework in identifying defects by comparing it against a\ncontrol group of engineers with diverse AM expertise. Our evaluation\ndemonstrated that LLM-based agents not only accurately identify common 3D\nprinting errors, such as inconsistent extrusion, stringing, warping, and layer\nadhesion, but also effectively determine the parameters causing these failures\nand autonomously correct them without any need for human intervention."
                },
                "authors": [
                    {
                        "name": "Yayati Jadhav"
                    },
                    {
                        "name": "Peter Pak"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10594v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10594v3",
                "updated": "2024-08-26T14:30:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    30,
                    38,
                    0,
                    239,
                    0
                ],
                "published": "2024-06-15T11:03:33Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    11,
                    3,
                    33,
                    5,
                    167,
                    0
                ],
                "title": "BlockPruner: Fine-grained Pruning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockPruner: Fine-grained Pruning for Large Language Models"
                },
                "summary": "With the rapid growth in the size and complexity of large language models\n(LLMs), the costs associated with their training and inference have escalated\nsignificantly. Research indicates that certain layers in LLMs harbor\nsubstantial redundancy, and pruning these layers has minimal impact on the\noverall performance. While various layer pruning methods have been developed\nbased on this insight, they generally overlook the finer-grained redundancies\nwithin the layers themselves. In this paper, we delve deeper into the\narchitecture of LLMs and demonstrate that finer-grained pruning can be achieved\nby targeting redundancies in multi-head attention (MHA) and multi-layer\nperceptron (MLP) blocks. We propose a novel, training-free structured pruning\napproach called BlockPruner. Unlike existing layer pruning methods, BlockPruner\nsegments each Transformer layer into MHA and MLP blocks. It then assesses the\nimportance of these blocks using perplexity measures and applies a heuristic\nsearch for iterative pruning. We applied BlockPruner to LLMs of various sizes\nand architectures and validated its performance across a wide range of\ndownstream tasks. Experimental results show that BlockPruner achieves more\ngranular and effective pruning compared to state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth in the size and complexity of large language models\n(LLMs), the costs associated with their training and inference have escalated\nsignificantly. Research indicates that certain layers in LLMs harbor\nsubstantial redundancy, and pruning these layers has minimal impact on the\noverall performance. While various layer pruning methods have been developed\nbased on this insight, they generally overlook the finer-grained redundancies\nwithin the layers themselves. In this paper, we delve deeper into the\narchitecture of LLMs and demonstrate that finer-grained pruning can be achieved\nby targeting redundancies in multi-head attention (MHA) and multi-layer\nperceptron (MLP) blocks. We propose a novel, training-free structured pruning\napproach called BlockPruner. Unlike existing layer pruning methods, BlockPruner\nsegments each Transformer layer into MHA and MLP blocks. It then assesses the\nimportance of these blocks using perplexity measures and applies a heuristic\nsearch for iterative pruning. We applied BlockPruner to LLMs of various sizes\nand architectures and validated its performance across a wide range of\ndownstream tasks. Experimental results show that BlockPruner achieves more\ngranular and effective pruning compared to state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Longguang Zhong"
                    },
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Ruijun Chen"
                    },
                    {
                        "name": "Xiaojun Quan"
                    },
                    {
                        "name": "Liangzhi Li"
                    }
                ],
                "author_detail": {
                    "name": "Liangzhi Li"
                },
                "author": "Liangzhi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10594v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10594v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14293v1",
                "updated": "2024-08-26T14:25:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    25,
                    30,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T14:25:30Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    25,
                    30,
                    0,
                    239,
                    0
                ],
                "title": "Investigating the Effectiveness of Bayesian Spam Filters in Detecting\n  LLM-modified Spam Mails",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating the Effectiveness of Bayesian Spam Filters in Detecting\n  LLM-modified Spam Mails"
                },
                "summary": "Spam and phishing remain critical threats in cybersecurity, responsible for\nnearly 90% of security incidents. As these attacks grow in sophistication, the\nneed for robust defensive mechanisms intensifies. Bayesian spam filters, like\nthe widely adopted open-source SpamAssassin, are essential tools in this fight.\nHowever, the emergence of large language models (LLMs) such as ChatGPT presents\nnew challenges. These models are not only powerful and accessible, but also\ninexpensive to use, raising concerns about their misuse in crafting\nsophisticated spam emails that evade traditional spam filters. This work aims\nto evaluate the robustness and effectiveness of SpamAssassin against\nLLM-modified email content. We developed a pipeline to test this vulnerability.\nOur pipeline modifies spam emails using GPT-3.5 Turbo and assesses\nSpamAssassin's ability to classify these modified emails correctly. The results\nshow that SpamAssassin misclassified up to 73.7% of LLM-modified spam emails as\nlegitimate. In contrast, a simpler dictionary-replacement attack showed a\nmaximum success rate of only 0.4%. These findings highlight the significant\nthreat posed by LLM-modified spam, especially given the cost-efficiency of such\nattacks (0.17 cents per email). This paper provides crucial insights into the\nvulnerabilities of current spam filters and the need for continuous improvement\nin cybersecurity measures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spam and phishing remain critical threats in cybersecurity, responsible for\nnearly 90% of security incidents. As these attacks grow in sophistication, the\nneed for robust defensive mechanisms intensifies. Bayesian spam filters, like\nthe widely adopted open-source SpamAssassin, are essential tools in this fight.\nHowever, the emergence of large language models (LLMs) such as ChatGPT presents\nnew challenges. These models are not only powerful and accessible, but also\ninexpensive to use, raising concerns about their misuse in crafting\nsophisticated spam emails that evade traditional spam filters. This work aims\nto evaluate the robustness and effectiveness of SpamAssassin against\nLLM-modified email content. We developed a pipeline to test this vulnerability.\nOur pipeline modifies spam emails using GPT-3.5 Turbo and assesses\nSpamAssassin's ability to classify these modified emails correctly. The results\nshow that SpamAssassin misclassified up to 73.7% of LLM-modified spam emails as\nlegitimate. In contrast, a simpler dictionary-replacement attack showed a\nmaximum success rate of only 0.4%. These findings highlight the significant\nthreat posed by LLM-modified spam, especially given the cost-efficiency of such\nattacks (0.17 cents per email). This paper provides crucial insights into the\nvulnerabilities of current spam filters and the need for continuous improvement\nin cybersecurity measures."
                },
                "authors": [
                    {
                        "name": "Malte Josten"
                    },
                    {
                        "name": "Torben Weis"
                    }
                ],
                "author_detail": {
                    "name": "Torben Weis"
                },
                "author": "Torben Weis",
                "arxiv_comment": "EAI International Conference on Digital Forensics & Cyber Crime 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14287v1",
                "updated": "2024-08-26T14:16:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    16,
                    22,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T14:16:22Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    16,
                    22,
                    0,
                    239,
                    0
                ],
                "title": "Cosmology and nuclear-physics implications of a subsolar\n  gravitational-wave event",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmology and nuclear-physics implications of a subsolar\n  gravitational-wave event"
                },
                "summary": "Detecting a compact subsolar object would have profound implications in\nphysics, the reach of which depends on the nature of the object. Here we\nexplore such consequences for a putative subsolar-mass gravitational wave event\ndetected by the LIGO-Virgo-KAGRA Collaboration. We forecast that the nature of\na subsolar binary (made of light neutron stars, primordial black holes, or more\nexotic compact objects) can be inferred with a great statistical confidence\nlevel already during the ongoing fourth observing run, based on the large tidal\ndeformability effects on the signal. The detection of a primordial black hole\nwould have implications for cosmology and dark matter scenarios, while the\nmeasurement of the tidal deformability of a subsolar neutron star could rule\nout or confirm the existence of strange stars made of quarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting a compact subsolar object would have profound implications in\nphysics, the reach of which depends on the nature of the object. Here we\nexplore such consequences for a putative subsolar-mass gravitational wave event\ndetected by the LIGO-Virgo-KAGRA Collaboration. We forecast that the nature of\na subsolar binary (made of light neutron stars, primordial black holes, or more\nexotic compact objects) can be inferred with a great statistical confidence\nlevel already during the ongoing fourth observing run, based on the large tidal\ndeformability effects on the signal. The detection of a primordial black hole\nwould have implications for cosmology and dark matter scenarios, while the\nmeasurement of the tidal deformability of a subsolar neutron star could rule\nout or confirm the existence of strange stars made of quarks."
                },
                "authors": [
                    {
                        "name": "Francesco Crescimbeni"
                    },
                    {
                        "name": "Gabriele Franciolini"
                    },
                    {
                        "name": "Paolo Pani"
                    },
                    {
                        "name": "Massimo Vaglio"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Vaglio"
                },
                "author": "Massimo Vaglio",
                "arxiv_comment": "5+4 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14277v1",
                "updated": "2024-08-26T13:53:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    13,
                    53,
                    4,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T13:53:04Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    13,
                    53,
                    4,
                    0,
                    239,
                    0
                ],
                "title": "Epidemic Information Extraction for Event-Based Surveillance using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Epidemic Information Extraction for Event-Based Surveillance using Large\n  Language Models"
                },
                "summary": "This paper presents a novel approach to epidemic surveillance, leveraging the\npower of Artificial Intelligence and Large Language Models (LLMs) for effective\ninterpretation of unstructured big data sources, like the popular ProMED and\nWHO Disease Outbreak News. We explore several LLMs, evaluating their\ncapabilities in extracting valuable epidemic information. We further enhance\nthe capabilities of the LLMs using in-context learning, and test the\nperformance of an ensemble model incorporating multiple open-source LLMs. The\nfindings indicate that LLMs can significantly enhance the accuracy and\ntimeliness of epidemic modelling and forecasting, offering a promising tool for\nmanaging future pandemic events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach to epidemic surveillance, leveraging the\npower of Artificial Intelligence and Large Language Models (LLMs) for effective\ninterpretation of unstructured big data sources, like the popular ProMED and\nWHO Disease Outbreak News. We explore several LLMs, evaluating their\ncapabilities in extracting valuable epidemic information. We further enhance\nthe capabilities of the LLMs using in-context learning, and test the\nperformance of an ensemble model incorporating multiple open-source LLMs. The\nfindings indicate that LLMs can significantly enhance the accuracy and\ntimeliness of epidemic modelling and forecasting, offering a promising tool for\nmanaging future pandemic events."
                },
                "authors": [
                    {
                        "name": "Sergio Consoli"
                    },
                    {
                        "name": "Peter Markov"
                    },
                    {
                        "name": "Nikolaos I. Stilianakis"
                    },
                    {
                        "name": "Lorenzo Bertolini"
                    },
                    {
                        "name": "Antonio Puertas Gallardo"
                    },
                    {
                        "name": "Mario Ceresa"
                    }
                ],
                "author_detail": {
                    "name": "Mario Ceresa"
                },
                "author": "Mario Ceresa",
                "arxiv_doi": "10.1007/978-981-97-4581-4_17",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-981-97-4581-4_17",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.14277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 4 figures, Ninth International Congress on Information and\n  Communication Technology (ICICT 2024)",
                "arxiv_journal_ref": "Lecture Notes in Networks and Systems, 2024, vol 1011, pages\n  241-252. Springer, Singapore",
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01, 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.2.7; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09431v2",
                "updated": "2024-08-26T13:41:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    13,
                    41,
                    14,
                    0,
                    239,
                    0
                ],
                "published": "2024-04-15T03:12:12Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    3,
                    12,
                    12,
                    0,
                    106,
                    0
                ],
                "title": "VFMM3D: Releasing the Potential of Image by Vision Foundation Model for\n  Monocular 3D Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VFMM3D: Releasing the Potential of Image by Vision Foundation Model for\n  Monocular 3D Object Detection"
                },
                "summary": "Due to its cost-effectiveness and widespread availability, monocular 3D\nobject detection, which relies solely on a single camera during inference,\nholds significant importance across various applications, including autonomous\ndriving and robotics. Nevertheless, directly predicting the coordinates of\nobjects in 3D space from monocular images poses challenges. Therefore, an\neffective solution involves transforming monocular images into LiDAR-like\nrepresentations and employing a LiDAR-based 3D object detector to predict the\n3D coordinates of objects. The key step in this method is accurately converting\nthe monocular image into a reliable point cloud form. In this paper, we present\nVFMM3D, an innovative framework that leverages the capabilities of Vision\nFoundation Models (VFMs) to accurately transform single-view images into LiDAR\npoint cloud representations. VFMM3D utilizes the Segment Anything Model (SAM)\nand Depth Anything Model (DAM) to generate high-quality pseudo-LiDAR data\nenriched with rich foreground information. Specifically, the Depth Anything\nModel (DAM) is employed to generate dense depth maps. Subsequently, the Segment\nAnything Model (SAM) is utilized to differentiate foreground and background\nregions by predicting instance masks. These predicted instance masks and depth\nmaps are then combined and projected into 3D space to generate pseudo-LiDAR\npoints. Finally, any object detectors based on point clouds can be utilized to\npredict the 3D coordinates of objects. Comprehensive experiments are conducted\non two challenging 3D object detection datasets, KITTI and Waymo. Our VFMM3D\nestablishes a new state-of-the-art performance on both datasets. Additionally,\nexperimental results demonstrate the generality of VFMM3D, showcasing its\nseamless integration into various LiDAR-based 3D object detectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to its cost-effectiveness and widespread availability, monocular 3D\nobject detection, which relies solely on a single camera during inference,\nholds significant importance across various applications, including autonomous\ndriving and robotics. Nevertheless, directly predicting the coordinates of\nobjects in 3D space from monocular images poses challenges. Therefore, an\neffective solution involves transforming monocular images into LiDAR-like\nrepresentations and employing a LiDAR-based 3D object detector to predict the\n3D coordinates of objects. The key step in this method is accurately converting\nthe monocular image into a reliable point cloud form. In this paper, we present\nVFMM3D, an innovative framework that leverages the capabilities of Vision\nFoundation Models (VFMs) to accurately transform single-view images into LiDAR\npoint cloud representations. VFMM3D utilizes the Segment Anything Model (SAM)\nand Depth Anything Model (DAM) to generate high-quality pseudo-LiDAR data\nenriched with rich foreground information. Specifically, the Depth Anything\nModel (DAM) is employed to generate dense depth maps. Subsequently, the Segment\nAnything Model (SAM) is utilized to differentiate foreground and background\nregions by predicting instance masks. These predicted instance masks and depth\nmaps are then combined and projected into 3D space to generate pseudo-LiDAR\npoints. Finally, any object detectors based on point clouds can be utilized to\npredict the 3D coordinates of objects. Comprehensive experiments are conducted\non two challenging 3D object detection datasets, KITTI and Waymo. Our VFMM3D\nestablishes a new state-of-the-art performance on both datasets. Additionally,\nexperimental results demonstrate the generality of VFMM3D, showcasing its\nseamless integration into various LiDAR-based 3D object detectors."
                },
                "authors": [
                    {
                        "name": "Bonan Ding"
                    },
                    {
                        "name": "Jin Xie"
                    },
                    {
                        "name": "Jing Nie"
                    },
                    {
                        "name": "Jiale Cao"
                    },
                    {
                        "name": "Xuelong Li"
                    },
                    {
                        "name": "Yanwei Pang"
                    }
                ],
                "author_detail": {
                    "name": "Yanwei Pang"
                },
                "author": "Yanwei Pang",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14259v1",
                "updated": "2024-08-26T13:26:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    13,
                    26,
                    44,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T13:26:44Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    13,
                    26,
                    44,
                    0,
                    239,
                    0
                ],
                "title": "Towards Synthetic Trace Generation of Modeling Operations using\n  In-Context Learning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Synthetic Trace Generation of Modeling Operations using\n  In-Context Learning Approach"
                },
                "summary": "Producing accurate software models is crucial in model-driven software\nengineering (MDE). However, modeling complex systems is an error-prone task\nthat requires deep application domain knowledge. In the past decade, several\nautomated techniques have been proposed to support academic and industrial\npractitioners by providing relevant modeling operations. Nevertheless, those\ntechniques require a huge amount of training data that cannot be available due\nto several factors, e.g., privacy issues. The advent of large language models\n(LLMs) can support the generation of synthetic data although state-of-the-art\napproaches are not yet supporting the generation of modeling operations. To\nfill the gap, we propose a conceptual framework that combines modeling event\nlogs, intelligent modeling assistants, and the generation of modeling\noperations using LLMs. In particular, the architecture comprises modeling\ncomponents that help the designer specify the system, record its operation\nwithin a graphical modeling environment, and automatically recommend relevant\noperations. In addition, we generate a completely new dataset of modeling\nevents by telling on the most prominent LLMs currently available. As a proof of\nconcept, we instantiate the proposed framework using a set of existing modeling\ntools employed in industrial use cases within different European projects. To\nassess the proposed methodology, we first evaluate the capability of the\nexamined LLMs to generate realistic modeling operations by relying on\nwell-founded distance metrics. Then, we evaluate the recommended operations by\nconsidering real-world industrial modeling artifacts. Our findings demonstrate\nthat LLMs can generate modeling events even though the overall accuracy is\nhigher when considering human-based operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Producing accurate software models is crucial in model-driven software\nengineering (MDE). However, modeling complex systems is an error-prone task\nthat requires deep application domain knowledge. In the past decade, several\nautomated techniques have been proposed to support academic and industrial\npractitioners by providing relevant modeling operations. Nevertheless, those\ntechniques require a huge amount of training data that cannot be available due\nto several factors, e.g., privacy issues. The advent of large language models\n(LLMs) can support the generation of synthetic data although state-of-the-art\napproaches are not yet supporting the generation of modeling operations. To\nfill the gap, we propose a conceptual framework that combines modeling event\nlogs, intelligent modeling assistants, and the generation of modeling\noperations using LLMs. In particular, the architecture comprises modeling\ncomponents that help the designer specify the system, record its operation\nwithin a graphical modeling environment, and automatically recommend relevant\noperations. In addition, we generate a completely new dataset of modeling\nevents by telling on the most prominent LLMs currently available. As a proof of\nconcept, we instantiate the proposed framework using a set of existing modeling\ntools employed in industrial use cases within different European projects. To\nassess the proposed methodology, we first evaluate the capability of the\nexamined LLMs to generate realistic modeling operations by relying on\nwell-founded distance metrics. Then, we evaluate the recommended operations by\nconsidering real-world industrial modeling artifacts. Our findings demonstrate\nthat LLMs can generate modeling events even though the overall accuracy is\nhigher when considering human-based operations."
                },
                "authors": [
                    {
                        "name": "Vittoriano Muttillo"
                    },
                    {
                        "name": "Claudio Di Sipio"
                    },
                    {
                        "name": "Riccardo Rubei"
                    },
                    {
                        "name": "Luca Berardinelli"
                    },
                    {
                        "name": "MohammadHadi Dehghani"
                    }
                ],
                "author_detail": {
                    "name": "MohammadHadi Dehghani"
                },
                "author": "MohammadHadi Dehghani",
                "arxiv_comment": "Accepted at the 39th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20584v2",
                "updated": "2024-08-26T13:19:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    13,
                    19,
                    48,
                    0,
                    239,
                    0
                ],
                "published": "2024-07-30T06:33:44Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    6,
                    33,
                    44,
                    1,
                    212,
                    0
                ],
                "title": "Pruning Large Language Models with Semi-Structural Adaptive Sparse\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning Large Language Models with Semi-Structural Adaptive Sparse\n  Training"
                },
                "summary": "The tremendous success of Large Language Models (LLMs) across various complex\ntasks relies heavily on their substantial scale, which raises challenges during\nmodel deployment due to their large memory consumption. Recently, numerous\nstudies have attempted to compress LLMs using one-shot pruning methods.\nHowever, these methods often experience considerable performance degradation on\ncomplex language understanding tasks, calling into question the feasibility of\npruning in LLMs. To address this issue, we propose a pruning pipeline for\nsemi-structured sparse models via retraining, termed Adaptive Sparse Trainer\n(AST). Unlike previous one-shot pruning methods, AST incrementally transforms\ndense models into sparse ones by applying decay to masked weights while\nallowing the model to adaptively select masks throughout the training process.\nFurthermore, we observe that using distillation with a dense model as the\nteacher can prevent the sparse model from falling into local optima and\naccelerate convergence. In addition, we incorporate extra well-initialized\nparameters to further enhance model performance with minimal increase in memory\nfootprint. AST can significantly enhance model performance, approaching the\nlevel of dense models. When applied to the LLaMA2-7B model, AST reduces the\nzero-shot accuracy gap between dense and semi-structured sparse models to 1.12%\nacross multiple zero-shot tasks, utilizing less than 0.4% of the pretraining\ntokens. Our work demonstrates the feasibility of deploying semi-structured\nsparse large language models and introduces a novel method for achieving highly\ncompressed models when combined with existing quantization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The tremendous success of Large Language Models (LLMs) across various complex\ntasks relies heavily on their substantial scale, which raises challenges during\nmodel deployment due to their large memory consumption. Recently, numerous\nstudies have attempted to compress LLMs using one-shot pruning methods.\nHowever, these methods often experience considerable performance degradation on\ncomplex language understanding tasks, calling into question the feasibility of\npruning in LLMs. To address this issue, we propose a pruning pipeline for\nsemi-structured sparse models via retraining, termed Adaptive Sparse Trainer\n(AST). Unlike previous one-shot pruning methods, AST incrementally transforms\ndense models into sparse ones by applying decay to masked weights while\nallowing the model to adaptively select masks throughout the training process.\nFurthermore, we observe that using distillation with a dense model as the\nteacher can prevent the sparse model from falling into local optima and\naccelerate convergence. In addition, we incorporate extra well-initialized\nparameters to further enhance model performance with minimal increase in memory\nfootprint. AST can significantly enhance model performance, approaching the\nlevel of dense models. When applied to the LLaMA2-7B model, AST reduces the\nzero-shot accuracy gap between dense and semi-structured sparse models to 1.12%\nacross multiple zero-shot tasks, utilizing less than 0.4% of the pretraining\ntokens. Our work demonstrates the feasibility of deploying semi-structured\nsparse large language models and introduces a novel method for achieving highly\ncompressed models when combined with existing quantization techniques."
                },
                "authors": [
                    {
                        "name": "Weiyu Huang"
                    },
                    {
                        "name": "Yuezhou Hu"
                    },
                    {
                        "name": "Guohao Jian"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Jianfei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Chen"
                },
                "author": "Jianfei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00161v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00161v3",
                "updated": "2024-08-26T13:17:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    13,
                    17,
                    53,
                    0,
                    239,
                    0
                ],
                "published": "2024-04-30T19:24:56Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    19,
                    24,
                    56,
                    1,
                    121,
                    0
                ],
                "title": "Estimating Heterogeneous Treatment Effects with Item-Level Outcome Data:\n  Insights from Item Response Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating Heterogeneous Treatment Effects with Item-Level Outcome Data:\n  Insights from Item Response Theory"
                },
                "summary": "Analyses of heterogeneous treatment effects (HTE) are common in applied\ncausal inference research. However, when outcomes are latent variables assessed\nvia psychometric instruments such as educational tests, standard methods ignore\nthe potential HTE that may exist among the individual items of the outcome\nmeasure. Failing to account for ``item-level'' HTE (IL-HTE) can lead to both\nestimated standard errors that are too small and identification challenges in\nthe estimation of treatment-by-covariate interaction effects. We demonstrate\nhow Item Response Theory (IRT) models that estimate a treatment effect for each\nassessment item can both address these challenges and provide new insights into\nHTE generally. This study articulates the theoretical rationale for the IL-HTE\nmodel and demonstrates its practical value using 73 data sets from 46\nrandomized controlled trials containing 5.8 million item responses in\neconomics, education, and health research. Our results show that the IL-HTE\nmodel reveals item-level variation masked by single-number scores, provides\nmore meaningful standard errors in many settings, allows for estimates of the\ngeneralizability of causal effects to untested items, resolves identification\nproblems in the estimation of interaction effects, and provides estimates of\nstandardized treatment effect sizes corrected for attenuation due to\nmeasurement error.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyses of heterogeneous treatment effects (HTE) are common in applied\ncausal inference research. However, when outcomes are latent variables assessed\nvia psychometric instruments such as educational tests, standard methods ignore\nthe potential HTE that may exist among the individual items of the outcome\nmeasure. Failing to account for ``item-level'' HTE (IL-HTE) can lead to both\nestimated standard errors that are too small and identification challenges in\nthe estimation of treatment-by-covariate interaction effects. We demonstrate\nhow Item Response Theory (IRT) models that estimate a treatment effect for each\nassessment item can both address these challenges and provide new insights into\nHTE generally. This study articulates the theoretical rationale for the IL-HTE\nmodel and demonstrates its practical value using 73 data sets from 46\nrandomized controlled trials containing 5.8 million item responses in\neconomics, education, and health research. Our results show that the IL-HTE\nmodel reveals item-level variation masked by single-number scores, provides\nmore meaningful standard errors in many settings, allows for estimates of the\ngeneralizability of causal effects to untested items, resolves identification\nproblems in the estimation of interaction effects, and provides estimates of\nstandardized treatment effect sizes corrected for attenuation due to\nmeasurement error."
                },
                "authors": [
                    {
                        "name": "Joshua B. Gilbert"
                    },
                    {
                        "name": "Zachary Himmelsbach"
                    },
                    {
                        "name": "James Soland"
                    },
                    {
                        "name": "Mridul Joshi"
                    },
                    {
                        "name": "Benjamin W. Domingue"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin W. Domingue"
                },
                "author": "Benjamin W. Domingue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00161v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00161v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14244v1",
                "updated": "2024-08-26T12:59:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    12,
                    59,
                    32,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T12:59:32Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    12,
                    59,
                    32,
                    0,
                    239,
                    0
                ],
                "title": "Cascaded Temporal Updating Network for Efficient Video Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cascaded Temporal Updating Network for Efficient Video Super-Resolution"
                },
                "summary": "Existing video super-resolution (VSR) methods generally adopt a recurrent\npropagation network to extract spatio-temporal information from the entire\nvideo sequences, exhibiting impressive performance. However, the key components\nin recurrent-based VSR networks significantly impact model efficiency, e.g.,\nthe alignment module occupies a substantial portion of model parameters, while\nthe bidirectional propagation mechanism significantly amplifies the inference\ntime. Consequently, developing a compact and efficient VSR method that can be\ndeployed on resource-constrained devices, e.g., smartphones, remains\nchallenging. To this end, we propose a cascaded temporal updating network\n(CTUN) for efficient VSR. We first develop an implicit cascaded alignment\nmodule to explore spatio-temporal correspondences from adjacent frames.\nMoreover, we propose a unidirectional propagation updating network to\nefficiently explore long-range temporal information, which is crucial for\nhigh-quality video reconstruction. Specifically, we develop a simple yet\neffective hidden updater that can leverage future information to update hidden\nfeatures during forward propagation, significantly reducing inference time\nwhile maintaining performance. Finally, we formulate all of these components\ninto an end-to-end trainable VSR network. Extensive experimental results show\nthat our CTUN achieves a favorable trade-off between efficiency and performance\ncompared to existing methods. Notably, compared with BasicVSR, our method\nobtains better results while employing only about 30% of the parameters and\nrunning time. The source code and pre-trained models will be available at\nhttps://github.com/House-Leo/CTUN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing video super-resolution (VSR) methods generally adopt a recurrent\npropagation network to extract spatio-temporal information from the entire\nvideo sequences, exhibiting impressive performance. However, the key components\nin recurrent-based VSR networks significantly impact model efficiency, e.g.,\nthe alignment module occupies a substantial portion of model parameters, while\nthe bidirectional propagation mechanism significantly amplifies the inference\ntime. Consequently, developing a compact and efficient VSR method that can be\ndeployed on resource-constrained devices, e.g., smartphones, remains\nchallenging. To this end, we propose a cascaded temporal updating network\n(CTUN) for efficient VSR. We first develop an implicit cascaded alignment\nmodule to explore spatio-temporal correspondences from adjacent frames.\nMoreover, we propose a unidirectional propagation updating network to\nefficiently explore long-range temporal information, which is crucial for\nhigh-quality video reconstruction. Specifically, we develop a simple yet\neffective hidden updater that can leverage future information to update hidden\nfeatures during forward propagation, significantly reducing inference time\nwhile maintaining performance. Finally, we formulate all of these components\ninto an end-to-end trainable VSR network. Extensive experimental results show\nthat our CTUN achieves a favorable trade-off between efficiency and performance\ncompared to existing methods. Notably, compared with BasicVSR, our method\nobtains better results while employing only about 30% of the parameters and\nrunning time. The source code and pre-trained models will be available at\nhttps://github.com/House-Leo/CTUN."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Jiangxin Dong"
                    },
                    {
                        "name": "Jinshan Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jinshan Pan"
                },
                "author": "Jinshan Pan",
                "arxiv_comment": "Project website: https://github.com/House-Leo/CTUN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12848v2",
                "updated": "2024-08-26T12:55:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    12,
                    55,
                    44,
                    0,
                    239,
                    0
                ],
                "published": "2024-03-19T15:54:48Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    15,
                    54,
                    48,
                    1,
                    79,
                    0
                ],
                "title": "Planner3D: LLM-enhanced graph prior meets 3D indoor scene explicit\n  regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planner3D: LLM-enhanced graph prior meets 3D indoor scene explicit\n  regularization"
                },
                "summary": "Compositional 3D scene synthesis has diverse applications across a spectrum\nof industries such as robotics, films, and video games, as it closely mirrors\nthe complexity of real-world multi-object environments. Conventional works\ntypically employ shape retrieval based frameworks which naturally suffer from\nlimited shape diversity. Recent progresses have been made in object shape\ngeneration with generative models such as diffusion models, which increases the\nshape fidelity. However, these approaches separately treat 3D shape generation\nand layout generation. The synthesized scenes are usually hampered by layout\ncollision, which suggests that the scene-level fidelity is still\nunder-explored. In this paper, we aim at generating realistic and reasonable 3D\nindoor scenes from scene graph. To enrich the priors of the given scene graph\ninputs, large language model is utilized to aggregate the global-wise features\nwith local node-wise and edge-wise features. With a unified graph encoder,\ngraph features are extracted to guide joint layout-shape generation. Additional\nregularization is introduced to explicitly constrain the produced 3D layouts.\nBenchmarked on the SG-FRONT dataset, our method achieves better 3D scene\nsynthesis, especially in terms of scene-level fidelity. The source code will be\nreleased after publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional 3D scene synthesis has diverse applications across a spectrum\nof industries such as robotics, films, and video games, as it closely mirrors\nthe complexity of real-world multi-object environments. Conventional works\ntypically employ shape retrieval based frameworks which naturally suffer from\nlimited shape diversity. Recent progresses have been made in object shape\ngeneration with generative models such as diffusion models, which increases the\nshape fidelity. However, these approaches separately treat 3D shape generation\nand layout generation. The synthesized scenes are usually hampered by layout\ncollision, which suggests that the scene-level fidelity is still\nunder-explored. In this paper, we aim at generating realistic and reasonable 3D\nindoor scenes from scene graph. To enrich the priors of the given scene graph\ninputs, large language model is utilized to aggregate the global-wise features\nwith local node-wise and edge-wise features. With a unified graph encoder,\ngraph features are extracted to guide joint layout-shape generation. Additional\nregularization is introduced to explicitly constrain the produced 3D layouts.\nBenchmarked on the SG-FRONT dataset, our method achieves better 3D scene\nsynthesis, especially in terms of scene-level fidelity. The source code will be\nreleased after publication."
                },
                "authors": [
                    {
                        "name": "Yao Wei"
                    },
                    {
                        "name": "Martin Renqiang Min"
                    },
                    {
                        "name": "George Vosselman"
                    },
                    {
                        "name": "Li Erran Li"
                    },
                    {
                        "name": "Michael Ying Yang"
                    }
                ],
                "author_detail": {
                    "name": "Michael Ying Yang"
                },
                "author": "Michael Ying Yang",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14238v1",
                "updated": "2024-08-26T12:52:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    12,
                    52,
                    2,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T12:52:02Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    12,
                    52,
                    2,
                    0,
                    239,
                    0
                ],
                "title": "Are LLM-based Recommenders Already the Best? Simple Scaled Cross-entropy\n  Unleashes the Potential of Traditional Sequential Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLM-based Recommenders Already the Best? Simple Scaled Cross-entropy\n  Unleashes the Potential of Traditional Sequential Recommenders"
                },
                "summary": "Large language models (LLMs) have been garnering increasing attention in the\nrecommendation community. Some studies have observed that LLMs, when fine-tuned\nby the cross-entropy (CE) loss with a full softmax, could achieve\n`state-of-the-art' performance in sequential recommendation. However, most of\nthe baselines used for comparison are trained using a pointwise/pairwise loss\nfunction. This inconsistent experimental setting leads to the underestimation\nof traditional methods and further fosters over-confidence in the ranking\ncapability of LLMs.\n  In this study, we provide theoretical justification for the superiority of\nthe cross-entropy loss by demonstrating its two desirable properties: tightness\nand coverage. Furthermore, this study sheds light on additional novel insights:\n1) Taking into account only the recommendation performance, CE is not yet\noptimal as it is not a quite tight bound in terms of some ranking metrics. 2)\nIn scenarios that full softmax cannot be performed, an effective alternative is\nto scale up the sampled normalizing term. These findings then help unleash the\npotential of traditional recommendation models, allowing them to surpass\nLLM-based counterparts. Given the substantial computational burden, existing\nLLM-based methods are not as effective as claimed for sequential\nrecommendation. We hope that these theoretical understandings in conjunction\nwith the empirical results will facilitate an objective evaluation of LLM-based\nrecommendation in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been garnering increasing attention in the\nrecommendation community. Some studies have observed that LLMs, when fine-tuned\nby the cross-entropy (CE) loss with a full softmax, could achieve\n`state-of-the-art' performance in sequential recommendation. However, most of\nthe baselines used for comparison are trained using a pointwise/pairwise loss\nfunction. This inconsistent experimental setting leads to the underestimation\nof traditional methods and further fosters over-confidence in the ranking\ncapability of LLMs.\n  In this study, we provide theoretical justification for the superiority of\nthe cross-entropy loss by demonstrating its two desirable properties: tightness\nand coverage. Furthermore, this study sheds light on additional novel insights:\n1) Taking into account only the recommendation performance, CE is not yet\noptimal as it is not a quite tight bound in terms of some ranking metrics. 2)\nIn scenarios that full softmax cannot be performed, an effective alternative is\nto scale up the sampled normalizing term. These findings then help unleash the\npotential of traditional recommendation models, allowing them to surpass\nLLM-based counterparts. Given the substantial computational burden, existing\nLLM-based methods are not as effective as claimed for sequential\nrecommendation. We hope that these theoretical understandings in conjunction\nwith the empirical results will facilitate an objective evaluation of LLM-based\nrecommendation in the future."
                },
                "authors": [
                    {
                        "name": "Cong Xu"
                    },
                    {
                        "name": "Zhangchi Zhu"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Jianyong Wang"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "18 pages. arXiv admin note: substantial text overlap with\n  arXiv:2402.06216",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11341v2",
                "updated": "2024-08-26T12:14:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    12,
                    14,
                    31,
                    0,
                    239,
                    0
                ],
                "published": "2024-04-17T13:00:52Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    13,
                    0,
                    52,
                    2,
                    108,
                    0
                ],
                "title": "The Causal Chambers: Real Physical Systems as a Testbed for AI\n  Methodology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Causal Chambers: Real Physical Systems as a Testbed for AI\n  Methodology"
                },
                "summary": "In some fields of AI, machine learning and statistics, the validation of new\nmethods and algorithms is often hindered by the scarcity of suitable real-world\ndatasets. Researchers must often turn to simulated data, which yields limited\ninformation about the applicability of the proposed methods to real problems.\nAs a step forward, we have constructed two devices that allow us to quickly and\ninexpensively produce large datasets from non-trivial but well-understood\nphysical systems. The devices, which we call causal chambers, are\ncomputer-controlled laboratories that allow us to manipulate and measure an\narray of variables from these physical systems, providing a rich testbed for\nalgorithms from a variety of fields. We illustrate potential applications\nthrough a series of case studies in fields such as causal discovery,\nout-of-distribution generalization, change point detection, independent\ncomponent analysis, and symbolic regression. For applications to causal\ninference, the chambers allow us to carefully perform interventions. We also\nprovide and empirically validate a causal model of each chamber, which can be\nused as ground truth for different tasks. All hardware and software is made\nopen source, and the datasets are publicly available at causalchamber.org or\nthrough the Python package causalchamber.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In some fields of AI, machine learning and statistics, the validation of new\nmethods and algorithms is often hindered by the scarcity of suitable real-world\ndatasets. Researchers must often turn to simulated data, which yields limited\ninformation about the applicability of the proposed methods to real problems.\nAs a step forward, we have constructed two devices that allow us to quickly and\ninexpensively produce large datasets from non-trivial but well-understood\nphysical systems. The devices, which we call causal chambers, are\ncomputer-controlled laboratories that allow us to manipulate and measure an\narray of variables from these physical systems, providing a rich testbed for\nalgorithms from a variety of fields. We illustrate potential applications\nthrough a series of case studies in fields such as causal discovery,\nout-of-distribution generalization, change point detection, independent\ncomponent analysis, and symbolic regression. For applications to causal\ninference, the chambers allow us to carefully perform interventions. We also\nprovide and empirically validate a causal model of each chamber, which can be\nused as ground truth for different tasks. All hardware and software is made\nopen source, and the datasets are publicly available at causalchamber.org or\nthrough the Python package causalchamber."
                },
                "authors": [
                    {
                        "name": "Juan L. Gamella"
                    },
                    {
                        "name": "Jonas Peters"
                    },
                    {
                        "name": "Peter Bühlmann"
                    }
                ],
                "author_detail": {
                    "name": "Peter Bühlmann"
                },
                "author": "Peter Bühlmann",
                "arxiv_comment": "40 pages, 20 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13985v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13985v2",
                "updated": "2024-08-26T12:08:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    12,
                    8,
                    12,
                    0,
                    239,
                    0
                ],
                "published": "2024-03-20T21:38:04Z",
                "published_parsed": [
                    2024,
                    3,
                    20,
                    21,
                    38,
                    4,
                    2,
                    80,
                    0
                ],
                "title": "Cosmology with Persistent Homology: a Fisher Forecast",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmology with Persistent Homology: a Fisher Forecast"
                },
                "summary": "Persistent homology naturally addresses the multi-scale topological\ncharacteristics of the large-scale structure as a distribution of clusters,\nloops, and voids. We apply this tool to the dark matter halo catalogs from the\nQuijote simulations, and build a summary statistic for comparison with the\njoint power spectrum and bispectrum statistic regarding their information\ncontent on cosmological parameters and primordial non-Gaussianity. Through a\nFisher analysis, we find that constraints from persistent homology are tighter\nfor 8 out of the 10 parameters by margins of 13-50%. The complementarity of the\ntwo statistics breaks parameter degeneracies, allowing for a further gain in\nconstraining power when combined. We run a series of consistency checks to\nconsolidate our results, and conclude that our findings motivate incorporating\npersistent homology into inference pipelines for cosmological survey data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent homology naturally addresses the multi-scale topological\ncharacteristics of the large-scale structure as a distribution of clusters,\nloops, and voids. We apply this tool to the dark matter halo catalogs from the\nQuijote simulations, and build a summary statistic for comparison with the\njoint power spectrum and bispectrum statistic regarding their information\ncontent on cosmological parameters and primordial non-Gaussianity. Through a\nFisher analysis, we find that constraints from persistent homology are tighter\nfor 8 out of the 10 parameters by margins of 13-50%. The complementarity of the\ntwo statistics breaks parameter degeneracies, allowing for a further gain in\nconstraining power when combined. We run a series of consistency checks to\nconsolidate our results, and conclude that our findings motivate incorporating\npersistent homology into inference pipelines for cosmological survey data."
                },
                "authors": [
                    {
                        "name": "Jacky H. T. Yip"
                    },
                    {
                        "name": "Matteo Biagetti"
                    },
                    {
                        "name": "Alex Cole"
                    },
                    {
                        "name": "Karthik Viswanathan"
                    },
                    {
                        "name": "Gary Shiu"
                    }
                ],
                "author_detail": {
                    "name": "Gary Shiu"
                },
                "author": "Gary Shiu",
                "arxiv_comment": "24+18 pages, 22 figures, 4 tables. Accepted for publication in JCAP.\n  Replaced with the accepted version (minor changes)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13985v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13985v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14202v1",
                "updated": "2024-08-26T12:00:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    12,
                    0,
                    17,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T12:00:17Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    12,
                    0,
                    17,
                    0,
                    239,
                    0
                ],
                "title": "Bounding the number of reticulation events for displaying multiple trees\n  in a phylogenetic network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bounding the number of reticulation events for displaying multiple trees\n  in a phylogenetic network"
                },
                "summary": "Reconstructing a parsimonious phylogenetic network that displays multiple\nphylogenetic trees is an important problem in theory of phylogenetics, where\nthe complexity of the inferred networks is measured by reticulation numbers.\nThe reticulation number for a set of trees is defined as the minimum number of\nreticulations in a phylogenetic network that displays those trees. A\nmathematical problem is bounding the reticulation number for multiple trees\nover a fixed number of taxa. While this problem has been extensively studied\nfor two trees, much less is known about the upper bounds on the reticulation\nnumbers for three or more arbitrary trees. In this paper, we present a few\nnon-trivial upper bounds on reticulation numbers for three or more trees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing a parsimonious phylogenetic network that displays multiple\nphylogenetic trees is an important problem in theory of phylogenetics, where\nthe complexity of the inferred networks is measured by reticulation numbers.\nThe reticulation number for a set of trees is defined as the minimum number of\nreticulations in a phylogenetic network that displays those trees. A\nmathematical problem is bounding the reticulation number for multiple trees\nover a fixed number of taxa. While this problem has been extensively studied\nfor two trees, much less is known about the upper bounds on the reticulation\nnumbers for three or more arbitrary trees. In this paper, we present a few\nnon-trivial upper bounds on reticulation numbers for three or more trees."
                },
                "authors": [
                    {
                        "name": "Yufeng Wu"
                    },
                    {
                        "name": "Louxin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Louxin Zhang"
                },
                "author": "Louxin Zhang",
                "arxiv_comment": "9 figures, 18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "5C30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.02595v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.02595v3",
                "updated": "2024-08-26T11:35:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    11,
                    35,
                    52,
                    0,
                    239,
                    0
                ],
                "published": "2023-04-02T02:19:15Z",
                "published_parsed": [
                    2023,
                    4,
                    2,
                    2,
                    19,
                    15,
                    6,
                    92,
                    0
                ],
                "title": "Bayesian neural networks via MCMC: a Python-based tutorial",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian neural networks via MCMC: a Python-based tutorial"
                },
                "summary": "Bayesian inference provides a methodology for parameter estimation and\nuncertainty quantification in machine learning and deep learning methods.\nVariational inference and Markov Chain Monte-Carlo (MCMC) sampling methods are\nused to implement Bayesian inference. In the past three decades, MCMC sampling\nmethods have faced some challenges in being adapted to larger models (such as\nin deep learning) and big data problems. Advanced proposal distributions that\nincorporate gradients, such as a Langevin proposal distribution, provide a\nmeans to address some of the limitations of MCMC sampling for Bayesian neural\nnetworks. Furthermore, MCMC methods have typically been constrained to\nstatisticians and currently not well-known among deep learning researchers. We\npresent a tutorial for MCMC methods that covers simple Bayesian linear and\nlogistic models, and Bayesian neural networks. The aim of this tutorial is to\nbridge the gap between theory and implementation via coding, given a general\nsparsity of libraries and tutorials to this end. This tutorial provides code in\nPython with data and instructions that enable their use and extension. We\nprovide results for some benchmark problems showing the strengths and\nweaknesses of implementing the respective Bayesian models via MCMC. We\nhighlight the challenges in sampling multi-modal posterior distributions for\nthe case of Bayesian neural networks and the need for further improvement of\nconvergence diagnosis methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference provides a methodology for parameter estimation and\nuncertainty quantification in machine learning and deep learning methods.\nVariational inference and Markov Chain Monte-Carlo (MCMC) sampling methods are\nused to implement Bayesian inference. In the past three decades, MCMC sampling\nmethods have faced some challenges in being adapted to larger models (such as\nin deep learning) and big data problems. Advanced proposal distributions that\nincorporate gradients, such as a Langevin proposal distribution, provide a\nmeans to address some of the limitations of MCMC sampling for Bayesian neural\nnetworks. Furthermore, MCMC methods have typically been constrained to\nstatisticians and currently not well-known among deep learning researchers. We\npresent a tutorial for MCMC methods that covers simple Bayesian linear and\nlogistic models, and Bayesian neural networks. The aim of this tutorial is to\nbridge the gap between theory and implementation via coding, given a general\nsparsity of libraries and tutorials to this end. This tutorial provides code in\nPython with data and instructions that enable their use and extension. We\nprovide results for some benchmark problems showing the strengths and\nweaknesses of implementing the respective Bayesian models via MCMC. We\nhighlight the challenges in sampling multi-modal posterior distributions for\nthe case of Bayesian neural networks and the need for further improvement of\nconvergence diagnosis methods."
                },
                "authors": [
                    {
                        "name": "Rohitash Chandra"
                    },
                    {
                        "name": "Joshua Simmons"
                    }
                ],
                "author_detail": {
                    "name": "Joshua Simmons"
                },
                "author": "Joshua Simmons",
                "arxiv_doi": "10.1109/ACCESS.2024.3401234",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2024.3401234",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2304.02595v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.02595v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "IEEE Access (2024)",
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14185v1",
                "updated": "2024-08-26T11:19:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    11,
                    19,
                    58,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T11:19:58Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    11,
                    19,
                    58,
                    0,
                    239,
                    0
                ],
                "title": "DynamicRouteGPT: A Real-Time Multi-Vehicle Dynamic Navigation Framework\n  Based on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicRouteGPT: A Real-Time Multi-Vehicle Dynamic Navigation Framework\n  Based on Large Language Models"
                },
                "summary": "Real-time dynamic path planning in complex traffic environments presents\nchallenges, such as varying traffic volumes and signal wait times. Traditional\nstatic routing algorithms like Dijkstra and A* compute shortest paths but often\nfail under dynamic conditions. Recent Reinforcement Learning (RL) approaches\noffer improvements but tend to focus on local optima, risking dead-ends or\nboundary issues. This paper proposes a novel approach based on causal inference\nfor real-time dynamic path planning, balancing global and local optimality. We\nfirst use the static Dijkstra algorithm to compute a globally optimal baseline\npath. A distributed control strategy then guides vehicles along this path. At\nintersections, DynamicRouteGPT performs real-time decision-making for local\npath selection, considering real-time traffic, driving preferences, and\nunexpected events. DynamicRouteGPT integrates Markov chains, Bayesian\ninference, and large-scale pretrained language models like Llama3 8B to provide\nan efficient path planning solution. It dynamically adjusts to traffic\nscenarios and driver preferences and requires no pre-training, offering broad\napplicability across road networks. A key innovation is the construction of\ncausal graphs for counterfactual reasoning, optimizing path decisions.\nExperimental results show that our method achieves state-of-the-art performance\nin real-time dynamic path planning for multiple vehicles while providing\nexplainable path selections, offering a novel and efficient solution for\ncomplex traffic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time dynamic path planning in complex traffic environments presents\nchallenges, such as varying traffic volumes and signal wait times. Traditional\nstatic routing algorithms like Dijkstra and A* compute shortest paths but often\nfail under dynamic conditions. Recent Reinforcement Learning (RL) approaches\noffer improvements but tend to focus on local optima, risking dead-ends or\nboundary issues. This paper proposes a novel approach based on causal inference\nfor real-time dynamic path planning, balancing global and local optimality. We\nfirst use the static Dijkstra algorithm to compute a globally optimal baseline\npath. A distributed control strategy then guides vehicles along this path. At\nintersections, DynamicRouteGPT performs real-time decision-making for local\npath selection, considering real-time traffic, driving preferences, and\nunexpected events. DynamicRouteGPT integrates Markov chains, Bayesian\ninference, and large-scale pretrained language models like Llama3 8B to provide\nan efficient path planning solution. It dynamically adjusts to traffic\nscenarios and driver preferences and requires no pre-training, offering broad\napplicability across road networks. A key innovation is the construction of\ncausal graphs for counterfactual reasoning, optimizing path decisions.\nExperimental results show that our method achieves state-of-the-art performance\nin real-time dynamic path planning for multiple vehicles while providing\nexplainable path selections, offering a novel and efficient solution for\ncomplex traffic environments."
                },
                "authors": [
                    {
                        "name": "Ziai Zhou"
                    },
                    {
                        "name": "Bin Zhou"
                    },
                    {
                        "name": "Hao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Liu"
                },
                "author": "Hao Liu",
                "arxiv_comment": "This paper is 12 pages long and represents the initial draft, version\n  1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05141v2",
                "updated": "2024-08-26T10:53:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    10,
                    53,
                    28,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-09T15:53:55Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    15,
                    53,
                    55,
                    4,
                    222,
                    0
                ],
                "title": "A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning"
                },
                "summary": "Retrieval-augmented generation (RAG) is a framework enabling large language\nmodels (LLMs) to enhance their accuracy and reduce hallucinations by\nintegrating external knowledge bases. In this paper, we introduce a hybrid RAG\nsystem enhanced through a comprehensive suite of optimizations that\nsignificantly improve retrieval quality, augment reasoning capabilities, and\nrefine numerical computation ability. We refined the text chunks and tables in\nweb pages, added attribute predictors to reduce hallucinations, conducted LLM\nKnowledge Extractor and Knowledge Graph Extractor, and finally built a\nreasoning strategy with all the references. We evaluated our system on the CRAG\ndataset through the Meta CRAG KDD Cup 2024 Competition. Both the local and\nonline evaluations demonstrate that our system significantly enhances complex\nreasoning capabilities. In local evaluations, we have significantly improved\naccuracy and reduced error rates compared to the baseline model, achieving a\nnotable increase in scores. In the meanwhile, we have attained outstanding\nresults in online assessments, demonstrating the performance and generalization\ncapabilities of the proposed system. The source code for our system is released\nin \\url{https://gitlab.aicrowd.com/shizueyy/crag-new}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) is a framework enabling large language\nmodels (LLMs) to enhance their accuracy and reduce hallucinations by\nintegrating external knowledge bases. In this paper, we introduce a hybrid RAG\nsystem enhanced through a comprehensive suite of optimizations that\nsignificantly improve retrieval quality, augment reasoning capabilities, and\nrefine numerical computation ability. We refined the text chunks and tables in\nweb pages, added attribute predictors to reduce hallucinations, conducted LLM\nKnowledge Extractor and Knowledge Graph Extractor, and finally built a\nreasoning strategy with all the references. We evaluated our system on the CRAG\ndataset through the Meta CRAG KDD Cup 2024 Competition. Both the local and\nonline evaluations demonstrate that our system significantly enhances complex\nreasoning capabilities. In local evaluations, we have significantly improved\naccuracy and reduced error rates compared to the baseline model, achieving a\nnotable increase in scores. In the meanwhile, we have attained outstanding\nresults in online assessments, demonstrating the performance and generalization\ncapabilities of the proposed system. The source code for our system is released\nin \\url{https://gitlab.aicrowd.com/shizueyy/crag-new}."
                },
                "authors": [
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Chengwu Liu"
                    },
                    {
                        "name": "Jingyang Yuan"
                    },
                    {
                        "name": "Gongbo Sun"
                    },
                    {
                        "name": "Siqi Li"
                    },
                    {
                        "name": "Ming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zhang"
                },
                "author": "Ming Zhang",
                "arxiv_comment": "Technical report for 3rd prize in Task 1 of Meta CRAG KDD Cup 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14177v1",
                "updated": "2024-08-26T10:50:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    10,
                    50,
                    14,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T10:50:14Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    10,
                    50,
                    14,
                    0,
                    239,
                    0
                ],
                "title": "NimbleD: Enhancing Self-supervised Monocular Depth Estimation with\n  Pseudo-labels and Large-scale Video Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NimbleD: Enhancing Self-supervised Monocular Depth Estimation with\n  Pseudo-labels and Large-scale Video Pre-training"
                },
                "summary": "We introduce NimbleD, an efficient self-supervised monocular depth estimation\nlearning framework that incorporates supervision from pseudo-labels generated\nby a large vision model. This framework does not require camera intrinsics,\nenabling large-scale pre-training on publicly available videos. Our\nstraightforward yet effective learning strategy significantly enhances the\nperformance of fast and lightweight models without introducing any overhead,\nallowing them to achieve performance comparable to state-of-the-art\nself-supervised monocular depth estimation models. This advancement is\nparticularly beneficial for virtual and augmented reality applications\nrequiring low latency inference. The source code, model weights, and\nacknowledgments are available at https://github.com/xapaxca/nimbled .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce NimbleD, an efficient self-supervised monocular depth estimation\nlearning framework that incorporates supervision from pseudo-labels generated\nby a large vision model. This framework does not require camera intrinsics,\nenabling large-scale pre-training on publicly available videos. Our\nstraightforward yet effective learning strategy significantly enhances the\nperformance of fast and lightweight models without introducing any overhead,\nallowing them to achieve performance comparable to state-of-the-art\nself-supervised monocular depth estimation models. This advancement is\nparticularly beneficial for virtual and augmented reality applications\nrequiring low latency inference. The source code, model weights, and\nacknowledgments are available at https://github.com/xapaxca/nimbled ."
                },
                "authors": [
                    {
                        "name": "Albert Luginov"
                    },
                    {
                        "name": "Muhammad Shahzad"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Shahzad"
                },
                "author": "Muhammad Shahzad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14158v1",
                "updated": "2024-08-26T10:11:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    10,
                    11,
                    56,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T10:11:56Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    10,
                    11,
                    56,
                    0,
                    239,
                    0
                ],
                "title": "Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep\n  Learning"
                },
                "summary": "The rapid progress in Deep Learning (DL) and Large Language Models (LLMs) has\nexponentially increased demands of computational power and bandwidth. This,\ncombined with the high costs of faster computing chips and interconnects, has\nsignificantly inflated High Performance Computing (HPC) construction costs. To\naddress these challenges, we introduce the Fire-Flyer AI-HPC architecture, a\nsynergistic hardware-software co-design framework and its best practices. For\nDL training, we deployed the Fire-Flyer 2 with 10,000 PCIe A100 GPUs, achieved\nperformance approximating the DGX-A100 while reducing costs by half and energy\nconsumption by 40%. We specifically engineered HFReduce to accelerate allreduce\ncommunication and implemented numerous measures to keep our Computation-Storage\nIntegrated Network congestion-free. Through our software stack, including\nHaiScale, 3FS, and HAI-Platform, we achieved substantial scalability by\noverlapping computation and communication. Our system-oriented experience from\nDL training provides valuable insights to drive future advancements in AI-HPC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress in Deep Learning (DL) and Large Language Models (LLMs) has\nexponentially increased demands of computational power and bandwidth. This,\ncombined with the high costs of faster computing chips and interconnects, has\nsignificantly inflated High Performance Computing (HPC) construction costs. To\naddress these challenges, we introduce the Fire-Flyer AI-HPC architecture, a\nsynergistic hardware-software co-design framework and its best practices. For\nDL training, we deployed the Fire-Flyer 2 with 10,000 PCIe A100 GPUs, achieved\nperformance approximating the DGX-A100 while reducing costs by half and energy\nconsumption by 40%. We specifically engineered HFReduce to accelerate allreduce\ncommunication and implemented numerous measures to keep our Computation-Storage\nIntegrated Network congestion-free. Through our software stack, including\nHaiScale, 3FS, and HAI-Platform, we achieved substantial scalability by\noverlapping computation and communication. Our system-oriented experience from\nDL training provides valuable insights to drive future advancements in AI-HPC."
                },
                "authors": [
                    {
                        "name": "Wei An"
                    },
                    {
                        "name": "Xiao Bi"
                    },
                    {
                        "name": "Guanting Chen"
                    },
                    {
                        "name": "Shanhuang Chen"
                    },
                    {
                        "name": "Chengqi Deng"
                    },
                    {
                        "name": "Honghui Ding"
                    },
                    {
                        "name": "Kai Dong"
                    },
                    {
                        "name": "Qiushi Du"
                    },
                    {
                        "name": "Wenjun Gao"
                    },
                    {
                        "name": "Kang Guan"
                    },
                    {
                        "name": "Jianzhong Guo"
                    },
                    {
                        "name": "Yongqiang Guo"
                    },
                    {
                        "name": "Zhe Fu"
                    },
                    {
                        "name": "Ying He"
                    },
                    {
                        "name": "Panpan Huang"
                    },
                    {
                        "name": "Jiashi Li"
                    },
                    {
                        "name": "Wenfeng Liang"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Yiyuan Liu"
                    },
                    {
                        "name": "Yuxuan Liu"
                    },
                    {
                        "name": "Shanghao Lu"
                    },
                    {
                        "name": "Xuan Lu"
                    },
                    {
                        "name": "Xiaotao Nie"
                    },
                    {
                        "name": "Tian Pei"
                    },
                    {
                        "name": "Junjie Qiu"
                    },
                    {
                        "name": "Hui Qu"
                    },
                    {
                        "name": "Zehui Ren"
                    },
                    {
                        "name": "Zhangli Sha"
                    },
                    {
                        "name": "Xuecheng Su"
                    },
                    {
                        "name": "Xiaowen Sun"
                    },
                    {
                        "name": "Yixuan Tan"
                    },
                    {
                        "name": "Minghui Tang"
                    },
                    {
                        "name": "Shiyu Wang"
                    },
                    {
                        "name": "Yaohui Wang"
                    },
                    {
                        "name": "Yongji Wang"
                    },
                    {
                        "name": "Ziwei Xie"
                    },
                    {
                        "name": "Yiliang Xiong"
                    },
                    {
                        "name": "Yanhong Xu"
                    },
                    {
                        "name": "Shengfeng Ye"
                    },
                    {
                        "name": "Shuiping Yu"
                    },
                    {
                        "name": "Yukun Zha"
                    },
                    {
                        "name": "Liyue Zhang"
                    },
                    {
                        "name": "Haowei Zhang"
                    },
                    {
                        "name": "Mingchuan Zhang"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Yichao Zhang"
                    },
                    {
                        "name": "Chenggang Zhao"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Shangyan Zhou"
                    },
                    {
                        "name": "Shunfeng Zhou"
                    },
                    {
                        "name": "Yuheng Zou"
                    }
                ],
                "author_detail": {
                    "name": "Yuheng Zou"
                },
                "author": "Yuheng Zou",
                "arxiv_comment": "This is the preprint version of the paper accepted for presentation\n  at the 2024 International Conference for High Performance Computing,\n  Networking, Storage, and Analysis (SC'24). \\c{opyright} 2024 IEEE. Personal\n  use of this material is permitted. For other uses, permission from IEEE must\n  be obtained. Please refer to IEEE Xplore for the final published version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06977v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06977v2",
                "updated": "2024-08-26T10:02:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    10,
                    2,
                    14,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-13T15:33:27Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    33,
                    27,
                    1,
                    226,
                    0
                ],
                "title": "Endogeneity Corrections in Binary Outcome Models with Nonlinear\n  Transformations: Identification and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Endogeneity Corrections in Binary Outcome Models with Nonlinear\n  Transformations: Identification and Inference"
                },
                "summary": "For binary outcome models, an endogeneity correction based on nonlinear\nrank-based transformations is proposed. Identification without external\ninstruments is achieved under one of two assumptions: either the endogenous\nregressor is a nonlinear function of one component of the error term,\nconditional on the exogenous regressors, or the dependence between the\nendogenous and exogenous regressors is nonlinear. Under these conditions, we\nprove consistency and asymptotic normality. Monte Carlo simulations and an\napplication on German insolvency data illustrate the usefulness of the method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For binary outcome models, an endogeneity correction based on nonlinear\nrank-based transformations is proposed. Identification without external\ninstruments is achieved under one of two assumptions: either the endogenous\nregressor is a nonlinear function of one component of the error term,\nconditional on the exogenous regressors, or the dependence between the\nendogenous and exogenous regressors is nonlinear. Under these conditions, we\nprove consistency and asymptotic normality. Monte Carlo simulations and an\napplication on German insolvency data illustrate the usefulness of the method."
                },
                "authors": [
                    {
                        "name": "Alexander Mayer"
                    },
                    {
                        "name": "Dominik Wied"
                    }
                ],
                "author_detail": {
                    "name": "Dominik Wied"
                },
                "author": "Dominik Wied",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06977v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06977v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13431v2",
                "updated": "2024-08-26T09:58:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    9,
                    58,
                    4,
                    0,
                    239,
                    0
                ],
                "published": "2024-07-18T12:00:32Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    12,
                    0,
                    32,
                    3,
                    200,
                    0
                ],
                "title": "Improving Out-of-Distribution Generalization of Trajectory Prediction\n  for Autonomous Driving via Polynomial Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Out-of-Distribution Generalization of Trajectory Prediction\n  for Autonomous Driving via Polynomial Representations"
                },
                "summary": "Robustness against Out-of-Distribution (OoD) samples is a key performance\nindicator of a trajectory prediction model. However, the development and\nranking of state-of-the-art (SotA) models are driven by their In-Distribution\n(ID) performance on individual competition datasets. We present an OoD testing\nprotocol that homogenizes datasets and prediction tasks across two large-scale\nmotion datasets. We introduce a novel prediction algorithm based on polynomial\nrepresentations for agent trajectory and road geometry on both the input and\noutput sides of the model. With a much smaller model size, training effort, and\ninference time, we reach near SotA performance for ID testing and significantly\nimprove robustness in OoD testing. Within our OoD testing protocol, we further\nstudy two augmentation strategies of SotA models and their effects on model\ngeneralization. Highlighting the contrast between ID and OoD performance, we\nsuggest adding OoD testing to the evaluation criteria of trajectory prediction\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustness against Out-of-Distribution (OoD) samples is a key performance\nindicator of a trajectory prediction model. However, the development and\nranking of state-of-the-art (SotA) models are driven by their In-Distribution\n(ID) performance on individual competition datasets. We present an OoD testing\nprotocol that homogenizes datasets and prediction tasks across two large-scale\nmotion datasets. We introduce a novel prediction algorithm based on polynomial\nrepresentations for agent trajectory and road geometry on both the input and\noutput sides of the model. With a much smaller model size, training effort, and\ninference time, we reach near SotA performance for ID testing and significantly\nimprove robustness in OoD testing. Within our OoD testing protocol, we further\nstudy two augmentation strategies of SotA models and their effects on model\ngeneralization. Highlighting the contrast between ID and OoD performance, we\nsuggest adding OoD testing to the evaluation criteria of trajectory prediction\nmodels."
                },
                "authors": [
                    {
                        "name": "Yue Yao"
                    },
                    {
                        "name": "Shengchao Yan"
                    },
                    {
                        "name": "Daniel Goehring"
                    },
                    {
                        "name": "Wolfram Burgard"
                    },
                    {
                        "name": "Joerg Reichardt"
                    }
                ],
                "author_detail": {
                    "name": "Joerg Reichardt"
                },
                "author": "Joerg Reichardt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04660v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04660v3",
                "updated": "2024-08-26T09:37:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    9,
                    37,
                    46,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-05T20:01:10Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    20,
                    1,
                    10,
                    0,
                    218,
                    0
                ],
                "title": "XMainframe: A Large Language Model for Mainframe Modernization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XMainframe: A Large Language Model for Mainframe Modernization"
                },
                "summary": "Mainframe operating systems, despite their inception in the 1940s, continue\nto support critical sectors like finance and government. However, these systems\nare often viewed as outdated, requiring extensive maintenance and\nmodernization. Addressing this challenge necessitates innovative tools that can\nunderstand and interact with legacy codebases. To this end, we introduce\nXMainframe, a state-of-the-art large language model (LLM) specifically designed\nwith knowledge of mainframe legacy systems and COBOL codebases. Our solution\ninvolves the creation of an extensive data collection pipeline to produce\nhigh-quality training datasets, enhancing XMainframe's performance in this\nspecialized domain. Additionally, we present MainframeBench, a comprehensive\nbenchmark for assessing mainframe knowledge, including multiple-choice\nquestions, question answering, and COBOL code summarization. Our empirical\nevaluations demonstrate that XMainframe consistently outperforms existing\nstate-of-the-art LLMs across these tasks. Specifically, XMainframe achieves 30%\nhigher accuracy than DeepSeek-Coder on multiple-choice questions, doubles the\nBLEU score of Mixtral-Instruct 8x7B on question answering, and scores six times\nhigher than GPT-3.5 on COBOL summarization. Our work highlights the potential\nof XMainframe to drive significant advancements in managing and modernizing\nlegacy systems, thereby enhancing productivity and saving time for software\ndevelopers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainframe operating systems, despite their inception in the 1940s, continue\nto support critical sectors like finance and government. However, these systems\nare often viewed as outdated, requiring extensive maintenance and\nmodernization. Addressing this challenge necessitates innovative tools that can\nunderstand and interact with legacy codebases. To this end, we introduce\nXMainframe, a state-of-the-art large language model (LLM) specifically designed\nwith knowledge of mainframe legacy systems and COBOL codebases. Our solution\ninvolves the creation of an extensive data collection pipeline to produce\nhigh-quality training datasets, enhancing XMainframe's performance in this\nspecialized domain. Additionally, we present MainframeBench, a comprehensive\nbenchmark for assessing mainframe knowledge, including multiple-choice\nquestions, question answering, and COBOL code summarization. Our empirical\nevaluations demonstrate that XMainframe consistently outperforms existing\nstate-of-the-art LLMs across these tasks. Specifically, XMainframe achieves 30%\nhigher accuracy than DeepSeek-Coder on multiple-choice questions, doubles the\nBLEU score of Mixtral-Instruct 8x7B on question answering, and scores six times\nhigher than GPT-3.5 on COBOL summarization. Our work highlights the potential\nof XMainframe to drive significant advancements in managing and modernizing\nlegacy systems, thereby enhancing productivity and saving time for software\ndevelopers."
                },
                "authors": [
                    {
                        "name": "Anh T. V. Dau"
                    },
                    {
                        "name": "Hieu Trung Dao"
                    },
                    {
                        "name": "Anh Tuan Nguyen"
                    },
                    {
                        "name": "Hieu Trung Tran"
                    },
                    {
                        "name": "Phong X. Nguyen"
                    },
                    {
                        "name": "Nghi D. Q. Bui"
                    }
                ],
                "author_detail": {
                    "name": "Nghi D. Q. Bui"
                },
                "author": "Nghi D. Q. Bui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04660v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04660v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14141v1",
                "updated": "2024-08-26T09:37:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    9,
                    37,
                    42,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T09:37:42Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    9,
                    37,
                    42,
                    0,
                    239,
                    0
                ],
                "title": "Crowd-Calibrator: Can Annotator Disagreement Inform Calibration in\n  Subjective Tasks?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crowd-Calibrator: Can Annotator Disagreement Inform Calibration in\n  Subjective Tasks?"
                },
                "summary": "Subjective tasks in NLP have been mostly relegated to objective standards,\nwhere the gold label is decided by taking the majority vote. This obfuscates\nannotator disagreement and the inherent uncertainty of the label. We argue that\nsubjectivity should factor into model decisions and play a direct role via\ncalibration under a selective prediction setting. Specifically, instead of\ncalibrating confidence purely from the model's perspective, we calibrate models\nfor subjective tasks based on crowd worker agreement. Our method,\nCrowd-Calibrator, models the distance between the distribution of crowd worker\nlabels and the model's own distribution over labels to inform whether the model\nshould abstain from a decision. On two highly subjective tasks, hate speech\ndetection and natural language inference, our experiments show Crowd-Calibrator\neither outperforms or achieves competitive performance with existing selective\nprediction baselines. Our findings highlight the value of bringing human\ndecision-making into model predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Subjective tasks in NLP have been mostly relegated to objective standards,\nwhere the gold label is decided by taking the majority vote. This obfuscates\nannotator disagreement and the inherent uncertainty of the label. We argue that\nsubjectivity should factor into model decisions and play a direct role via\ncalibration under a selective prediction setting. Specifically, instead of\ncalibrating confidence purely from the model's perspective, we calibrate models\nfor subjective tasks based on crowd worker agreement. Our method,\nCrowd-Calibrator, models the distance between the distribution of crowd worker\nlabels and the model's own distribution over labels to inform whether the model\nshould abstain from a decision. On two highly subjective tasks, hate speech\ndetection and natural language inference, our experiments show Crowd-Calibrator\neither outperforms or achieves competitive performance with existing selective\nprediction baselines. Our findings highlight the value of bringing human\ndecision-making into model predictions."
                },
                "authors": [
                    {
                        "name": "Urja Khurana"
                    },
                    {
                        "name": "Eric Nalisnick"
                    },
                    {
                        "name": "Antske Fokkens"
                    },
                    {
                        "name": "Swabha Swayamdipta"
                    }
                ],
                "author_detail": {
                    "name": "Swabha Swayamdipta"
                },
                "author": "Swabha Swayamdipta",
                "arxiv_comment": "Accepted at COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14134v1",
                "updated": "2024-08-26T09:29:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    9,
                    29,
                    56,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T09:29:56Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    9,
                    29,
                    56,
                    0,
                    239,
                    0
                ],
                "title": "Exploring the Potential of Large Language Models for Heterophilic Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Potential of Large Language Models for Heterophilic Graphs"
                },
                "summary": "Graph Neural Networks (GNNs) are essential for various graph-based learning\ntasks. Notably, classical GNN architectures operate under the assumption of\nhomophily, which posits that connected nodes are likely to share similar\nfeatures. However, this assumption limits the effectiveness of GNNs in handling\nheterophilic graphs where connected nodes often exhibit dissimilar\ncharacteristics. Existing approaches for homophily graphs such as non-local\nneighbor extension and architectural refinement overlook the rich textual data\nassociated with nodes, which could unlock deeper insights into these\nheterophilic contexts. With advancements in Large Language Models (LLMs), there\nis significant promise to enhance GNNs by leveraging the extensive open-world\nknowledge within LLMs to more effectively interpret and utilize textual data\nfor characterizing heterophilic graphs. In this work, we explore the potential\nof LLMs for modeling heterophilic graphs and propose a novel two-stage\nframework: LLM-enhanced edge discriminator and LLM-guided edge reweighting.\nSpecifically, in the first stage, we fine-tune the LLM to better identify\nhomophilic and heterophilic edges based on the textual information of their\nnodes. In the second stage, we adaptively manage message propagation in GNNs\nfor different edge types based on node features, structures, and heterophilic\nor homophilic characteristics. To cope with the computational demands when\ndeploying LLMs in practical scenarios, we further explore model distillation\ntechniques to fine-tune smaller, more efficient models that maintain\ncompetitive performance. Extensive experiments validate the effectiveness of\nour framework, demonstrating the feasibility of using LLMs to enhance GNNs for\nnode classification on heterophilic graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are essential for various graph-based learning\ntasks. Notably, classical GNN architectures operate under the assumption of\nhomophily, which posits that connected nodes are likely to share similar\nfeatures. However, this assumption limits the effectiveness of GNNs in handling\nheterophilic graphs where connected nodes often exhibit dissimilar\ncharacteristics. Existing approaches for homophily graphs such as non-local\nneighbor extension and architectural refinement overlook the rich textual data\nassociated with nodes, which could unlock deeper insights into these\nheterophilic contexts. With advancements in Large Language Models (LLMs), there\nis significant promise to enhance GNNs by leveraging the extensive open-world\nknowledge within LLMs to more effectively interpret and utilize textual data\nfor characterizing heterophilic graphs. In this work, we explore the potential\nof LLMs for modeling heterophilic graphs and propose a novel two-stage\nframework: LLM-enhanced edge discriminator and LLM-guided edge reweighting.\nSpecifically, in the first stage, we fine-tune the LLM to better identify\nhomophilic and heterophilic edges based on the textual information of their\nnodes. In the second stage, we adaptively manage message propagation in GNNs\nfor different edge types based on node features, structures, and heterophilic\nor homophilic characteristics. To cope with the computational demands when\ndeploying LLMs in practical scenarios, we further explore model distillation\ntechniques to fine-tune smaller, more efficient models that maintain\ncompetitive performance. Extensive experiments validate the effectiveness of\nour framework, demonstrating the feasibility of using LLMs to enhance GNNs for\nnode classification on heterophilic graphs."
                },
                "authors": [
                    {
                        "name": "Yuxia Wu"
                    },
                    {
                        "name": "Shujie Li"
                    },
                    {
                        "name": "Yuan Fang"
                    },
                    {
                        "name": "Chuan Shi"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Shi"
                },
                "author": "Chuan Shi",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.11534v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.11534v6",
                "updated": "2024-08-26T08:52:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    8,
                    52,
                    44,
                    0,
                    239,
                    0
                ],
                "published": "2023-08-21T06:51:56Z",
                "published_parsed": [
                    2023,
                    8,
                    21,
                    6,
                    51,
                    56,
                    0,
                    233,
                    0
                ],
                "title": "PlatoLM: Teaching LLMs in Multi-Round Dialogue via a User Simulator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PlatoLM: Teaching LLMs in Multi-Round Dialogue via a User Simulator"
                },
                "summary": "The unparalleled performance of closed-sourced ChatGPT has sparked efforts\ntowards its democratization, with notable strides made by leveraging real user\nand ChatGPT dialogues, as evidenced by Vicuna. However, due to challenges in\ngathering dialogues involving human participation, current endeavors like Baize\nand UltraChat rely on ChatGPT conducting roleplay to simulate humans based on\ninstructions, resulting in overdependence on seeds, diminished human-likeness,\nlimited topic diversity, and an absence of genuine multi-round conversational\ndynamics. To address the above issues, we propose a paradigm to simulate human\nbehavior better and explore the benefits of incorporating more human-like\nquestions in multi-turn conversations. Specifically, we directly target human\nquestions extracted from genuine human-machine conversations as a learning goal\nand provide a novel user simulator called `Socratic'. The experimental results\nshow our response model, `PlatoLM', achieves SoTA performance among LLaMA-based\n7B models in MT-Bench. Our findings further demonstrate that our method\nintroduces highly human-like questioning patterns and rich topic structures,\nwhich can teach the response model better than previous works in multi-round\nconversations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The unparalleled performance of closed-sourced ChatGPT has sparked efforts\ntowards its democratization, with notable strides made by leveraging real user\nand ChatGPT dialogues, as evidenced by Vicuna. However, due to challenges in\ngathering dialogues involving human participation, current endeavors like Baize\nand UltraChat rely on ChatGPT conducting roleplay to simulate humans based on\ninstructions, resulting in overdependence on seeds, diminished human-likeness,\nlimited topic diversity, and an absence of genuine multi-round conversational\ndynamics. To address the above issues, we propose a paradigm to simulate human\nbehavior better and explore the benefits of incorporating more human-like\nquestions in multi-turn conversations. Specifically, we directly target human\nquestions extracted from genuine human-machine conversations as a learning goal\nand provide a novel user simulator called `Socratic'. The experimental results\nshow our response model, `PlatoLM', achieves SoTA performance among LLaMA-based\n7B models in MT-Bench. Our findings further demonstrate that our method\nintroduces highly human-like questioning patterns and rich topic structures,\nwhich can teach the response model better than previous works in multi-round\nconversations."
                },
                "authors": [
                    {
                        "name": "Chuyi Kong"
                    },
                    {
                        "name": "Yaxin Fan"
                    },
                    {
                        "name": "Xiang Wan"
                    },
                    {
                        "name": "Feng Jiang"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "arxiv_comment": "23 pages",
                "arxiv_journal_ref": "ACL 2024 Main Conference, 7841-7863",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.11534v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.11534v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10833v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10833v2",
                "updated": "2024-08-26T08:47:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    8,
                    47,
                    54,
                    0,
                    239,
                    0
                ],
                "published": "2024-06-16T08:03:24Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    8,
                    3,
                    24,
                    6,
                    168,
                    0
                ],
                "title": "A Comprehensive Survey of Scientific Large Language Models and Their\n  Applications in Scientific Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey of Scientific Large Language Models and Their\n  Applications in Scientific Discovery"
                },
                "summary": "In many scientific fields, large language models (LLMs) have revolutionized\nthe way text and other modalities of data (e.g., molecules and proteins) are\nhandled, achieving superior performance in various applications and augmenting\nthe scientific discovery process. Nevertheless, previous surveys on scientific\nLLMs often concentrate on one or two fields or a single modality. In this\npaper, we aim to provide a more holistic view of the research landscape by\nunveiling cross-field and cross-modal connections between scientific LLMs\nregarding their architectures and pre-training techniques. To this end, we\ncomprehensively survey over 250 scientific LLMs, discuss their commonalities\nand differences, as well as summarize pre-training datasets and evaluation\ntasks for each field and modality. Moreover, we investigate how LLMs have been\ndeployed to benefit scientific discovery. Resources related to this survey are\navailable at https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many scientific fields, large language models (LLMs) have revolutionized\nthe way text and other modalities of data (e.g., molecules and proteins) are\nhandled, achieving superior performance in various applications and augmenting\nthe scientific discovery process. Nevertheless, previous surveys on scientific\nLLMs often concentrate on one or two fields or a single modality. In this\npaper, we aim to provide a more holistic view of the research landscape by\nunveiling cross-field and cross-modal connections between scientific LLMs\nregarding their architectures and pre-training techniques. To this end, we\ncomprehensively survey over 250 scientific LLMs, discuss their commonalities\nand differences, as well as summarize pre-training datasets and evaluation\ntasks for each field and modality. Moreover, we investigate how LLMs have been\ndeployed to benefit scientific discovery. Resources related to this survey are\navailable at https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Xiusi Chen"
                    },
                    {
                        "name": "Bowen Jin"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Shuiwang Ji"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Jiawei Han"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Han"
                },
                "author": "Jiawei Han",
                "arxiv_comment": "34 pages (GitHub:\n  https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10833v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10833v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11322v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11322v4",
                "updated": "2024-08-26T08:25:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    8,
                    25,
                    1,
                    0,
                    239,
                    0
                ],
                "published": "2024-03-17T19:54:16Z",
                "published_parsed": [
                    2024,
                    3,
                    17,
                    19,
                    54,
                    16,
                    6,
                    77,
                    0
                ],
                "title": "StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows"
                },
                "summary": "It is a notable trend to use Large Language Models (LLMs) to tackle complex\ntasks, e.g., tasks that require a sequence of actions and dynamic interaction\nwith tools and external environments. In this paper, we propose StateFlow, a\nnovel LLM-based task-solving paradigm that conceptualizes complex task-solving\nprocesses as state machines. In StateFlow, we distinguish between \"process\ngrounding\" (via state and state transitions) and \"sub-task solving\" (through\nactions within a state), enhancing control and interpretability of the\ntask-solving procedure. A state represents the status of a running process. The\ntransitions between states are controlled by heuristic rules or decisions made\nby the LLM, allowing for a dynamic and adaptive progression. Upon entering a\nstate, a series of actions is executed, involving not only calling LLMs guided\nby different prompts, but also the utilization of external tools as needed. Our\nresults show that StateFlow significantly enhances LLMs' efficiency. For\ninstance, StateFlow achieves 13% and 28% higher success rates compared to ReAct\nin InterCode SQL and ALFWorld benchmark, with 5x and 3x less cost respectively.\nWe also show that StateFlow can be combined with iterative refining methods\nlike Reflexion to further improve performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is a notable trend to use Large Language Models (LLMs) to tackle complex\ntasks, e.g., tasks that require a sequence of actions and dynamic interaction\nwith tools and external environments. In this paper, we propose StateFlow, a\nnovel LLM-based task-solving paradigm that conceptualizes complex task-solving\nprocesses as state machines. In StateFlow, we distinguish between \"process\ngrounding\" (via state and state transitions) and \"sub-task solving\" (through\nactions within a state), enhancing control and interpretability of the\ntask-solving procedure. A state represents the status of a running process. The\ntransitions between states are controlled by heuristic rules or decisions made\nby the LLM, allowing for a dynamic and adaptive progression. Upon entering a\nstate, a series of actions is executed, involving not only calling LLMs guided\nby different prompts, but also the utilization of external tools as needed. Our\nresults show that StateFlow significantly enhances LLMs' efficiency. For\ninstance, StateFlow achieves 13% and 28% higher success rates compared to ReAct\nin InterCode SQL and ALFWorld benchmark, with 5x and 3x less cost respectively.\nWe also show that StateFlow can be combined with iterative refining methods\nlike Reflexion to further improve performance."
                },
                "authors": [
                    {
                        "name": "Yiran Wu"
                    },
                    {
                        "name": "Tianwei Yue"
                    },
                    {
                        "name": "Shaokun Zhang"
                    },
                    {
                        "name": "Chi Wang"
                    },
                    {
                        "name": "Qingyun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Qingyun Wu"
                },
                "author": "Qingyun Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.11322v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11322v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.08334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.08334v2",
                "updated": "2024-08-26T08:24:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    8,
                    24,
                    14,
                    0,
                    239,
                    0
                ],
                "published": "2024-05-14T06:09:08Z",
                "published_parsed": [
                    2024,
                    5,
                    14,
                    6,
                    9,
                    8,
                    1,
                    135,
                    0
                ],
                "title": "Could Chemical LLMs benefit from Message Passing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Could Chemical LLMs benefit from Message Passing"
                },
                "summary": "Pretrained language models (LMs) showcase significant capabilities in\nprocessing molecular text, while concurrently, message passing neural networks\n(MPNNs) demonstrate resilience and versatility in the domain of molecular\nscience. Despite these advancements, we find there are limited studies\ninvestigating the bidirectional interactions between molecular structures and\ntheir corresponding textual representations. Therefore, in this paper, we\npropose two strategies to evaluate whether an information integration can\nenhance the performance: contrast learning, which involves utilizing an MPNN to\nsupervise the training of the LM, and fusion, which exploits information from\nboth models. Our empirical analysis reveals that the integration approaches\nexhibit superior performance compared to baselines when applied to smaller\nmolecular graphs, while these integration approaches do not yield performance\nenhancements on large scale graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained language models (LMs) showcase significant capabilities in\nprocessing molecular text, while concurrently, message passing neural networks\n(MPNNs) demonstrate resilience and versatility in the domain of molecular\nscience. Despite these advancements, we find there are limited studies\ninvestigating the bidirectional interactions between molecular structures and\ntheir corresponding textual representations. Therefore, in this paper, we\npropose two strategies to evaluate whether an information integration can\nenhance the performance: contrast learning, which involves utilizing an MPNN to\nsupervise the training of the LM, and fusion, which exploits information from\nboth models. Our empirical analysis reveals that the integration approaches\nexhibit superior performance compared to baselines when applied to smaller\nmolecular graphs, while these integration approaches do not yield performance\nenhancements on large scale graphs."
                },
                "authors": [
                    {
                        "name": "Jiaqing Xie"
                    },
                    {
                        "name": "Ziheng Chi"
                    }
                ],
                "author_detail": {
                    "name": "Ziheng Chi"
                },
                "author": "Ziheng Chi",
                "arxiv_comment": "Accepted at ACL @ Languages and Molecules 2024. In Proceedings of ACL\n  2024",
                "arxiv_journal_ref": "In Proceedings of the 1st Workshop on Language + Molecules (L+M\n  2024), pages 10 20, Bangkok, Thailand. Association for Computational\n  Linguistics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.08334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.08334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03624v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03624v2",
                "updated": "2024-08-26T08:09:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    8,
                    9,
                    39,
                    0,
                    239,
                    0
                ],
                "published": "2024-07-04T04:19:50Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    4,
                    19,
                    50,
                    3,
                    186,
                    0
                ],
                "title": "Question-Analysis Prompting Improves LLM Performance in Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question-Analysis Prompting Improves LLM Performance in Reasoning Tasks"
                },
                "summary": "Although LLMs have the potential to transform many fields, they still\nunderperform humans in reasoning tasks. Existing methods induce the model to\nproduce step-by-step calculations, but this research explores the question:\nDoes making the LLM analyze the question improve its performance? We propose a\nnovel prompting strategy called Question Analysis Prompting (QAP), in which the\nmodel is prompted to explain the question in $n$ words before solving. The\nvalue of $n$ influences the length of response generated by the model. QAP is\nevaluated on GPT 3.5 Turbo and GPT 4 Turbo on arithmetic datasets GSM8K, AQuA,\nand SAT and commonsense dataset StrategyQA. QAP is compared with other\nstate-of-the-art prompts including Chain-of-Thought (CoT), Plan and Solve\nPrompting (PS+) and Take A Deep Breath (TADB). QAP outperforms all\nstate-of-the-art prompts on AQuA and SAT datasets on both GPT3.5 and GPT4. QAP\nconsistently ranks among the top-2 prompts on 75\\% of the tests. A key factor\nof QAP performance can be attributed to response length, where detailed\nresponses are beneficial when answering harder questions, but can negatively\naffect easy questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although LLMs have the potential to transform many fields, they still\nunderperform humans in reasoning tasks. Existing methods induce the model to\nproduce step-by-step calculations, but this research explores the question:\nDoes making the LLM analyze the question improve its performance? We propose a\nnovel prompting strategy called Question Analysis Prompting (QAP), in which the\nmodel is prompted to explain the question in $n$ words before solving. The\nvalue of $n$ influences the length of response generated by the model. QAP is\nevaluated on GPT 3.5 Turbo and GPT 4 Turbo on arithmetic datasets GSM8K, AQuA,\nand SAT and commonsense dataset StrategyQA. QAP is compared with other\nstate-of-the-art prompts including Chain-of-Thought (CoT), Plan and Solve\nPrompting (PS+) and Take A Deep Breath (TADB). QAP outperforms all\nstate-of-the-art prompts on AQuA and SAT datasets on both GPT3.5 and GPT4. QAP\nconsistently ranks among the top-2 prompts on 75\\% of the tests. A key factor\nof QAP performance can be attributed to response length, where detailed\nresponses are beneficial when answering harder questions, but can negatively\naffect easy questions."
                },
                "authors": [
                    {
                        "name": "Dharunish Yugeswardeenoo"
                    },
                    {
                        "name": "Kevin Zhu"
                    },
                    {
                        "name": "Sean O'Brien"
                    }
                ],
                "author_detail": {
                    "name": "Sean O'Brien"
                },
                "author": "Sean O'Brien",
                "arxiv_comment": "Accepted in Proceedings of the 62nd Annual Meeting of the Association\n  for Computational Linguistics: Student Research Workshop (ACL-SRW 2024) 11\n  pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03624v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03624v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14077v2",
                "updated": "2024-08-27T03:36:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    3,
                    36,
                    54,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-26T08:00:47Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    8,
                    0,
                    47,
                    0,
                    239,
                    0
                ],
                "title": "The effect of vorticity on the dynamical magnetic fields in heavy-ion\n  collisions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effect of vorticity on the dynamical magnetic fields in heavy-ion\n  collisions"
                },
                "summary": "Magnetic fields in heavy-ion collisions are pivotal and subject to diverse\nfactors. In this study, we quantitatively investigate the impact of fluid\nvorticity on the evolution of magnetic fields in the 20-50\\% centrality class\nin Au+Au collisions, with collision energies of $\\sqrt{s_{NN}}=(7.7, 14.5,\n19.6, 27, 39, 62.4, 200)$ GeV. Our results indicate that fluid vorticity leads\nto a delay in the evolution of the magnetic field, in which this effect becomes\nmore pronounced as the collision energy decreases. Additionally, we have\ncalculated the mean magnetic field values on the freeze-out hypersurface for\nvarious collision energies. Our simulation results align with the values\ninferred from experimental data of $\\bar{\\Lambda}-\\Lambda$, within the error\nmargins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetic fields in heavy-ion collisions are pivotal and subject to diverse\nfactors. In this study, we quantitatively investigate the impact of fluid\nvorticity on the evolution of magnetic fields in the 20-50\\% centrality class\nin Au+Au collisions, with collision energies of $\\sqrt{s_{NN}}=(7.7, 14.5,\n19.6, 27, 39, 62.4, 200)$ GeV. Our results indicate that fluid vorticity leads\nto a delay in the evolution of the magnetic field, in which this effect becomes\nmore pronounced as the collision energy decreases. Additionally, we have\ncalculated the mean magnetic field values on the freeze-out hypersurface for\nvarious collision energies. Our simulation results align with the values\ninferred from experimental data of $\\bar{\\Lambda}-\\Lambda$, within the error\nmargins."
                },
                "authors": [
                    {
                        "name": "Anping Huang"
                    },
                    {
                        "name": "Xiang-Yu Wu"
                    },
                    {
                        "name": "Mei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Mei Huang"
                },
                "author": "Mei Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09893v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09893v2",
                "updated": "2024-08-26T07:54:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    7,
                    54,
                    27,
                    0,
                    239,
                    0
                ],
                "published": "2024-07-13T13:58:24Z",
                "published_parsed": [
                    2024,
                    7,
                    13,
                    13,
                    58,
                    24,
                    5,
                    195,
                    0
                ],
                "title": "Synergistic Multi-Agent Framework with Trajectory Learning for\n  Knowledge-Intensive Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergistic Multi-Agent Framework with Trajectory Learning for\n  Knowledge-Intensive Tasks"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have led to significant\nbreakthroughs in various natural language processing tasks. However, generating\nfactually consistent responses in knowledge-intensive scenarios remains a\nchallenge due to issues such as hallucination, difficulty in acquiring\nlong-tailed knowledge, and limited memory expansion. This paper introduces\nSMART, a novel multi-agent framework that leverages external knowledge to\nenhance the interpretability and factual consistency of LLM-generated\nresponses. SMART comprises four specialized agents, each performing a specific\nsub-trajectory action to navigate complex knowledge-intensive tasks. We propose\na multi-agent co-training paradigm, Long-Short Trajectory Learning, which\nensures synergistic collaboration among agents while maintaining fine-grained\nexecution by each agent. Extensive experiments on five knowledge-intensive\ntasks demonstrate SMART's superior performance compared to widely adopted\nknowledge internalization and knowledge enhancement methods. Our framework can\nextend beyond knowledge-intensive tasks to more complex scenarios. Our code is\navailable at https://github.com/yueshengbin/SMART.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have led to significant\nbreakthroughs in various natural language processing tasks. However, generating\nfactually consistent responses in knowledge-intensive scenarios remains a\nchallenge due to issues such as hallucination, difficulty in acquiring\nlong-tailed knowledge, and limited memory expansion. This paper introduces\nSMART, a novel multi-agent framework that leverages external knowledge to\nenhance the interpretability and factual consistency of LLM-generated\nresponses. SMART comprises four specialized agents, each performing a specific\nsub-trajectory action to navigate complex knowledge-intensive tasks. We propose\na multi-agent co-training paradigm, Long-Short Trajectory Learning, which\nensures synergistic collaboration among agents while maintaining fine-grained\nexecution by each agent. Extensive experiments on five knowledge-intensive\ntasks demonstrate SMART's superior performance compared to widely adopted\nknowledge internalization and knowledge enhancement methods. Our framework can\nextend beyond knowledge-intensive tasks to more complex scenarios. Our code is\navailable at https://github.com/yueshengbin/SMART."
                },
                "authors": [
                    {
                        "name": "Shengbin Yue"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09893v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09893v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10903v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10903v4",
                "updated": "2024-08-27T02:58:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    58,
                    39,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-20T14:47:38Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    47,
                    38,
                    1,
                    233,
                    0
                ],
                "title": "BEYOND DIALOGUE: A Profile-Dialogue Alignment Framework Towards General\n  Role-Playing Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEYOND DIALOGUE: A Profile-Dialogue Alignment Framework Towards General\n  Role-Playing Language Model"
                },
                "summary": "The rapid advancement of large language models (LLMs) has revolutionized\nrole-playing, enabling the development of general role-playing models. However,\ncurrent role-playing training has two significant issues: (I) Using a\npredefined role profile to prompt dialogue training for specific scenarios\nusually leads to inconsistencies and even conflicts between the dialogue and\nthe profile, resulting in training biases. (II) The model learns to imitate the\nrole based solely on the profile, neglecting profile-dialogue alignment at the\nsentence level. In this work, we propose a simple yet effective framework\ncalled BEYOND DIALOGUE, designed to overcome these hurdles. This framework\ninnovatively introduces \"beyond dialogue\" tasks to align dialogue with profile\ntraits based on each specific scenario, thereby eliminating biases during\ntraining. Furthermore, by adopting an innovative prompting mechanism that\ngenerates reasoning outcomes for training, the framework allows the model to\nachieve fine-grained alignment between profile and dialogue at the sentence\nlevel. The aforementioned methods are fully automated and low-cost.\nAdditionally, the integration of automated dialogue and objective evaluation\nmethods forms a comprehensive framework, paving the way for general\nrole-playing. Experimental results demonstrate that our model excels in\nadhering to and reflecting various dimensions of role profiles, outperforming\nmost proprietary general and specialized role-playing baselines. All code and\ndatasets are available at https://github.com/yuyouyu32/BeyondDialogue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has revolutionized\nrole-playing, enabling the development of general role-playing models. However,\ncurrent role-playing training has two significant issues: (I) Using a\npredefined role profile to prompt dialogue training for specific scenarios\nusually leads to inconsistencies and even conflicts between the dialogue and\nthe profile, resulting in training biases. (II) The model learns to imitate the\nrole based solely on the profile, neglecting profile-dialogue alignment at the\nsentence level. In this work, we propose a simple yet effective framework\ncalled BEYOND DIALOGUE, designed to overcome these hurdles. This framework\ninnovatively introduces \"beyond dialogue\" tasks to align dialogue with profile\ntraits based on each specific scenario, thereby eliminating biases during\ntraining. Furthermore, by adopting an innovative prompting mechanism that\ngenerates reasoning outcomes for training, the framework allows the model to\nachieve fine-grained alignment between profile and dialogue at the sentence\nlevel. The aforementioned methods are fully automated and low-cost.\nAdditionally, the integration of automated dialogue and objective evaluation\nmethods forms a comprehensive framework, paving the way for general\nrole-playing. Experimental results demonstrate that our model excels in\nadhering to and reflecting various dimensions of role profiles, outperforming\nmost proprietary general and specialized role-playing baselines. All code and\ndatasets are available at https://github.com/yuyouyu32/BeyondDialogue."
                },
                "authors": [
                    {
                        "name": "Yeyong Yu"
                    },
                    {
                        "name": "Runsheng Yu"
                    },
                    {
                        "name": "Haojie Wei"
                    },
                    {
                        "name": "Zhanqiu Zhang"
                    },
                    {
                        "name": "Quan Qian"
                    }
                ],
                "author_detail": {
                    "name": "Quan Qian"
                },
                "author": "Quan Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10903v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10903v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14055v1",
                "updated": "2024-08-26T07:27:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    7,
                    27,
                    12,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T07:27:12Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    7,
                    27,
                    12,
                    0,
                    239,
                    0
                ],
                "title": "HAPM -- Hardware Aware Pruning Method for CNN hardware accelerators in\n  resource constrained devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAPM -- Hardware Aware Pruning Method for CNN hardware accelerators in\n  resource constrained devices"
                },
                "summary": "During the last years, algorithms known as Convolutional Neural Networks\n(CNNs) had become increasingly popular, expanding its application range to\nseveral areas. In particular, the image processing field has experienced a\nremarkable advance thanks to this algorithms. In IoT, a wide research field\naims to develop hardware capable of execute them at the lowest possible energy\ncost, but keeping acceptable image inference time. One can get around this\napparently conflicting objectives by applying design and training techniques.\nThe present work proposes a generic hardware architecture ready to be\nimplemented on FPGA devices, supporting a wide range of configurations which\nallows the system to run different neural network architectures, dynamically\nexploiting the sparsity caused by pruning techniques in the mathematical\noperations present in this kind of algorithms. The inference speed of the\ndesign is evaluated over different resource constrained FPGA devices. Finally,\nthe standard pruning algorithm is compared against a custom pruning technique\nspecifically designed to exploit the scheduling properties of this hardware\naccelerator. We demonstrate that our hardware-aware pruning algorithm achieves\na remarkable improvement of a 45 % in inference time compared to a network\npruned using the standard algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "During the last years, algorithms known as Convolutional Neural Networks\n(CNNs) had become increasingly popular, expanding its application range to\nseveral areas. In particular, the image processing field has experienced a\nremarkable advance thanks to this algorithms. In IoT, a wide research field\naims to develop hardware capable of execute them at the lowest possible energy\ncost, but keeping acceptable image inference time. One can get around this\napparently conflicting objectives by applying design and training techniques.\nThe present work proposes a generic hardware architecture ready to be\nimplemented on FPGA devices, supporting a wide range of configurations which\nallows the system to run different neural network architectures, dynamically\nexploiting the sparsity caused by pruning techniques in the mathematical\noperations present in this kind of algorithms. The inference speed of the\ndesign is evaluated over different resource constrained FPGA devices. Finally,\nthe standard pruning algorithm is compared against a custom pruning technique\nspecifically designed to exploit the scheduling properties of this hardware\naccelerator. We demonstrate that our hardware-aware pruning algorithm achieves\na remarkable improvement of a 45 % in inference time compared to a network\npruned using the standard algorithm."
                },
                "authors": [
                    {
                        "name": "Federico Nicolas Peccia"
                    },
                    {
                        "name": "Luciano Ferreyro"
                    },
                    {
                        "name": "Alejandro Furfaro"
                    }
                ],
                "author_detail": {
                    "name": "Alejandro Furfaro"
                },
                "author": "Alejandro Furfaro",
                "arxiv_comment": "8 pages, 7 figure, thesis for the title of Electronic Engineer\n  attained in 2021 at the Universidad Tecnologica Nacional (UTN), Argentina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14051v1",
                "updated": "2024-08-26T07:17:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    7,
                    17,
                    5,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T07:17:05Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    7,
                    17,
                    5,
                    0,
                    239,
                    0
                ],
                "title": "Let Video Teaches You More: Video-to-Image Knowledge Distillation using\n  DEtection TRansformer for Medical Video Lesion Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let Video Teaches You More: Video-to-Image Knowledge Distillation using\n  DEtection TRansformer for Medical Video Lesion Detection"
                },
                "summary": "AI-assisted lesion detection models play a crucial role in the early\nscreening of cancer. However, previous image-based models ignore the\ninter-frame contextual information present in videos. On the other hand,\nvideo-based models capture the inter-frame context but are computationally\nexpensive. To mitigate this contradiction, we delve into Video-to-Image\nknowledge distillation leveraging DEtection TRansformer (V2I-DETR) for the task\nof medical video lesion detection. V2I-DETR adopts a teacher-student network\nparadigm. The teacher network aims at extracting temporal contexts from\nmultiple frames and transferring them to the student network, and the student\nnetwork is an image-based model dedicated to fast prediction in inference. By\ndistilling multi-frame contexts into a single frame, the proposed V2I-DETR\ncombines the advantages of utilizing temporal contexts from video-based models\nand the inference speed of image-based models. Through extensive experiments,\nV2I-DETR outperforms previous state-of-the-art methods by a large margin while\nachieving the real-time inference speed (30 FPS) as the image-based model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-assisted lesion detection models play a crucial role in the early\nscreening of cancer. However, previous image-based models ignore the\ninter-frame contextual information present in videos. On the other hand,\nvideo-based models capture the inter-frame context but are computationally\nexpensive. To mitigate this contradiction, we delve into Video-to-Image\nknowledge distillation leveraging DEtection TRansformer (V2I-DETR) for the task\nof medical video lesion detection. V2I-DETR adopts a teacher-student network\nparadigm. The teacher network aims at extracting temporal contexts from\nmultiple frames and transferring them to the student network, and the student\nnetwork is an image-based model dedicated to fast prediction in inference. By\ndistilling multi-frame contexts into a single frame, the proposed V2I-DETR\ncombines the advantages of utilizing temporal contexts from video-based models\nand the inference speed of image-based models. Through extensive experiments,\nV2I-DETR outperforms previous state-of-the-art methods by a large margin while\nachieving the real-time inference speed (30 FPS) as the image-based model."
                },
                "authors": [
                    {
                        "name": "Yuncheng Jiang"
                    },
                    {
                        "name": "Zixun Zhang"
                    },
                    {
                        "name": "Jun Wei"
                    },
                    {
                        "name": "Chun-Mei Feng"
                    },
                    {
                        "name": "Guanbin Li"
                    },
                    {
                        "name": "Xiang Wan"
                    },
                    {
                        "name": "Shuguang Cui"
                    },
                    {
                        "name": "Zhen Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Li"
                },
                "author": "Zhen Li",
                "arxiv_comment": "BIBM2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03791v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03791v2",
                "updated": "2024-08-26T07:13:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    7,
                    13,
                    47,
                    0,
                    239,
                    0
                ],
                "published": "2024-07-04T09:55:04Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    9,
                    55,
                    4,
                    3,
                    186,
                    0
                ],
                "title": "M5 -- A Diverse Benchmark to Assess the Performance of Large Multimodal\n  Models Across Multilingual and Multicultural Vision-Language Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M5 -- A Diverse Benchmark to Assess the Performance of Large Multimodal\n  Models Across Multilingual and Multicultural Vision-Language Tasks"
                },
                "summary": "Since the release of ChatGPT, the field of Natural Language Processing has\nexperienced rapid advancements, particularly in Large Language Models (LLMs)\nand their multimodal counterparts, Large Multimodal Models (LMMs). Despite\ntheir impressive capabilities, LLMs often exhibit significant performance\ndisparities across different languages and cultural contexts, as demonstrated\nby various text-only benchmarks. However, current research lacks such\nbenchmarks for multimodal visio-linguistic settings. This work fills this gap\nby introducing M5, the first comprehensive benchmark designed to evaluate LMMs\non diverse vision-language tasks within a multilingual and multicultural\ncontext. M5 includes eight datasets covering five tasks and $41$ languages,\nwith a focus on underrepresented languages and culturally diverse images.\nFurthermore, we introduce two novel datasets, M5-VGR and M5-VLOD, including a\nnew Visio-Linguistic Outlier Detection task, in which all evaluated open-source\nmodels fail to significantly surpass the random baseline. Through extensive\nevaluation and analyses, we highlight substantial task-agnostic performance\ndisparities between high- and low-resource languages. Moreover, we show that\nlarger models do not necessarily outperform smaller ones in a multilingual\nsetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the release of ChatGPT, the field of Natural Language Processing has\nexperienced rapid advancements, particularly in Large Language Models (LLMs)\nand their multimodal counterparts, Large Multimodal Models (LMMs). Despite\ntheir impressive capabilities, LLMs often exhibit significant performance\ndisparities across different languages and cultural contexts, as demonstrated\nby various text-only benchmarks. However, current research lacks such\nbenchmarks for multimodal visio-linguistic settings. This work fills this gap\nby introducing M5, the first comprehensive benchmark designed to evaluate LMMs\non diverse vision-language tasks within a multilingual and multicultural\ncontext. M5 includes eight datasets covering five tasks and $41$ languages,\nwith a focus on underrepresented languages and culturally diverse images.\nFurthermore, we introduce two novel datasets, M5-VGR and M5-VLOD, including a\nnew Visio-Linguistic Outlier Detection task, in which all evaluated open-source\nmodels fail to significantly surpass the random baseline. Through extensive\nevaluation and analyses, we highlight substantial task-agnostic performance\ndisparities between high- and low-resource languages. Moreover, we show that\nlarger models do not necessarily outperform smaller ones in a multilingual\nsetting."
                },
                "authors": [
                    {
                        "name": "Florian Schneider"
                    },
                    {
                        "name": "Sunayana Sitaram"
                    }
                ],
                "author_detail": {
                    "name": "Sunayana Sitaram"
                },
                "author": "Sunayana Sitaram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03791v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03791v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.12236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.12236v2",
                "updated": "2024-08-26T07:09:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    7,
                    9,
                    52,
                    0,
                    239,
                    0
                ],
                "published": "2023-05-20T17:01:52Z",
                "published_parsed": [
                    2023,
                    5,
                    20,
                    17,
                    1,
                    52,
                    5,
                    140,
                    0
                ],
                "title": "Searching a Compact Architecture for Robust Multi-Exposure Image Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Searching a Compact Architecture for Robust Multi-Exposure Image Fusion"
                },
                "summary": "In recent years, learning-based methods have achieved significant\nadvancements in multi-exposure image fusion. However, two major stumbling\nblocks hinder the development, including pixel misalignment and inefficient\ninference. Reliance on aligned image pairs in existing methods causes\nsusceptibility to artifacts due to device motion. Additionally, existing\ntechniques often rely on handcrafted architectures with huge network\nengineering, resulting in redundant parameters, adversely impacting inference\nefficiency and flexibility. To mitigate these limitations, this study\nintroduces an architecture search-based paradigm incorporating self-alignment\nand detail repletion modules for robust multi-exposure image fusion.\n  Specifically, targeting the extreme discrepancy of exposure, we propose the\nself-alignment module, leveraging scene relighting to constrain the\nillumination degree for following alignment and feature extraction. Detail\nrepletion is proposed to enhance the texture details of scenes. Additionally,\nincorporating a hardware-sensitive constraint, we present the fusion-oriented\narchitecture search to explore compact and efficient networks for fusion. The\nproposed method outperforms various competitive schemes, achieving a noteworthy\n3.19\\% improvement in PSNR for general scenarios and an impressive 23.5\\%\nenhancement in misaligned scenarios. Moreover, it significantly reduces\ninference time by 69.1\\%. The code will be available at\nhttps://github.com/LiuZhu-CV/CRMEF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, learning-based methods have achieved significant\nadvancements in multi-exposure image fusion. However, two major stumbling\nblocks hinder the development, including pixel misalignment and inefficient\ninference. Reliance on aligned image pairs in existing methods causes\nsusceptibility to artifacts due to device motion. Additionally, existing\ntechniques often rely on handcrafted architectures with huge network\nengineering, resulting in redundant parameters, adversely impacting inference\nefficiency and flexibility. To mitigate these limitations, this study\nintroduces an architecture search-based paradigm incorporating self-alignment\nand detail repletion modules for robust multi-exposure image fusion.\n  Specifically, targeting the extreme discrepancy of exposure, we propose the\nself-alignment module, leveraging scene relighting to constrain the\nillumination degree for following alignment and feature extraction. Detail\nrepletion is proposed to enhance the texture details of scenes. Additionally,\nincorporating a hardware-sensitive constraint, we present the fusion-oriented\narchitecture search to explore compact and efficient networks for fusion. The\nproposed method outperforms various competitive schemes, achieving a noteworthy\n3.19\\% improvement in PSNR for general scenarios and an impressive 23.5\\%\nenhancement in misaligned scenarios. Moreover, it significantly reduces\ninference time by 69.1\\%. The code will be available at\nhttps://github.com/LiuZhu-CV/CRMEF."
                },
                "authors": [
                    {
                        "name": "Zhu Liu"
                    },
                    {
                        "name": "Jinyuan Liu"
                    },
                    {
                        "name": "Guanyao Wu"
                    },
                    {
                        "name": "Zihang Chen"
                    },
                    {
                        "name": "Xin Fan"
                    },
                    {
                        "name": "Risheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Risheng Liu"
                },
                "author": "Risheng Liu",
                "arxiv_comment": "14 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.12236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.12236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14045v1",
                "updated": "2024-08-26T06:57:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    6,
                    57,
                    22,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T06:57:22Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    6,
                    57,
                    22,
                    0,
                    239,
                    0
                ],
                "title": "Beyond Detection: Leveraging Large Language Models for Cyber Attack\n  Prediction in IoT Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Detection: Leveraging Large Language Models for Cyber Attack\n  Prediction in IoT Networks"
                },
                "summary": "In recent years, numerous large-scale cyberattacks have exploited Internet of\nThings (IoT) devices, a phenomenon that is expected to escalate with the\ncontinuing proliferation of IoT technology. Despite considerable efforts in\nattack detection, intrusion detection systems remain mostly reactive,\nresponding to specific patterns or observed anomalies. This work proposes a\nproactive approach to anticipate and mitigate malicious activities before they\ncause damage. This paper proposes a novel network intrusion prediction\nframework that combines Large Language Models (LLMs) with Long Short Term\nMemory (LSTM) networks. The framework incorporates two LLMs in a feedback loop:\na fine-tuned Generative Pre-trained Transformer (GPT) model for predicting\nnetwork traffic and a fine-tuned Bidirectional Encoder Representations from\nTransformers (BERT) for evaluating the predicted traffic. The LSTM classifier\nmodel then identifies malicious packets among these predictions. Our framework,\nevaluated on the CICIoT2023 IoT attack dataset, demonstrates a significant\nimprovement in predictive capabilities, achieving an overall accuracy of 98%,\noffering a robust solution to IoT cybersecurity challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, numerous large-scale cyberattacks have exploited Internet of\nThings (IoT) devices, a phenomenon that is expected to escalate with the\ncontinuing proliferation of IoT technology. Despite considerable efforts in\nattack detection, intrusion detection systems remain mostly reactive,\nresponding to specific patterns or observed anomalies. This work proposes a\nproactive approach to anticipate and mitigate malicious activities before they\ncause damage. This paper proposes a novel network intrusion prediction\nframework that combines Large Language Models (LLMs) with Long Short Term\nMemory (LSTM) networks. The framework incorporates two LLMs in a feedback loop:\na fine-tuned Generative Pre-trained Transformer (GPT) model for predicting\nnetwork traffic and a fine-tuned Bidirectional Encoder Representations from\nTransformers (BERT) for evaluating the predicted traffic. The LSTM classifier\nmodel then identifies malicious packets among these predictions. Our framework,\nevaluated on the CICIoT2023 IoT attack dataset, demonstrates a significant\nimprovement in predictive capabilities, achieving an overall accuracy of 98%,\noffering a robust solution to IoT cybersecurity challenges."
                },
                "authors": [
                    {
                        "name": "Alaeddine Diaf"
                    },
                    {
                        "name": "Abdelaziz Amara Korba"
                    },
                    {
                        "name": "Nour Elislem Karabadji"
                    },
                    {
                        "name": "Yacine Ghamri-Doudane"
                    }
                ],
                "author_detail": {
                    "name": "Yacine Ghamri-Doudane"
                },
                "author": "Yacine Ghamri-Doudane",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03633v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03633v3",
                "updated": "2024-08-26T06:19:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    6,
                    19,
                    53,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-07T08:44:44Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    8,
                    44,
                    44,
                    2,
                    220,
                    0
                ],
                "title": "CARE: A Clue-guided Assistant for CSRs to Read User Manuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARE: A Clue-guided Assistant for CSRs to Read User Manuals"
                },
                "summary": "It is time-saving to build a reading assistant for customer service\nrepresentations (CSRs) when reading user manuals, especially information-rich\nones. Current solutions don't fit the online custom service scenarios well due\nto the lack of attention to user questions and possible responses. Hence, we\npropose to develop a time-saving and careful reading assistant for CSRs, named\nCARE. It can help the CSRs quickly find proper responses from the user manuals\nvia explicit clue chains. Specifically, each of the clue chains is formed by\ninferring over the user manuals, starting from the question clue aligned with\nthe user question and ending at a possible response. To overcome the shortage\nof supervised data, we adopt the self-supervised strategy for model learning.\nThe offline experiment shows that CARE is efficient in automatically inferring\naccurate responses from the user manual. The online experiment further\ndemonstrates the superiority of CARE to reduce CSRs' reading burden and keep\nhigh service quality, in particular with >35% decrease in time spent and\nkeeping a >0.75 ICC score.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is time-saving to build a reading assistant for customer service\nrepresentations (CSRs) when reading user manuals, especially information-rich\nones. Current solutions don't fit the online custom service scenarios well due\nto the lack of attention to user questions and possible responses. Hence, we\npropose to develop a time-saving and careful reading assistant for CSRs, named\nCARE. It can help the CSRs quickly find proper responses from the user manuals\nvia explicit clue chains. Specifically, each of the clue chains is formed by\ninferring over the user manuals, starting from the question clue aligned with\nthe user question and ending at a possible response. To overcome the shortage\nof supervised data, we adopt the self-supervised strategy for model learning.\nThe offline experiment shows that CARE is efficient in automatically inferring\naccurate responses from the user manual. The online experiment further\ndemonstrates the superiority of CARE to reduce CSRs' reading burden and keep\nhigh service quality, in particular with >35% decrease in time spent and\nkeeping a >0.75 ICC score."
                },
                "authors": [
                    {
                        "name": "Weihong Du"
                    },
                    {
                        "name": "Jia Liu"
                    },
                    {
                        "name": "Zujie Wen"
                    },
                    {
                        "name": "Dingnan Jin"
                    },
                    {
                        "name": "Hongru Liang"
                    },
                    {
                        "name": "Wenqiang Lei"
                    }
                ],
                "author_detail": {
                    "name": "Wenqiang Lei"
                },
                "author": "Wenqiang Lei",
                "arxiv_comment": "Accepted to The 62nd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03633v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03633v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02462v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02462v2",
                "updated": "2024-08-26T06:12:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    6,
                    12,
                    5,
                    0,
                    239,
                    0
                ],
                "published": "2024-04-03T05:04:55Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    5,
                    4,
                    55,
                    2,
                    94,
                    0
                ],
                "title": "A Unified Membership Inference Method for Visual Self-supervised Encoder\n  via Part-aware Capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Membership Inference Method for Visual Self-supervised Encoder\n  via Part-aware Capability"
                },
                "summary": "Self-supervised learning shows promise in harnessing extensive unlabeled\ndata, but it also confronts significant privacy concerns, especially in vision.\nIn this paper, we aim to perform membership inference on visual self-supervised\nmodels in a more realistic setting: self-supervised training method and details\nare unknown for an adversary when attacking as he usually faces a black-box\nsystem in practice. In this setting, considering that self-supervised model\ncould be trained by completely different self-supervised paradigms, e.g.,\nmasked image modeling and contrastive learning, with complex training details,\nwe propose a unified membership inference method called PartCrop. It is\nmotivated by the shared part-aware capability among models and stronger part\nresponse on the training data. Specifically, PartCrop crops parts of objects in\nan image to query responses with the image in representation space. We conduct\nextensive attacks on self-supervised models with different training protocols\nand structures using three widely used image datasets. The results verify the\neffectiveness and generalization of PartCrop. Moreover, to defend against\nPartCrop, we evaluate two common approaches, i.e., early stop and differential\nprivacy, and propose a tailored method called shrinking crop scale range. The\ndefense experiments indicate that all of them are effective. Our code is\navailable at https://github.com/JiePKU/PartCrop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-supervised learning shows promise in harnessing extensive unlabeled\ndata, but it also confronts significant privacy concerns, especially in vision.\nIn this paper, we aim to perform membership inference on visual self-supervised\nmodels in a more realistic setting: self-supervised training method and details\nare unknown for an adversary when attacking as he usually faces a black-box\nsystem in practice. In this setting, considering that self-supervised model\ncould be trained by completely different self-supervised paradigms, e.g.,\nmasked image modeling and contrastive learning, with complex training details,\nwe propose a unified membership inference method called PartCrop. It is\nmotivated by the shared part-aware capability among models and stronger part\nresponse on the training data. Specifically, PartCrop crops parts of objects in\nan image to query responses with the image in representation space. We conduct\nextensive attacks on self-supervised models with different training protocols\nand structures using three widely used image datasets. The results verify the\neffectiveness and generalization of PartCrop. Moreover, to defend against\nPartCrop, we evaluate two common approaches, i.e., early stop and differential\nprivacy, and propose a tailored method called shrinking crop scale range. The\ndefense experiments indicate that all of them are effective. Our code is\navailable at https://github.com/JiePKU/PartCrop."
                },
                "authors": [
                    {
                        "name": "Jie Zhu"
                    },
                    {
                        "name": "Jirong Zha"
                    },
                    {
                        "name": "Ding Li"
                    },
                    {
                        "name": "Leye Wang"
                    }
                ],
                "author_detail": {
                    "name": "Leye Wang"
                },
                "author": "Leye Wang",
                "arxiv_comment": "Accepted by ACM CCS2024, Full version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02462v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02462v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14033v1",
                "updated": "2024-08-26T05:55:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    5,
                    55,
                    48,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T05:55:48Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    5,
                    55,
                    48,
                    0,
                    239,
                    0
                ],
                "title": "MLR-Copilot: Autonomous Machine Learning Research based on Large\n  Language Models Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLR-Copilot: Autonomous Machine Learning Research based on Large\n  Language Models Agents"
                },
                "summary": "Machine learning research, crucial for technological advancements and\ninnovation, often faces significant challenges due to its inherent complexity,\nslow pace of experimentation, and the necessity for specialized expertise.\nMotivated by this, we present a new systematic framework, autonomous Machine\nLearning Research with large language models (MLR-Copilot), designed to enhance\nmachine learning research productivity through the automatic generation and\nimplementation of research ideas using Large Language Model (LLM) agents. The\nframework consists of three phases: research idea generation, experiment\nimplementation, and implementation execution. First, existing research papers\nare used to generate hypotheses and experimental plans vis IdeaAgent powered by\nLLMs. Next, the implementation generation phase translates these plans into\nexecutables with ExperimentAgent. This phase leverages retrieved prototype code\nand optionally retrieves candidate models and data. Finally, the execution\nphase, also managed by ExperimentAgent, involves running experiments with\nmechanisms for human feedback and iterative debugging to enhance the likelihood\nof achieving executable research outcomes. We evaluate our framework on five\nmachine learning research tasks and the experimental results show the\nframework's potential to facilitate the research progress and innovations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning research, crucial for technological advancements and\ninnovation, often faces significant challenges due to its inherent complexity,\nslow pace of experimentation, and the necessity for specialized expertise.\nMotivated by this, we present a new systematic framework, autonomous Machine\nLearning Research with large language models (MLR-Copilot), designed to enhance\nmachine learning research productivity through the automatic generation and\nimplementation of research ideas using Large Language Model (LLM) agents. The\nframework consists of three phases: research idea generation, experiment\nimplementation, and implementation execution. First, existing research papers\nare used to generate hypotheses and experimental plans vis IdeaAgent powered by\nLLMs. Next, the implementation generation phase translates these plans into\nexecutables with ExperimentAgent. This phase leverages retrieved prototype code\nand optionally retrieves candidate models and data. Finally, the execution\nphase, also managed by ExperimentAgent, involves running experiments with\nmechanisms for human feedback and iterative debugging to enhance the likelihood\nof achieving executable research outcomes. We evaluate our framework on five\nmachine learning research tasks and the experimental results show the\nframework's potential to facilitate the research progress and innovations."
                },
                "authors": [
                    {
                        "name": "Ruochen Li"
                    },
                    {
                        "name": "Teerth Patel"
                    },
                    {
                        "name": "Qingyun Wang"
                    },
                    {
                        "name": "Xinya Du"
                    }
                ],
                "author_detail": {
                    "name": "Xinya Du"
                },
                "author": "Xinya Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14822v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14822v2",
                "updated": "2024-08-26T05:43:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    5,
                    43,
                    34,
                    0,
                    239,
                    0
                ],
                "published": "2024-06-21T01:46:32Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    1,
                    46,
                    32,
                    4,
                    173,
                    0
                ],
                "title": "Explaining muon excess in cosmic rays using the gluon condensation model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explaining muon excess in cosmic rays using the gluon condensation model"
                },
                "summary": "Ultrahigh-energy cosmic rays are often characterized indirectly by analyzing\nthe properties of secondary cosmic ray particles produced in the collisions\nwith air nuclei. The particle number $N_\\mu$ of muon and the depth of shower\nmaximum $X_\\mathrm{max}$ after air shower cascade are mostly studied to infer\nthe energy and mass of the incident cosmic rays. Research have shown that there\nis a significant excess in the observed number of muons arriving at the ground\nfrom extensive air showers (EAS) compared to the simulations using the existing\ncosmic ray hadronic interaction model. To explain this muon excess phenomenon,\na new theoretical model, the gluon condensation model (GC model), is introduced\nin this paper and simulated by using the AIRES engine. We assume that the GC\neffect appears mainly in the first collision of the cascade leading to a\nsignificant increase in the strangeness production, consequently, the\nproduction rate of kaons is increased and $n_K/n_\\pi$ is greater than the value\nof the usual hadronic interaction process. In the calculation, the model\nassumes that only pions and kaons are produced in GC state. The increase of\nstrange particle yield would mean that the energy transferred from the hadronic\ncascade to electromagnetic cascade through $\\pi^{0} \\rightarrow 2\\gamma$ decay\nis reduced. This would in turn increase the number of muons at the ground level\ndue to meson decays.Our model provides a new theoretical possibility to explain\nthe muon excess puzzle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultrahigh-energy cosmic rays are often characterized indirectly by analyzing\nthe properties of secondary cosmic ray particles produced in the collisions\nwith air nuclei. The particle number $N_\\mu$ of muon and the depth of shower\nmaximum $X_\\mathrm{max}$ after air shower cascade are mostly studied to infer\nthe energy and mass of the incident cosmic rays. Research have shown that there\nis a significant excess in the observed number of muons arriving at the ground\nfrom extensive air showers (EAS) compared to the simulations using the existing\ncosmic ray hadronic interaction model. To explain this muon excess phenomenon,\na new theoretical model, the gluon condensation model (GC model), is introduced\nin this paper and simulated by using the AIRES engine. We assume that the GC\neffect appears mainly in the first collision of the cascade leading to a\nsignificant increase in the strangeness production, consequently, the\nproduction rate of kaons is increased and $n_K/n_\\pi$ is greater than the value\nof the usual hadronic interaction process. In the calculation, the model\nassumes that only pions and kaons are produced in GC state. The increase of\nstrange particle yield would mean that the energy transferred from the hadronic\ncascade to electromagnetic cascade through $\\pi^{0} \\rightarrow 2\\gamma$ decay\nis reduced. This would in turn increase the number of muons at the ground level\ndue to meson decays.Our model provides a new theoretical possibility to explain\nthe muon excess puzzle."
                },
                "authors": [
                    {
                        "name": "Bingyang Liu"
                    },
                    {
                        "name": "Zhixiang Yang"
                    },
                    {
                        "name": "Jianhong Ruan"
                    }
                ],
                "author_detail": {
                    "name": "Jianhong Ruan"
                },
                "author": "Jianhong Ruan",
                "arxiv_doi": "10.3847/1538-4357/ad6b9a",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad6b9a",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.14822v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14822v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.05561v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.05561v5",
                "updated": "2024-08-26T05:31:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    5,
                    31,
                    38,
                    0,
                    239,
                    0
                ],
                "published": "2024-01-10T22:07:21Z",
                "published_parsed": [
                    2024,
                    1,
                    10,
                    22,
                    7,
                    21,
                    2,
                    10,
                    0
                ],
                "title": "TrustLLM: Trustworthiness in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrustLLM: Trustworthiness in Large Language Models"
                },
                "summary": "Large language models (LLMs), exemplified by ChatGPT, have gained\nconsiderable attention for their excellent natural language processing\ncapabilities. Nonetheless, these LLMs present many challenges, particularly in\nthe realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs\nemerges as an important topic. This paper introduces TrustLLM, a comprehensive\nstudy of trustworthiness in LLMs, including principles for different dimensions\nof trustworthiness, established benchmark, evaluation, and analysis of\ntrustworthiness for mainstream LLMs, and discussion of open challenges and\nfuture directions. Specifically, we first propose a set of principles for\ntrustworthy LLMs that span eight different dimensions. Based on these\nprinciples, we further establish a benchmark across six dimensions including\ntruthfulness, safety, fairness, robustness, privacy, and machine ethics. We\nthen present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of\nover 30 datasets. Our findings firstly show that in general trustworthiness and\nutility (i.e., functional effectiveness) are positively related. Secondly, our\nobservations reveal that proprietary LLMs generally outperform most open-source\ncounterparts in terms of trustworthiness, raising concerns about the potential\nrisks of widely accessible open-source LLMs. However, a few open-source LLMs\ncome very close to proprietary ones. Thirdly, it is important to note that some\nLLMs may be overly calibrated towards exhibiting trustworthiness, to the extent\nthat they compromise their utility by mistakenly treating benign prompts as\nharmful and consequently not responding. Finally, we emphasize the importance\nof ensuring transparency not only in the models themselves but also in the\ntechnologies that underpin trustworthiness. Knowing the specific trustworthy\ntechnologies that have been employed is crucial for analyzing their\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), exemplified by ChatGPT, have gained\nconsiderable attention for their excellent natural language processing\ncapabilities. Nonetheless, these LLMs present many challenges, particularly in\nthe realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs\nemerges as an important topic. This paper introduces TrustLLM, a comprehensive\nstudy of trustworthiness in LLMs, including principles for different dimensions\nof trustworthiness, established benchmark, evaluation, and analysis of\ntrustworthiness for mainstream LLMs, and discussion of open challenges and\nfuture directions. Specifically, we first propose a set of principles for\ntrustworthy LLMs that span eight different dimensions. Based on these\nprinciples, we further establish a benchmark across six dimensions including\ntruthfulness, safety, fairness, robustness, privacy, and machine ethics. We\nthen present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of\nover 30 datasets. Our findings firstly show that in general trustworthiness and\nutility (i.e., functional effectiveness) are positively related. Secondly, our\nobservations reveal that proprietary LLMs generally outperform most open-source\ncounterparts in terms of trustworthiness, raising concerns about the potential\nrisks of widely accessible open-source LLMs. However, a few open-source LLMs\ncome very close to proprietary ones. Thirdly, it is important to note that some\nLLMs may be overly calibrated towards exhibiting trustworthiness, to the extent\nthat they compromise their utility by mistakenly treating benign prompts as\nharmful and consequently not responding. Finally, we emphasize the importance\nof ensuring transparency not only in the models themselves but also in the\ntechnologies that underpin trustworthiness. Knowing the specific trustworthy\ntechnologies that have been employed is crucial for analyzing their\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Haoran Wang"
                    },
                    {
                        "name": "Siyuan Wu"
                    },
                    {
                        "name": "Qihui Zhang"
                    },
                    {
                        "name": "Yuan Li"
                    },
                    {
                        "name": "Chujie Gao"
                    },
                    {
                        "name": "Yixin Huang"
                    },
                    {
                        "name": "Wenhan Lyu"
                    },
                    {
                        "name": "Yixuan Zhang"
                    },
                    {
                        "name": "Xiner Li"
                    },
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Yijue Wang"
                    },
                    {
                        "name": "Zhikun Zhang"
                    },
                    {
                        "name": "Bertie Vidgen"
                    },
                    {
                        "name": "Bhavya Kailkhura"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Chaowei Xiao"
                    },
                    {
                        "name": "Chunyuan Li"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Hongyi Wang"
                    },
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Huaxiu Yao"
                    },
                    {
                        "name": "Manolis Kellis"
                    },
                    {
                        "name": "Marinka Zitnik"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Jian Pei"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Jieyu Zhao"
                    },
                    {
                        "name": "Jiliang Tang"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Joaquin Vanschoren"
                    },
                    {
                        "name": "John Mitchell"
                    },
                    {
                        "name": "Kai Shu"
                    },
                    {
                        "name": "Kaidi Xu"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Lifang He"
                    },
                    {
                        "name": "Lifu Huang"
                    },
                    {
                        "name": "Michael Backes"
                    },
                    {
                        "name": "Neil Zhenqiang Gong"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Ran Xu"
                    },
                    {
                        "name": "Rex Ying"
                    },
                    {
                        "name": "Shuiwang Ji"
                    },
                    {
                        "name": "Suman Jana"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "William Wang"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Xun Chen"
                    },
                    {
                        "name": "Xuyu Wang"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Yanfang Ye"
                    },
                    {
                        "name": "Yinzhi Cao"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Yue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhao"
                },
                "author": "Yue Zhao",
                "arxiv_comment": "This work is still under work and we welcome your contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.05561v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.05561v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14023v1",
                "updated": "2024-08-26T05:27:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    5,
                    27,
                    14,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T05:27:14Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    5,
                    27,
                    14,
                    0,
                    239,
                    0
                ],
                "title": "Video-CCAM: Enhancing Video-Language Understanding with Causal\n  Cross-Attention Masks for Short and Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-CCAM: Enhancing Video-Language Understanding with Causal\n  Cross-Attention Masks for Short and Long Videos"
                },
                "summary": "Multi-modal large language models (MLLMs) have demonstrated considerable\npotential across various downstream tasks that require cross-domain knowledge.\nMLLMs capable of processing videos, known as Video-MLLMs, have attracted broad\ninterest in video-language understanding. However, videos, especially long\nvideos, contain more visual tokens than images, making them difficult for LLMs\nto process. Existing works either downsample visual features or extend the LLM\ncontext size, risking the loss of high-resolution information or slowing down\ninference speed. To address these limitations, we apply cross-attention layers\nin the intermediate projector between the visual encoder and the large language\nmodel (LLM). As the naive cross-attention mechanism is insensitive to temporal\norder, we further introduce causal cross-attention masks (CCAMs) within the\ncross-attention layers. This Video-MLLM, named Video-CCAM, is trained in a\nstraightforward two-stage fashion: feature alignment and visual instruction\ntuning. We develop several Video-CCAM models based on LLMs of different sizes\n(4B, 9B, and 14B). Video-CCAM proves to be a robust Video-MLLM and shows\noutstanding performance from short videos to long ones. Among standard video\nbenchmarks like MVBench and VideoChatGPT-QA, Video-CCAM shows outstanding\nperformances (1st/2nd/3rd in MVBench and TGIF-QA, 2nd/3rd/4th in MSVD-QA,\nMSRVTT-QA, and ActivityNet-QA). In benchmarks encompassing long videos,\nVideo-CCAM models can be directly adapted to long video understanding and still\nachieve exceptional scores despite being trained solely with images and\n16-frame videos. Using 96 frames (6$\\times$ the training number of frames),\nVideo-CCAM models rank 1st/2nd/3rd in VideoVista and 1st/2nd/4th in MLVU among\nall open-source Video-MLLMs, respectively. The code is publicly available in\n\\url{https://github.com/QQ-MM/Video-CCAM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal large language models (MLLMs) have demonstrated considerable\npotential across various downstream tasks that require cross-domain knowledge.\nMLLMs capable of processing videos, known as Video-MLLMs, have attracted broad\ninterest in video-language understanding. However, videos, especially long\nvideos, contain more visual tokens than images, making them difficult for LLMs\nto process. Existing works either downsample visual features or extend the LLM\ncontext size, risking the loss of high-resolution information or slowing down\ninference speed. To address these limitations, we apply cross-attention layers\nin the intermediate projector between the visual encoder and the large language\nmodel (LLM). As the naive cross-attention mechanism is insensitive to temporal\norder, we further introduce causal cross-attention masks (CCAMs) within the\ncross-attention layers. This Video-MLLM, named Video-CCAM, is trained in a\nstraightforward two-stage fashion: feature alignment and visual instruction\ntuning. We develop several Video-CCAM models based on LLMs of different sizes\n(4B, 9B, and 14B). Video-CCAM proves to be a robust Video-MLLM and shows\noutstanding performance from short videos to long ones. Among standard video\nbenchmarks like MVBench and VideoChatGPT-QA, Video-CCAM shows outstanding\nperformances (1st/2nd/3rd in MVBench and TGIF-QA, 2nd/3rd/4th in MSVD-QA,\nMSRVTT-QA, and ActivityNet-QA). In benchmarks encompassing long videos,\nVideo-CCAM models can be directly adapted to long video understanding and still\nachieve exceptional scores despite being trained solely with images and\n16-frame videos. Using 96 frames (6$\\times$ the training number of frames),\nVideo-CCAM models rank 1st/2nd/3rd in VideoVista and 1st/2nd/4th in MLVU among\nall open-source Video-MLLMs, respectively. The code is publicly available in\n\\url{https://github.com/QQ-MM/Video-CCAM}."
                },
                "authors": [
                    {
                        "name": "Jiajun Fei"
                    },
                    {
                        "name": "Dian Li"
                    },
                    {
                        "name": "Zhidong Deng"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Gang Liu"
                    },
                    {
                        "name": "Hui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hui Wang"
                },
                "author": "Hui Wang",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10668v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10668v3",
                "updated": "2024-08-26T05:27:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    5,
                    27,
                    13,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-20T09:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    11,
                    21,
                    1,
                    233,
                    0
                ],
                "title": "Probing the Safety Response Boundary of Large Language Models via Unsafe\n  Decoding Path Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing the Safety Response Boundary of Large Language Models via Unsafe\n  Decoding Path Generation"
                },
                "summary": "Large Language Models (LLMs) are implicit troublemakers. While they provide\nvaluable insights and assist in problem-solving, they can also potentially\nserve as a resource for malicious activities. Implementing safety alignment\ncould mitigate the risk of LLMs generating harmful responses. We argue that:\neven when an LLM appears to successfully block harmful queries, there may still\nbe hidden vulnerabilities that could act as ticking time bombs. To identify\nthese underlying weaknesses, we propose to use a cost value model as both a\ndetector and an attacker. Trained on external or self-generated harmful\ndatasets, the cost value model could successfully influence the original safe\nLLM to output toxic content in decoding process. For instance, LLaMA-2-chat 7B\noutputs 39.18% concrete toxic content, along with only 22.16% refusals without\nany harmful suffixes. These potential weaknesses can then be exploited via\nprompt optimization such as soft prompts on images. We name this decoding\nstrategy: Jailbreak Value Decoding (JVD), emphasizing that seemingly secure\nLLMs may not be as safe as we initially believe. They could be used to gather\nharmful data or launch covert attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are implicit troublemakers. While they provide\nvaluable insights and assist in problem-solving, they can also potentially\nserve as a resource for malicious activities. Implementing safety alignment\ncould mitigate the risk of LLMs generating harmful responses. We argue that:\neven when an LLM appears to successfully block harmful queries, there may still\nbe hidden vulnerabilities that could act as ticking time bombs. To identify\nthese underlying weaknesses, we propose to use a cost value model as both a\ndetector and an attacker. Trained on external or self-generated harmful\ndatasets, the cost value model could successfully influence the original safe\nLLM to output toxic content in decoding process. For instance, LLaMA-2-chat 7B\noutputs 39.18% concrete toxic content, along with only 22.16% refusals without\nany harmful suffixes. These potential weaknesses can then be exploited via\nprompt optimization such as soft prompts on images. We name this decoding\nstrategy: Jailbreak Value Decoding (JVD), emphasizing that seemingly secure\nLLMs may not be as safe as we initially believe. They could be used to gather\nharmful data or launch covert attacks."
                },
                "authors": [
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Bingzhe Wu"
                    },
                    {
                        "name": "Yatao Bian"
                    },
                    {
                        "name": "Yongzhe Chang"
                    },
                    {
                        "name": "Xueqian Wang"
                    },
                    {
                        "name": "Peilin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Peilin Zhao"
                },
                "author": "Peilin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10668v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10668v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10566v2",
                "updated": "2024-08-26T05:08:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    5,
                    8,
                    29,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-20T06:05:52Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    6,
                    5,
                    52,
                    1,
                    233,
                    0
                ],
                "title": "SparseGrow: Addressing Growth-Induced Forgetting in Task-Agnostic\n  Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseGrow: Addressing Growth-Induced Forgetting in Task-Agnostic\n  Continual Learning"
                },
                "summary": "In continual learning (CL), model growth enhances adaptability over new data,\nimproving knowledge retention for more tasks. However, improper model growth\ncan lead to severe degradation of previously learned knowledge, an issue we\nname as growth-induced forgetting (GIFt), especially in task-agnostic CL using\nentire grown model for inference. Existing works, despite adopting model growth\nand random initialization for better adaptability, often fail to recognize the\npresence of GIFt caused by improper model growth. This oversight limits\ncomprehensive control of forgetting and hinders full utilization of model\ngrowth. We are the first in CL to identify this issue and conduct an in-depth\nstudy on root cause of GIFt, where layer expansion stands out among model\ngrowth strategies, widening layers without affecting model functionality. Yet,\ndirect adoption of layer expansion presents challenges. It lacks data-driven\ncontrol and initialization of expanded parameters to balance adaptability and\nknowledge retention. This paper presents a novel SparseGrow approach to\novercome the issue of GIFt while enhancing adaptability over new data.\nSparseGrow employs data-driven sparse layer expansion to control efficient\nparameter usage during growth, reducing GIFt from excessive growth and\nfunctionality changes. It also combines sparse growth with on-data\ninitialization at training late-stage to create partially 0-valued expansions\nthat fit learned distribution, enhancing retention and adaptability. To further\nminimize forgetting, freezing is applied by calculating the sparse mask,\nallowing data-driven preservation of important parameters. Through experiments\nacross datasets with various settings, cases and task numbers, we demonstrate\nthe necessity of layer expansion and showcase the effectiveness of SparseGrow\nin overcoming GIFt, highlighting its adaptability and knowledge retention for\nincremental tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In continual learning (CL), model growth enhances adaptability over new data,\nimproving knowledge retention for more tasks. However, improper model growth\ncan lead to severe degradation of previously learned knowledge, an issue we\nname as growth-induced forgetting (GIFt), especially in task-agnostic CL using\nentire grown model for inference. Existing works, despite adopting model growth\nand random initialization for better adaptability, often fail to recognize the\npresence of GIFt caused by improper model growth. This oversight limits\ncomprehensive control of forgetting and hinders full utilization of model\ngrowth. We are the first in CL to identify this issue and conduct an in-depth\nstudy on root cause of GIFt, where layer expansion stands out among model\ngrowth strategies, widening layers without affecting model functionality. Yet,\ndirect adoption of layer expansion presents challenges. It lacks data-driven\ncontrol and initialization of expanded parameters to balance adaptability and\nknowledge retention. This paper presents a novel SparseGrow approach to\novercome the issue of GIFt while enhancing adaptability over new data.\nSparseGrow employs data-driven sparse layer expansion to control efficient\nparameter usage during growth, reducing GIFt from excessive growth and\nfunctionality changes. It also combines sparse growth with on-data\ninitialization at training late-stage to create partially 0-valued expansions\nthat fit learned distribution, enhancing retention and adaptability. To further\nminimize forgetting, freezing is applied by calculating the sparse mask,\nallowing data-driven preservation of important parameters. Through experiments\nacross datasets with various settings, cases and task numbers, we demonstrate\nthe necessity of layer expansion and showcase the effectiveness of SparseGrow\nin overcoming GIFt, highlighting its adaptability and knowledge retention for\nincremental tasks."
                },
                "authors": [
                    {
                        "name": "Yuqing Zhao"
                    },
                    {
                        "name": "Divya Saxena"
                    },
                    {
                        "name": "Jiannong Cao"
                    },
                    {
                        "name": "Xiaoyun Liu"
                    },
                    {
                        "name": "Changlin Song"
                    }
                ],
                "author_detail": {
                    "name": "Changlin Song"
                },
                "author": "Changlin Song",
                "arxiv_comment": "This paper has been submitted to the AAAI conference. If accepted,\n  the final version will be updated to reflect the conference proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14017v1",
                "updated": "2024-08-26T05:03:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    5,
                    3,
                    32,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T05:03:32Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    5,
                    3,
                    32,
                    0,
                    239,
                    0
                ],
                "title": "Making Formulog Fast: An Argument for Unconventional Datalog Evaluation\n  (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Formulog Fast: An Argument for Unconventional Datalog Evaluation\n  (Extended Version)"
                },
                "summary": "By combining Datalog, SMT solving, and functional programming, the language\nFormulog provides an appealing mix of features for implementing SMT-based\nstatic analyses (e.g., refinement type checking, symbolic execution) in a\nnatural, declarative way. At the same time, the performance of its custom\nDatalog solver can be an impediment to using Formulog beyond prototyping -- a\ncommon problem for Datalog variants that aspire to solve large problem\ninstances. In this work we speed up Formulog evaluation, with surprising\nresults: while 2.2x speedups are obtained by using the conventional techniques\nfor high-performance Datalog (e.g., compilation, specialized data structures),\nthe big wins come by abandoning the central assumption in modern performant\nDatalog engines, semi-naive Datalog evaluation. In its place, we develop eager\nevaluation, a concurrent Datalog evaluation algorithm that explores the logical\ninference space via a depth-first traversal order. In practice, eager\nevaluation leads to an advantageous distribution of Formulog's SMT workload to\nexternal SMT solvers and improved SMT solving times: our eager evaluation\nextensions to the Formulog interpreter and Souffl\\'e's code generator achieve\nmean 5.2x and 7.6x speedups, respectively, over the optimized code generated by\noff-the-shelf Souffl\\'e on SMT-heavy Formulog benchmarks.\n  Using compilation and eager evaluation, Formulog implementations of\nrefinement type checking, bottom-up pointer analysis, and symbolic execution\nachieve speedups on 20 out of 23 benchmarks over previously published,\nhand-tuned analyses written in F#, Java, and C++, providing strong evidence\nthat Formulog can be the basis of a realistic platform for SMT-based static\nanalysis. Moreover, our experience adds nuance to the conventional wisdom that\nsemi-naive evaluation is the one-size-fits-all best Datalog evaluation\nalgorithm for static analysis workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By combining Datalog, SMT solving, and functional programming, the language\nFormulog provides an appealing mix of features for implementing SMT-based\nstatic analyses (e.g., refinement type checking, symbolic execution) in a\nnatural, declarative way. At the same time, the performance of its custom\nDatalog solver can be an impediment to using Formulog beyond prototyping -- a\ncommon problem for Datalog variants that aspire to solve large problem\ninstances. In this work we speed up Formulog evaluation, with surprising\nresults: while 2.2x speedups are obtained by using the conventional techniques\nfor high-performance Datalog (e.g., compilation, specialized data structures),\nthe big wins come by abandoning the central assumption in modern performant\nDatalog engines, semi-naive Datalog evaluation. In its place, we develop eager\nevaluation, a concurrent Datalog evaluation algorithm that explores the logical\ninference space via a depth-first traversal order. In practice, eager\nevaluation leads to an advantageous distribution of Formulog's SMT workload to\nexternal SMT solvers and improved SMT solving times: our eager evaluation\nextensions to the Formulog interpreter and Souffl\\'e's code generator achieve\nmean 5.2x and 7.6x speedups, respectively, over the optimized code generated by\noff-the-shelf Souffl\\'e on SMT-heavy Formulog benchmarks.\n  Using compilation and eager evaluation, Formulog implementations of\nrefinement type checking, bottom-up pointer analysis, and symbolic execution\nachieve speedups on 20 out of 23 benchmarks over previously published,\nhand-tuned analyses written in F#, Java, and C++, providing strong evidence\nthat Formulog can be the basis of a realistic platform for SMT-based static\nanalysis. Moreover, our experience adds nuance to the conventional wisdom that\nsemi-naive evaluation is the one-size-fits-all best Datalog evaluation\nalgorithm for static analysis workloads."
                },
                "authors": [
                    {
                        "name": "Aaron Bembenek"
                    },
                    {
                        "name": "Michael Greenberg"
                    },
                    {
                        "name": "Stephen Chong"
                    }
                ],
                "author_detail": {
                    "name": "Stephen Chong"
                },
                "arxiv_affiliation": "Harvard University",
                "author": "Stephen Chong",
                "arxiv_comment": "Please cite the official PACMPL version of this article, available at\n  https://doi.org/10.1145/3689754",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14016v1",
                "updated": "2024-08-26T04:56:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    56,
                    41,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T04:56:41Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    56,
                    41,
                    0,
                    239,
                    0
                ],
                "title": "Pixel-Aligned Multi-View Generation with Depth Guided Decoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pixel-Aligned Multi-View Generation with Depth Guided Decoder"
                },
                "summary": "The task of image-to-multi-view generation refers to generating novel views\nof an instance from a single image. Recent methods achieve this by extending\ntext-to-image latent diffusion models to multi-view version, which contains an\nVAE image encoder and a U-Net diffusion model. Specifically, these generation\nmethods usually fix VAE and finetune the U-Net only. However, the significant\ndownscaling of the latent vectors computed from the input images and\nindependent decoding leads to notable pixel-level misalignment across multiple\nviews. To address this, we propose a novel method for pixel-level\nimage-to-multi-view generation. Unlike prior work, we incorporate attention\nlayers across multi-view images in the VAE decoder of a latent video diffusion\nmodel. Specifically, we introduce a depth-truncated epipolar attention,\nenabling the model to focus on spatially adjacent regions while remaining\nmemory efficient. Applying depth-truncated attn is challenging during inference\nas the ground-truth depth is usually difficult to obtain and pre-trained depth\nestimation models is hard to provide accurate depth. Thus, to enhance the\ngeneralization to inaccurate depth when ground truth depth is missing, we\nperturb depth inputs during training. During inference, we employ a rapid\nmulti-view to 3D reconstruction approach, NeuS, to obtain coarse depth for the\ndepth-truncated epipolar attention. Our model enables better pixel alignment\nacross multi-view images. Moreover, we demonstrate the efficacy of our approach\nin improving downstream multi-view to 3D reconstruction tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The task of image-to-multi-view generation refers to generating novel views\nof an instance from a single image. Recent methods achieve this by extending\ntext-to-image latent diffusion models to multi-view version, which contains an\nVAE image encoder and a U-Net diffusion model. Specifically, these generation\nmethods usually fix VAE and finetune the U-Net only. However, the significant\ndownscaling of the latent vectors computed from the input images and\nindependent decoding leads to notable pixel-level misalignment across multiple\nviews. To address this, we propose a novel method for pixel-level\nimage-to-multi-view generation. Unlike prior work, we incorporate attention\nlayers across multi-view images in the VAE decoder of a latent video diffusion\nmodel. Specifically, we introduce a depth-truncated epipolar attention,\nenabling the model to focus on spatially adjacent regions while remaining\nmemory efficient. Applying depth-truncated attn is challenging during inference\nas the ground-truth depth is usually difficult to obtain and pre-trained depth\nestimation models is hard to provide accurate depth. Thus, to enhance the\ngeneralization to inaccurate depth when ground truth depth is missing, we\nperturb depth inputs during training. During inference, we employ a rapid\nmulti-view to 3D reconstruction approach, NeuS, to obtain coarse depth for the\ndepth-truncated epipolar attention. Our model enables better pixel alignment\nacross multi-view images. Moreover, we demonstrate the efficacy of our approach\nin improving downstream multi-view to 3D reconstruction tasks."
                },
                "authors": [
                    {
                        "name": "Zhenggang Tang"
                    },
                    {
                        "name": "Peiye Zhuang"
                    },
                    {
                        "name": "Chaoyang Wang"
                    },
                    {
                        "name": "Aliaksandr Siarohin"
                    },
                    {
                        "name": "Yash Kant"
                    },
                    {
                        "name": "Alexander Schwing"
                    },
                    {
                        "name": "Sergey Tulyakov"
                    },
                    {
                        "name": "Hsin-Ying Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hsin-Ying Lee"
                },
                "author": "Hsin-Ying Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14008v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14008v1",
                "updated": "2024-08-26T04:29:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    29,
                    52,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T04:29:52Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    29,
                    52,
                    0,
                    239,
                    0
                ],
                "title": "LMM-VQA: Advancing Video Quality Assessment with Large Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMM-VQA: Advancing Video Quality Assessment with Large Multimodal Models"
                },
                "summary": "The explosive growth of videos on streaming media platforms has underscored\nthe urgent need for effective video quality assessment (VQA) algorithms to\nmonitor and perceptually optimize the quality of streaming videos. However, VQA\nremains an extremely challenging task due to the diverse video content and the\ncomplex spatial and temporal distortions, thus necessitating more advanced\nmethods to address these issues. Nowadays, large multimodal models (LMMs), such\nas GPT-4V, have exhibited strong capabilities for various visual understanding\ntasks, motivating us to leverage the powerful multimodal representation ability\nof LMMs to solve the VQA task. Therefore, we propose the first Large\nMulti-Modal Video Quality Assessment (LMM-VQA) model, which introduces a novel\nspatiotemporal visual modeling strategy for quality-aware feature extraction.\nSpecifically, we first reformulate the quality regression problem into a\nquestion and answering (Q&A) task and construct Q&A prompts for VQA instruction\ntuning. Then, we design a spatiotemporal vision encoder to extract spatial and\ntemporal features to represent the quality characteristics of videos, which are\nsubsequently mapped into the language space by the spatiotemporal projector for\nmodality alignment. Finally, the aligned visual tokens and the quality-inquired\ntext tokens are aggregated as inputs for the large language model (LLM) to\ngenerate the quality score and level. Extensive experiments demonstrate that\nLMM-VQA achieves state-of-the-art performance across five VQA benchmarks,\nexhibiting an average improvement of $5\\%$ in generalization ability over\nexisting methods. Furthermore, due to the advanced design of the spatiotemporal\nencoder and projector, LMM-VQA also performs exceptionally well on general\nvideo understanding tasks, further validating its effectiveness. Our code will\nbe released at https://github.com/Sueqk/LMM-VQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The explosive growth of videos on streaming media platforms has underscored\nthe urgent need for effective video quality assessment (VQA) algorithms to\nmonitor and perceptually optimize the quality of streaming videos. However, VQA\nremains an extremely challenging task due to the diverse video content and the\ncomplex spatial and temporal distortions, thus necessitating more advanced\nmethods to address these issues. Nowadays, large multimodal models (LMMs), such\nas GPT-4V, have exhibited strong capabilities for various visual understanding\ntasks, motivating us to leverage the powerful multimodal representation ability\nof LMMs to solve the VQA task. Therefore, we propose the first Large\nMulti-Modal Video Quality Assessment (LMM-VQA) model, which introduces a novel\nspatiotemporal visual modeling strategy for quality-aware feature extraction.\nSpecifically, we first reformulate the quality regression problem into a\nquestion and answering (Q&A) task and construct Q&A prompts for VQA instruction\ntuning. Then, we design a spatiotemporal vision encoder to extract spatial and\ntemporal features to represent the quality characteristics of videos, which are\nsubsequently mapped into the language space by the spatiotemporal projector for\nmodality alignment. Finally, the aligned visual tokens and the quality-inquired\ntext tokens are aggregated as inputs for the large language model (LLM) to\ngenerate the quality score and level. Extensive experiments demonstrate that\nLMM-VQA achieves state-of-the-art performance across five VQA benchmarks,\nexhibiting an average improvement of $5\\%$ in generalization ability over\nexisting methods. Furthermore, due to the advanced design of the spatiotemporal\nencoder and projector, LMM-VQA also performs exceptionally well on general\nvideo understanding tasks, further validating its effectiveness. Our code will\nbe released at https://github.com/Sueqk/LMM-VQA."
                },
                "authors": [
                    {
                        "name": "Qihang Ge"
                    },
                    {
                        "name": "Wei Sun"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Yunhao Li"
                    },
                    {
                        "name": "Zhongpeng Ji"
                    },
                    {
                        "name": "Fengyu Sun"
                    },
                    {
                        "name": "Shangling Jui"
                    },
                    {
                        "name": "Xiongkuo Min"
                    },
                    {
                        "name": "Guangtao Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Guangtao Zhai"
                },
                "author": "Guangtao Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14008v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12529v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12529v2",
                "updated": "2024-08-26T04:28:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    28,
                    41,
                    0,
                    239,
                    0
                ],
                "published": "2024-07-17T13:11:28Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    13,
                    11,
                    28,
                    2,
                    199,
                    0
                ],
                "title": "Crafting the Path: Robust Query Rewriting for Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crafting the Path: Robust Query Rewriting for Information Retrieval"
                },
                "summary": "Query rewriting aims to generate a new query that can complement the original\nquery to improve the information retrieval system. Recent studies on query\nrewriting, such as query2doc, query2expand and querey2cot, rely on the internal\nknowledge of Large Language Models (LLMs) to generate a relevant passage to add\ninformation to the query. Nevertheless, the efficacy of these methodologies may\nmarkedly decline in instances where the requisite knowledge is not encapsulated\nwithin the model's intrinsic parameters. In this paper, we propose a novel\nstructured query rewriting method called Crafting the Path tailored for\nretrieval systems. Crafting the Path involves a three-step process that crafts\nquery-related information necessary for finding the passages to be searched in\neach step. Specifically, the Crafting the Path begins with Query Concept\nComprehension, proceeds to Query Type Identification, and finally conducts\nExpected Answer Extraction. Experimental results show that our method\noutperforms previous rewriting methods, especially in less familiar domains for\nLLMs. We demonstrate that our method is less dependent on the internal\nparameter knowledge of the model and generates queries with fewer factual\ninaccuracies. Furthermore, we observe that \\name{} demonstrates superior\nperformance in the retrieval-augmented generation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query rewriting aims to generate a new query that can complement the original\nquery to improve the information retrieval system. Recent studies on query\nrewriting, such as query2doc, query2expand and querey2cot, rely on the internal\nknowledge of Large Language Models (LLMs) to generate a relevant passage to add\ninformation to the query. Nevertheless, the efficacy of these methodologies may\nmarkedly decline in instances where the requisite knowledge is not encapsulated\nwithin the model's intrinsic parameters. In this paper, we propose a novel\nstructured query rewriting method called Crafting the Path tailored for\nretrieval systems. Crafting the Path involves a three-step process that crafts\nquery-related information necessary for finding the passages to be searched in\neach step. Specifically, the Crafting the Path begins with Query Concept\nComprehension, proceeds to Query Type Identification, and finally conducts\nExpected Answer Extraction. Experimental results show that our method\noutperforms previous rewriting methods, especially in less familiar domains for\nLLMs. We demonstrate that our method is less dependent on the internal\nparameter knowledge of the model and generates queries with fewer factual\ninaccuracies. Furthermore, we observe that \\name{} demonstrates superior\nperformance in the retrieval-augmented generation scenarios."
                },
                "authors": [
                    {
                        "name": "Ingeol Baek"
                    },
                    {
                        "name": "Jimin Lee"
                    },
                    {
                        "name": "Joonho Yang"
                    },
                    {
                        "name": "Hwanhee Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hwanhee Lee"
                },
                "author": "Hwanhee Lee",
                "arxiv_comment": "3 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12529v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12529v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14005v1",
                "updated": "2024-08-26T04:15:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    15,
                    12,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T04:15:12Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    15,
                    12,
                    0,
                    239,
                    0
                ],
                "title": "The Calibration of Polycyclic Aromatic Hydrocarbon Dust Emission as a\n  Star Formation Rate Indicator in the AKARI NEP Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Calibration of Polycyclic Aromatic Hydrocarbon Dust Emission as a\n  Star Formation Rate Indicator in the AKARI NEP Survey"
                },
                "summary": "Polycyclic aromatic hydrocarbon (PAH) dust emission has been proposed as an\neffective extinction-independent star formation rate (SFR) indicator in the\nmid-infrared (MIR), but this may depend on conditions in the interstellar\nmedium. The coverage of the AKARI/Infrared Camera (IRC) allows us to study the\neffects of metallicity, starburst intensity, and active galactic nuclei on PAH\nemission in galaxies with $f_{\\nu}(L18W)\\lesssim 19$ AB mag. Observations\ninclude follow-up, rest-frame optical spectra of 443 galaxies within the AKARI\nNorth Ecliptic Pole survey that have IRC detections from 7-24 $\\mu$m. We use\noptical emission line diagnostics to infer SFR based on H$\\alpha$ and [O\nII]$\\lambda\\lambda 3726,3729$ emission line luminosities. The PAH 6.2 $\\mu$m\nand PAH 7.7 $\\mu$m luminosities ($L(PAH\\ 6.2\\ \\mu m)$ and $L(PAH\\ 7.7\\ \\mu m)$,\nrespectively) derived using multi-wavelength model fits are consistent with\nthose derived from slitless spectroscopy within 0.2 dex. $L(PAH\\ 6.2\\ \\mu m)$\nand $L(PAH\\ 7.7\\ \\mu m)$ correlate linearly with the 24 $\\mu$m-dust corrected\nH$\\alpha$ luminosity only for normal, star-forming ``main-sequence\" galaxies.\nAssuming multi-linear correlations, we quantify the additional dependencies on\nmetallicity and starburst intensity, which we use to correct our PAH SFR\ncalibrations at $0<z<1.2$ for the first time. We derive the cosmic star\nformation rate density (SFRD) per comoving volume from $0.15 \\lesssim z\n\\lesssim 1$. The PAH SFRD is consistent with that of the far-infrared and\nreaches an order of magnitude higher than that of uncorrected UV observations\nat $z\\sim1$. Starburst galaxies contribute $\\gtrsim 0.7$ of the total SFRD at\n$z\\sim1$ compared to main-sequence galaxies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polycyclic aromatic hydrocarbon (PAH) dust emission has been proposed as an\neffective extinction-independent star formation rate (SFR) indicator in the\nmid-infrared (MIR), but this may depend on conditions in the interstellar\nmedium. The coverage of the AKARI/Infrared Camera (IRC) allows us to study the\neffects of metallicity, starburst intensity, and active galactic nuclei on PAH\nemission in galaxies with $f_{\\nu}(L18W)\\lesssim 19$ AB mag. Observations\ninclude follow-up, rest-frame optical spectra of 443 galaxies within the AKARI\nNorth Ecliptic Pole survey that have IRC detections from 7-24 $\\mu$m. We use\noptical emission line diagnostics to infer SFR based on H$\\alpha$ and [O\nII]$\\lambda\\lambda 3726,3729$ emission line luminosities. The PAH 6.2 $\\mu$m\nand PAH 7.7 $\\mu$m luminosities ($L(PAH\\ 6.2\\ \\mu m)$ and $L(PAH\\ 7.7\\ \\mu m)$,\nrespectively) derived using multi-wavelength model fits are consistent with\nthose derived from slitless spectroscopy within 0.2 dex. $L(PAH\\ 6.2\\ \\mu m)$\nand $L(PAH\\ 7.7\\ \\mu m)$ correlate linearly with the 24 $\\mu$m-dust corrected\nH$\\alpha$ luminosity only for normal, star-forming ``main-sequence\" galaxies.\nAssuming multi-linear correlations, we quantify the additional dependencies on\nmetallicity and starburst intensity, which we use to correct our PAH SFR\ncalibrations at $0<z<1.2$ for the first time. We derive the cosmic star\nformation rate density (SFRD) per comoving volume from $0.15 \\lesssim z\n\\lesssim 1$. The PAH SFRD is consistent with that of the far-infrared and\nreaches an order of magnitude higher than that of uncorrected UV observations\nat $z\\sim1$. Starburst galaxies contribute $\\gtrsim 0.7$ of the total SFRD at\n$z\\sim1$ compared to main-sequence galaxies."
                },
                "authors": [
                    {
                        "name": "Helen Kyung Kim"
                    },
                    {
                        "name": "Matthew A. Malkan"
                    },
                    {
                        "name": "Toshinobu Takagi"
                    },
                    {
                        "name": "Nagisa Oi"
                    },
                    {
                        "name": "Denis Burgarella"
                    },
                    {
                        "name": "Takamitsu Miyaji"
                    },
                    {
                        "name": "Hyunjin Shim"
                    },
                    {
                        "name": "Hideo Matsuhara"
                    },
                    {
                        "name": "Tomotsugu Goto"
                    },
                    {
                        "name": "Yoichi Ohyama"
                    },
                    {
                        "name": "Veronique Buat"
                    },
                    {
                        "name": "Seong Jin Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seong Jin Kim"
                },
                "author": "Seong Jin Kim",
                "arxiv_comment": "Accepted for publication in The Astrophysical Journal. 50 pages, 27\n  figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13999v1",
                "updated": "2024-08-26T03:55:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    55,
                    21,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T03:55:21Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    55,
                    21,
                    0,
                    239,
                    0
                ],
                "title": "Variations in the Inferred Cosmic-Ray Spectral Index as Measured by\n  Neutron Monitors in Antarctica",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variations in the Inferred Cosmic-Ray Spectral Index as Measured by\n  Neutron Monitors in Antarctica"
                },
                "summary": "A technique has recently been developed for tracking short-term spectral\nvariations in Galactic cosmic rays (GCRs) using data from a single neutron\nmonitor (NM), by collecting histograms of the time delay between successive\nneutron counts and extracting the leader fraction $L$ as a proxy of the\nspectral index. Here we analyze $L$ from four Antarctic NMs during 2015 March\nto 2023 September. We have calibrated $L$ from the South Pole NM with respect\nto a daily spectral index determined from published data of GCR proton fluxes\nduring 2015--2019 from the Alpha Magnetic Spectrometer (AMS-02) aboard the\nInternational Space Station. Our results demonstrate a robust correlation\nbetween the leader fraction and the spectral index fit over the rigidity range\n2.97--16.6 GV for AMS-02 data, with uncertainty 0.018 in the daily spectral\nindex as inferred from $L$. In addition to the 11-year solar activity cycle, a\nwavelet analysis confirms a 27-day periodicity in the GCR flux and spectral\nindex corresponding to solar rotation, especially near sunspot minimum, while\nthe flux occasionally exhibited a strong harmonic at 13.5 days, and that the\nmagnetic field component along a nominal Parker spiral (i.e., the magnetic\nsector structure) is a strong determinant of such spectral and flux variations,\nwith the solar wind speed exerting an additional, nearly rigidity-independent\ninfluence on flux variations. Our investigation affirms the capability of\nground-based NM stations to accurately and continuously monitor cosmic ray\nspectral variations in the long-term future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A technique has recently been developed for tracking short-term spectral\nvariations in Galactic cosmic rays (GCRs) using data from a single neutron\nmonitor (NM), by collecting histograms of the time delay between successive\nneutron counts and extracting the leader fraction $L$ as a proxy of the\nspectral index. Here we analyze $L$ from four Antarctic NMs during 2015 March\nto 2023 September. We have calibrated $L$ from the South Pole NM with respect\nto a daily spectral index determined from published data of GCR proton fluxes\nduring 2015--2019 from the Alpha Magnetic Spectrometer (AMS-02) aboard the\nInternational Space Station. Our results demonstrate a robust correlation\nbetween the leader fraction and the spectral index fit over the rigidity range\n2.97--16.6 GV for AMS-02 data, with uncertainty 0.018 in the daily spectral\nindex as inferred from $L$. In addition to the 11-year solar activity cycle, a\nwavelet analysis confirms a 27-day periodicity in the GCR flux and spectral\nindex corresponding to solar rotation, especially near sunspot minimum, while\nthe flux occasionally exhibited a strong harmonic at 13.5 days, and that the\nmagnetic field component along a nominal Parker spiral (i.e., the magnetic\nsector structure) is a strong determinant of such spectral and flux variations,\nwith the solar wind speed exerting an additional, nearly rigidity-independent\ninfluence on flux variations. Our investigation affirms the capability of\nground-based NM stations to accurately and continuously monitor cosmic ray\nspectral variations in the long-term future."
                },
                "authors": [
                    {
                        "name": "Pradiphat Muangha"
                    },
                    {
                        "name": "David Ruffolo"
                    },
                    {
                        "name": "Alejandro Sáiz"
                    },
                    {
                        "name": "Chanoknan Banglieng"
                    },
                    {
                        "name": "Paul Evenson"
                    },
                    {
                        "name": "Surujhdeo Seunarine"
                    },
                    {
                        "name": "Suyeon Oh"
                    },
                    {
                        "name": "Jongil Jung"
                    },
                    {
                        "name": "Marc Duldig"
                    },
                    {
                        "name": "John Humble"
                    }
                ],
                "author_detail": {
                    "name": "John Humble"
                },
                "arxiv_affiliation": "School of Natural Sciences, University of Tasmania, Hobart, Tasmania, Australia",
                "author": "John Humble",
                "arxiv_comment": "17 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12616v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12616v2",
                "updated": "2024-08-26T03:47:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    47,
                    6,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-08T16:46:14Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    16,
                    46,
                    14,
                    3,
                    221,
                    0
                ],
                "title": "Semantic Communication based on Large Language Model for Underwater\n  Image Transmission",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Communication based on Large Language Model for Underwater\n  Image Transmission"
                },
                "summary": "Underwater communication is essential for environmental monitoring, marine\nbiology research, and underwater exploration. Traditional underwater\ncommunication faces limitations like low bandwidth, high latency, and\nsusceptibility to noise, while semantic communication (SC) offers a promising\nsolution by focusing on the exchange of semantics rather than symbols or bits.\nHowever, SC encounters challenges in underwater environments, including\nsemantic information mismatch and difficulties in accurately identifying and\ntransmitting critical information that aligns with the diverse requirements of\nunderwater applications. To address these challenges, we propose a novel\nSemantic Communication (SC) framework based on Large Language Models (LLMs).\nOur framework leverages visual LLMs to perform semantic compression and\nprioritization of underwater image data according to the query from users. By\nidentifying and encoding key semantic elements within the images, the system\nselectively transmits high-priority information while applying higher\ncompression rates to less critical regions. On the receiver side, an LLM-based\nrecovery mechanism, along with Global Vision ControlNet and Key Region\nControlNet networks, aids in reconstructing the images, thereby enhancing\ncommunication efficiency and robustness. Our framework reduces the overall data\nsize to 0.8\\% of the original. Experimental results demonstrate that our method\nsignificantly outperforms existing approaches, ensuring high-quality,\nsemantically accurate image reconstruction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Underwater communication is essential for environmental monitoring, marine\nbiology research, and underwater exploration. Traditional underwater\ncommunication faces limitations like low bandwidth, high latency, and\nsusceptibility to noise, while semantic communication (SC) offers a promising\nsolution by focusing on the exchange of semantics rather than symbols or bits.\nHowever, SC encounters challenges in underwater environments, including\nsemantic information mismatch and difficulties in accurately identifying and\ntransmitting critical information that aligns with the diverse requirements of\nunderwater applications. To address these challenges, we propose a novel\nSemantic Communication (SC) framework based on Large Language Models (LLMs).\nOur framework leverages visual LLMs to perform semantic compression and\nprioritization of underwater image data according to the query from users. By\nidentifying and encoding key semantic elements within the images, the system\nselectively transmits high-priority information while applying higher\ncompression rates to less critical regions. On the receiver side, an LLM-based\nrecovery mechanism, along with Global Vision ControlNet and Key Region\nControlNet networks, aids in reconstructing the images, thereby enhancing\ncommunication efficiency and robustness. Our framework reduces the overall data\nsize to 0.8\\% of the original. Experimental results demonstrate that our method\nsignificantly outperforms existing approaches, ensuring high-quality,\nsemantically accurate image reconstruction."
                },
                "authors": [
                    {
                        "name": "Weilong Chen"
                    },
                    {
                        "name": "Wenxuan Xu"
                    },
                    {
                        "name": "Haoran Chen"
                    },
                    {
                        "name": "Xinran Zhang"
                    },
                    {
                        "name": "Zhijin Qin"
                    },
                    {
                        "name": "Yanru Zhang"
                    },
                    {
                        "name": "Zhu Han"
                    }
                ],
                "author_detail": {
                    "name": "Zhu Han"
                },
                "author": "Zhu Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12616v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12616v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10848v2",
                "updated": "2024-08-26T03:19:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    19,
                    45,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-20T13:40:25Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    40,
                    25,
                    1,
                    233,
                    0
                ],
                "title": "Perception-guided Jailbreak against Text-to-Image Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception-guided Jailbreak against Text-to-Image Models"
                },
                "summary": "In recent years, Text-to-Image (T2I) models have garnered significant\nattention due to their remarkable advancements. However, security concerns have\nemerged due to their potential to generate inappropriate or Not-Safe-For-Work\n(NSFW) images. In this paper, inspired by the observation that texts with\ndifferent semantics can lead to similar human perceptions, we propose an\nLLM-driven perception-guided jailbreak method, termed PGJ. It is a black-box\njailbreak method that requires no specific T2I model (model-free) and generates\nhighly natural attack prompts. Specifically, we propose identifying a safe\nphrase that is similar in human perception yet inconsistent in text semantics\nwith the target unsafe word and using it as a substitution. The experiments\nconducted on six open-source models and commercial online services with\nthousands of prompts have verified the effectiveness of PGJ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Text-to-Image (T2I) models have garnered significant\nattention due to their remarkable advancements. However, security concerns have\nemerged due to their potential to generate inappropriate or Not-Safe-For-Work\n(NSFW) images. In this paper, inspired by the observation that texts with\ndifferent semantics can lead to similar human perceptions, we propose an\nLLM-driven perception-guided jailbreak method, termed PGJ. It is a black-box\njailbreak method that requires no specific T2I model (model-free) and generates\nhighly natural attack prompts. Specifically, we propose identifying a safe\nphrase that is similar in human perception yet inconsistent in text semantics\nwith the target unsafe word and using it as a substitution. The experiments\nconducted on six open-source models and commercial online services with\nthousands of prompts have verified the effectiveness of PGJ."
                },
                "authors": [
                    {
                        "name": "Yihao Huang"
                    },
                    {
                        "name": "Le Liang"
                    },
                    {
                        "name": "Tianlin Li"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Run Wang"
                    },
                    {
                        "name": "Weikai Miao"
                    },
                    {
                        "name": "Geguang Pu"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08713v2",
                "updated": "2024-08-26T03:03:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    3,
                    47,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-16T12:51:52Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    51,
                    52,
                    4,
                    229,
                    0
                ],
                "title": "Beyond KAN: Introducing KarSein for Adaptive High-Order Feature\n  Interaction Modeling in CTR Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond KAN: Introducing KarSein for Adaptive High-Order Feature\n  Interaction Modeling in CTR Prediction"
                },
                "summary": "Modeling feature interactions is crucial for click-through rate (CTR)\nprediction, particularly when it comes to high-order explicit interactions.\nTraditional methods struggle with this task because they often predefine a\nmaximum interaction order, which relies heavily on prior knowledge and can\nlimit the model's effectiveness. Additionally, modeling high-order interactions\ntypically leads to increased computational costs. Therefore, the challenge lies\nin adaptively modeling high-order feature interactions while maintaining\nefficiency. To address this issue, we introduce Kolmogorov-Arnold Represented\nSparse Efficient Interaction Network (KarSein), designed to optimize both\npredictive accuracy and computational efficiency. We firstly identify\nlimitations of directly applying Kolmogorov-Arnold Networks (KAN) to CTR and\nthen introduce KarSein to overcome these issues. It features a novel\narchitecture that reduces the computational costs of KAN and supports embedding\nvectors as feature inputs. Additionally, KarSein employs guided symbolic\nregression to address the challenge of KAN in spontaneously learning\nmultiplicative relationships. Extensive experiments demonstrate KarSein's\nsuperior performance, achieving significant predictive accuracy with minimal\ncomputational overhead. Furthermore, KarSein maintains strong global\nexplainability while enabling the removal of redundant features, resulting in a\nsparse network structure. These advantages also position KarSein as a promising\nmethod for efficient inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling feature interactions is crucial for click-through rate (CTR)\nprediction, particularly when it comes to high-order explicit interactions.\nTraditional methods struggle with this task because they often predefine a\nmaximum interaction order, which relies heavily on prior knowledge and can\nlimit the model's effectiveness. Additionally, modeling high-order interactions\ntypically leads to increased computational costs. Therefore, the challenge lies\nin adaptively modeling high-order feature interactions while maintaining\nefficiency. To address this issue, we introduce Kolmogorov-Arnold Represented\nSparse Efficient Interaction Network (KarSein), designed to optimize both\npredictive accuracy and computational efficiency. We firstly identify\nlimitations of directly applying Kolmogorov-Arnold Networks (KAN) to CTR and\nthen introduce KarSein to overcome these issues. It features a novel\narchitecture that reduces the computational costs of KAN and supports embedding\nvectors as feature inputs. Additionally, KarSein employs guided symbolic\nregression to address the challenge of KAN in spontaneously learning\nmultiplicative relationships. Extensive experiments demonstrate KarSein's\nsuperior performance, achieving significant predictive accuracy with minimal\ncomputational overhead. Furthermore, KarSein maintains strong global\nexplainability while enabling the removal of redundant features, resulting in a\nsparse network structure. These advantages also position KarSein as a promising\nmethod for efficient inference."
                },
                "authors": [
                    {
                        "name": "Yunxiao Shi"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Haimin Zhang"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    },
                    {
                        "name": "Min Xu"
                    }
                ],
                "author_detail": {
                    "name": "Min Xu"
                },
                "author": "Min Xu",
                "arxiv_comment": "KarSein for CTR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13987v1",
                "updated": "2024-08-26T02:53:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    2,
                    53,
                    24,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T02:53:24Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    2,
                    53,
                    24,
                    0,
                    239,
                    0
                ],
                "title": "Focused Large Language Models are Stable Many-Shot Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Focused Large Language Models are Stable Many-Shot Learners"
                },
                "summary": "In-Context Learning (ICL) enables large language models (LLMs) to achieve\nrapid task adaptation by learning from demonstrations. With the increase in\navailable context length of LLMs, recent experiments have shown that the\nperformance of ICL does not necessarily scale well in many-shot (demonstration)\nsettings. We theoretically and experimentally confirm that the reason lies in\nmore demonstrations dispersing the model attention from the query, hindering\nits understanding of key content. Inspired by how humans learn from examples,\nwe propose a training-free method FocusICL, which conducts triviality filtering\nto avoid attention being diverted by unimportant contents at token-level and\noperates hierarchical attention to further ensure sufficient attention towards\ncurrent query at demonstration-level. We also design an efficient\nhyperparameter searching strategy for FocusICL based on model perplexity of\ndemonstrations. Comprehensive experiments validate that FocusICL achieves an\naverage performance improvement of 5.2% over vanilla ICL and scales well with\nmany-shot demonstrations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning (ICL) enables large language models (LLMs) to achieve\nrapid task adaptation by learning from demonstrations. With the increase in\navailable context length of LLMs, recent experiments have shown that the\nperformance of ICL does not necessarily scale well in many-shot (demonstration)\nsettings. We theoretically and experimentally confirm that the reason lies in\nmore demonstrations dispersing the model attention from the query, hindering\nits understanding of key content. Inspired by how humans learn from examples,\nwe propose a training-free method FocusICL, which conducts triviality filtering\nto avoid attention being diverted by unimportant contents at token-level and\noperates hierarchical attention to further ensure sufficient attention towards\ncurrent query at demonstration-level. We also design an efficient\nhyperparameter searching strategy for FocusICL based on model perplexity of\ndemonstrations. Comprehensive experiments validate that FocusICL achieves an\naverage performance improvement of 5.2% over vanilla ICL and scales well with\nmany-shot demonstrations."
                },
                "authors": [
                    {
                        "name": "Peiwen Yuan"
                    },
                    {
                        "name": "Shaoxiong Feng"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Xinglin Wang"
                    },
                    {
                        "name": "Yueqi Zhang"
                    },
                    {
                        "name": "Chuyi Tan"
                    },
                    {
                        "name": "Boyuan Pan"
                    },
                    {
                        "name": "Heda Wang"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kan Li"
                    }
                ],
                "author_detail": {
                    "name": "Kan Li"
                },
                "author": "Kan Li",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13986v1",
                "updated": "2024-08-26T02:36:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    2,
                    36,
                    55,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T02:36:55Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    2,
                    36,
                    55,
                    0,
                    239,
                    0
                ],
                "title": "AgentMove: Predicting Human Mobility Anywhere Using Large Language Model\n  based Agentic Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentMove: Predicting Human Mobility Anywhere Using Large Language Model\n  based Agentic Framework"
                },
                "summary": "Human mobility prediction plays a crucial role in various real-world\napplications. Although deep learning based models have shown promising results\nover the past decade, their reliance on extensive private mobility data for\ntraining and their inability to perform zero-shot predictions, have hindered\nfurther advancements. Recently, attempts have been made to apply large language\nmodels (LLMs) to mobility prediction task. However, their performance has been\nconstrained by the absence of a systematic design of workflow. They directly\ngenerate the final output using LLMs, which limits the potential of LLMs to\nuncover complex mobility patterns and underestimates their extensive reserve of\nglobal geospatial knowledge. In this paper, we introduce AgentMove, a\nsystematic agentic prediction framework to achieve generalized mobility\nprediction for any cities worldwide. In AgentMove, we first decompose the\nmobility prediction task into three sub-tasks and then design corresponding\nmodules to complete these subtasks, including spatial-temporal memory for\nindividual mobility pattern mining, world knowledge generator for modeling the\neffects of urban structure and collective knowledge extractor for capturing the\nshared patterns among population. Finally, we combine the results of three\nmodules and conduct a reasoning step to generate the final predictions.\nExtensive experiments on mobility data from two sources in 12 cities\ndemonstrate that AgentMove outperforms the best baseline more than 8% in\nvarious metrics and it shows robust predictions with various LLMs as base and\nalso less geographical bias across cities. Codes and data can be found in\nhttps://github.com/tsinghua-fib-lab/AgentMove.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human mobility prediction plays a crucial role in various real-world\napplications. Although deep learning based models have shown promising results\nover the past decade, their reliance on extensive private mobility data for\ntraining and their inability to perform zero-shot predictions, have hindered\nfurther advancements. Recently, attempts have been made to apply large language\nmodels (LLMs) to mobility prediction task. However, their performance has been\nconstrained by the absence of a systematic design of workflow. They directly\ngenerate the final output using LLMs, which limits the potential of LLMs to\nuncover complex mobility patterns and underestimates their extensive reserve of\nglobal geospatial knowledge. In this paper, we introduce AgentMove, a\nsystematic agentic prediction framework to achieve generalized mobility\nprediction for any cities worldwide. In AgentMove, we first decompose the\nmobility prediction task into three sub-tasks and then design corresponding\nmodules to complete these subtasks, including spatial-temporal memory for\nindividual mobility pattern mining, world knowledge generator for modeling the\neffects of urban structure and collective knowledge extractor for capturing the\nshared patterns among population. Finally, we combine the results of three\nmodules and conduct a reasoning step to generate the final predictions.\nExtensive experiments on mobility data from two sources in 12 cities\ndemonstrate that AgentMove outperforms the best baseline more than 8% in\nvarious metrics and it shows robust predictions with various LLMs as base and\nalso less geographical bias across cities. Codes and data can be found in\nhttps://github.com/tsinghua-fib-lab/AgentMove."
                },
                "authors": [
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Yuwei Du"
                    },
                    {
                        "name": "Jie Zhao"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13985v1",
                "updated": "2024-08-26T02:35:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    2,
                    35,
                    37,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T02:35:37Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    2,
                    35,
                    37,
                    0,
                    239,
                    0
                ],
                "title": "TF-Attack: Transferable and Fast Adversarial Attacks on Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TF-Attack: Transferable and Fast Adversarial Attacks on Large Language\n  Models"
                },
                "summary": "With the great advancements in large language models (LLMs), adversarial\nattacks against LLMs have recently attracted increasing attention. We found\nthat pre-existing adversarial attack methodologies exhibit limited\ntransferability and are notably inefficient, particularly when applied to LLMs.\nIn this paper, we analyze the core mechanisms of previous predominant\nadversarial attack methods, revealing that 1) the distributions of importance\nscore differ markedly among victim models, restricting the transferability; 2)\nthe sequential attack processes induces substantial time overheads. Based on\nthe above two insights, we introduce a new scheme, named TF-Attack, for\nTransferable and Fast adversarial attacks on LLMs. TF-Attack employs an\nexternal LLM as a third-party overseer rather than the victim model to identify\ncritical units within sentences. Moreover, TF-Attack introduces the concept of\nImportance Level, which allows for parallel substitutions of attacks. We\nconduct extensive experiments on 6 widely adopted benchmarks, evaluating the\nproposed method through both automatic and human metrics. Results show that our\nmethod consistently surpasses previous methods in transferability and delivers\nsignificant speed improvements, up to 20 times faster than earlier attack\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the great advancements in large language models (LLMs), adversarial\nattacks against LLMs have recently attracted increasing attention. We found\nthat pre-existing adversarial attack methodologies exhibit limited\ntransferability and are notably inefficient, particularly when applied to LLMs.\nIn this paper, we analyze the core mechanisms of previous predominant\nadversarial attack methods, revealing that 1) the distributions of importance\nscore differ markedly among victim models, restricting the transferability; 2)\nthe sequential attack processes induces substantial time overheads. Based on\nthe above two insights, we introduce a new scheme, named TF-Attack, for\nTransferable and Fast adversarial attacks on LLMs. TF-Attack employs an\nexternal LLM as a third-party overseer rather than the victim model to identify\ncritical units within sentences. Moreover, TF-Attack introduces the concept of\nImportance Level, which allows for parallel substitutions of attacks. We\nconduct extensive experiments on 6 widely adopted benchmarks, evaluating the\nproposed method through both automatic and human metrics. Results show that our\nmethod consistently surpasses previous methods in transferability and delivers\nsignificant speed improvements, up to 20 times faster than earlier attack\nstrategies."
                },
                "authors": [
                    {
                        "name": "Zelin Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Mingming Yang"
                    },
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.04560v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.04560v3",
                "updated": "2024-08-26T02:11:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    2,
                    11,
                    58,
                    0,
                    239,
                    0
                ],
                "published": "2024-01-09T13:56:37Z",
                "published_parsed": [
                    2024,
                    1,
                    9,
                    13,
                    56,
                    37,
                    1,
                    9,
                    0
                ],
                "title": "Phase-shifted remote photoplethysmography for estimating heart rate and\n  blood pressure from facial video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phase-shifted remote photoplethysmography for estimating heart rate and\n  blood pressure from facial video"
                },
                "summary": "Human health can be critically affected by cardiovascular diseases, such as\nhypertension, arrhythmias, and stroke. Heart rate and blood pressure are\nimportant biometric information for the monitoring of cardiovascular system and\nearly diagnosis of cardiovascular diseases. Existing methods for estimating the\nheart rate are based on electrocardiography and photoplethyomography, which\nrequire contacting the sensor to the skin surface. Moreover, catheter and\ncuff-based methods for measuring blood pressure cause inconvenience and have\nlimited applicability. Therefore, in this thesis, we propose a vision-based\nmethod for estimating the heart rate and blood pressure. This thesis proposes a\n2-stage deep learning framework consisting of a dual remote\nphotoplethysmography network (DRP-Net) and bounded blood pressure network\n(BBP-Net). In the first stage, DRP-Net infers remote photoplethysmography\n(rPPG) signals for the acral and facial regions, and these phase-shifted rPPG\nsignals are utilized to estimate the heart rate. In the second stage, BBP-Net\nintegrates temporal features and analyzes phase discrepancy between the acral\nand facial rPPG signals to estimate SBP and DBP values. To improve the accuracy\nof estimating the heart rate, we employed a data augmentation method based on a\nframe interpolation model. Moreover, we designed BBP-Net to infer blood\npressure within a predefined range by incorporating a scaled sigmoid function.\nOur method resulted in estimating the heart rate with the mean absolute error\n(MAE) of 1.78 BPM, reducing the MAE by 34.31 % compared to the recent method,\non the MMSE-HR dataset. The MAE for estimating the systolic blood pressure\n(SBP) and diastolic blood pressure (DBP) were 10.19 mmHg and 7.09 mmHg. On the\nV4V dataset, the MAE for the heart rate, SBP, and DBP were 3.83 BPM, 13.64\nmmHg, and 9.4 mmHg, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human health can be critically affected by cardiovascular diseases, such as\nhypertension, arrhythmias, and stroke. Heart rate and blood pressure are\nimportant biometric information for the monitoring of cardiovascular system and\nearly diagnosis of cardiovascular diseases. Existing methods for estimating the\nheart rate are based on electrocardiography and photoplethyomography, which\nrequire contacting the sensor to the skin surface. Moreover, catheter and\ncuff-based methods for measuring blood pressure cause inconvenience and have\nlimited applicability. Therefore, in this thesis, we propose a vision-based\nmethod for estimating the heart rate and blood pressure. This thesis proposes a\n2-stage deep learning framework consisting of a dual remote\nphotoplethysmography network (DRP-Net) and bounded blood pressure network\n(BBP-Net). In the first stage, DRP-Net infers remote photoplethysmography\n(rPPG) signals for the acral and facial regions, and these phase-shifted rPPG\nsignals are utilized to estimate the heart rate. In the second stage, BBP-Net\nintegrates temporal features and analyzes phase discrepancy between the acral\nand facial rPPG signals to estimate SBP and DBP values. To improve the accuracy\nof estimating the heart rate, we employed a data augmentation method based on a\nframe interpolation model. Moreover, we designed BBP-Net to infer blood\npressure within a predefined range by incorporating a scaled sigmoid function.\nOur method resulted in estimating the heart rate with the mean absolute error\n(MAE) of 1.78 BPM, reducing the MAE by 34.31 % compared to the recent method,\non the MMSE-HR dataset. The MAE for estimating the systolic blood pressure\n(SBP) and diastolic blood pressure (DBP) were 10.19 mmHg and 7.09 mmHg. On the\nV4V dataset, the MAE for the heart rate, SBP, and DBP were 3.83 BPM, 13.64\nmmHg, and 9.4 mmHg, respectively."
                },
                "authors": [
                    {
                        "name": "Gyutae Hwang"
                    },
                    {
                        "name": "Sang Jun Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sang Jun Lee"
                },
                "author": "Sang Jun Lee",
                "arxiv_comment": "33 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.04560v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.04560v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.03328v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.03328v3",
                "updated": "2024-08-26T02:05:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    2,
                    5,
                    37,
                    0,
                    239,
                    0
                ],
                "published": "2023-10-05T05:55:06Z",
                "published_parsed": [
                    2023,
                    10,
                    5,
                    5,
                    55,
                    6,
                    3,
                    278,
                    0
                ],
                "title": "Reformulating Domain Adaptation of Large Language Models as\n  Adapt-Retrieve-Revise: A Case Study on Chinese Legal Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reformulating Domain Adaptation of Large Language Models as\n  Adapt-Retrieve-Revise: A Case Study on Chinese Legal Domain"
                },
                "summary": "While large language models (LLMs) like GPT-4 have recently demonstrated\nastonishing zero-shot capabilities in general domain tasks, they often generate\ncontent with hallucinations in specific domains such as Chinese law, hindering\ntheir application in these areas. This is typically due to the absence of\ntraining data that encompasses such a specific domain, preventing GPT-4 from\nacquiring in-domain knowledge. A pressing challenge is that it's not plausible\nto continue training LLMs of such scale on in-domain data.\n  This paper introduces a simple and effective domain adaptation framework for\nGPT-4 by reformulating generation as an \\textbf{adapt-retrieve-revise} process.\nThe initial step is to \\textbf{adapt} an affordable 7B LLM to the target domain\nby continuing learning on in-domain data. When solving a task, we leverage the\nadapted LLM to generate a draft answer given a task query. Then, the draft\nanswer will be used to \\textbf{retrieve} supporting evidence candidates from an\nexternal in-domain knowledge base. Finally, the draft answer and retrieved\nevidence are concatenated into a whole prompt to let GPT-4 assess the evidence\nand \\textbf{revise} the draft answer to generate the final answer.\n  Our proposal combines the advantages of the efficiency of adapting a smaller\n7B model with the evidence-assessing capability of GPT-4 and effectively\nprevents GPT-4 from generating hallucinatory content. In the zero-shot setting\nof four Chinese legal tasks, our method improves accuracy by 33.3\\% compared to\nthe direct generation by GPT-4. When compared to two stronger retrieval-based\nbaselines, our method outperforms them by 15.4\\% and 23.9\\%. Our code will be\nreleased",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) like GPT-4 have recently demonstrated\nastonishing zero-shot capabilities in general domain tasks, they often generate\ncontent with hallucinations in specific domains such as Chinese law, hindering\ntheir application in these areas. This is typically due to the absence of\ntraining data that encompasses such a specific domain, preventing GPT-4 from\nacquiring in-domain knowledge. A pressing challenge is that it's not plausible\nto continue training LLMs of such scale on in-domain data.\n  This paper introduces a simple and effective domain adaptation framework for\nGPT-4 by reformulating generation as an \\textbf{adapt-retrieve-revise} process.\nThe initial step is to \\textbf{adapt} an affordable 7B LLM to the target domain\nby continuing learning on in-domain data. When solving a task, we leverage the\nadapted LLM to generate a draft answer given a task query. Then, the draft\nanswer will be used to \\textbf{retrieve} supporting evidence candidates from an\nexternal in-domain knowledge base. Finally, the draft answer and retrieved\nevidence are concatenated into a whole prompt to let GPT-4 assess the evidence\nand \\textbf{revise} the draft answer to generate the final answer.\n  Our proposal combines the advantages of the efficiency of adapting a smaller\n7B model with the evidence-assessing capability of GPT-4 and effectively\nprevents GPT-4 from generating hallucinatory content. In the zero-shot setting\nof four Chinese legal tasks, our method improves accuracy by 33.3\\% compared to\nthe direct generation by GPT-4. When compared to two stronger retrieval-based\nbaselines, our method outperforms them by 15.4\\% and 23.9\\%. Our code will be\nreleased"
                },
                "authors": [
                    {
                        "name": "Zhen wan"
                    },
                    {
                        "name": "Yating Zhang"
                    },
                    {
                        "name": "Yexiang Wang"
                    },
                    {
                        "name": "Fei Cheng"
                    },
                    {
                        "name": "Sadao Kurohashi"
                    }
                ],
                "author_detail": {
                    "name": "Sadao Kurohashi"
                },
                "author": "Sadao Kurohashi",
                "arxiv_comment": "Accepted by ACL 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.03328v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.03328v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13977v1",
                "updated": "2024-08-26T01:50:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    1,
                    50,
                    29,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T01:50:29Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    1,
                    50,
                    29,
                    0,
                    239,
                    0
                ],
                "title": "Say Your Reason: Extract Contextual Rules In Situ for Context-aware\n  Service Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Say Your Reason: Extract Contextual Rules In Situ for Context-aware\n  Service Recommendation"
                },
                "summary": "This paper introduces SayRea, an interactive system that facilitates the\nextraction of contextual rules for personalized context-aware service\nrecommendations in mobile scenarios. The system monitors a user's execution of\nregistered services on their smartphones (via accessibility service) and\nproactively requests a single-sentence reason from the user. By utilizing a\nLarge Language Model (LLM), SayRea parses the reason and predicts contextual\nrelationships between the observed service and potential contexts (such as\nsetting the alarm clock deep in the evening). In this way, SayRea can\nsignificantly reduce the cognitive load on users in anticipating future needs\nand selecting contextual attributes. A 10-day field study involving 20\nparticipants showed that SayRea accumulated an average of 62.4 rules per user\nand successfully recommended 45% of service usage. The participants provided\npositive feedback on the system's usability, interpretability, and\ncontrollability. The findings highlight SayRea's effectiveness in personalized\nservice recommendations and its potential to enhance user experience in mobile\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces SayRea, an interactive system that facilitates the\nextraction of contextual rules for personalized context-aware service\nrecommendations in mobile scenarios. The system monitors a user's execution of\nregistered services on their smartphones (via accessibility service) and\nproactively requests a single-sentence reason from the user. By utilizing a\nLarge Language Model (LLM), SayRea parses the reason and predicts contextual\nrelationships between the observed service and potential contexts (such as\nsetting the alarm clock deep in the evening). In this way, SayRea can\nsignificantly reduce the cognitive load on users in anticipating future needs\nand selecting contextual attributes. A 10-day field study involving 20\nparticipants showed that SayRea accumulated an average of 62.4 rules per user\nand successfully recommended 45% of service usage. The participants provided\npositive feedback on the system's usability, interpretability, and\ncontrollability. The findings highlight SayRea's effectiveness in personalized\nservice recommendations and its potential to enhance user experience in mobile\nscenarios."
                },
                "authors": [
                    {
                        "name": "Yuxuan Li"
                    },
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Lihang Pan"
                    },
                    {
                        "name": "Chun Yu"
                    },
                    {
                        "name": "Yuanchun Shi"
                    }
                ],
                "author_detail": {
                    "name": "Yuanchun Shi"
                },
                "author": "Yuanchun Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13976v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13976v2",
                "updated": "2024-08-27T06:49:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    6,
                    49,
                    19,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-26T01:48:57Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    1,
                    48,
                    57,
                    0,
                    239,
                    0
                ],
                "title": "Sifting through the Chaff: On Utilizing Execution Feedback for Ranking\n  the Generated Code Candidates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sifting through the Chaff: On Utilizing Execution Feedback for Ranking\n  the Generated Code Candidates"
                },
                "summary": "Large Language Models (LLMs), such as GPT-4, StarCoder, and CodeLlama, are\ntransforming the way developers approach programming by automatically\ngenerating code based on given natural language descriptions. Despite\nadvancements, generating syntactically and semantically correct code remains\nchallenging, especially for complex programming tasks. Typically, individuals\ngenerate multiple candidate solutions using LLMs to increase the likelihood of\nproducing correct code. However, selecting the correct code from these\ncandidates-a process known as code ranking-remains a major challenge. Current\nresearch on code ranking can be categorized into execution-based and\nnon-execution-based methods. Execution-based methods, although effective,\nencounter notable limitations, such as scarcity of quality unit tests and\nsecurity risks. Non-execution-based methods like CodeRanker, which rely solely\non classification labels to train a code ranker, struggle to capture subtle\nerrors and provide detailed error insights. Recognizing the strengths and\nlimitations of both approaches, we propose a new method. The key insight of our\nwork is that an effective code ranker is expected to genuinely comprehend the\nunderlying causes of erroneous code, as relying solely on classification labels\nis insufficient. Inspired by this, this paper puts forward RankEF, an\ninnovative approach for code ranking that leverages execution feedback. RankEF\nemploys multi-task learning to integrate code classification with execution\nfeedback generation. This approach enables the model to understand the reasons\nbehind incorrect code, distinguishing between correct and incorrect solutions\nwithout the need to execute the code during the ranking phase. Experiments on\nthree code generation benchmarks demonstrate that RankEF significantly\noutperforms the state-of-the-art CodeRanker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT-4, StarCoder, and CodeLlama, are\ntransforming the way developers approach programming by automatically\ngenerating code based on given natural language descriptions. Despite\nadvancements, generating syntactically and semantically correct code remains\nchallenging, especially for complex programming tasks. Typically, individuals\ngenerate multiple candidate solutions using LLMs to increase the likelihood of\nproducing correct code. However, selecting the correct code from these\ncandidates-a process known as code ranking-remains a major challenge. Current\nresearch on code ranking can be categorized into execution-based and\nnon-execution-based methods. Execution-based methods, although effective,\nencounter notable limitations, such as scarcity of quality unit tests and\nsecurity risks. Non-execution-based methods like CodeRanker, which rely solely\non classification labels to train a code ranker, struggle to capture subtle\nerrors and provide detailed error insights. Recognizing the strengths and\nlimitations of both approaches, we propose a new method. The key insight of our\nwork is that an effective code ranker is expected to genuinely comprehend the\nunderlying causes of erroneous code, as relying solely on classification labels\nis insufficient. Inspired by this, this paper puts forward RankEF, an\ninnovative approach for code ranking that leverages execution feedback. RankEF\nemploys multi-task learning to integrate code classification with execution\nfeedback generation. This approach enables the model to understand the reasons\nbehind incorrect code, distinguishing between correct and incorrect solutions\nwithout the need to execute the code during the ranking phase. Experiments on\nthree code generation benchmarks demonstrate that RankEF significantly\noutperforms the state-of-the-art CodeRanker."
                },
                "authors": [
                    {
                        "name": "Zhihong Sun"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Ge Li"
                    },
                    {
                        "name": "Chen Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Chen Lyu"
                },
                "author": "Chen Lyu",
                "arxiv_comment": "Accepted by the 39th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13976v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13976v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.02310v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.02310v2",
                "updated": "2024-08-26T01:39:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    1,
                    39,
                    49,
                    0,
                    239,
                    0
                ],
                "published": "2023-02-05T05:39:56Z",
                "published_parsed": [
                    2023,
                    2,
                    5,
                    5,
                    39,
                    56,
                    6,
                    36,
                    0
                ],
                "title": "$\\ell_1$-penalized Multinomial Regression: Estimation, inference, and\n  prediction, with an application to risk factor identification for different\n  dementia subtypes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\ell_1$-penalized Multinomial Regression: Estimation, inference, and\n  prediction, with an application to risk factor identification for different\n  dementia subtypes"
                },
                "summary": "High-dimensional multinomial regression models are very useful in practice\nbut have received less research attention than logistic regression models,\nespecially from the perspective of statistical inference. In this work, we\nanalyze the estimation and prediction error of the contrast-based\n$\\ell_1$-penalized multinomial regression model and extend the debiasing method\nto the multinomial case, providing a valid confidence interval for each\ncoefficient and $p$-value of the individual hypothesis test. We also examine\ncases of model misspecification and non-identically distributed data to\ndemonstrate the robustness of our method when some assumptions are violated. We\napply the debiasing method to identify important predictors in the progression\ninto dementia of different subtypes. Results from extensive simulations show\nthe superiority of the debiasing method compared to other inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-dimensional multinomial regression models are very useful in practice\nbut have received less research attention than logistic regression models,\nespecially from the perspective of statistical inference. In this work, we\nanalyze the estimation and prediction error of the contrast-based\n$\\ell_1$-penalized multinomial regression model and extend the debiasing method\nto the multinomial case, providing a valid confidence interval for each\ncoefficient and $p$-value of the individual hypothesis test. We also examine\ncases of model misspecification and non-identically distributed data to\ndemonstrate the robustness of our method when some assumptions are violated. We\napply the debiasing method to identify important predictors in the progression\ninto dementia of different subtypes. Results from extensive simulations show\nthe superiority of the debiasing method compared to other inference methods."
                },
                "authors": [
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Henry Rusinek"
                    },
                    {
                        "name": "Arjun V. Masurkar"
                    },
                    {
                        "name": "Yang Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Feng"
                },
                "author": "Yang Feng",
                "arxiv_comment": "48 pages, 5 figures, 44 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.02310v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.02310v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12574v2",
                "updated": "2024-08-25T23:58:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    23,
                    58,
                    25,
                    6,
                    238,
                    0
                ],
                "published": "2024-08-22T17:41:45Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    41,
                    45,
                    3,
                    235,
                    0
                ],
                "title": "MuMA-ToM: Multi-modal Multi-Agent Theory of Mind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MuMA-ToM: Multi-modal Multi-Agent Theory of Mind"
                },
                "summary": "Understanding people's social interactions in complex real-world scenarios\noften relies on intricate mental reasoning. To truly understand how and why\npeople interact with one another, we must infer the underlying mental states\nthat give rise to the social interactions, i.e., Theory of Mind reasoning in\nmulti-agent interactions. Additionally, social interactions are often\nmulti-modal -- we can watch people's actions, hear their conversations, and/or\nread about their past behaviors. For AI systems to successfully and safely\ninteract with people in real-world environments, they also need to understand\npeople's mental states as well as their inferences about each other's mental\nstates based on multi-modal information about their interactions. For this, we\nintroduce MuMA-ToM, a Multi-modal Multi-Agent Theory of Mind benchmark.\nMuMA-ToM is the first multi-modal Theory of Mind benchmark that evaluates\nmental reasoning in embodied multi-agent interactions. In MuMA-ToM, we provide\nvideo and text descriptions of people's multi-modal behavior in realistic\nhousehold environments. Based on the context, we then ask questions about\npeople's goals, beliefs, and beliefs about others' goals. We validated MuMA-ToM\nin a human experiment and provided a human baseline. We also proposed a novel\nmulti-modal, multi-agent ToM model, LIMP (Language model-based Inverse\nMulti-agent Planning). Our experimental results show that LIMP significantly\noutperforms state-of-the-art methods, including large multi-modal models (e.g.,\nGPT-4o, Gemini-1.5 Pro) and a recent multi-modal ToM model, BIP-ALM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding people's social interactions in complex real-world scenarios\noften relies on intricate mental reasoning. To truly understand how and why\npeople interact with one another, we must infer the underlying mental states\nthat give rise to the social interactions, i.e., Theory of Mind reasoning in\nmulti-agent interactions. Additionally, social interactions are often\nmulti-modal -- we can watch people's actions, hear their conversations, and/or\nread about their past behaviors. For AI systems to successfully and safely\ninteract with people in real-world environments, they also need to understand\npeople's mental states as well as their inferences about each other's mental\nstates based on multi-modal information about their interactions. For this, we\nintroduce MuMA-ToM, a Multi-modal Multi-Agent Theory of Mind benchmark.\nMuMA-ToM is the first multi-modal Theory of Mind benchmark that evaluates\nmental reasoning in embodied multi-agent interactions. In MuMA-ToM, we provide\nvideo and text descriptions of people's multi-modal behavior in realistic\nhousehold environments. Based on the context, we then ask questions about\npeople's goals, beliefs, and beliefs about others' goals. We validated MuMA-ToM\nin a human experiment and provided a human baseline. We also proposed a novel\nmulti-modal, multi-agent ToM model, LIMP (Language model-based Inverse\nMulti-agent Planning). Our experimental results show that LIMP significantly\noutperforms state-of-the-art methods, including large multi-modal models (e.g.,\nGPT-4o, Gemini-1.5 Pro) and a recent multi-modal ToM model, BIP-ALM."
                },
                "authors": [
                    {
                        "name": "Haojun Shi"
                    },
                    {
                        "name": "Suyu Ye"
                    },
                    {
                        "name": "Xinyu Fang"
                    },
                    {
                        "name": "Chuanyang Jin"
                    },
                    {
                        "name": "Leyla Isik"
                    },
                    {
                        "name": "Yen-Ling Kuo"
                    },
                    {
                        "name": "Tianmin Shu"
                    }
                ],
                "author_detail": {
                    "name": "Tianmin Shu"
                },
                "author": "Tianmin Shu",
                "arxiv_comment": "Project website: https://scai.cs.jhu.edu/projects/MuMA-ToM/ Code:\n  https://github.com/SCAI-JHU/MuMA-ToM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13960v2",
                "updated": "2024-08-27T15:06:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    6,
                    17,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-25T23:48:11Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    23,
                    48,
                    11,
                    6,
                    238,
                    0
                ],
                "title": "Time Series Analysis for Education: Methods, Applications, and Future\n  Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time Series Analysis for Education: Methods, Applications, and Future\n  Directions"
                },
                "summary": "Recent advancements in the collection and analysis of sequential educational\ndata have brought time series analysis to a pivotal position in educational\nresearch, highlighting its essential role in facilitating data-driven\ndecision-making. However, there is a lack of comprehensive summaries that\nconsolidate these advancements. To the best of our knowledge, this paper is the\nfirst to provide a comprehensive review of time series analysis techniques\nspecifically within the educational context. We begin by exploring the\nlandscape of educational data analytics, categorizing various data sources and\ntypes relevant to education. We then review four prominent time series\nmethods-forecasting, classification, clustering, and anomaly\ndetection-illustrating their specific application points in educational\nsettings. Subsequently, we present a range of educational scenarios and\napplications, focusing on how these methods are employed to address diverse\neducational tasks, which highlights the practical integration of multiple time\nseries methods to solve complex educational problems. Finally, we conclude with\na discussion on future directions, including personalized learning analytics,\nmultimodal data fusion, and the role of large language models (LLMs) in\neducational time series. The contributions of this paper include a detailed\ntaxonomy of educational data, a synthesis of time series techniques with\nspecific educational applications, and a forward-looking perspective on\nemerging trends and future research opportunities in educational analysis. The\nrelated papers and resources are available and regularly updated at the project\npage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in the collection and analysis of sequential educational\ndata have brought time series analysis to a pivotal position in educational\nresearch, highlighting its essential role in facilitating data-driven\ndecision-making. However, there is a lack of comprehensive summaries that\nconsolidate these advancements. To the best of our knowledge, this paper is the\nfirst to provide a comprehensive review of time series analysis techniques\nspecifically within the educational context. We begin by exploring the\nlandscape of educational data analytics, categorizing various data sources and\ntypes relevant to education. We then review four prominent time series\nmethods-forecasting, classification, clustering, and anomaly\ndetection-illustrating their specific application points in educational\nsettings. Subsequently, we present a range of educational scenarios and\napplications, focusing on how these methods are employed to address diverse\neducational tasks, which highlights the practical integration of multiple time\nseries methods to solve complex educational problems. Finally, we conclude with\na discussion on future directions, including personalized learning analytics,\nmultimodal data fusion, and the role of large language models (LLMs) in\neducational time series. The contributions of this paper include a detailed\ntaxonomy of educational data, a synthesis of time series techniques with\nspecific educational applications, and a forward-looking perspective on\nemerging trends and future research opportunities in educational analysis. The\nrelated papers and resources are available and regularly updated at the project\npage."
                },
                "authors": [
                    {
                        "name": "Shengzhong Mao"
                    },
                    {
                        "name": "Chaoli Zhang"
                    },
                    {
                        "name": "Yichi Song"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Xiao-Jun Zeng"
                    },
                    {
                        "name": "Zenglin Xu"
                    },
                    {
                        "name": "Qingsong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Qingsong Wen"
                },
                "author": "Qingsong Wen",
                "arxiv_comment": "24 pages, 3 figures, 6 tables, project page: see\n  https://github.com/ai-for-edu/time-series-analysis-for-education",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13959v1",
                "updated": "2024-08-25T23:46:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    23,
                    46,
                    35,
                    6,
                    238,
                    0
                ],
                "published": "2024-08-25T23:46:35Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    23,
                    46,
                    35,
                    6,
                    238,
                    0
                ],
                "title": "Bidirectional Awareness Induction in Autoregressive Seq2Seq Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional Awareness Induction in Autoregressive Seq2Seq Models"
                },
                "summary": "Autoregressive Sequence-To-Sequence models are the foundation of many Deep\nLearning achievements in major research fields such as Vision and Natural\nLanguage Processing. Despite that, they still present significant limitations.\nFor instance, when errors occur in the early steps of the prediction, the whole\noutput is severely affected. Such reliance on previously predicted tokens and\nthe inherent computational unfriendliness of sequential algorithms, motivated\nresearchers to explore different architectures and methods in the search for\nbidirectional approaches. In this work, we introduce the Bidirectional\nAwareness Induction (BAI), a training method that leverages a subset of\nelements in the network, the Pivots, to perform bidirectional learning without\nbreaking the autoregressive constraints. To showcase its flexibility, we apply\nthe method to three architectures, the Transformer, ExpansionNet v2 and GPT,\nthen perform experiments over three tasks. Experimental results showcase BAI's\neffectiveness on all selected tasks and architectures. In particular, we\nobserved an increase of up to 2.4 CIDEr in Image-Captioning, 4.96 BLEU in\nNeural Machine Translation, and 1.16 ROUGE in Text Summarization compared to\nthe respective baselines. Notably, BAI not only has a positive impact on models\ntrained from scratch but on pre-trained models as well. Such an aspect,\ncombined with the absence of architectural requirements synergizes well with\nthe current trend of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Sequence-To-Sequence models are the foundation of many Deep\nLearning achievements in major research fields such as Vision and Natural\nLanguage Processing. Despite that, they still present significant limitations.\nFor instance, when errors occur in the early steps of the prediction, the whole\noutput is severely affected. Such reliance on previously predicted tokens and\nthe inherent computational unfriendliness of sequential algorithms, motivated\nresearchers to explore different architectures and methods in the search for\nbidirectional approaches. In this work, we introduce the Bidirectional\nAwareness Induction (BAI), a training method that leverages a subset of\nelements in the network, the Pivots, to perform bidirectional learning without\nbreaking the autoregressive constraints. To showcase its flexibility, we apply\nthe method to three architectures, the Transformer, ExpansionNet v2 and GPT,\nthen perform experiments over three tasks. Experimental results showcase BAI's\neffectiveness on all selected tasks and architectures. In particular, we\nobserved an increase of up to 2.4 CIDEr in Image-Captioning, 4.96 BLEU in\nNeural Machine Translation, and 1.16 ROUGE in Text Summarization compared to\nthe respective baselines. Notably, BAI not only has a positive impact on models\ntrained from scratch but on pre-trained models as well. Such an aspect,\ncombined with the absence of architectural requirements synergizes well with\nthe current trend of LLMs."
                },
                "authors": [
                    {
                        "name": "Jia Cheng Hu"
                    },
                    {
                        "name": "Roberto Cavicchioli"
                    },
                    {
                        "name": "Alessandro Capotondi"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Capotondi"
                },
                "author": "Alessandro Capotondi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13949v1",
                "updated": "2024-08-25T22:07:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    22,
                    7,
                    6,
                    6,
                    238,
                    0
                ],
                "published": "2024-08-25T22:07:06Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    22,
                    7,
                    6,
                    6,
                    238,
                    0
                ],
                "title": "Inference on Consensus Ranking of Distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on Consensus Ranking of Distributions"
                },
                "summary": "Instead of testing for unanimous agreement, I propose learning how broad of a\nconsensus favors one distribution over another (of earnings, productivity,\nasset returns, test scores, etc.). Specifically, given a sample from each of\ntwo distributions, I propose statistical inference methods to learn about the\nset of utility functions for which the first distribution has higher expected\nutility than the second distribution. With high probability, an \"inner\"\nconfidence set is contained within this true set, while an \"outer\" confidence\nset contains the true set. Such confidence sets can be formed by inverting a\nproposed multiple testing procedure that controls the familywise error rate.\nTheoretical justification comes from empirical process results, given that very\nlarge classes of utility functions are generally Donsker (subject to finite\nmoments). The theory additionally justifies a uniform (over utility functions)\nconfidence band of expected utility differences, as well as tests with a\nutility-based \"restricted stochastic dominance\" as either the null or\nalternative hypothesis. Simulated and empirical examples illustrate the\nmethodology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instead of testing for unanimous agreement, I propose learning how broad of a\nconsensus favors one distribution over another (of earnings, productivity,\nasset returns, test scores, etc.). Specifically, given a sample from each of\ntwo distributions, I propose statistical inference methods to learn about the\nset of utility functions for which the first distribution has higher expected\nutility than the second distribution. With high probability, an \"inner\"\nconfidence set is contained within this true set, while an \"outer\" confidence\nset contains the true set. Such confidence sets can be formed by inverting a\nproposed multiple testing procedure that controls the familywise error rate.\nTheoretical justification comes from empirical process results, given that very\nlarge classes of utility functions are generally Donsker (subject to finite\nmoments). The theory additionally justifies a uniform (over utility functions)\nconfidence band of expected utility differences, as well as tests with a\nutility-based \"restricted stochastic dominance\" as either the null or\nalternative hypothesis. Simulated and empirical examples illustrate the\nmethodology."
                },
                "authors": [
                    {
                        "name": "David M. Kaplan"
                    }
                ],
                "author_detail": {
                    "name": "David M. Kaplan"
                },
                "author": "David M. Kaplan",
                "arxiv_doi": "10.1080/07350015.2023.2252040",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1080/07350015.2023.2252040",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.13949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted manuscript",
                "arxiv_journal_ref": "Journal of Business & Economic Statistics 42 (2024) 839-850",
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13945v1",
                "updated": "2024-08-25T21:49:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    21,
                    49,
                    10,
                    6,
                    238,
                    0
                ],
                "published": "2024-08-25T21:49:10Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    21,
                    49,
                    10,
                    6,
                    238,
                    0
                ],
                "title": "Personalized Topology-Informed 12-Lead ECG Electrode Localization from\n  Incomplete Cardiac MRIs for Efficient Cardiac Digital Twins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Topology-Informed 12-Lead ECG Electrode Localization from\n  Incomplete Cardiac MRIs for Efficient Cardiac Digital Twins"
                },
                "summary": "Cardiac digital twins (CDTs) offer personalized \\textit{in-silico} cardiac\nrepresentations for the inference of multi-scale properties tied to cardiac\nmechanisms. The creation of CDTs requires precise information about the\nelectrode position on the torso, especially for the personalized\nelectrocardiogram (ECG) calibration. However, current studies commonly rely on\nadditional acquisition of torso imaging and manual/semi-automatic methods for\nECG electrode localization. In this study, we propose a novel and efficient\ntopology-informed model to fully automatically extract personalized ECG\nelectrode locations from 2D clinically standard cardiac MRIs. Specifically, we\nobtain the sparse torso contours from the cardiac MRIs and then localize the\nelectrodes from the contours. Cardiac MRIs aim at imaging of the heart instead\nof the torso, leading to incomplete torso geometry within the imaging. To\ntackle the missing topology, we incorporate the electrodes as a subset of the\nkeypoints, which can be explicitly aligned with the 3D torso topology. The\nexperimental results demonstrate that the proposed model outperforms the\ntime-consuming conventional method in terms of accuracy (Euclidean distance:\n$1.24 \\pm 0.293$ cm vs. $1.48 \\pm 0.362$ cm) and efficiency ($2$~s vs.\n$30$-$35$~min). We further demonstrate the effectiveness of using the detected\nelectrodes for \\textit{in-silico} ECG simulation, highlighting their potential\nfor creating accurate and efficient CDT models. The code will be released\npublicly after the manuscript is accepted for publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cardiac digital twins (CDTs) offer personalized \\textit{in-silico} cardiac\nrepresentations for the inference of multi-scale properties tied to cardiac\nmechanisms. The creation of CDTs requires precise information about the\nelectrode position on the torso, especially for the personalized\nelectrocardiogram (ECG) calibration. However, current studies commonly rely on\nadditional acquisition of torso imaging and manual/semi-automatic methods for\nECG electrode localization. In this study, we propose a novel and efficient\ntopology-informed model to fully automatically extract personalized ECG\nelectrode locations from 2D clinically standard cardiac MRIs. Specifically, we\nobtain the sparse torso contours from the cardiac MRIs and then localize the\nelectrodes from the contours. Cardiac MRIs aim at imaging of the heart instead\nof the torso, leading to incomplete torso geometry within the imaging. To\ntackle the missing topology, we incorporate the electrodes as a subset of the\nkeypoints, which can be explicitly aligned with the 3D torso topology. The\nexperimental results demonstrate that the proposed model outperforms the\ntime-consuming conventional method in terms of accuracy (Euclidean distance:\n$1.24 \\pm 0.293$ cm vs. $1.48 \\pm 0.362$ cm) and efficiency ($2$~s vs.\n$30$-$35$~min). We further demonstrate the effectiveness of using the detected\nelectrodes for \\textit{in-silico} ECG simulation, highlighting their potential\nfor creating accurate and efficient CDT models. The code will be released\npublicly after the manuscript is accepted for publication."
                },
                "authors": [
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Hannah Smith"
                    },
                    {
                        "name": "Yilin Lyu"
                    },
                    {
                        "name": "Julia Camps"
                    },
                    {
                        "name": "Blanca Rodriguez"
                    },
                    {
                        "name": "Abhirup Banerjee"
                    },
                    {
                        "name": "Vicente Grau"
                    }
                ],
                "author_detail": {
                    "name": "Vicente Grau"
                },
                "author": "Vicente Grau",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13940v1",
                "updated": "2024-08-25T21:20:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    21,
                    20,
                    17,
                    6,
                    238,
                    0
                ],
                "published": "2024-08-25T21:20:17Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    21,
                    20,
                    17,
                    6,
                    238,
                    0
                ],
                "title": "CoT Rerailer: Enhancing the Reliability of Large Language Models in\n  Complex Reasoning Tasks through Error Detection and Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoT Rerailer: Enhancing the Reliability of Large Language Models in\n  Complex Reasoning Tasks through Error Detection and Correction"
                },
                "summary": "Chain-of-Thought (CoT) prompting enhances Large Language Models (LLMs)\ncomplex reasoning abilities by generating intermediate steps. However, these\nsteps can introduce hallucinations and accumulate errors. We propose the CoT\nRerailer to address these challenges, employing self-consistency and\nmulti-agent debate systems to identify and rectify errors in the reasoning\nprocess. The CoT Rerailer first selects the most logically correct Reasoning\nPath (RP) using consistency checks and critical evaluation by automated agents.\nIt then engages a multi-agent debate system to propose and validate corrections\nto ensure the generation of an error-free intermediate logical path. The\ncorrected steps are then used to generate a revised reasoning chain to further\nreduce hallucinations and enhance answer quality. We demonstrate the\neffectiveness of our approach across diverse question-answering datasets in\nvarious knowledge domains. The CoT Rerailer enhances the reliability of\nLLM-generated reasoning, contributing to more trustworthy AI driven\ndecision-making processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting enhances Large Language Models (LLMs)\ncomplex reasoning abilities by generating intermediate steps. However, these\nsteps can introduce hallucinations and accumulate errors. We propose the CoT\nRerailer to address these challenges, employing self-consistency and\nmulti-agent debate systems to identify and rectify errors in the reasoning\nprocess. The CoT Rerailer first selects the most logically correct Reasoning\nPath (RP) using consistency checks and critical evaluation by automated agents.\nIt then engages a multi-agent debate system to propose and validate corrections\nto ensure the generation of an error-free intermediate logical path. The\ncorrected steps are then used to generate a revised reasoning chain to further\nreduce hallucinations and enhance answer quality. We demonstrate the\neffectiveness of our approach across diverse question-answering datasets in\nvarious knowledge domains. The CoT Rerailer enhances the reliability of\nLLM-generated reasoning, contributing to more trustworthy AI driven\ndecision-making processes."
                },
                "authors": [
                    {
                        "name": "Guangya Wan"
                    },
                    {
                        "name": "Yuqi Wu"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Sheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Li"
                },
                "author": "Sheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.14471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14471v1",
                "updated": "2024-08-26T17:59:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    59,
                    1,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:59:01Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    59,
                    1,
                    0,
                    239,
                    0
                ],
                "title": "A Practitioner's Guide to Continual Multimodal Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Practitioner's Guide to Continual Multimodal Pretraining"
                },
                "summary": "Multimodal foundation models serve numerous applications at the intersection\nof vision and language. Still, despite being pretrained on extensive data, they\nbecome outdated over time. To keep models updated, research into continual\npretraining mainly explores scenarios with either (1) infrequent,\nindiscriminate updates on large-scale new data, or (2) frequent, sample-level\nupdates. However, practical model deployment often operates in the gap between\nthese two limit cases, as real-world applications often demand adaptation to\nspecific subdomains, tasks or concepts -- spread over the entire, varying life\ncycle of a model. In this work, we complement current perspectives on continual\npretraining through a research test bed as well as provide comprehensive\nguidance for effective continual model updates in such scenarios. We first\nintroduce FoMo-in-Flux, a continual multimodal pretraining benchmark with\nrealistic compute constraints and practical deployment requirements,\nconstructed over 63 datasets with diverse visual and semantic coverage. Using\nFoMo-in-Flux, we explore the complex landscape of practical continual\npretraining through multiple perspectives: (1) A data-centric investigation of\ndata mixtures and stream orderings that emulate real-world deployment\nsituations, (2) a method-centric investigation ranging from simple fine-tuning\nand traditional continual learning strategies to parameter-efficient updates\nand model merging, (3) meta learning rate schedules and mechanistic design\nchoices, and (4) the influence of model and compute scaling. Together, our\ninsights provide a practitioner's guide to continual multimodal pretraining for\nreal-world deployment. Our benchmark and code is here:\nhttps://github.com/ExplainableML/fomo_in_flux.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal foundation models serve numerous applications at the intersection\nof vision and language. Still, despite being pretrained on extensive data, they\nbecome outdated over time. To keep models updated, research into continual\npretraining mainly explores scenarios with either (1) infrequent,\nindiscriminate updates on large-scale new data, or (2) frequent, sample-level\nupdates. However, practical model deployment often operates in the gap between\nthese two limit cases, as real-world applications often demand adaptation to\nspecific subdomains, tasks or concepts -- spread over the entire, varying life\ncycle of a model. In this work, we complement current perspectives on continual\npretraining through a research test bed as well as provide comprehensive\nguidance for effective continual model updates in such scenarios. We first\nintroduce FoMo-in-Flux, a continual multimodal pretraining benchmark with\nrealistic compute constraints and practical deployment requirements,\nconstructed over 63 datasets with diverse visual and semantic coverage. Using\nFoMo-in-Flux, we explore the complex landscape of practical continual\npretraining through multiple perspectives: (1) A data-centric investigation of\ndata mixtures and stream orderings that emulate real-world deployment\nsituations, (2) a method-centric investigation ranging from simple fine-tuning\nand traditional continual learning strategies to parameter-efficient updates\nand model merging, (3) meta learning rate schedules and mechanistic design\nchoices, and (4) the influence of model and compute scaling. Together, our\ninsights provide a practitioner's guide to continual multimodal pretraining for\nreal-world deployment. Our benchmark and code is here:\nhttps://github.com/ExplainableML/fomo_in_flux."
                },
                "authors": [
                    {
                        "name": "Karsten Roth"
                    },
                    {
                        "name": "Vishaal Udandarao"
                    },
                    {
                        "name": "Sebastian Dziadzio"
                    },
                    {
                        "name": "Ameya Prabhu"
                    },
                    {
                        "name": "Mehdi Cherti"
                    },
                    {
                        "name": "Oriol Vinyals"
                    },
                    {
                        "name": "Olivier Hénaff"
                    },
                    {
                        "name": "Samuel Albanie"
                    },
                    {
                        "name": "Matthias Bethge"
                    },
                    {
                        "name": "Zeynep Akata"
                    }
                ],
                "author_detail": {
                    "name": "Zeynep Akata"
                },
                "author": "Zeynep Akata",
                "arxiv_comment": "Technical Report. 52 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14470v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14470v2",
                "updated": "2024-08-27T03:56:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    3,
                    56,
                    11,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-26T17:58:53Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    58,
                    53,
                    0,
                    239,
                    0
                ],
                "title": "Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large\n  Language Models"
                },
                "summary": "Fine-tuning large language models (LLMs) on downstream tasks requires\nsubstantial computational resources. A class of parameter-efficient fine-tuning\n(PEFT) aims to mitigate these computational challenges by selectively\nfine-tuning only a small fraction of the model parameters. Although\ncomputationally efficient, these techniques often fail to match the performance\nof fully fine-tuned models, primarily due to inherent biases introduced during\nparameter selection. Traditional selective PEFT techniques use a fixed set of\nparameters based on a predefined budget (a process also known as unmasking),\nfailing to capture parameter importance dynamically and often ending up\nexceeding the budget. We introduce $\\text{ID}^3$, a novel selective PEFT method\nthat calculates parameter importance continually and dynamically unmasks\nparameters by balancing exploration and exploitation in parameter selection.\nOur empirical study on 15 tasks spanning natural language understanding and\ngenerative tasks demonstrates the effectiveness of our method compared to\nfixed-masking-based PEFT techniques. We analytically show that $\\text{ID}^3$\nreduces the number of gradient updates by a factor of two, enhancing\ncomputational efficiency. $\\text{ID}^3$ is robust to random initialization of\nneurons and, therefore, can be seamlessly integrated into existing additive and\nreparametrization-based PEFT modules such as adapters and LoRA for dynamic\nsparsification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) on downstream tasks requires\nsubstantial computational resources. A class of parameter-efficient fine-tuning\n(PEFT) aims to mitigate these computational challenges by selectively\nfine-tuning only a small fraction of the model parameters. Although\ncomputationally efficient, these techniques often fail to match the performance\nof fully fine-tuned models, primarily due to inherent biases introduced during\nparameter selection. Traditional selective PEFT techniques use a fixed set of\nparameters based on a predefined budget (a process also known as unmasking),\nfailing to capture parameter importance dynamically and often ending up\nexceeding the budget. We introduce $\\text{ID}^3$, a novel selective PEFT method\nthat calculates parameter importance continually and dynamically unmasks\nparameters by balancing exploration and exploitation in parameter selection.\nOur empirical study on 15 tasks spanning natural language understanding and\ngenerative tasks demonstrates the effectiveness of our method compared to\nfixed-masking-based PEFT techniques. We analytically show that $\\text{ID}^3$\nreduces the number of gradient updates by a factor of two, enhancing\ncomputational efficiency. $\\text{ID}^3$ is robust to random initialization of\nneurons and, therefore, can be seamlessly integrated into existing additive and\nreparametrization-based PEFT modules such as adapters and LoRA for dynamic\nsparsification."
                },
                "authors": [
                    {
                        "name": "Aradhye Agarwal"
                    },
                    {
                        "name": "Suhas K Ramesh"
                    },
                    {
                        "name": "Ayan Sengupta"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "arxiv_comment": "15 pages, 7 tables, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14470v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14470v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14467v1",
                "updated": "2024-08-26T17:58:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    58,
                    17,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:58:17Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    58,
                    17,
                    0,
                    239,
                    0
                ],
                "title": "Explicit Inductive Inference using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explicit Inductive Inference using Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are reported to hold undesirable attestation\nbias on inference tasks: when asked to predict if a premise P entails a\nhypothesis H, instead of considering H's conditional truthfulness entailed by\nP, LLMs tend to use the out-of-context truth label of H as a fragile proxy. In\nthis paper, we propose a pipeline that exploits this bias to do explicit\ninductive inference. Our pipeline uses an LLM to transform a premise into a set\nof attested alternatives, and then aggregate answers of the derived new\nentailment inquiries to support the original inference prediction. On a\ndirectional predicate entailment benchmark, we demonstrate that by applying\nthis simple pipeline, we can improve the overall performance of LLMs on\ninference and substantially alleviate the impact of their attestation bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are reported to hold undesirable attestation\nbias on inference tasks: when asked to predict if a premise P entails a\nhypothesis H, instead of considering H's conditional truthfulness entailed by\nP, LLMs tend to use the out-of-context truth label of H as a fragile proxy. In\nthis paper, we propose a pipeline that exploits this bias to do explicit\ninductive inference. Our pipeline uses an LLM to transform a premise into a set\nof attested alternatives, and then aggregate answers of the derived new\nentailment inquiries to support the original inference prediction. On a\ndirectional predicate entailment benchmark, we demonstrate that by applying\nthis simple pipeline, we can improve the overall performance of LLMs on\ninference and substantially alleviate the impact of their attestation bias."
                },
                "authors": [
                    {
                        "name": "Tianyang Liu"
                    },
                    {
                        "name": "Tianyi Li"
                    },
                    {
                        "name": "Liang Cheng"
                    },
                    {
                        "name": "Mark Steedman"
                    }
                ],
                "author_detail": {
                    "name": "Mark Steedman"
                },
                "author": "Mark Steedman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11796v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11796v2",
                "updated": "2024-08-26T17:50:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    50,
                    46,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-21T17:38:48Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    38,
                    48,
                    2,
                    234,
                    0
                ],
                "title": "LLM Pruning and Distillation in Practice: The Minitron Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Pruning and Distillation in Practice: The Minitron Approach"
                },
                "summary": "We present a comprehensive report on compressing the Llama 3.1 8B and Mistral\nNeMo 12B models to 4B and 8B parameters, respectively, using pruning and\ndistillation. We explore two distinct pruning strategies: (1) depth pruning and\n(2) joint hidden/attention/MLP (width) pruning, and evaluate the results on\ncommon benchmarks from the LM Evaluation Harness. The models are then aligned\nwith NeMo Aligner and tested in instruct-tuned versions. This approach produces\na compelling 4B model from Llama 3.1 8B and a state-of-the-art\nMistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo\n12B. We found that with no access to the original data, it is beneficial to\nslightly fine-tune teacher models on the distillation dataset. We open-source\nour base model weights on Hugging Face with a permissive license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive report on compressing the Llama 3.1 8B and Mistral\nNeMo 12B models to 4B and 8B parameters, respectively, using pruning and\ndistillation. We explore two distinct pruning strategies: (1) depth pruning and\n(2) joint hidden/attention/MLP (width) pruning, and evaluate the results on\ncommon benchmarks from the LM Evaluation Harness. The models are then aligned\nwith NeMo Aligner and tested in instruct-tuned versions. This approach produces\na compelling 4B model from Llama 3.1 8B and a state-of-the-art\nMistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo\n12B. We found that with no access to the original data, it is beneficial to\nslightly fine-tune teacher models on the distillation dataset. We open-source\nour base model weights on Hugging Face with a permissive license."
                },
                "authors": [
                    {
                        "name": "Sharath Turuvekere Sreenivas"
                    },
                    {
                        "name": "Saurav Muralidharan"
                    },
                    {
                        "name": "Raviraj Joshi"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "Mostofa Patwary"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "arxiv_comment": "v2: Added missing references. Cleaned up runtime performance section",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11796v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11796v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.13840v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.13840v3",
                "updated": "2024-08-26T17:34:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    34,
                    44,
                    0,
                    239,
                    0
                ],
                "published": "2023-06-24T02:25:56Z",
                "published_parsed": [
                    2023,
                    6,
                    24,
                    2,
                    25,
                    56,
                    5,
                    175,
                    0
                ],
                "title": "Beyond Scale: The Diversity Coefficient as a Data Quality Metric for\n  Variability in Natural Language Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Scale: The Diversity Coefficient as a Data Quality Metric for\n  Variability in Natural Language Data"
                },
                "summary": "Current trends in pre-training Large Language Models (LLMs) primarily focus\non the scaling of model and dataset size. While the quality of pre-training\ndata is considered an important factor for training powerful LLMs, it remains a\nnebulous concept that has not been rigorously characterized. To this end, we\npropose a formalization of one key aspect of data quality -- measuring the\nvariability of natural language data -- specifically via a measure we call the\ndiversity coefficient. Our empirical analysis shows that the proposed diversity\ncoefficient aligns with the intuitive properties of diversity and variability,\ne.g., it increases as the number of latent concepts increases. Then, we measure\nthe diversity coefficient of publicly available pre-training datasets and\ndemonstrate that their formal diversity is high compared to theoretical lower\nand upper bounds. Finally, we conduct a comprehensive set of controlled\ninterventional experiments with GPT-2 and LLaMAv2 that demonstrate the\ndiversity coefficient of pre-training data characterizes useful aspects of\ndownstream model evaluation performance -- totaling 44 models of various sizes\n(51M to 7B parameters). We conclude that our formal notion of diversity is an\nimportant aspect of data quality that captures variability and causally leads\nto improved evaluation performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current trends in pre-training Large Language Models (LLMs) primarily focus\non the scaling of model and dataset size. While the quality of pre-training\ndata is considered an important factor for training powerful LLMs, it remains a\nnebulous concept that has not been rigorously characterized. To this end, we\npropose a formalization of one key aspect of data quality -- measuring the\nvariability of natural language data -- specifically via a measure we call the\ndiversity coefficient. Our empirical analysis shows that the proposed diversity\ncoefficient aligns with the intuitive properties of diversity and variability,\ne.g., it increases as the number of latent concepts increases. Then, we measure\nthe diversity coefficient of publicly available pre-training datasets and\ndemonstrate that their formal diversity is high compared to theoretical lower\nand upper bounds. Finally, we conduct a comprehensive set of controlled\ninterventional experiments with GPT-2 and LLaMAv2 that demonstrate the\ndiversity coefficient of pre-training data characterizes useful aspects of\ndownstream model evaluation performance -- totaling 44 models of various sizes\n(51M to 7B parameters). We conclude that our formal notion of diversity is an\nimportant aspect of data quality that captures variability and causally leads\nto improved evaluation performance."
                },
                "authors": [
                    {
                        "name": "Brando Miranda"
                    },
                    {
                        "name": "Alycia Lee"
                    },
                    {
                        "name": "Sudharsan Sundar"
                    },
                    {
                        "name": "Allison Casasola"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    }
                ],
                "author_detail": {
                    "name": "Sanmi Koyejo"
                },
                "author": "Sanmi Koyejo",
                "arxiv_journal_ref": "Published as workshop paper in the Data-centric Machine Learning\n  Research (DMLR) Workshop, ICLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.13840v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.13840v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10468v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10468v3",
                "updated": "2024-08-26T17:28:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    28,
                    23,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-20T00:40:49Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    0,
                    40,
                    49,
                    1,
                    233,
                    0
                ],
                "title": "Tracing Privacy Leakage of Language Models to Training Data via Adjusted\n  Influence Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracing Privacy Leakage of Language Models to Training Data via Adjusted\n  Influence Functions"
                },
                "summary": "The responses generated by Large Language Models (LLMs) can include sensitive\ninformation from individuals and organizations, leading to potential privacy\nleakage. This work implements Influence Functions (IFs) to trace privacy\nleakage back to the training data, thereby mitigating privacy concerns of\nLanguage Models (LMs). However, we notice that current IFs struggle to\naccurately estimate the influence of tokens with large gradient norms,\npotentially overestimating their influence. When tracing the most influential\nsamples, this leads to frequently tracing back to samples with large gradient\nnorm tokens, overshadowing the actual most influential samples even if their\ninfluences are well estimated. To address this issue, we propose Heuristically\nAdjusted IF (HAIF), which reduces the weight of tokens with large gradient\nnorms, thereby significantly improving the accuracy of tracing the most\ninfluential samples. To establish easily obtained groundtruth for tracing\nprivacy leakage, we construct two datasets, PII-E and PII-CR, representing two\ndistinct scenarios: one with identical text in the model outputs and\npre-training data, and the other where models leverage their reasoning\nabilities to generate text divergent from pre-training data. HAIF significantly\nimproves tracing accuracy, enhancing it by 20.96% to 73.71% on the PII-E\ndataset and 3.21% to 45.93% on the PII-CR dataset, compared to the best SOTA\nIFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs\non real-world pretraining data CLUECorpus2020, demonstrating strong robustness\nregardless prompt and response lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The responses generated by Large Language Models (LLMs) can include sensitive\ninformation from individuals and organizations, leading to potential privacy\nleakage. This work implements Influence Functions (IFs) to trace privacy\nleakage back to the training data, thereby mitigating privacy concerns of\nLanguage Models (LMs). However, we notice that current IFs struggle to\naccurately estimate the influence of tokens with large gradient norms,\npotentially overestimating their influence. When tracing the most influential\nsamples, this leads to frequently tracing back to samples with large gradient\nnorm tokens, overshadowing the actual most influential samples even if their\ninfluences are well estimated. To address this issue, we propose Heuristically\nAdjusted IF (HAIF), which reduces the weight of tokens with large gradient\nnorms, thereby significantly improving the accuracy of tracing the most\ninfluential samples. To establish easily obtained groundtruth for tracing\nprivacy leakage, we construct two datasets, PII-E and PII-CR, representing two\ndistinct scenarios: one with identical text in the model outputs and\npre-training data, and the other where models leverage their reasoning\nabilities to generate text divergent from pre-training data. HAIF significantly\nimproves tracing accuracy, enhancing it by 20.96% to 73.71% on the PII-E\ndataset and 3.21% to 45.93% on the PII-CR dataset, compared to the best SOTA\nIFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs\non real-world pretraining data CLUECorpus2020, demonstrating strong robustness\nregardless prompt and response lengths."
                },
                "authors": [
                    {
                        "name": "Jinxin Liu"
                    },
                    {
                        "name": "Zao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zao Yang"
                },
                "author": "Zao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10468v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10468v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14418v1",
                "updated": "2024-08-26T17:04:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    4,
                    0,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:04:00Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    4,
                    0,
                    0,
                    239,
                    0
                ],
                "title": "MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR\n  Errors with LLM-generated Synthetic Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR\n  Errors with LLM-generated Synthetic Dialogues"
                },
                "summary": "Automatic Speech Recognition (ASR) systems are pivotal in transcribing speech\ninto text, yet the errors they introduce can significantly degrade the\nperformance of downstream tasks like summarization. This issue is particularly\npronounced in clinical dialogue summarization, a low-resource domain where\nsupervised data for fine-tuning is scarce, necessitating the use of ASR models\nas black-box solutions. Employing conventional data augmentation for enhancing\nthe noise robustness of summarization models is not feasible either due to the\nunavailability of sufficient medical dialogue audio recordings and\ncorresponding ASR transcripts. To address this challenge, we propose MEDSAGE,\nan approach for generating synthetic samples for data augmentation using Large\nLanguage Models (LLMs). Specifically, we leverage the in-context learning\ncapabilities of LLMs and instruct them to generate ASR-like errors based on a\nfew available medical dialogue examples with audio recordings. Experimental\nresults show that LLMs can effectively model ASR noise, and incorporating this\nnoisy data into the training process significantly improves the robustness and\naccuracy of medical dialogue summarization systems. This approach addresses the\nchallenges of noisy ASR outputs in critical applications, offering a robust\nsolution to enhance the reliability of clinical dialogue summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Speech Recognition (ASR) systems are pivotal in transcribing speech\ninto text, yet the errors they introduce can significantly degrade the\nperformance of downstream tasks like summarization. This issue is particularly\npronounced in clinical dialogue summarization, a low-resource domain where\nsupervised data for fine-tuning is scarce, necessitating the use of ASR models\nas black-box solutions. Employing conventional data augmentation for enhancing\nthe noise robustness of summarization models is not feasible either due to the\nunavailability of sufficient medical dialogue audio recordings and\ncorresponding ASR transcripts. To address this challenge, we propose MEDSAGE,\nan approach for generating synthetic samples for data augmentation using Large\nLanguage Models (LLMs). Specifically, we leverage the in-context learning\ncapabilities of LLMs and instruct them to generate ASR-like errors based on a\nfew available medical dialogue examples with audio recordings. Experimental\nresults show that LLMs can effectively model ASR noise, and incorporating this\nnoisy data into the training process significantly improves the robustness and\naccuracy of medical dialogue summarization systems. This approach addresses the\nchallenges of noisy ASR outputs in critical applications, offering a robust\nsolution to enhance the reliability of clinical dialogue summarization."
                },
                "authors": [
                    {
                        "name": "Kuluhan Binici"
                    },
                    {
                        "name": "Abhinav Ramesh Kashyap"
                    },
                    {
                        "name": "Viktor Schlegel"
                    },
                    {
                        "name": "Andy T. Liu"
                    },
                    {
                        "name": "Vijay Prakash Dwivedi"
                    },
                    {
                        "name": "Thanh-Tung Nguyen"
                    },
                    {
                        "name": "Xiaoxue Gao"
                    },
                    {
                        "name": "Nancy F. Chen"
                    },
                    {
                        "name": "Stefan Winkler"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Winkler"
                },
                "author": "Stefan Winkler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05720v2",
                "updated": "2024-08-26T16:48:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    48,
                    8,
                    0,
                    239,
                    0
                ],
                "published": "2024-03-08T23:17:55Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    23,
                    17,
                    55,
                    4,
                    68,
                    0
                ],
                "title": "A Dataset and Benchmark for Hospital Course Summarization with Adapted\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Dataset and Benchmark for Hospital Course Summarization with Adapted\n  Large Language Models"
                },
                "summary": "Brief hospital course (BHC) summaries are clinical documents that summarize a\npatient's hospital stay. While large language models (LLMs) depict remarkable\ncapabilities in automating real-world tasks, their capabilities for healthcare\napplications such as synthesizing BHCs from clinical notes have not been shown.\nWe introduce a novel pre-processed dataset, the MIMIC-IV-BHC, encapsulating\nclinical note and brief hospital course (BHC) pairs to adapt LLMs for BHC\nsynthesis. Furthermore, we introduce a benchmark of the summarization\nperformance of two general-purpose LLMs and three healthcare-adapted LLMs.\n  Using clinical notes as input, we apply prompting-based (using in-context\nlearning) and fine-tuning-based adaptation strategies to three open-source LLMs\n(Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5,\nGPT-4). We evaluate these LLMs across multiple context-length inputs using\nnatural language similarity metrics. We further conduct a clinical study with\nfive clinicians, comparing clinician-written and LLM-generated BHCs across 30\nsamples, focusing on their potential to enhance clinical decision-making\nthrough improved summary quality. We observe that the Llama2-13B fine-tuned LLM\noutperforms other domain-adapted models given quantitative evaluation metrics\nof BLEU and BERT-Score. GPT-4 with in-context learning shows more robustness to\nincreasing context lengths of clinical note inputs than fine-tuned Llama2-13B.\nDespite comparable quantitative metrics, the reader study depicts a significant\npreference for summaries generated by GPT-4 with in-context learning compared\nto both Llama2-13B fine-tuned summaries and the original summaries,\nhighlighting the need for qualitative clinical evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brief hospital course (BHC) summaries are clinical documents that summarize a\npatient's hospital stay. While large language models (LLMs) depict remarkable\ncapabilities in automating real-world tasks, their capabilities for healthcare\napplications such as synthesizing BHCs from clinical notes have not been shown.\nWe introduce a novel pre-processed dataset, the MIMIC-IV-BHC, encapsulating\nclinical note and brief hospital course (BHC) pairs to adapt LLMs for BHC\nsynthesis. Furthermore, we introduce a benchmark of the summarization\nperformance of two general-purpose LLMs and three healthcare-adapted LLMs.\n  Using clinical notes as input, we apply prompting-based (using in-context\nlearning) and fine-tuning-based adaptation strategies to three open-source LLMs\n(Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5,\nGPT-4). We evaluate these LLMs across multiple context-length inputs using\nnatural language similarity metrics. We further conduct a clinical study with\nfive clinicians, comparing clinician-written and LLM-generated BHCs across 30\nsamples, focusing on their potential to enhance clinical decision-making\nthrough improved summary quality. We observe that the Llama2-13B fine-tuned LLM\noutperforms other domain-adapted models given quantitative evaluation metrics\nof BLEU and BERT-Score. GPT-4 with in-context learning shows more robustness to\nincreasing context lengths of clinical note inputs than fine-tuned Llama2-13B.\nDespite comparable quantitative metrics, the reader study depicts a significant\npreference for summaries generated by GPT-4 with in-context learning compared\nto both Llama2-13B fine-tuned summaries and the original summaries,\nhighlighting the need for qualitative clinical evaluation."
                },
                "authors": [
                    {
                        "name": "Asad Aali"
                    },
                    {
                        "name": "Dave Van Veen"
                    },
                    {
                        "name": "Yamin Ishraq Arefeen"
                    },
                    {
                        "name": "Jason Hom"
                    },
                    {
                        "name": "Christian Bluethgen"
                    },
                    {
                        "name": "Eduardo Pontes Reis"
                    },
                    {
                        "name": "Sergios Gatidis"
                    },
                    {
                        "name": "Namuun Clifford"
                    },
                    {
                        "name": "Joseph Daws"
                    },
                    {
                        "name": "Arash S. Tehrani"
                    },
                    {
                        "name": "Jangwon Kim"
                    },
                    {
                        "name": "Akshay S. Chaudhari"
                    }
                ],
                "author_detail": {
                    "name": "Akshay S. Chaudhari"
                },
                "author": "Akshay S. Chaudhari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.00042v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.00042v4",
                "updated": "2024-08-26T16:47:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    47,
                    4,
                    0,
                    239,
                    0
                ],
                "published": "2023-02-28T19:31:11Z",
                "published_parsed": [
                    2023,
                    2,
                    28,
                    19,
                    31,
                    11,
                    1,
                    59,
                    0
                ],
                "title": "Design, Kinematics, and Deployment of a Continuum Underwater\n  Vehicle-Manipulator System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design, Kinematics, and Deployment of a Continuum Underwater\n  Vehicle-Manipulator System"
                },
                "summary": "Underwater vehicle-manipulator systems (UVMSs) are underwater robots equipped\nwith one or more manipulators to perform intervention missions. This paper\nprovides the mechanical, electrical, and software design of a novel UVMS\nequipped with a continuum manipulator, referred to as a continuum-UVMS. A\nkinematic model for the continuum-UVMS is derived in order to build an\nalgorithm to resolve the robot's redundancy and generate joint space commands.\nDifferent methods to optimize the trajectory for specific tasks are proposed\nusing both the weighted least norm solution and the gradient projection method.\nKinematic simulation results are analyzed to assess the performance of the\nproposed algorithm. Finally, the continuum-UVMS is deployed in an experimental\ndemonstration in which both teleoperation and autonomous control are tested for\na given reference trajectory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Underwater vehicle-manipulator systems (UVMSs) are underwater robots equipped\nwith one or more manipulators to perform intervention missions. This paper\nprovides the mechanical, electrical, and software design of a novel UVMS\nequipped with a continuum manipulator, referred to as a continuum-UVMS. A\nkinematic model for the continuum-UVMS is derived in order to build an\nalgorithm to resolve the robot's redundancy and generate joint space commands.\nDifferent methods to optimize the trajectory for specific tasks are proposed\nusing both the weighted least norm solution and the gradient projection method.\nKinematic simulation results are analyzed to assess the performance of the\nproposed algorithm. Finally, the continuum-UVMS is deployed in an experimental\ndemonstration in which both teleoperation and autonomous control are tested for\na given reference trajectory."
                },
                "authors": [
                    {
                        "name": "Justin L. Sitler"
                    },
                    {
                        "name": "Long Wang"
                    }
                ],
                "author_detail": {
                    "name": "Long Wang"
                },
                "author": "Long Wang",
                "arxiv_comment": "14 pages, ASME Journal of Mechanisms and Robotics, accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.00042v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.00042v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14398v1",
                "updated": "2024-08-26T16:29:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    29,
                    13,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T16:29:13Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    29,
                    13,
                    0,
                    239,
                    0
                ],
                "title": "Language-specific Calibration for Pruning Multilingual Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-specific Calibration for Pruning Multilingual Language Models"
                },
                "summary": "Recent advances in large language model (LLM) pruning have shown\nstate-of-the-art compression results in post-training and retraining-free\nsettings while maintaining high predictive performance. However, such research\nmainly considers calibrating pruning using English text, despite the\nmultilingual nature of modern LLMs and their frequent uses in non-English\nlanguages. In this paper, we set out to explore effective strategies for\ncalibrating the pruning of multilingual language models. We present the first\ncomprehensive empirical study, comparing different calibration languages for\npruning multilingual models across diverse tasks, models, and state-of-the-art\npruning techniques. Our results present practical suggestions, for example,\ncalibrating in the target language can efficiently yield lower perplexity, but\ndoes not necessarily benefit downstream tasks. Our further analysis experiments\nunveil that calibration in the target language mainly contributes to preserving\nlanguage-specific features related to fluency and coherence, but might not\ncontribute to capturing language-agnostic features such as language\nunderstanding and reasoning. Last, we provide practical recommendations for\nfuture practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language model (LLM) pruning have shown\nstate-of-the-art compression results in post-training and retraining-free\nsettings while maintaining high predictive performance. However, such research\nmainly considers calibrating pruning using English text, despite the\nmultilingual nature of modern LLMs and their frequent uses in non-English\nlanguages. In this paper, we set out to explore effective strategies for\ncalibrating the pruning of multilingual language models. We present the first\ncomprehensive empirical study, comparing different calibration languages for\npruning multilingual models across diverse tasks, models, and state-of-the-art\npruning techniques. Our results present practical suggestions, for example,\ncalibrating in the target language can efficiently yield lower perplexity, but\ndoes not necessarily benefit downstream tasks. Our further analysis experiments\nunveil that calibration in the target language mainly contributes to preserving\nlanguage-specific features related to fluency and coherence, but might not\ncontribute to capturing language-agnostic features such as language\nunderstanding and reasoning. Last, we provide practical recommendations for\nfuture practitioners."
                },
                "authors": [
                    {
                        "name": "Simon Kurz"
                    },
                    {
                        "name": "Zhixue Zhao"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "Lucie Flek"
                    }
                ],
                "author_detail": {
                    "name": "Lucie Flek"
                },
                "author": "Lucie Flek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14387v1",
                "updated": "2024-08-26T16:11:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    11,
                    53,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T16:11:53Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    11,
                    53,
                    0,
                    239,
                    0
                ],
                "title": "Reprogramming Foundational Large Language Models(LLMs) for Enterprise\n  Adoption for Spatio-Temporal Forecasting Applications: Unveiling a New Era in\n  Copilot-Guided Cross-Modal Time Series Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reprogramming Foundational Large Language Models(LLMs) for Enterprise\n  Adoption for Spatio-Temporal Forecasting Applications: Unveiling a New Era in\n  Copilot-Guided Cross-Modal Time Series Representation Learning"
                },
                "summary": "Spatio-temporal forecasting plays a crucial role in various sectors such as\ntransportation systems, logistics, and supply chain management. However,\nexisting methods are limited by their ability to handle large, complex\ndatasets. To overcome this limitation, we introduce a hybrid approach that\ncombines the strengths of open-source large and small-scale language models\n(LLMs and LMs) with traditional forecasting methods. We augment traditional\nmethods with dynamic prompting and a grouped-query, multi-head attention\nmechanism to more effectively capture both intra-series and inter-series\ndependencies in evolving nonlinear time series data. In addition, we facilitate\non-premises customization by fine-tuning smaller open-source LMs for time\nseries trend analysis utilizing descriptions generated by open-source large LMs\non consumer-grade hardware using Low-Rank Adaptation with Activation Memory\nReduction (LoRA-AMR) technique to reduce computational overhead and activation\nstorage memory demands while preserving inference latency. We combine language\nmodel processing for time series trend analysis with traditional time series\nrepresentation learning method for cross-modal integration, achieving robust\nand accurate forecasts. The framework effectiveness is demonstrated through\nextensive experiments on various real-world datasets, outperforming existing\nmethods by significant margins in terms of forecast accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatio-temporal forecasting plays a crucial role in various sectors such as\ntransportation systems, logistics, and supply chain management. However,\nexisting methods are limited by their ability to handle large, complex\ndatasets. To overcome this limitation, we introduce a hybrid approach that\ncombines the strengths of open-source large and small-scale language models\n(LLMs and LMs) with traditional forecasting methods. We augment traditional\nmethods with dynamic prompting and a grouped-query, multi-head attention\nmechanism to more effectively capture both intra-series and inter-series\ndependencies in evolving nonlinear time series data. In addition, we facilitate\non-premises customization by fine-tuning smaller open-source LMs for time\nseries trend analysis utilizing descriptions generated by open-source large LMs\non consumer-grade hardware using Low-Rank Adaptation with Activation Memory\nReduction (LoRA-AMR) technique to reduce computational overhead and activation\nstorage memory demands while preserving inference latency. We combine language\nmodel processing for time series trend analysis with traditional time series\nrepresentation learning method for cross-modal integration, achieving robust\nand accurate forecasts. The framework effectiveness is demonstrated through\nextensive experiments on various real-world datasets, outperforming existing\nmethods by significant margins in terms of forecast accuracy."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Chidaksh Ravuru"
                    },
                    {
                        "name": "Geethan Sannidhi"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "arxiv_comment": "Paper published at the Deployable AI (DAI) workshop at AAAI-2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14380v1",
                "updated": "2024-08-26T16:00:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    0,
                    41,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T16:00:41Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    0,
                    41,
                    0,
                    239,
                    0
                ],
                "title": "Probing Causality Manipulation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing Causality Manipulation of Large Language Models"
                },
                "summary": "Large language models (LLMs) have shown various ability on natural language\nprocessing, including problems about causality. It is not intuitive for LLMs to\ncommand causality, since pretrained models usually work on statistical\nassociations, and do not focus on causes and effects in sentences. So that\nprobing internal manipulation of causality is necessary for LLMs. This paper\nproposes a novel approach to probe causality manipulation hierarchically, by\nproviding different shortcuts to models and observe behaviors. We exploit\nretrieval augmented generation (RAG) and in-context learning (ICL) for models\non a designed causality classification task. We conduct experiments on\nmainstream LLMs, including GPT-4 and some smaller and domain-specific models.\nOur results suggest that LLMs can detect entities related to causality and\nrecognize direct causal relationships. However, LLMs lack specialized cognition\nfor causality, merely treating them as part of the global semantic of the\nsentence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown various ability on natural language\nprocessing, including problems about causality. It is not intuitive for LLMs to\ncommand causality, since pretrained models usually work on statistical\nassociations, and do not focus on causes and effects in sentences. So that\nprobing internal manipulation of causality is necessary for LLMs. This paper\nproposes a novel approach to probe causality manipulation hierarchically, by\nproviding different shortcuts to models and observe behaviors. We exploit\nretrieval augmented generation (RAG) and in-context learning (ICL) for models\non a designed causality classification task. We conduct experiments on\nmainstream LLMs, including GPT-4 and some smaller and domain-specific models.\nOur results suggest that LLMs can detect entities related to causality and\nrecognize direct causal relationships. However, LLMs lack specialized cognition\nfor causality, merely treating them as part of the global semantic of the\nsentence."
                },
                "authors": [
                    {
                        "name": "Chenyang Zhang"
                    },
                    {
                        "name": "Haibo Tong"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Dongyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongyu Zhang"
                },
                "author": "Dongyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14357v1",
                "updated": "2024-08-26T15:31:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    31,
                    58,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T15:31:58Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    31,
                    58,
                    0,
                    239,
                    0
                ],
                "title": "Exploring ChatGPT App Ecosystem: Distribution, Deployment and Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring ChatGPT App Ecosystem: Distribution, Deployment and Security"
                },
                "summary": "ChatGPT has enabled third-party developers to create plugins to expand\nChatGPT's capabilities.These plugins are distributed through OpenAI's plugin\nstore, making them easily accessible to users. With ChatGPT as the backbone,\nthis app ecosystem has illustrated great business potential by offering users\npersonalized services in a conversational manner. Nonetheless, many crucial\naspects regarding app development, deployment, and security of this ecosystem\nhave yet to be thoroughly studied in the research community, potentially\nhindering a broader adoption by both developers and users. In this work, we\nconduct the first comprehensive study of the ChatGPT app ecosystem, aiming to\nilluminate its landscape for our research community. Our study examines the\ndistribution and deployment models in the integration of LLMs and third-party\napps, and assesses their security and privacy implications. We uncover an\nuneven distribution of functionality among ChatGPT plugins, highlighting\nprevalent and emerging topics. We also identify severe flaws in the\nauthentication and user data protection for third-party app APIs integrated\nwithin LLMs, revealing a concerning status quo of security and privacy in this\napp ecosystem. Our work provides insights for the secure and sustainable\ndevelopment of this rapidly evolving ecosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatGPT has enabled third-party developers to create plugins to expand\nChatGPT's capabilities.These plugins are distributed through OpenAI's plugin\nstore, making them easily accessible to users. With ChatGPT as the backbone,\nthis app ecosystem has illustrated great business potential by offering users\npersonalized services in a conversational manner. Nonetheless, many crucial\naspects regarding app development, deployment, and security of this ecosystem\nhave yet to be thoroughly studied in the research community, potentially\nhindering a broader adoption by both developers and users. In this work, we\nconduct the first comprehensive study of the ChatGPT app ecosystem, aiming to\nilluminate its landscape for our research community. Our study examines the\ndistribution and deployment models in the integration of LLMs and third-party\napps, and assesses their security and privacy implications. We uncover an\nuneven distribution of functionality among ChatGPT plugins, highlighting\nprevalent and emerging topics. We also identify severe flaws in the\nauthentication and user data protection for third-party app APIs integrated\nwithin LLMs, revealing a concerning status quo of security and privacy in this\napp ecosystem. Our work provides insights for the secure and sustainable\ndevelopment of this rapidly evolving ecosystem."
                },
                "authors": [
                    {
                        "name": "Chuan Yan"
                    },
                    {
                        "name": "Ruomai Ren"
                    },
                    {
                        "name": "Mark Huasong Meng"
                    },
                    {
                        "name": "Liuhuo Wan"
                    },
                    {
                        "name": "Tian Yang Ooi"
                    },
                    {
                        "name": "Guangdong Bai"
                    }
                ],
                "author_detail": {
                    "name": "Guangdong Bai"
                },
                "author": "Guangdong Bai",
                "arxiv_comment": "Accepted by the 39th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14354v1",
                "updated": "2024-08-26T15:30:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    30,
                    5,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T15:30:05Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    30,
                    5,
                    0,
                    239,
                    0
                ],
                "title": "SWE-bench-java: A GitHub Issue Resolving Benchmark for Java",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-bench-java: A GitHub Issue Resolving Benchmark for Java"
                },
                "summary": "GitHub issue resolving is a critical task in software engineering, recently\ngaining significant attention in both industry and academia. Within this task,\nSWE-bench has been released to evaluate issue resolving capabilities of large\nlanguage models (LLMs), but has so far only focused on Python version. However,\nsupporting more programming languages is also important, as there is a strong\ndemand in industry. As a first step toward multilingual support, we have\ndeveloped a Java version of SWE-bench, called SWE-bench-java. We have publicly\nreleased the dataset, along with the corresponding Docker-based evaluation\nenvironment and leaderboard, which will be continuously maintained and updated\nin the coming months. To verify the reliability of SWE-bench-java, we implement\na classic method SWE-agent and test several powerful LLMs on it. As is well\nknown, developing a high-quality multi-lingual benchmark is time-consuming and\nlabor-intensive, so we welcome contributions through pull requests or\ncollaboration to accelerate its iteration and refinement, paving the way for\nfully automated programming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GitHub issue resolving is a critical task in software engineering, recently\ngaining significant attention in both industry and academia. Within this task,\nSWE-bench has been released to evaluate issue resolving capabilities of large\nlanguage models (LLMs), but has so far only focused on Python version. However,\nsupporting more programming languages is also important, as there is a strong\ndemand in industry. As a first step toward multilingual support, we have\ndeveloped a Java version of SWE-bench, called SWE-bench-java. We have publicly\nreleased the dataset, along with the corresponding Docker-based evaluation\nenvironment and leaderboard, which will be continuously maintained and updated\nin the coming months. To verify the reliability of SWE-bench-java, we implement\na classic method SWE-agent and test several powerful LLMs on it. As is well\nknown, developing a high-quality multi-lingual benchmark is time-consuming and\nlabor-intensive, so we welcome contributions through pull requests or\ncollaboration to accelerate its iteration and refinement, paving the way for\nfully automated programming."
                },
                "authors": [
                    {
                        "name": "Daoguang Zan"
                    },
                    {
                        "name": "Zhirong Huang"
                    },
                    {
                        "name": "Ailun Yu"
                    },
                    {
                        "name": "Shaoxin Lin"
                    },
                    {
                        "name": "Yifan Shi"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Dong Chen"
                    },
                    {
                        "name": "Zongshuai Qi"
                    },
                    {
                        "name": "Hao Yu"
                    },
                    {
                        "name": "Lei Yu"
                    },
                    {
                        "name": "Dezhi Ran"
                    },
                    {
                        "name": "Muhan Zeng"
                    },
                    {
                        "name": "Bo Shen"
                    },
                    {
                        "name": "Pan Bian"
                    },
                    {
                        "name": "Guangtai Liang"
                    },
                    {
                        "name": "Bei Guan"
                    },
                    {
                        "name": "Pengjie Huang"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Yongji Wang"
                    },
                    {
                        "name": "Qianxiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qianxiang Wang"
                },
                "author": "Qianxiang Wang",
                "arxiv_comment": "This work is in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14352v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14352v1",
                "updated": "2024-08-26T15:29:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    29,
                    34,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T15:29:34Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    29,
                    34,
                    0,
                    239,
                    0
                ],
                "title": "Assessing Contamination in Large Language Models: Introducing the\n  LogProber method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Contamination in Large Language Models: Introducing the\n  LogProber method"
                },
                "summary": "In machine learning, contamination refers to situations where testing data\nleak into the training set. The issue is particularly relevant for the\nevaluation of the performance of Large Language Models (LLMs), which are\ngenerally trained on gargantuan, and generally opaque, corpora of text scraped\nfrom the world wide web. Developing tools to detect contamination is therefore\ncrucial to be able to fairly and properly track the evolution of the\nperformance of LLMs. Most recent works in the field are not tailored to\nquantify contamination on short sequences of text like we find in psychology\nquestionnaires. In the present paper we introduce LogProber, a novel,\nefficient, algorithm that we show able to detect contamination using token\nprobability in given sentences. In the second part we investigate the\nlimitations of the method and discuss how different training methods can\ncontaminate models without leaving traces in the token probabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In machine learning, contamination refers to situations where testing data\nleak into the training set. The issue is particularly relevant for the\nevaluation of the performance of Large Language Models (LLMs), which are\ngenerally trained on gargantuan, and generally opaque, corpora of text scraped\nfrom the world wide web. Developing tools to detect contamination is therefore\ncrucial to be able to fairly and properly track the evolution of the\nperformance of LLMs. Most recent works in the field are not tailored to\nquantify contamination on short sequences of text like we find in psychology\nquestionnaires. In the present paper we introduce LogProber, a novel,\nefficient, algorithm that we show able to detect contamination using token\nprobability in given sentences. In the second part we investigate the\nlimitations of the method and discuss how different training methods can\ncontaminate models without leaving traces in the token probabilities."
                },
                "authors": [
                    {
                        "name": "Nicolas Yax"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    },
                    {
                        "name": "Stefano Palminteri"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Palminteri"
                },
                "author": "Stefano Palminteri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14352v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14352v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14340v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14340v2",
                "updated": "2024-08-27T14:09:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    9,
                    44,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-26T15:13:14Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    13,
                    14,
                    0,
                    239,
                    0
                ],
                "title": "Foundation Models for Music: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Models for Music: A Survey"
                },
                "summary": "In recent years, foundation models (FMs) such as large language models (LLMs)\nand latent diffusion models (LDMs) have profoundly impacted diverse sectors,\nincluding music. This comprehensive review examines state-of-the-art (SOTA)\npre-trained models and foundation models in music, spanning from representation\nlearning, generative learning and multimodal learning. We first contextualise\nthe significance of music in various industries and trace the evolution of AI\nin music. By delineating the modalities targeted by foundation models, we\ndiscover many of the music representations are underexplored in FM development.\nThen, emphasis is placed on the lack of versatility of previous methods on\ndiverse music applications, along with the potential of FMs in music\nunderstanding, generation and medical application. By comprehensively exploring\nthe details of the model pre-training paradigm, architectural choices,\ntokenisation, finetuning methodologies and controllability, we emphasise the\nimportant topics that should have been well explored, like instruction tuning\nand in-context learning, scaling law and emergent ability, as well as\nlong-sequence modelling etc. A dedicated section presents insights into music\nagents, accompanied by a thorough analysis of datasets and evaluations\nessential for pre-training and downstream tasks. Finally, by underscoring the\nvital importance of ethical considerations, we advocate that following research\non FM for music should focus more on such issues as interpretability,\ntransparency, human responsibility, and copyright issues. The paper offers\ninsights into future challenges and trends on FMs for music, aiming to shape\nthe trajectory of human-AI collaboration in the music realm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, foundation models (FMs) such as large language models (LLMs)\nand latent diffusion models (LDMs) have profoundly impacted diverse sectors,\nincluding music. This comprehensive review examines state-of-the-art (SOTA)\npre-trained models and foundation models in music, spanning from representation\nlearning, generative learning and multimodal learning. We first contextualise\nthe significance of music in various industries and trace the evolution of AI\nin music. By delineating the modalities targeted by foundation models, we\ndiscover many of the music representations are underexplored in FM development.\nThen, emphasis is placed on the lack of versatility of previous methods on\ndiverse music applications, along with the potential of FMs in music\nunderstanding, generation and medical application. By comprehensively exploring\nthe details of the model pre-training paradigm, architectural choices,\ntokenisation, finetuning methodologies and controllability, we emphasise the\nimportant topics that should have been well explored, like instruction tuning\nand in-context learning, scaling law and emergent ability, as well as\nlong-sequence modelling etc. A dedicated section presents insights into music\nagents, accompanied by a thorough analysis of datasets and evaluations\nessential for pre-training and downstream tasks. Finally, by underscoring the\nvital importance of ethical considerations, we advocate that following research\non FM for music should focus more on such issues as interpretability,\ntransparency, human responsibility, and copyright issues. The paper offers\ninsights into future challenges and trends on FMs for music, aiming to shape\nthe trajectory of human-AI collaboration in the music realm."
                },
                "authors": [
                    {
                        "name": "Yinghao Ma"
                    },
                    {
                        "name": "Anders Øland"
                    },
                    {
                        "name": "Anton Ragni"
                    },
                    {
                        "name": "Bleiz MacSen Del Sette"
                    },
                    {
                        "name": "Charalampos Saitis"
                    },
                    {
                        "name": "Chris Donahue"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Christos Plachouras"
                    },
                    {
                        "name": "Emmanouil Benetos"
                    },
                    {
                        "name": "Elio Quinton"
                    },
                    {
                        "name": "Elona Shatri"
                    },
                    {
                        "name": "Fabio Morreale"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "György Fazekas"
                    },
                    {
                        "name": "Gus Xia"
                    },
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Ilaria Manco"
                    },
                    {
                        "name": "Jiawen Huang"
                    },
                    {
                        "name": "Julien Guinot"
                    },
                    {
                        "name": "Liwei Lin"
                    },
                    {
                        "name": "Luca Marinelli"
                    },
                    {
                        "name": "Max W. Y. Lam"
                    },
                    {
                        "name": "Megha Sharma"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Roger B. Dannenberg"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Shangda Wu"
                    },
                    {
                        "name": "Shih-Lun Wu"
                    },
                    {
                        "name": "Shuqi Dai"
                    },
                    {
                        "name": "Shun Lei"
                    },
                    {
                        "name": "Shiyin Kang"
                    },
                    {
                        "name": "Simon Dixon"
                    },
                    {
                        "name": "Wenhu Chen"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Xingjian Du"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Zeyue Tian"
                    },
                    {
                        "name": "Zhiyong Wu"
                    },
                    {
                        "name": "Zhizheng Wu"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Ziyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ziyu Wang"
                },
                "author": "Ziyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14340v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14340v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16528v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16528v2",
                "updated": "2024-08-26T14:59:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    59,
                    53,
                    0,
                    239,
                    0
                ],
                "published": "2024-05-26T11:29:57Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    11,
                    29,
                    57,
                    6,
                    147,
                    0
                ],
                "title": "LoQT: Low Rank Adapters for Quantized Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoQT: Low Rank Adapters for Quantized Training"
                },
                "summary": "Training of large neural networks requires significant computational\nresources. Despite advances using low-rank adapters and quantization,\npretraining of models such as LLMs on consumer hardware has not been possible\nwithout model sharding, offloading during training, or per-layer gradient\nupdates. To address these limitations, we propose LoQT, a method for\nefficiently training quantized models. LoQT uses gradient-based tensor\nfactorization to initialize low-rank trainable weight matrices that are\nperiodically merged into quantized full-rank weight matrices. Our approach is\nsuitable for both pretraining and fine-tuning of models, which we demonstrate\nexperimentally for language modeling and downstream task adaptation. We find\nthat LoQT enables efficient training of models up to 7B parameters on a\nconsumer-grade 24GB GPU. We also demonstrate the feasibility of training a 13B\nparameter model using per-layer gradient updates on the same hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training of large neural networks requires significant computational\nresources. Despite advances using low-rank adapters and quantization,\npretraining of models such as LLMs on consumer hardware has not been possible\nwithout model sharding, offloading during training, or per-layer gradient\nupdates. To address these limitations, we propose LoQT, a method for\nefficiently training quantized models. LoQT uses gradient-based tensor\nfactorization to initialize low-rank trainable weight matrices that are\nperiodically merged into quantized full-rank weight matrices. Our approach is\nsuitable for both pretraining and fine-tuning of models, which we demonstrate\nexperimentally for language modeling and downstream task adaptation. We find\nthat LoQT enables efficient training of models up to 7B parameters on a\nconsumer-grade 24GB GPU. We also demonstrate the feasibility of training a 13B\nparameter model using per-layer gradient updates on the same hardware."
                },
                "authors": [
                    {
                        "name": "Sebastian Loeschcke"
                    },
                    {
                        "name": "Mads Toftrup"
                    },
                    {
                        "name": "Michael J. Kastoryano"
                    },
                    {
                        "name": "Serge Belongie"
                    },
                    {
                        "name": "Vésteinn Snæbjarnarson"
                    }
                ],
                "author_detail": {
                    "name": "Vésteinn Snæbjarnarson"
                },
                "author": "Vésteinn Snæbjarnarson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16528v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16528v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14331v1",
                "updated": "2024-08-26T14:55:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    55,
                    40,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T14:55:40Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    55,
                    40,
                    0,
                    239,
                    0
                ],
                "title": "Automated Machine Learning in Insurance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Machine Learning in Insurance"
                },
                "summary": "Machine Learning (ML) has gained popularity in actuarial research and\ninsurance industrial applications. However, the performance of most ML tasks\nheavily depends on data preprocessing, model selection, and hyperparameter\noptimization, which are considered to be intensive in terms of domain\nknowledge, experience, and manual labor. Automated Machine Learning (AutoML)\naims to automatically complete the full life-cycle of ML tasks and provides\nstate-of-the-art ML models without human intervention or supervision. This\npaper introduces an AutoML workflow that allows users without domain knowledge\nor prior experience to achieve robust and effortless ML deployment by writing\nonly a few lines of code. This proposed AutoML is specifically tailored for the\ninsurance application, with features like the balancing step in data\npreprocessing, ensemble pipelines, and customized loss functions. These\nfeatures are designed to address the unique challenges of the insurance domain,\nincluding the imbalanced nature of common insurance datasets. The full code and\ndocumentation are available on the GitHub repository.\n(https://github.com/PanyiDong/InsurAutoML)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Learning (ML) has gained popularity in actuarial research and\ninsurance industrial applications. However, the performance of most ML tasks\nheavily depends on data preprocessing, model selection, and hyperparameter\noptimization, which are considered to be intensive in terms of domain\nknowledge, experience, and manual labor. Automated Machine Learning (AutoML)\naims to automatically complete the full life-cycle of ML tasks and provides\nstate-of-the-art ML models without human intervention or supervision. This\npaper introduces an AutoML workflow that allows users without domain knowledge\nor prior experience to achieve robust and effortless ML deployment by writing\nonly a few lines of code. This proposed AutoML is specifically tailored for the\ninsurance application, with features like the balancing step in data\npreprocessing, ensemble pipelines, and customized loss functions. These\nfeatures are designed to address the unique challenges of the insurance domain,\nincluding the imbalanced nature of common insurance datasets. The full code and\ndocumentation are available on the GitHub repository.\n(https://github.com/PanyiDong/InsurAutoML)"
                },
                "authors": [
                    {
                        "name": "Panyi Dong"
                    },
                    {
                        "name": "Zhiyu Quan"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Quan"
                },
                "author": "Zhiyu Quan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14329v1",
                "updated": "2024-08-26T14:55:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    55,
                    23,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T14:55:23Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    55,
                    23,
                    0,
                    239,
                    0
                ],
                "title": "PHEVA: A Privacy-preserving Human-centric Video Anomaly Detection\n  Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PHEVA: A Privacy-preserving Human-centric Video Anomaly Detection\n  Dataset"
                },
                "summary": "PHEVA, a Privacy-preserving Human-centric Ethical Video Anomaly detection\ndataset. By removing pixel information and providing only de-identified human\nannotations, PHEVA safeguards personally identifiable information. The dataset\nincludes seven indoor/outdoor scenes, featuring one novel, context-specific\ncamera, and offers over 5x the pose-annotated frames compared to the largest\nprevious dataset. This study benchmarks state-of-the-art methods on PHEVA using\na comprehensive set of metrics, including the 10% Error Rate (10ER), a metric\nused for anomaly detection for the first time providing insights relevant to\nreal-world deployment. As the first of its kind, PHEVA bridges the gap between\nconventional training and real-world deployment by introducing continual\nlearning benchmarks, with models outperforming traditional methods in 82.14% of\ncases. The dataset is publicly available at\nhttps://github.com/TeCSAR-UNCC/PHEVA.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PHEVA, a Privacy-preserving Human-centric Ethical Video Anomaly detection\ndataset. By removing pixel information and providing only de-identified human\nannotations, PHEVA safeguards personally identifiable information. The dataset\nincludes seven indoor/outdoor scenes, featuring one novel, context-specific\ncamera, and offers over 5x the pose-annotated frames compared to the largest\nprevious dataset. This study benchmarks state-of-the-art methods on PHEVA using\na comprehensive set of metrics, including the 10% Error Rate (10ER), a metric\nused for anomaly detection for the first time providing insights relevant to\nreal-world deployment. As the first of its kind, PHEVA bridges the gap between\nconventional training and real-world deployment by introducing continual\nlearning benchmarks, with models outperforming traditional methods in 82.14% of\ncases. The dataset is publicly available at\nhttps://github.com/TeCSAR-UNCC/PHEVA.git."
                },
                "authors": [
                    {
                        "name": "Ghazal Alinezhad Noghre"
                    },
                    {
                        "name": "Shanle Yao"
                    },
                    {
                        "name": "Armin Danesh Pazho"
                    },
                    {
                        "name": "Babak Rahimi Ardabili"
                    },
                    {
                        "name": "Vinit Katariya"
                    },
                    {
                        "name": "Hamed Tabkhi"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Tabkhi"
                },
                "author": "Hamed Tabkhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14317v1",
                "updated": "2024-08-26T14:45:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    45,
                    3,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T14:45:03Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    45,
                    3,
                    0,
                    239,
                    0
                ],
                "title": "Claim Verification in the Age of Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Claim Verification in the Age of Large Language Models: A Survey"
                },
                "summary": "The large and ever-increasing amount of data available on the Internet\ncoupled with the laborious task of manual claim and fact verification has\nsparked the interest in the development of automated claim verification\nsystems. Several deep learning and transformer-based models have been proposed\nfor this task over the years. With the introduction of Large Language Models\n(LLMs) and their superior performance in several NLP tasks, we have seen a\nsurge of LLM-based approaches to claim verification along with the use of novel\nmethods such as Retrieval Augmented Generation (RAG). In this survey, we\npresent a comprehensive account of recent claim verification frameworks using\nLLMs. We describe the different components of the claim verification pipeline\nused in these frameworks in detail including common approaches to retrieval,\nprompting, and fine-tuning. Finally, we describe publicly available English\ndatasets created for this task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The large and ever-increasing amount of data available on the Internet\ncoupled with the laborious task of manual claim and fact verification has\nsparked the interest in the development of automated claim verification\nsystems. Several deep learning and transformer-based models have been proposed\nfor this task over the years. With the introduction of Large Language Models\n(LLMs) and their superior performance in several NLP tasks, we have seen a\nsurge of LLM-based approaches to claim verification along with the use of novel\nmethods such as Retrieval Augmented Generation (RAG). In this survey, we\npresent a comprehensive account of recent claim verification frameworks using\nLLMs. We describe the different components of the claim verification pipeline\nused in these frameworks in detail including common approaches to retrieval,\nprompting, and fine-tuning. Finally, we describe publicly available English\ndatasets created for this task."
                },
                "authors": [
                    {
                        "name": "Alphaeus Dmonte"
                    },
                    {
                        "name": "Roland Oruche"
                    },
                    {
                        "name": "Marcos Zampieri"
                    },
                    {
                        "name": "Prasad Calyam"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14307v1",
                "updated": "2024-08-26T14:38:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    38,
                    19,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T14:38:19Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    38,
                    19,
                    0,
                    239,
                    0
                ],
                "title": "LLM-3D Print: Large Language Models To Monitor and Control 3D Printing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-3D Print: Large Language Models To Monitor and Control 3D Printing"
                },
                "summary": "Industry 4.0 has revolutionized manufacturing by driving digitalization and\nshifting the paradigm toward additive manufacturing (AM). Fused Deposition\nModeling (FDM), a key AM technology, enables the creation of highly customized,\ncost-effective products with minimal material waste through layer-by-layer\nextrusion, posing a significant challenge to traditional subtractive methods.\nHowever, the susceptibility of material extrusion techniques to errors often\nrequires expert intervention to detect and mitigate defects that can severely\ncompromise product quality. While automated error detection and machine\nlearning models exist, their generalizability across diverse 3D printer setups,\nfirmware, and sensors is limited, and deep learning methods require extensive\nlabeled datasets, hindering scalability and adaptability. To address these\nchallenges, we present a process monitoring and control framework that\nleverages pre-trained Large Language Models (LLMs) alongside 3D printers to\ndetect and address printing defects. The LLM evaluates print quality by\nanalyzing images captured after each layer or print segment, identifying\nfailure modes and querying the printer for relevant parameters. It then\ngenerates and executes a corrective action plan. We validated the effectiveness\nof the proposed framework in identifying defects by comparing it against a\ncontrol group of engineers with diverse AM expertise. Our evaluation\ndemonstrated that LLM-based agents not only accurately identify common 3D\nprinting errors, such as inconsistent extrusion, stringing, warping, and layer\nadhesion, but also effectively determine the parameters causing these failures\nand autonomously correct them without any need for human intervention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industry 4.0 has revolutionized manufacturing by driving digitalization and\nshifting the paradigm toward additive manufacturing (AM). Fused Deposition\nModeling (FDM), a key AM technology, enables the creation of highly customized,\ncost-effective products with minimal material waste through layer-by-layer\nextrusion, posing a significant challenge to traditional subtractive methods.\nHowever, the susceptibility of material extrusion techniques to errors often\nrequires expert intervention to detect and mitigate defects that can severely\ncompromise product quality. While automated error detection and machine\nlearning models exist, their generalizability across diverse 3D printer setups,\nfirmware, and sensors is limited, and deep learning methods require extensive\nlabeled datasets, hindering scalability and adaptability. To address these\nchallenges, we present a process monitoring and control framework that\nleverages pre-trained Large Language Models (LLMs) alongside 3D printers to\ndetect and address printing defects. The LLM evaluates print quality by\nanalyzing images captured after each layer or print segment, identifying\nfailure modes and querying the printer for relevant parameters. It then\ngenerates and executes a corrective action plan. We validated the effectiveness\nof the proposed framework in identifying defects by comparing it against a\ncontrol group of engineers with diverse AM expertise. Our evaluation\ndemonstrated that LLM-based agents not only accurately identify common 3D\nprinting errors, such as inconsistent extrusion, stringing, warping, and layer\nadhesion, but also effectively determine the parameters causing these failures\nand autonomously correct them without any need for human intervention."
                },
                "authors": [
                    {
                        "name": "Yayati Jadhav"
                    },
                    {
                        "name": "Peter Pak"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18294v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18294v4",
                "updated": "2024-08-26T14:32:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    32,
                    33,
                    0,
                    239,
                    0
                ],
                "published": "2024-02-28T12:38:49Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    12,
                    38,
                    49,
                    2,
                    59,
                    0
                ],
                "title": "Whole-body Humanoid Robot Locomotion with Human Reference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whole-body Humanoid Robot Locomotion with Human Reference"
                },
                "summary": "Recently, humanoid robots have made significant advances in their ability to\nperform challenging tasks due to the deployment of Reinforcement Learning (RL),\nhowever, the inherent complexity of humanoid robots, including the difficulty\nof designing complicated reward functions and training entire sophisticated\nsystems, still poses a notable challenge. To conquer these challenges, after\nmany iterations and in-depth investigations, we have meticulously developed a\nfull-size humanoid robot, \"Adam\", whose innovative structural design greatly\nimproves the efficiency and effectiveness of the imitation learning process. In\naddition, we have developed a novel imitation learning framework based on an\nadversarial motion prior, which applies not only to Adam but also to humanoid\nrobots in general. Using the framework, Adam can exhibit unprecedented\nhuman-like characteristics in locomotion tasks. Our experimental results\ndemonstrate that the proposed framework enables Adam to achieve\nhuman-comparable performance in complex locomotion tasks, marking the first\ntime that human locomotion data has been used for imitation learning in a\nfull-size humanoid robot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, humanoid robots have made significant advances in their ability to\nperform challenging tasks due to the deployment of Reinforcement Learning (RL),\nhowever, the inherent complexity of humanoid robots, including the difficulty\nof designing complicated reward functions and training entire sophisticated\nsystems, still poses a notable challenge. To conquer these challenges, after\nmany iterations and in-depth investigations, we have meticulously developed a\nfull-size humanoid robot, \"Adam\", whose innovative structural design greatly\nimproves the efficiency and effectiveness of the imitation learning process. In\naddition, we have developed a novel imitation learning framework based on an\nadversarial motion prior, which applies not only to Adam but also to humanoid\nrobots in general. Using the framework, Adam can exhibit unprecedented\nhuman-like characteristics in locomotion tasks. Our experimental results\ndemonstrate that the proposed framework enables Adam to achieve\nhuman-comparable performance in complex locomotion tasks, marking the first\ntime that human locomotion data has been used for imitation learning in a\nfull-size humanoid robot."
                },
                "authors": [
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Peter Cui"
                    },
                    {
                        "name": "David Yan"
                    },
                    {
                        "name": "Jingkai Sun"
                    },
                    {
                        "name": "Yiqun Duan"
                    },
                    {
                        "name": "Gang Han"
                    },
                    {
                        "name": "Wen Zhao"
                    },
                    {
                        "name": "Weining Zhang"
                    },
                    {
                        "name": "Yijie Guo"
                    },
                    {
                        "name": "Arthur Zhang"
                    },
                    {
                        "name": "Renjing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Renjing Xu"
                },
                "author": "Renjing Xu",
                "arxiv_comment": "7pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18294v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18294v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10594v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10594v3",
                "updated": "2024-08-26T14:30:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    30,
                    38,
                    0,
                    239,
                    0
                ],
                "published": "2024-06-15T11:03:33Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    11,
                    3,
                    33,
                    5,
                    167,
                    0
                ],
                "title": "BlockPruner: Fine-grained Pruning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockPruner: Fine-grained Pruning for Large Language Models"
                },
                "summary": "With the rapid growth in the size and complexity of large language models\n(LLMs), the costs associated with their training and inference have escalated\nsignificantly. Research indicates that certain layers in LLMs harbor\nsubstantial redundancy, and pruning these layers has minimal impact on the\noverall performance. While various layer pruning methods have been developed\nbased on this insight, they generally overlook the finer-grained redundancies\nwithin the layers themselves. In this paper, we delve deeper into the\narchitecture of LLMs and demonstrate that finer-grained pruning can be achieved\nby targeting redundancies in multi-head attention (MHA) and multi-layer\nperceptron (MLP) blocks. We propose a novel, training-free structured pruning\napproach called BlockPruner. Unlike existing layer pruning methods, BlockPruner\nsegments each Transformer layer into MHA and MLP blocks. It then assesses the\nimportance of these blocks using perplexity measures and applies a heuristic\nsearch for iterative pruning. We applied BlockPruner to LLMs of various sizes\nand architectures and validated its performance across a wide range of\ndownstream tasks. Experimental results show that BlockPruner achieves more\ngranular and effective pruning compared to state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth in the size and complexity of large language models\n(LLMs), the costs associated with their training and inference have escalated\nsignificantly. Research indicates that certain layers in LLMs harbor\nsubstantial redundancy, and pruning these layers has minimal impact on the\noverall performance. While various layer pruning methods have been developed\nbased on this insight, they generally overlook the finer-grained redundancies\nwithin the layers themselves. In this paper, we delve deeper into the\narchitecture of LLMs and demonstrate that finer-grained pruning can be achieved\nby targeting redundancies in multi-head attention (MHA) and multi-layer\nperceptron (MLP) blocks. We propose a novel, training-free structured pruning\napproach called BlockPruner. Unlike existing layer pruning methods, BlockPruner\nsegments each Transformer layer into MHA and MLP blocks. It then assesses the\nimportance of these blocks using perplexity measures and applies a heuristic\nsearch for iterative pruning. We applied BlockPruner to LLMs of various sizes\nand architectures and validated its performance across a wide range of\ndownstream tasks. Experimental results show that BlockPruner achieves more\ngranular and effective pruning compared to state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Longguang Zhong"
                    },
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Ruijun Chen"
                    },
                    {
                        "name": "Xiaojun Quan"
                    },
                    {
                        "name": "Liangzhi Li"
                    }
                ],
                "author_detail": {
                    "name": "Liangzhi Li"
                },
                "author": "Liangzhi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10594v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10594v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14293v1",
                "updated": "2024-08-26T14:25:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    25,
                    30,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T14:25:30Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    25,
                    30,
                    0,
                    239,
                    0
                ],
                "title": "Investigating the Effectiveness of Bayesian Spam Filters in Detecting\n  LLM-modified Spam Mails",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating the Effectiveness of Bayesian Spam Filters in Detecting\n  LLM-modified Spam Mails"
                },
                "summary": "Spam and phishing remain critical threats in cybersecurity, responsible for\nnearly 90% of security incidents. As these attacks grow in sophistication, the\nneed for robust defensive mechanisms intensifies. Bayesian spam filters, like\nthe widely adopted open-source SpamAssassin, are essential tools in this fight.\nHowever, the emergence of large language models (LLMs) such as ChatGPT presents\nnew challenges. These models are not only powerful and accessible, but also\ninexpensive to use, raising concerns about their misuse in crafting\nsophisticated spam emails that evade traditional spam filters. This work aims\nto evaluate the robustness and effectiveness of SpamAssassin against\nLLM-modified email content. We developed a pipeline to test this vulnerability.\nOur pipeline modifies spam emails using GPT-3.5 Turbo and assesses\nSpamAssassin's ability to classify these modified emails correctly. The results\nshow that SpamAssassin misclassified up to 73.7% of LLM-modified spam emails as\nlegitimate. In contrast, a simpler dictionary-replacement attack showed a\nmaximum success rate of only 0.4%. These findings highlight the significant\nthreat posed by LLM-modified spam, especially given the cost-efficiency of such\nattacks (0.17 cents per email). This paper provides crucial insights into the\nvulnerabilities of current spam filters and the need for continuous improvement\nin cybersecurity measures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spam and phishing remain critical threats in cybersecurity, responsible for\nnearly 90% of security incidents. As these attacks grow in sophistication, the\nneed for robust defensive mechanisms intensifies. Bayesian spam filters, like\nthe widely adopted open-source SpamAssassin, are essential tools in this fight.\nHowever, the emergence of large language models (LLMs) such as ChatGPT presents\nnew challenges. These models are not only powerful and accessible, but also\ninexpensive to use, raising concerns about their misuse in crafting\nsophisticated spam emails that evade traditional spam filters. This work aims\nto evaluate the robustness and effectiveness of SpamAssassin against\nLLM-modified email content. We developed a pipeline to test this vulnerability.\nOur pipeline modifies spam emails using GPT-3.5 Turbo and assesses\nSpamAssassin's ability to classify these modified emails correctly. The results\nshow that SpamAssassin misclassified up to 73.7% of LLM-modified spam emails as\nlegitimate. In contrast, a simpler dictionary-replacement attack showed a\nmaximum success rate of only 0.4%. These findings highlight the significant\nthreat posed by LLM-modified spam, especially given the cost-efficiency of such\nattacks (0.17 cents per email). This paper provides crucial insights into the\nvulnerabilities of current spam filters and the need for continuous improvement\nin cybersecurity measures."
                },
                "authors": [
                    {
                        "name": "Malte Josten"
                    },
                    {
                        "name": "Torben Weis"
                    }
                ],
                "author_detail": {
                    "name": "Torben Weis"
                },
                "author": "Torben Weis",
                "arxiv_comment": "EAI International Conference on Digital Forensics & Cyber Crime 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.01210v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.01210v4",
                "updated": "2024-08-26T13:57:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    13,
                    57,
                    31,
                    0,
                    239,
                    0
                ],
                "published": "2023-12-02T19:39:50Z",
                "published_parsed": [
                    2023,
                    12,
                    2,
                    19,
                    39,
                    50,
                    5,
                    336,
                    0
                ],
                "title": "When accurate prediction models yield harmful self-fulfilling prophecies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When accurate prediction models yield harmful self-fulfilling prophecies"
                },
                "summary": "Prediction models are popular in medical research and practice. By predicting\nan outcome of interest for specific patients, these models may help inform\ndifficult treatment decisions, and are often hailed as the poster children for\npersonalized, data-driven healthcare. We show however, that using prediction\nmodels for decision making can lead to harmful decisions, even when the\npredictions exhibit good discrimination after deployment. These models are\nharmful self-fulfilling prophecies: their deployment harms a group of patients\nbut the worse outcome of these patients does not invalidate the predictive\npower of the model. Our main result is a formal characterization of a set of\nsuch prediction models. Next we show that models that are well calibrated\nbefore and after deployment are useless for decision making as they made no\nchange in the data distribution. These results point to the need to revise\nstandard practices for validation, deployment and evaluation of prediction\nmodels that are used in medical decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prediction models are popular in medical research and practice. By predicting\nan outcome of interest for specific patients, these models may help inform\ndifficult treatment decisions, and are often hailed as the poster children for\npersonalized, data-driven healthcare. We show however, that using prediction\nmodels for decision making can lead to harmful decisions, even when the\npredictions exhibit good discrimination after deployment. These models are\nharmful self-fulfilling prophecies: their deployment harms a group of patients\nbut the worse outcome of these patients does not invalidate the predictive\npower of the model. Our main result is a formal characterization of a set of\nsuch prediction models. Next we show that models that are well calibrated\nbefore and after deployment are useless for decision making as they made no\nchange in the data distribution. These results point to the need to revise\nstandard practices for validation, deployment and evaluation of prediction\nmodels that are used in medical decisions."
                },
                "authors": [
                    {
                        "name": "Wouter A. C. van Amsterdam"
                    },
                    {
                        "name": "Nan van Geloven"
                    },
                    {
                        "name": "Jesse H. Krijthe"
                    },
                    {
                        "name": "Rajesh Ranganath"
                    },
                    {
                        "name": "Giovanni Ciná"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Ciná"
                },
                "author": "Giovanni Ciná",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.01210v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.01210v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14277v1",
                "updated": "2024-08-26T13:53:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    13,
                    53,
                    4,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T13:53:04Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    13,
                    53,
                    4,
                    0,
                    239,
                    0
                ],
                "title": "Epidemic Information Extraction for Event-Based Surveillance using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Epidemic Information Extraction for Event-Based Surveillance using Large\n  Language Models"
                },
                "summary": "This paper presents a novel approach to epidemic surveillance, leveraging the\npower of Artificial Intelligence and Large Language Models (LLMs) for effective\ninterpretation of unstructured big data sources, like the popular ProMED and\nWHO Disease Outbreak News. We explore several LLMs, evaluating their\ncapabilities in extracting valuable epidemic information. We further enhance\nthe capabilities of the LLMs using in-context learning, and test the\nperformance of an ensemble model incorporating multiple open-source LLMs. The\nfindings indicate that LLMs can significantly enhance the accuracy and\ntimeliness of epidemic modelling and forecasting, offering a promising tool for\nmanaging future pandemic events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach to epidemic surveillance, leveraging the\npower of Artificial Intelligence and Large Language Models (LLMs) for effective\ninterpretation of unstructured big data sources, like the popular ProMED and\nWHO Disease Outbreak News. We explore several LLMs, evaluating their\ncapabilities in extracting valuable epidemic information. We further enhance\nthe capabilities of the LLMs using in-context learning, and test the\nperformance of an ensemble model incorporating multiple open-source LLMs. The\nfindings indicate that LLMs can significantly enhance the accuracy and\ntimeliness of epidemic modelling and forecasting, offering a promising tool for\nmanaging future pandemic events."
                },
                "authors": [
                    {
                        "name": "Sergio Consoli"
                    },
                    {
                        "name": "Peter Markov"
                    },
                    {
                        "name": "Nikolaos I. Stilianakis"
                    },
                    {
                        "name": "Lorenzo Bertolini"
                    },
                    {
                        "name": "Antonio Puertas Gallardo"
                    },
                    {
                        "name": "Mario Ceresa"
                    }
                ],
                "author_detail": {
                    "name": "Mario Ceresa"
                },
                "author": "Mario Ceresa",
                "arxiv_doi": "10.1007/978-981-97-4581-4_17",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-981-97-4581-4_17",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.14277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 4 figures, Ninth International Congress on Information and\n  Communication Technology (ICICT 2024)",
                "arxiv_journal_ref": "Lecture Notes in Networks and Systems, 2024, vol 1011, pages\n  241-252. Springer, Singapore",
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01, 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.2.7; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14259v1",
                "updated": "2024-08-26T13:26:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    13,
                    26,
                    44,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T13:26:44Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    13,
                    26,
                    44,
                    0,
                    239,
                    0
                ],
                "title": "Towards Synthetic Trace Generation of Modeling Operations using\n  In-Context Learning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Synthetic Trace Generation of Modeling Operations using\n  In-Context Learning Approach"
                },
                "summary": "Producing accurate software models is crucial in model-driven software\nengineering (MDE). However, modeling complex systems is an error-prone task\nthat requires deep application domain knowledge. In the past decade, several\nautomated techniques have been proposed to support academic and industrial\npractitioners by providing relevant modeling operations. Nevertheless, those\ntechniques require a huge amount of training data that cannot be available due\nto several factors, e.g., privacy issues. The advent of large language models\n(LLMs) can support the generation of synthetic data although state-of-the-art\napproaches are not yet supporting the generation of modeling operations. To\nfill the gap, we propose a conceptual framework that combines modeling event\nlogs, intelligent modeling assistants, and the generation of modeling\noperations using LLMs. In particular, the architecture comprises modeling\ncomponents that help the designer specify the system, record its operation\nwithin a graphical modeling environment, and automatically recommend relevant\noperations. In addition, we generate a completely new dataset of modeling\nevents by telling on the most prominent LLMs currently available. As a proof of\nconcept, we instantiate the proposed framework using a set of existing modeling\ntools employed in industrial use cases within different European projects. To\nassess the proposed methodology, we first evaluate the capability of the\nexamined LLMs to generate realistic modeling operations by relying on\nwell-founded distance metrics. Then, we evaluate the recommended operations by\nconsidering real-world industrial modeling artifacts. Our findings demonstrate\nthat LLMs can generate modeling events even though the overall accuracy is\nhigher when considering human-based operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Producing accurate software models is crucial in model-driven software\nengineering (MDE). However, modeling complex systems is an error-prone task\nthat requires deep application domain knowledge. In the past decade, several\nautomated techniques have been proposed to support academic and industrial\npractitioners by providing relevant modeling operations. Nevertheless, those\ntechniques require a huge amount of training data that cannot be available due\nto several factors, e.g., privacy issues. The advent of large language models\n(LLMs) can support the generation of synthetic data although state-of-the-art\napproaches are not yet supporting the generation of modeling operations. To\nfill the gap, we propose a conceptual framework that combines modeling event\nlogs, intelligent modeling assistants, and the generation of modeling\noperations using LLMs. In particular, the architecture comprises modeling\ncomponents that help the designer specify the system, record its operation\nwithin a graphical modeling environment, and automatically recommend relevant\noperations. In addition, we generate a completely new dataset of modeling\nevents by telling on the most prominent LLMs currently available. As a proof of\nconcept, we instantiate the proposed framework using a set of existing modeling\ntools employed in industrial use cases within different European projects. To\nassess the proposed methodology, we first evaluate the capability of the\nexamined LLMs to generate realistic modeling operations by relying on\nwell-founded distance metrics. Then, we evaluate the recommended operations by\nconsidering real-world industrial modeling artifacts. Our findings demonstrate\nthat LLMs can generate modeling events even though the overall accuracy is\nhigher when considering human-based operations."
                },
                "authors": [
                    {
                        "name": "Vittoriano Muttillo"
                    },
                    {
                        "name": "Claudio Di Sipio"
                    },
                    {
                        "name": "Riccardo Rubei"
                    },
                    {
                        "name": "Luca Berardinelli"
                    },
                    {
                        "name": "MohammadHadi Dehghani"
                    }
                ],
                "author_detail": {
                    "name": "MohammadHadi Dehghani"
                },
                "author": "MohammadHadi Dehghani",
                "arxiv_comment": "Accepted at the 39th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20584v2",
                "updated": "2024-08-26T13:19:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    13,
                    19,
                    48,
                    0,
                    239,
                    0
                ],
                "published": "2024-07-30T06:33:44Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    6,
                    33,
                    44,
                    1,
                    212,
                    0
                ],
                "title": "Pruning Large Language Models with Semi-Structural Adaptive Sparse\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning Large Language Models with Semi-Structural Adaptive Sparse\n  Training"
                },
                "summary": "The tremendous success of Large Language Models (LLMs) across various complex\ntasks relies heavily on their substantial scale, which raises challenges during\nmodel deployment due to their large memory consumption. Recently, numerous\nstudies have attempted to compress LLMs using one-shot pruning methods.\nHowever, these methods often experience considerable performance degradation on\ncomplex language understanding tasks, calling into question the feasibility of\npruning in LLMs. To address this issue, we propose a pruning pipeline for\nsemi-structured sparse models via retraining, termed Adaptive Sparse Trainer\n(AST). Unlike previous one-shot pruning methods, AST incrementally transforms\ndense models into sparse ones by applying decay to masked weights while\nallowing the model to adaptively select masks throughout the training process.\nFurthermore, we observe that using distillation with a dense model as the\nteacher can prevent the sparse model from falling into local optima and\naccelerate convergence. In addition, we incorporate extra well-initialized\nparameters to further enhance model performance with minimal increase in memory\nfootprint. AST can significantly enhance model performance, approaching the\nlevel of dense models. When applied to the LLaMA2-7B model, AST reduces the\nzero-shot accuracy gap between dense and semi-structured sparse models to 1.12%\nacross multiple zero-shot tasks, utilizing less than 0.4% of the pretraining\ntokens. Our work demonstrates the feasibility of deploying semi-structured\nsparse large language models and introduces a novel method for achieving highly\ncompressed models when combined with existing quantization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The tremendous success of Large Language Models (LLMs) across various complex\ntasks relies heavily on their substantial scale, which raises challenges during\nmodel deployment due to their large memory consumption. Recently, numerous\nstudies have attempted to compress LLMs using one-shot pruning methods.\nHowever, these methods often experience considerable performance degradation on\ncomplex language understanding tasks, calling into question the feasibility of\npruning in LLMs. To address this issue, we propose a pruning pipeline for\nsemi-structured sparse models via retraining, termed Adaptive Sparse Trainer\n(AST). Unlike previous one-shot pruning methods, AST incrementally transforms\ndense models into sparse ones by applying decay to masked weights while\nallowing the model to adaptively select masks throughout the training process.\nFurthermore, we observe that using distillation with a dense model as the\nteacher can prevent the sparse model from falling into local optima and\naccelerate convergence. In addition, we incorporate extra well-initialized\nparameters to further enhance model performance with minimal increase in memory\nfootprint. AST can significantly enhance model performance, approaching the\nlevel of dense models. When applied to the LLaMA2-7B model, AST reduces the\nzero-shot accuracy gap between dense and semi-structured sparse models to 1.12%\nacross multiple zero-shot tasks, utilizing less than 0.4% of the pretraining\ntokens. Our work demonstrates the feasibility of deploying semi-structured\nsparse large language models and introduces a novel method for achieving highly\ncompressed models when combined with existing quantization techniques."
                },
                "authors": [
                    {
                        "name": "Weiyu Huang"
                    },
                    {
                        "name": "Yuezhou Hu"
                    },
                    {
                        "name": "Guohao Jian"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Jianfei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Chen"
                },
                "author": "Jianfei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14254v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14254v1",
                "updated": "2024-08-26T13:16:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    13,
                    16,
                    42,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T13:16:42Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    13,
                    16,
                    42,
                    0,
                    239,
                    0
                ],
                "title": "Integrated Brain Connectivity Analysis with fMRI, DTI, and sMRI Powered\n  by Interpretable Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated Brain Connectivity Analysis with fMRI, DTI, and sMRI Powered\n  by Interpretable Graph Neural Networks"
                },
                "summary": "Multimodal neuroimaging modeling has becomes a widely used approach but\nconfronts considerable challenges due to heterogeneity, which encompasses\nvariability in data types, scales, and formats across modalities. This\nvariability necessitates the deployment of advanced computational methods to\nintegrate and interpret these diverse datasets within a cohesive analytical\nframework. In our research, we amalgamate functional magnetic resonance\nimaging, diffusion tensor imaging, and structural MRI into a cohesive\nframework. This integration capitalizes on the unique strengths of each\nmodality and their inherent interconnections, aiming for a comprehensive\nunderstanding of the brain's connectivity and anatomical characteristics.\nUtilizing the Glasser atlas for parcellation, we integrate imaging derived\nfeatures from various modalities: functional connectivity from fMRI, structural\nconnectivity from DTI, and anatomical features from sMRI within consistent\nregions. Our approach incorporates a masking strategy to differentially weight\nneural connections, thereby facilitating a holistic amalgamation of multimodal\nimaging data. This technique enhances interpretability at connectivity level,\ntranscending traditional analyses centered on singular regional attributes. The\nmodel is applied to the Human Connectome Project's Development study to\nelucidate the associations between multimodal imaging and cognitive functions\nthroughout youth. The analysis demonstrates improved predictive accuracy and\nuncovers crucial anatomical features and essential neural connections,\ndeepening our understanding of brain structure and function.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal neuroimaging modeling has becomes a widely used approach but\nconfronts considerable challenges due to heterogeneity, which encompasses\nvariability in data types, scales, and formats across modalities. This\nvariability necessitates the deployment of advanced computational methods to\nintegrate and interpret these diverse datasets within a cohesive analytical\nframework. In our research, we amalgamate functional magnetic resonance\nimaging, diffusion tensor imaging, and structural MRI into a cohesive\nframework. This integration capitalizes on the unique strengths of each\nmodality and their inherent interconnections, aiming for a comprehensive\nunderstanding of the brain's connectivity and anatomical characteristics.\nUtilizing the Glasser atlas for parcellation, we integrate imaging derived\nfeatures from various modalities: functional connectivity from fMRI, structural\nconnectivity from DTI, and anatomical features from sMRI within consistent\nregions. Our approach incorporates a masking strategy to differentially weight\nneural connections, thereby facilitating a holistic amalgamation of multimodal\nimaging data. This technique enhances interpretability at connectivity level,\ntranscending traditional analyses centered on singular regional attributes. The\nmodel is applied to the Human Connectome Project's Development study to\nelucidate the associations between multimodal imaging and cognitive functions\nthroughout youth. The analysis demonstrates improved predictive accuracy and\nuncovers crucial anatomical features and essential neural connections,\ndeepening our understanding of brain structure and function."
                },
                "authors": [
                    {
                        "name": "Gang Qu"
                    },
                    {
                        "name": "Ziyu Zhou"
                    },
                    {
                        "name": "Vince D. Calhoun"
                    },
                    {
                        "name": "Aiying Zhang"
                    },
                    {
                        "name": "Yu-Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Ping Wang"
                },
                "author": "Yu-Ping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14254v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12848v2",
                "updated": "2024-08-26T12:55:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    12,
                    55,
                    44,
                    0,
                    239,
                    0
                ],
                "published": "2024-03-19T15:54:48Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    15,
                    54,
                    48,
                    1,
                    79,
                    0
                ],
                "title": "Planner3D: LLM-enhanced graph prior meets 3D indoor scene explicit\n  regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planner3D: LLM-enhanced graph prior meets 3D indoor scene explicit\n  regularization"
                },
                "summary": "Compositional 3D scene synthesis has diverse applications across a spectrum\nof industries such as robotics, films, and video games, as it closely mirrors\nthe complexity of real-world multi-object environments. Conventional works\ntypically employ shape retrieval based frameworks which naturally suffer from\nlimited shape diversity. Recent progresses have been made in object shape\ngeneration with generative models such as diffusion models, which increases the\nshape fidelity. However, these approaches separately treat 3D shape generation\nand layout generation. The synthesized scenes are usually hampered by layout\ncollision, which suggests that the scene-level fidelity is still\nunder-explored. In this paper, we aim at generating realistic and reasonable 3D\nindoor scenes from scene graph. To enrich the priors of the given scene graph\ninputs, large language model is utilized to aggregate the global-wise features\nwith local node-wise and edge-wise features. With a unified graph encoder,\ngraph features are extracted to guide joint layout-shape generation. Additional\nregularization is introduced to explicitly constrain the produced 3D layouts.\nBenchmarked on the SG-FRONT dataset, our method achieves better 3D scene\nsynthesis, especially in terms of scene-level fidelity. The source code will be\nreleased after publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional 3D scene synthesis has diverse applications across a spectrum\nof industries such as robotics, films, and video games, as it closely mirrors\nthe complexity of real-world multi-object environments. Conventional works\ntypically employ shape retrieval based frameworks which naturally suffer from\nlimited shape diversity. Recent progresses have been made in object shape\ngeneration with generative models such as diffusion models, which increases the\nshape fidelity. However, these approaches separately treat 3D shape generation\nand layout generation. The synthesized scenes are usually hampered by layout\ncollision, which suggests that the scene-level fidelity is still\nunder-explored. In this paper, we aim at generating realistic and reasonable 3D\nindoor scenes from scene graph. To enrich the priors of the given scene graph\ninputs, large language model is utilized to aggregate the global-wise features\nwith local node-wise and edge-wise features. With a unified graph encoder,\ngraph features are extracted to guide joint layout-shape generation. Additional\nregularization is introduced to explicitly constrain the produced 3D layouts.\nBenchmarked on the SG-FRONT dataset, our method achieves better 3D scene\nsynthesis, especially in terms of scene-level fidelity. The source code will be\nreleased after publication."
                },
                "authors": [
                    {
                        "name": "Yao Wei"
                    },
                    {
                        "name": "Martin Renqiang Min"
                    },
                    {
                        "name": "George Vosselman"
                    },
                    {
                        "name": "Li Erran Li"
                    },
                    {
                        "name": "Michael Ying Yang"
                    }
                ],
                "author_detail": {
                    "name": "Michael Ying Yang"
                },
                "author": "Michael Ying Yang",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14238v1",
                "updated": "2024-08-26T12:52:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    12,
                    52,
                    2,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T12:52:02Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    12,
                    52,
                    2,
                    0,
                    239,
                    0
                ],
                "title": "Are LLM-based Recommenders Already the Best? Simple Scaled Cross-entropy\n  Unleashes the Potential of Traditional Sequential Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLM-based Recommenders Already the Best? Simple Scaled Cross-entropy\n  Unleashes the Potential of Traditional Sequential Recommenders"
                },
                "summary": "Large language models (LLMs) have been garnering increasing attention in the\nrecommendation community. Some studies have observed that LLMs, when fine-tuned\nby the cross-entropy (CE) loss with a full softmax, could achieve\n`state-of-the-art' performance in sequential recommendation. However, most of\nthe baselines used for comparison are trained using a pointwise/pairwise loss\nfunction. This inconsistent experimental setting leads to the underestimation\nof traditional methods and further fosters over-confidence in the ranking\ncapability of LLMs.\n  In this study, we provide theoretical justification for the superiority of\nthe cross-entropy loss by demonstrating its two desirable properties: tightness\nand coverage. Furthermore, this study sheds light on additional novel insights:\n1) Taking into account only the recommendation performance, CE is not yet\noptimal as it is not a quite tight bound in terms of some ranking metrics. 2)\nIn scenarios that full softmax cannot be performed, an effective alternative is\nto scale up the sampled normalizing term. These findings then help unleash the\npotential of traditional recommendation models, allowing them to surpass\nLLM-based counterparts. Given the substantial computational burden, existing\nLLM-based methods are not as effective as claimed for sequential\nrecommendation. We hope that these theoretical understandings in conjunction\nwith the empirical results will facilitate an objective evaluation of LLM-based\nrecommendation in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been garnering increasing attention in the\nrecommendation community. Some studies have observed that LLMs, when fine-tuned\nby the cross-entropy (CE) loss with a full softmax, could achieve\n`state-of-the-art' performance in sequential recommendation. However, most of\nthe baselines used for comparison are trained using a pointwise/pairwise loss\nfunction. This inconsistent experimental setting leads to the underestimation\nof traditional methods and further fosters over-confidence in the ranking\ncapability of LLMs.\n  In this study, we provide theoretical justification for the superiority of\nthe cross-entropy loss by demonstrating its two desirable properties: tightness\nand coverage. Furthermore, this study sheds light on additional novel insights:\n1) Taking into account only the recommendation performance, CE is not yet\noptimal as it is not a quite tight bound in terms of some ranking metrics. 2)\nIn scenarios that full softmax cannot be performed, an effective alternative is\nto scale up the sampled normalizing term. These findings then help unleash the\npotential of traditional recommendation models, allowing them to surpass\nLLM-based counterparts. Given the substantial computational burden, existing\nLLM-based methods are not as effective as claimed for sequential\nrecommendation. We hope that these theoretical understandings in conjunction\nwith the empirical results will facilitate an objective evaluation of LLM-based\nrecommendation in the future."
                },
                "authors": [
                    {
                        "name": "Cong Xu"
                    },
                    {
                        "name": "Zhangchi Zhu"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Jianyong Wang"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "18 pages. arXiv admin note: substantial text overlap with\n  arXiv:2402.06216",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14219v1",
                "updated": "2024-08-26T12:33:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    12,
                    33,
                    43,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T12:33:43Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    12,
                    33,
                    43,
                    0,
                    239,
                    0
                ],
                "title": "Visuo-Tactile Exploration of Unknown Rigid 3D Curvatures by\n  Vision-Augmented Unified Force-Impedance Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visuo-Tactile Exploration of Unknown Rigid 3D Curvatures by\n  Vision-Augmented Unified Force-Impedance Control"
                },
                "summary": "Despite recent advancements in torque-controlled tactile robots, integrating\nthem into manufacturing settings remains challenging, particularly in complex\nenvironments. Simplifying robotic skill programming for non-experts is crucial\nfor increasing robot deployment in manufacturing. This work proposes an\ninnovative approach, Vision-Augmented Unified Force-Impedance Control\n(VA-UFIC), aimed at intuitive visuo-tactile exploration of unknown 3D\ncurvatures. VA-UFIC stands out by seamlessly integrating vision and tactile\ndata, enabling the exploration of diverse contact shapes in three dimensions,\nincluding point contacts, flat contacts with concave and convex curvatures, and\nscenarios involving contact loss. A pivotal component of our method is a robust\nonline contact alignment monitoring system that considers tactile error, local\nsurface curvature, and orientation, facilitating adaptive adjustments of robot\nstiffness and force regulation during exploration. We introduce virtual energy\ntanks within the control framework to ensure safety and stability, effectively\naddressing inherent safety concerns in visuo-tactile exploration. Evaluation\nusing a Franka Emika research robot demonstrates the efficacy of VA-UFIC in\nexploring unknown 3D curvatures while adhering to arbitrarily defined\nforce-motion policies. By seamlessly integrating vision and tactile sensing,\nVA-UFIC offers a promising avenue for intuitive exploration of complex\nenvironments, with potential applications spanning manufacturing, inspection,\nand beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advancements in torque-controlled tactile robots, integrating\nthem into manufacturing settings remains challenging, particularly in complex\nenvironments. Simplifying robotic skill programming for non-experts is crucial\nfor increasing robot deployment in manufacturing. This work proposes an\ninnovative approach, Vision-Augmented Unified Force-Impedance Control\n(VA-UFIC), aimed at intuitive visuo-tactile exploration of unknown 3D\ncurvatures. VA-UFIC stands out by seamlessly integrating vision and tactile\ndata, enabling the exploration of diverse contact shapes in three dimensions,\nincluding point contacts, flat contacts with concave and convex curvatures, and\nscenarios involving contact loss. A pivotal component of our method is a robust\nonline contact alignment monitoring system that considers tactile error, local\nsurface curvature, and orientation, facilitating adaptive adjustments of robot\nstiffness and force regulation during exploration. We introduce virtual energy\ntanks within the control framework to ensure safety and stability, effectively\naddressing inherent safety concerns in visuo-tactile exploration. Evaluation\nusing a Franka Emika research robot demonstrates the efficacy of VA-UFIC in\nexploring unknown 3D curvatures while adhering to arbitrarily defined\nforce-motion policies. By seamlessly integrating vision and tactile sensing,\nVA-UFIC offers a promising avenue for intuitive exploration of complex\nenvironments, with potential applications spanning manufacturing, inspection,\nand beyond."
                },
                "authors": [
                    {
                        "name": "Kübra Karacan"
                    },
                    {
                        "name": "Anran Zhang"
                    },
                    {
                        "name": "Hamid Sadeghian"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Sami Haddadin"
                    }
                ],
                "author_detail": {
                    "name": "Sami Haddadin"
                },
                "author": "Sami Haddadin",
                "arxiv_comment": "8 pages, 3 figures, accepted by IROS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14199v1",
                "updated": "2024-08-26T11:54:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    11,
                    54,
                    27,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T11:54:27Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    11,
                    54,
                    27,
                    0,
                    239,
                    0
                ],
                "title": "A Survey on Small-Scale Testbeds for Connected and Automated Vehicles\n  and Robot Swarms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Small-Scale Testbeds for Connected and Automated Vehicles\n  and Robot Swarms"
                },
                "summary": "Connected and automated vehicles and robot swarms hold transformative\npotential for enhancing safety, efficiency, and sustainability in the\ntransportation and manufacturing sectors. Extensive testing and validation of\nthese technologies is crucial for their deployment in the real world. While\nsimulations are essential for initial testing, they often have limitations in\ncapturing the complex dynamics of real-world interactions. This limitation\nunderscores the importance of small-scale testbeds. These testbeds provide a\nrealistic, cost-effective, and controlled environment for testing and\nvalidating algorithms, acting as an essential intermediary between simulation\nand full-scale experiments. This work serves to facilitate researchers' efforts\nin identifying existing small-scale testbeds suitable for their experiments and\nprovide insights for those who want to build their own. In addition, it\ndelivers a comprehensive survey of the current landscape of these testbeds. We\nderive 62 characteristics of testbeds based on the well-known sense-plan-act\nparadigm and offer an online table comparing 22 small-scale testbeds based on\nthese characteristics. The online table is hosted on our designated public\nwebpage www.cpm-remote.de/testbeds, and we invite testbed creators and\ndevelopers to contribute to it. We closely examine nine testbeds in this paper,\ndemonstrating how the derived characteristics can be used to present testbeds.\nFurthermore, we discuss three ongoing challenges concerning small-scale\ntestbeds that we identified, i.e., small-scale to full-scale transition,\nsustainability, and power and resource management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Connected and automated vehicles and robot swarms hold transformative\npotential for enhancing safety, efficiency, and sustainability in the\ntransportation and manufacturing sectors. Extensive testing and validation of\nthese technologies is crucial for their deployment in the real world. While\nsimulations are essential for initial testing, they often have limitations in\ncapturing the complex dynamics of real-world interactions. This limitation\nunderscores the importance of small-scale testbeds. These testbeds provide a\nrealistic, cost-effective, and controlled environment for testing and\nvalidating algorithms, acting as an essential intermediary between simulation\nand full-scale experiments. This work serves to facilitate researchers' efforts\nin identifying existing small-scale testbeds suitable for their experiments and\nprovide insights for those who want to build their own. In addition, it\ndelivers a comprehensive survey of the current landscape of these testbeds. We\nderive 62 characteristics of testbeds based on the well-known sense-plan-act\nparadigm and offer an online table comparing 22 small-scale testbeds based on\nthese characteristics. The online table is hosted on our designated public\nwebpage www.cpm-remote.de/testbeds, and we invite testbed creators and\ndevelopers to contribute to it. We closely examine nine testbeds in this paper,\ndemonstrating how the derived characteristics can be used to present testbeds.\nFurthermore, we discuss three ongoing challenges concerning small-scale\ntestbeds that we identified, i.e., small-scale to full-scale transition,\nsustainability, and power and resource management."
                },
                "authors": [
                    {
                        "name": "Armin Mokhtarian"
                    },
                    {
                        "name": "Jianye Xu"
                    },
                    {
                        "name": "Patrick Scheffe"
                    },
                    {
                        "name": "Maximilian Kloock"
                    },
                    {
                        "name": "Simon Schäfer"
                    },
                    {
                        "name": "Heeseung Bang"
                    },
                    {
                        "name": "Viet-Anh Le"
                    },
                    {
                        "name": "Sangeet Ulhas"
                    },
                    {
                        "name": "Johannes Betz"
                    },
                    {
                        "name": "Sean Wilson"
                    },
                    {
                        "name": "Spring Berman"
                    },
                    {
                        "name": "Liam Paull"
                    },
                    {
                        "name": "Amanda Prorok"
                    },
                    {
                        "name": "Bassam Alrifaee"
                    }
                ],
                "author_detail": {
                    "name": "Bassam Alrifaee"
                },
                "author": "Bassam Alrifaee",
                "arxiv_doi": "10.13140/RG.2.2.16176.74248/1",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.13140/RG.2.2.16176.74248/1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.14199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 11 figures, 1 table. This work has been submitted to the\n  IEEE Robotics & Automation Magazine for possible publication. Copyright may\n  be transferred without notice, after which this version may no longer be\n  accessible",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05141v2",
                "updated": "2024-08-26T10:53:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    10,
                    53,
                    28,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-09T15:53:55Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    15,
                    53,
                    55,
                    4,
                    222,
                    0
                ],
                "title": "A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning"
                },
                "summary": "Retrieval-augmented generation (RAG) is a framework enabling large language\nmodels (LLMs) to enhance their accuracy and reduce hallucinations by\nintegrating external knowledge bases. In this paper, we introduce a hybrid RAG\nsystem enhanced through a comprehensive suite of optimizations that\nsignificantly improve retrieval quality, augment reasoning capabilities, and\nrefine numerical computation ability. We refined the text chunks and tables in\nweb pages, added attribute predictors to reduce hallucinations, conducted LLM\nKnowledge Extractor and Knowledge Graph Extractor, and finally built a\nreasoning strategy with all the references. We evaluated our system on the CRAG\ndataset through the Meta CRAG KDD Cup 2024 Competition. Both the local and\nonline evaluations demonstrate that our system significantly enhances complex\nreasoning capabilities. In local evaluations, we have significantly improved\naccuracy and reduced error rates compared to the baseline model, achieving a\nnotable increase in scores. In the meanwhile, we have attained outstanding\nresults in online assessments, demonstrating the performance and generalization\ncapabilities of the proposed system. The source code for our system is released\nin \\url{https://gitlab.aicrowd.com/shizueyy/crag-new}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) is a framework enabling large language\nmodels (LLMs) to enhance their accuracy and reduce hallucinations by\nintegrating external knowledge bases. In this paper, we introduce a hybrid RAG\nsystem enhanced through a comprehensive suite of optimizations that\nsignificantly improve retrieval quality, augment reasoning capabilities, and\nrefine numerical computation ability. We refined the text chunks and tables in\nweb pages, added attribute predictors to reduce hallucinations, conducted LLM\nKnowledge Extractor and Knowledge Graph Extractor, and finally built a\nreasoning strategy with all the references. We evaluated our system on the CRAG\ndataset through the Meta CRAG KDD Cup 2024 Competition. Both the local and\nonline evaluations demonstrate that our system significantly enhances complex\nreasoning capabilities. In local evaluations, we have significantly improved\naccuracy and reduced error rates compared to the baseline model, achieving a\nnotable increase in scores. In the meanwhile, we have attained outstanding\nresults in online assessments, demonstrating the performance and generalization\ncapabilities of the proposed system. The source code for our system is released\nin \\url{https://gitlab.aicrowd.com/shizueyy/crag-new}."
                },
                "authors": [
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Chengwu Liu"
                    },
                    {
                        "name": "Jingyang Yuan"
                    },
                    {
                        "name": "Gongbo Sun"
                    },
                    {
                        "name": "Siqi Li"
                    },
                    {
                        "name": "Ming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zhang"
                },
                "author": "Ming Zhang",
                "arxiv_comment": "Technical report for 3rd prize in Task 1 of Meta CRAG KDD Cup 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14158v1",
                "updated": "2024-08-26T10:11:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    10,
                    11,
                    56,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T10:11:56Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    10,
                    11,
                    56,
                    0,
                    239,
                    0
                ],
                "title": "Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep\n  Learning"
                },
                "summary": "The rapid progress in Deep Learning (DL) and Large Language Models (LLMs) has\nexponentially increased demands of computational power and bandwidth. This,\ncombined with the high costs of faster computing chips and interconnects, has\nsignificantly inflated High Performance Computing (HPC) construction costs. To\naddress these challenges, we introduce the Fire-Flyer AI-HPC architecture, a\nsynergistic hardware-software co-design framework and its best practices. For\nDL training, we deployed the Fire-Flyer 2 with 10,000 PCIe A100 GPUs, achieved\nperformance approximating the DGX-A100 while reducing costs by half and energy\nconsumption by 40%. We specifically engineered HFReduce to accelerate allreduce\ncommunication and implemented numerous measures to keep our Computation-Storage\nIntegrated Network congestion-free. Through our software stack, including\nHaiScale, 3FS, and HAI-Platform, we achieved substantial scalability by\noverlapping computation and communication. Our system-oriented experience from\nDL training provides valuable insights to drive future advancements in AI-HPC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress in Deep Learning (DL) and Large Language Models (LLMs) has\nexponentially increased demands of computational power and bandwidth. This,\ncombined with the high costs of faster computing chips and interconnects, has\nsignificantly inflated High Performance Computing (HPC) construction costs. To\naddress these challenges, we introduce the Fire-Flyer AI-HPC architecture, a\nsynergistic hardware-software co-design framework and its best practices. For\nDL training, we deployed the Fire-Flyer 2 with 10,000 PCIe A100 GPUs, achieved\nperformance approximating the DGX-A100 while reducing costs by half and energy\nconsumption by 40%. We specifically engineered HFReduce to accelerate allreduce\ncommunication and implemented numerous measures to keep our Computation-Storage\nIntegrated Network congestion-free. Through our software stack, including\nHaiScale, 3FS, and HAI-Platform, we achieved substantial scalability by\noverlapping computation and communication. Our system-oriented experience from\nDL training provides valuable insights to drive future advancements in AI-HPC."
                },
                "authors": [
                    {
                        "name": "Wei An"
                    },
                    {
                        "name": "Xiao Bi"
                    },
                    {
                        "name": "Guanting Chen"
                    },
                    {
                        "name": "Shanhuang Chen"
                    },
                    {
                        "name": "Chengqi Deng"
                    },
                    {
                        "name": "Honghui Ding"
                    },
                    {
                        "name": "Kai Dong"
                    },
                    {
                        "name": "Qiushi Du"
                    },
                    {
                        "name": "Wenjun Gao"
                    },
                    {
                        "name": "Kang Guan"
                    },
                    {
                        "name": "Jianzhong Guo"
                    },
                    {
                        "name": "Yongqiang Guo"
                    },
                    {
                        "name": "Zhe Fu"
                    },
                    {
                        "name": "Ying He"
                    },
                    {
                        "name": "Panpan Huang"
                    },
                    {
                        "name": "Jiashi Li"
                    },
                    {
                        "name": "Wenfeng Liang"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Yiyuan Liu"
                    },
                    {
                        "name": "Yuxuan Liu"
                    },
                    {
                        "name": "Shanghao Lu"
                    },
                    {
                        "name": "Xuan Lu"
                    },
                    {
                        "name": "Xiaotao Nie"
                    },
                    {
                        "name": "Tian Pei"
                    },
                    {
                        "name": "Junjie Qiu"
                    },
                    {
                        "name": "Hui Qu"
                    },
                    {
                        "name": "Zehui Ren"
                    },
                    {
                        "name": "Zhangli Sha"
                    },
                    {
                        "name": "Xuecheng Su"
                    },
                    {
                        "name": "Xiaowen Sun"
                    },
                    {
                        "name": "Yixuan Tan"
                    },
                    {
                        "name": "Minghui Tang"
                    },
                    {
                        "name": "Shiyu Wang"
                    },
                    {
                        "name": "Yaohui Wang"
                    },
                    {
                        "name": "Yongji Wang"
                    },
                    {
                        "name": "Ziwei Xie"
                    },
                    {
                        "name": "Yiliang Xiong"
                    },
                    {
                        "name": "Yanhong Xu"
                    },
                    {
                        "name": "Shengfeng Ye"
                    },
                    {
                        "name": "Shuiping Yu"
                    },
                    {
                        "name": "Yukun Zha"
                    },
                    {
                        "name": "Liyue Zhang"
                    },
                    {
                        "name": "Haowei Zhang"
                    },
                    {
                        "name": "Mingchuan Zhang"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Yichao Zhang"
                    },
                    {
                        "name": "Chenggang Zhao"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Shangyan Zhou"
                    },
                    {
                        "name": "Shunfeng Zhou"
                    },
                    {
                        "name": "Yuheng Zou"
                    }
                ],
                "author_detail": {
                    "name": "Yuheng Zou"
                },
                "author": "Yuheng Zou",
                "arxiv_comment": "This is the preprint version of the paper accepted for presentation\n  at the 2024 International Conference for High Performance Computing,\n  Networking, Storage, and Analysis (SC'24). \\c{opyright} 2024 IEEE. Personal\n  use of this material is permitted. For other uses, permission from IEEE must\n  be obtained. Please refer to IEEE Xplore for the final published version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04660v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04660v3",
                "updated": "2024-08-26T09:37:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    9,
                    37,
                    46,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-05T20:01:10Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    20,
                    1,
                    10,
                    0,
                    218,
                    0
                ],
                "title": "XMainframe: A Large Language Model for Mainframe Modernization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XMainframe: A Large Language Model for Mainframe Modernization"
                },
                "summary": "Mainframe operating systems, despite their inception in the 1940s, continue\nto support critical sectors like finance and government. However, these systems\nare often viewed as outdated, requiring extensive maintenance and\nmodernization. Addressing this challenge necessitates innovative tools that can\nunderstand and interact with legacy codebases. To this end, we introduce\nXMainframe, a state-of-the-art large language model (LLM) specifically designed\nwith knowledge of mainframe legacy systems and COBOL codebases. Our solution\ninvolves the creation of an extensive data collection pipeline to produce\nhigh-quality training datasets, enhancing XMainframe's performance in this\nspecialized domain. Additionally, we present MainframeBench, a comprehensive\nbenchmark for assessing mainframe knowledge, including multiple-choice\nquestions, question answering, and COBOL code summarization. Our empirical\nevaluations demonstrate that XMainframe consistently outperforms existing\nstate-of-the-art LLMs across these tasks. Specifically, XMainframe achieves 30%\nhigher accuracy than DeepSeek-Coder on multiple-choice questions, doubles the\nBLEU score of Mixtral-Instruct 8x7B on question answering, and scores six times\nhigher than GPT-3.5 on COBOL summarization. Our work highlights the potential\nof XMainframe to drive significant advancements in managing and modernizing\nlegacy systems, thereby enhancing productivity and saving time for software\ndevelopers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainframe operating systems, despite their inception in the 1940s, continue\nto support critical sectors like finance and government. However, these systems\nare often viewed as outdated, requiring extensive maintenance and\nmodernization. Addressing this challenge necessitates innovative tools that can\nunderstand and interact with legacy codebases. To this end, we introduce\nXMainframe, a state-of-the-art large language model (LLM) specifically designed\nwith knowledge of mainframe legacy systems and COBOL codebases. Our solution\ninvolves the creation of an extensive data collection pipeline to produce\nhigh-quality training datasets, enhancing XMainframe's performance in this\nspecialized domain. Additionally, we present MainframeBench, a comprehensive\nbenchmark for assessing mainframe knowledge, including multiple-choice\nquestions, question answering, and COBOL code summarization. Our empirical\nevaluations demonstrate that XMainframe consistently outperforms existing\nstate-of-the-art LLMs across these tasks. Specifically, XMainframe achieves 30%\nhigher accuracy than DeepSeek-Coder on multiple-choice questions, doubles the\nBLEU score of Mixtral-Instruct 8x7B on question answering, and scores six times\nhigher than GPT-3.5 on COBOL summarization. Our work highlights the potential\nof XMainframe to drive significant advancements in managing and modernizing\nlegacy systems, thereby enhancing productivity and saving time for software\ndevelopers."
                },
                "authors": [
                    {
                        "name": "Anh T. V. Dau"
                    },
                    {
                        "name": "Hieu Trung Dao"
                    },
                    {
                        "name": "Anh Tuan Nguyen"
                    },
                    {
                        "name": "Hieu Trung Tran"
                    },
                    {
                        "name": "Phong X. Nguyen"
                    },
                    {
                        "name": "Nghi D. Q. Bui"
                    }
                ],
                "author_detail": {
                    "name": "Nghi D. Q. Bui"
                },
                "author": "Nghi D. Q. Bui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04660v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04660v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14134v1",
                "updated": "2024-08-26T09:29:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    9,
                    29,
                    56,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T09:29:56Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    9,
                    29,
                    56,
                    0,
                    239,
                    0
                ],
                "title": "Exploring the Potential of Large Language Models for Heterophilic Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Potential of Large Language Models for Heterophilic Graphs"
                },
                "summary": "Graph Neural Networks (GNNs) are essential for various graph-based learning\ntasks. Notably, classical GNN architectures operate under the assumption of\nhomophily, which posits that connected nodes are likely to share similar\nfeatures. However, this assumption limits the effectiveness of GNNs in handling\nheterophilic graphs where connected nodes often exhibit dissimilar\ncharacteristics. Existing approaches for homophily graphs such as non-local\nneighbor extension and architectural refinement overlook the rich textual data\nassociated with nodes, which could unlock deeper insights into these\nheterophilic contexts. With advancements in Large Language Models (LLMs), there\nis significant promise to enhance GNNs by leveraging the extensive open-world\nknowledge within LLMs to more effectively interpret and utilize textual data\nfor characterizing heterophilic graphs. In this work, we explore the potential\nof LLMs for modeling heterophilic graphs and propose a novel two-stage\nframework: LLM-enhanced edge discriminator and LLM-guided edge reweighting.\nSpecifically, in the first stage, we fine-tune the LLM to better identify\nhomophilic and heterophilic edges based on the textual information of their\nnodes. In the second stage, we adaptively manage message propagation in GNNs\nfor different edge types based on node features, structures, and heterophilic\nor homophilic characteristics. To cope with the computational demands when\ndeploying LLMs in practical scenarios, we further explore model distillation\ntechniques to fine-tune smaller, more efficient models that maintain\ncompetitive performance. Extensive experiments validate the effectiveness of\nour framework, demonstrating the feasibility of using LLMs to enhance GNNs for\nnode classification on heterophilic graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are essential for various graph-based learning\ntasks. Notably, classical GNN architectures operate under the assumption of\nhomophily, which posits that connected nodes are likely to share similar\nfeatures. However, this assumption limits the effectiveness of GNNs in handling\nheterophilic graphs where connected nodes often exhibit dissimilar\ncharacteristics. Existing approaches for homophily graphs such as non-local\nneighbor extension and architectural refinement overlook the rich textual data\nassociated with nodes, which could unlock deeper insights into these\nheterophilic contexts. With advancements in Large Language Models (LLMs), there\nis significant promise to enhance GNNs by leveraging the extensive open-world\nknowledge within LLMs to more effectively interpret and utilize textual data\nfor characterizing heterophilic graphs. In this work, we explore the potential\nof LLMs for modeling heterophilic graphs and propose a novel two-stage\nframework: LLM-enhanced edge discriminator and LLM-guided edge reweighting.\nSpecifically, in the first stage, we fine-tune the LLM to better identify\nhomophilic and heterophilic edges based on the textual information of their\nnodes. In the second stage, we adaptively manage message propagation in GNNs\nfor different edge types based on node features, structures, and heterophilic\nor homophilic characteristics. To cope with the computational demands when\ndeploying LLMs in practical scenarios, we further explore model distillation\ntechniques to fine-tune smaller, more efficient models that maintain\ncompetitive performance. Extensive experiments validate the effectiveness of\nour framework, demonstrating the feasibility of using LLMs to enhance GNNs for\nnode classification on heterophilic graphs."
                },
                "authors": [
                    {
                        "name": "Yuxia Wu"
                    },
                    {
                        "name": "Shujie Li"
                    },
                    {
                        "name": "Yuan Fang"
                    },
                    {
                        "name": "Chuan Shi"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Shi"
                },
                "author": "Chuan Shi",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.11534v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.11534v6",
                "updated": "2024-08-26T08:52:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    8,
                    52,
                    44,
                    0,
                    239,
                    0
                ],
                "published": "2023-08-21T06:51:56Z",
                "published_parsed": [
                    2023,
                    8,
                    21,
                    6,
                    51,
                    56,
                    0,
                    233,
                    0
                ],
                "title": "PlatoLM: Teaching LLMs in Multi-Round Dialogue via a User Simulator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PlatoLM: Teaching LLMs in Multi-Round Dialogue via a User Simulator"
                },
                "summary": "The unparalleled performance of closed-sourced ChatGPT has sparked efforts\ntowards its democratization, with notable strides made by leveraging real user\nand ChatGPT dialogues, as evidenced by Vicuna. However, due to challenges in\ngathering dialogues involving human participation, current endeavors like Baize\nand UltraChat rely on ChatGPT conducting roleplay to simulate humans based on\ninstructions, resulting in overdependence on seeds, diminished human-likeness,\nlimited topic diversity, and an absence of genuine multi-round conversational\ndynamics. To address the above issues, we propose a paradigm to simulate human\nbehavior better and explore the benefits of incorporating more human-like\nquestions in multi-turn conversations. Specifically, we directly target human\nquestions extracted from genuine human-machine conversations as a learning goal\nand provide a novel user simulator called `Socratic'. The experimental results\nshow our response model, `PlatoLM', achieves SoTA performance among LLaMA-based\n7B models in MT-Bench. Our findings further demonstrate that our method\nintroduces highly human-like questioning patterns and rich topic structures,\nwhich can teach the response model better than previous works in multi-round\nconversations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The unparalleled performance of closed-sourced ChatGPT has sparked efforts\ntowards its democratization, with notable strides made by leveraging real user\nand ChatGPT dialogues, as evidenced by Vicuna. However, due to challenges in\ngathering dialogues involving human participation, current endeavors like Baize\nand UltraChat rely on ChatGPT conducting roleplay to simulate humans based on\ninstructions, resulting in overdependence on seeds, diminished human-likeness,\nlimited topic diversity, and an absence of genuine multi-round conversational\ndynamics. To address the above issues, we propose a paradigm to simulate human\nbehavior better and explore the benefits of incorporating more human-like\nquestions in multi-turn conversations. Specifically, we directly target human\nquestions extracted from genuine human-machine conversations as a learning goal\nand provide a novel user simulator called `Socratic'. The experimental results\nshow our response model, `PlatoLM', achieves SoTA performance among LLaMA-based\n7B models in MT-Bench. Our findings further demonstrate that our method\nintroduces highly human-like questioning patterns and rich topic structures,\nwhich can teach the response model better than previous works in multi-round\nconversations."
                },
                "authors": [
                    {
                        "name": "Chuyi Kong"
                    },
                    {
                        "name": "Yaxin Fan"
                    },
                    {
                        "name": "Xiang Wan"
                    },
                    {
                        "name": "Feng Jiang"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "arxiv_comment": "23 pages",
                "arxiv_journal_ref": "ACL 2024 Main Conference, 7841-7863",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.11534v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.11534v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10833v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10833v2",
                "updated": "2024-08-26T08:47:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    8,
                    47,
                    54,
                    0,
                    239,
                    0
                ],
                "published": "2024-06-16T08:03:24Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    8,
                    3,
                    24,
                    6,
                    168,
                    0
                ],
                "title": "A Comprehensive Survey of Scientific Large Language Models and Their\n  Applications in Scientific Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey of Scientific Large Language Models and Their\n  Applications in Scientific Discovery"
                },
                "summary": "In many scientific fields, large language models (LLMs) have revolutionized\nthe way text and other modalities of data (e.g., molecules and proteins) are\nhandled, achieving superior performance in various applications and augmenting\nthe scientific discovery process. Nevertheless, previous surveys on scientific\nLLMs often concentrate on one or two fields or a single modality. In this\npaper, we aim to provide a more holistic view of the research landscape by\nunveiling cross-field and cross-modal connections between scientific LLMs\nregarding their architectures and pre-training techniques. To this end, we\ncomprehensively survey over 250 scientific LLMs, discuss their commonalities\nand differences, as well as summarize pre-training datasets and evaluation\ntasks for each field and modality. Moreover, we investigate how LLMs have been\ndeployed to benefit scientific discovery. Resources related to this survey are\navailable at https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many scientific fields, large language models (LLMs) have revolutionized\nthe way text and other modalities of data (e.g., molecules and proteins) are\nhandled, achieving superior performance in various applications and augmenting\nthe scientific discovery process. Nevertheless, previous surveys on scientific\nLLMs often concentrate on one or two fields or a single modality. In this\npaper, we aim to provide a more holistic view of the research landscape by\nunveiling cross-field and cross-modal connections between scientific LLMs\nregarding their architectures and pre-training techniques. To this end, we\ncomprehensively survey over 250 scientific LLMs, discuss their commonalities\nand differences, as well as summarize pre-training datasets and evaluation\ntasks for each field and modality. Moreover, we investigate how LLMs have been\ndeployed to benefit scientific discovery. Resources related to this survey are\navailable at https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Xiusi Chen"
                    },
                    {
                        "name": "Bowen Jin"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Shuiwang Ji"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Jiawei Han"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Han"
                },
                "author": "Jiawei Han",
                "arxiv_comment": "34 pages (GitHub:\n  https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10833v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10833v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11322v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11322v4",
                "updated": "2024-08-26T08:25:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    8,
                    25,
                    1,
                    0,
                    239,
                    0
                ],
                "published": "2024-03-17T19:54:16Z",
                "published_parsed": [
                    2024,
                    3,
                    17,
                    19,
                    54,
                    16,
                    6,
                    77,
                    0
                ],
                "title": "StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows"
                },
                "summary": "It is a notable trend to use Large Language Models (LLMs) to tackle complex\ntasks, e.g., tasks that require a sequence of actions and dynamic interaction\nwith tools and external environments. In this paper, we propose StateFlow, a\nnovel LLM-based task-solving paradigm that conceptualizes complex task-solving\nprocesses as state machines. In StateFlow, we distinguish between \"process\ngrounding\" (via state and state transitions) and \"sub-task solving\" (through\nactions within a state), enhancing control and interpretability of the\ntask-solving procedure. A state represents the status of a running process. The\ntransitions between states are controlled by heuristic rules or decisions made\nby the LLM, allowing for a dynamic and adaptive progression. Upon entering a\nstate, a series of actions is executed, involving not only calling LLMs guided\nby different prompts, but also the utilization of external tools as needed. Our\nresults show that StateFlow significantly enhances LLMs' efficiency. For\ninstance, StateFlow achieves 13% and 28% higher success rates compared to ReAct\nin InterCode SQL and ALFWorld benchmark, with 5x and 3x less cost respectively.\nWe also show that StateFlow can be combined with iterative refining methods\nlike Reflexion to further improve performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is a notable trend to use Large Language Models (LLMs) to tackle complex\ntasks, e.g., tasks that require a sequence of actions and dynamic interaction\nwith tools and external environments. In this paper, we propose StateFlow, a\nnovel LLM-based task-solving paradigm that conceptualizes complex task-solving\nprocesses as state machines. In StateFlow, we distinguish between \"process\ngrounding\" (via state and state transitions) and \"sub-task solving\" (through\nactions within a state), enhancing control and interpretability of the\ntask-solving procedure. A state represents the status of a running process. The\ntransitions between states are controlled by heuristic rules or decisions made\nby the LLM, allowing for a dynamic and adaptive progression. Upon entering a\nstate, a series of actions is executed, involving not only calling LLMs guided\nby different prompts, but also the utilization of external tools as needed. Our\nresults show that StateFlow significantly enhances LLMs' efficiency. For\ninstance, StateFlow achieves 13% and 28% higher success rates compared to ReAct\nin InterCode SQL and ALFWorld benchmark, with 5x and 3x less cost respectively.\nWe also show that StateFlow can be combined with iterative refining methods\nlike Reflexion to further improve performance."
                },
                "authors": [
                    {
                        "name": "Yiran Wu"
                    },
                    {
                        "name": "Tianwei Yue"
                    },
                    {
                        "name": "Shaokun Zhang"
                    },
                    {
                        "name": "Chi Wang"
                    },
                    {
                        "name": "Qingyun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Qingyun Wu"
                },
                "author": "Qingyun Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.11322v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11322v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.08334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.08334v2",
                "updated": "2024-08-26T08:24:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    8,
                    24,
                    14,
                    0,
                    239,
                    0
                ],
                "published": "2024-05-14T06:09:08Z",
                "published_parsed": [
                    2024,
                    5,
                    14,
                    6,
                    9,
                    8,
                    1,
                    135,
                    0
                ],
                "title": "Could Chemical LLMs benefit from Message Passing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Could Chemical LLMs benefit from Message Passing"
                },
                "summary": "Pretrained language models (LMs) showcase significant capabilities in\nprocessing molecular text, while concurrently, message passing neural networks\n(MPNNs) demonstrate resilience and versatility in the domain of molecular\nscience. Despite these advancements, we find there are limited studies\ninvestigating the bidirectional interactions between molecular structures and\ntheir corresponding textual representations. Therefore, in this paper, we\npropose two strategies to evaluate whether an information integration can\nenhance the performance: contrast learning, which involves utilizing an MPNN to\nsupervise the training of the LM, and fusion, which exploits information from\nboth models. Our empirical analysis reveals that the integration approaches\nexhibit superior performance compared to baselines when applied to smaller\nmolecular graphs, while these integration approaches do not yield performance\nenhancements on large scale graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained language models (LMs) showcase significant capabilities in\nprocessing molecular text, while concurrently, message passing neural networks\n(MPNNs) demonstrate resilience and versatility in the domain of molecular\nscience. Despite these advancements, we find there are limited studies\ninvestigating the bidirectional interactions between molecular structures and\ntheir corresponding textual representations. Therefore, in this paper, we\npropose two strategies to evaluate whether an information integration can\nenhance the performance: contrast learning, which involves utilizing an MPNN to\nsupervise the training of the LM, and fusion, which exploits information from\nboth models. Our empirical analysis reveals that the integration approaches\nexhibit superior performance compared to baselines when applied to smaller\nmolecular graphs, while these integration approaches do not yield performance\nenhancements on large scale graphs."
                },
                "authors": [
                    {
                        "name": "Jiaqing Xie"
                    },
                    {
                        "name": "Ziheng Chi"
                    }
                ],
                "author_detail": {
                    "name": "Ziheng Chi"
                },
                "author": "Ziheng Chi",
                "arxiv_comment": "Accepted at ACL @ Languages and Molecules 2024. In Proceedings of ACL\n  2024",
                "arxiv_journal_ref": "In Proceedings of the 1st Workshop on Language + Molecules (L+M\n  2024), pages 10 20, Bangkok, Thailand. Association for Computational\n  Linguistics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.08334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.08334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14089v1",
                "updated": "2024-08-26T08:20:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    8,
                    20,
                    49,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T08:20:49Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    8,
                    20,
                    49,
                    0,
                    239,
                    0
                ],
                "title": "Mini-Slot-Assisted Short Packet URLLC:Differential or Coherent\n  Detection?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mini-Slot-Assisted Short Packet URLLC:Differential or Coherent\n  Detection?"
                },
                "summary": "One of the primary challenges in short packet ultra-reliable and low-latency\ncommunications (URLLC) is to achieve reliable channel estimation and data\ndetection while minimizing the impact on latency performance. Given the small\npacket size in mini-slot-assisted URLLC, relying solely on pilot-based coherent\ndetection is almost impossible to meet the seemingly contradictory requirements\nof high channel estimation accuracy, high reliability, low training overhead,\nand low latency. In this paper, we explore differential modulation both in the\nfrequency domain and in the time domain, and propose adopting an adaptive\napproach that integrates both differential and coherent detection to achieve\nmini-slot-assisted short packet URLLC, striking a balance among training\noverhead, system performance, and computational complexity. Specifically,\ndifferential (especially in the frequency domain) and coherent detection\nschemes can be dynamically activated based on application scenarios, channel\nstatistics, information payloads, mini-slot deployment options, and service\nrequirements. Furthermore, we derive the block error rate (BLER) for\npilot-based, frequency domain, and time domain differential OFDM using\nnon-asymptotic information-theoretic bounds. Simulation results validate the\nfeasibility and effectiveness of adaptive differential and coherent detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the primary challenges in short packet ultra-reliable and low-latency\ncommunications (URLLC) is to achieve reliable channel estimation and data\ndetection while minimizing the impact on latency performance. Given the small\npacket size in mini-slot-assisted URLLC, relying solely on pilot-based coherent\ndetection is almost impossible to meet the seemingly contradictory requirements\nof high channel estimation accuracy, high reliability, low training overhead,\nand low latency. In this paper, we explore differential modulation both in the\nfrequency domain and in the time domain, and propose adopting an adaptive\napproach that integrates both differential and coherent detection to achieve\nmini-slot-assisted short packet URLLC, striking a balance among training\noverhead, system performance, and computational complexity. Specifically,\ndifferential (especially in the frequency domain) and coherent detection\nschemes can be dynamically activated based on application scenarios, channel\nstatistics, information payloads, mini-slot deployment options, and service\nrequirements. Furthermore, we derive the block error rate (BLER) for\npilot-based, frequency domain, and time domain differential OFDM using\nnon-asymptotic information-theoretic bounds. Simulation results validate the\nfeasibility and effectiveness of adaptive differential and coherent detection."
                },
                "authors": [
                    {
                        "name": "Canjian Zheng"
                    },
                    {
                        "name": "Fu-Chun Zheng"
                    },
                    {
                        "name": "Jingjing Luo"
                    },
                    {
                        "name": "Pengcheng Zhu"
                    },
                    {
                        "name": "Xiaohu You"
                    },
                    {
                        "name": "Daquan Feng"
                    }
                ],
                "author_detail": {
                    "name": "Daquan Feng"
                },
                "author": "Daquan Feng",
                "arxiv_comment": "14 pages, 8 figures, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03624v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03624v2",
                "updated": "2024-08-26T08:09:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    8,
                    9,
                    39,
                    0,
                    239,
                    0
                ],
                "published": "2024-07-04T04:19:50Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    4,
                    19,
                    50,
                    3,
                    186,
                    0
                ],
                "title": "Question-Analysis Prompting Improves LLM Performance in Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question-Analysis Prompting Improves LLM Performance in Reasoning Tasks"
                },
                "summary": "Although LLMs have the potential to transform many fields, they still\nunderperform humans in reasoning tasks. Existing methods induce the model to\nproduce step-by-step calculations, but this research explores the question:\nDoes making the LLM analyze the question improve its performance? We propose a\nnovel prompting strategy called Question Analysis Prompting (QAP), in which the\nmodel is prompted to explain the question in $n$ words before solving. The\nvalue of $n$ influences the length of response generated by the model. QAP is\nevaluated on GPT 3.5 Turbo and GPT 4 Turbo on arithmetic datasets GSM8K, AQuA,\nand SAT and commonsense dataset StrategyQA. QAP is compared with other\nstate-of-the-art prompts including Chain-of-Thought (CoT), Plan and Solve\nPrompting (PS+) and Take A Deep Breath (TADB). QAP outperforms all\nstate-of-the-art prompts on AQuA and SAT datasets on both GPT3.5 and GPT4. QAP\nconsistently ranks among the top-2 prompts on 75\\% of the tests. A key factor\nof QAP performance can be attributed to response length, where detailed\nresponses are beneficial when answering harder questions, but can negatively\naffect easy questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although LLMs have the potential to transform many fields, they still\nunderperform humans in reasoning tasks. Existing methods induce the model to\nproduce step-by-step calculations, but this research explores the question:\nDoes making the LLM analyze the question improve its performance? We propose a\nnovel prompting strategy called Question Analysis Prompting (QAP), in which the\nmodel is prompted to explain the question in $n$ words before solving. The\nvalue of $n$ influences the length of response generated by the model. QAP is\nevaluated on GPT 3.5 Turbo and GPT 4 Turbo on arithmetic datasets GSM8K, AQuA,\nand SAT and commonsense dataset StrategyQA. QAP is compared with other\nstate-of-the-art prompts including Chain-of-Thought (CoT), Plan and Solve\nPrompting (PS+) and Take A Deep Breath (TADB). QAP outperforms all\nstate-of-the-art prompts on AQuA and SAT datasets on both GPT3.5 and GPT4. QAP\nconsistently ranks among the top-2 prompts on 75\\% of the tests. A key factor\nof QAP performance can be attributed to response length, where detailed\nresponses are beneficial when answering harder questions, but can negatively\naffect easy questions."
                },
                "authors": [
                    {
                        "name": "Dharunish Yugeswardeenoo"
                    },
                    {
                        "name": "Kevin Zhu"
                    },
                    {
                        "name": "Sean O'Brien"
                    }
                ],
                "author_detail": {
                    "name": "Sean O'Brien"
                },
                "author": "Sean O'Brien",
                "arxiv_comment": "Accepted in Proceedings of the 62nd Annual Meeting of the Association\n  for Computational Linguistics: Student Research Workshop (ACL-SRW 2024) 11\n  pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03624v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03624v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09893v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09893v2",
                "updated": "2024-08-26T07:54:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    7,
                    54,
                    27,
                    0,
                    239,
                    0
                ],
                "published": "2024-07-13T13:58:24Z",
                "published_parsed": [
                    2024,
                    7,
                    13,
                    13,
                    58,
                    24,
                    5,
                    195,
                    0
                ],
                "title": "Synergistic Multi-Agent Framework with Trajectory Learning for\n  Knowledge-Intensive Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergistic Multi-Agent Framework with Trajectory Learning for\n  Knowledge-Intensive Tasks"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have led to significant\nbreakthroughs in various natural language processing tasks. However, generating\nfactually consistent responses in knowledge-intensive scenarios remains a\nchallenge due to issues such as hallucination, difficulty in acquiring\nlong-tailed knowledge, and limited memory expansion. This paper introduces\nSMART, a novel multi-agent framework that leverages external knowledge to\nenhance the interpretability and factual consistency of LLM-generated\nresponses. SMART comprises four specialized agents, each performing a specific\nsub-trajectory action to navigate complex knowledge-intensive tasks. We propose\na multi-agent co-training paradigm, Long-Short Trajectory Learning, which\nensures synergistic collaboration among agents while maintaining fine-grained\nexecution by each agent. Extensive experiments on five knowledge-intensive\ntasks demonstrate SMART's superior performance compared to widely adopted\nknowledge internalization and knowledge enhancement methods. Our framework can\nextend beyond knowledge-intensive tasks to more complex scenarios. Our code is\navailable at https://github.com/yueshengbin/SMART.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have led to significant\nbreakthroughs in various natural language processing tasks. However, generating\nfactually consistent responses in knowledge-intensive scenarios remains a\nchallenge due to issues such as hallucination, difficulty in acquiring\nlong-tailed knowledge, and limited memory expansion. This paper introduces\nSMART, a novel multi-agent framework that leverages external knowledge to\nenhance the interpretability and factual consistency of LLM-generated\nresponses. SMART comprises four specialized agents, each performing a specific\nsub-trajectory action to navigate complex knowledge-intensive tasks. We propose\na multi-agent co-training paradigm, Long-Short Trajectory Learning, which\nensures synergistic collaboration among agents while maintaining fine-grained\nexecution by each agent. Extensive experiments on five knowledge-intensive\ntasks demonstrate SMART's superior performance compared to widely adopted\nknowledge internalization and knowledge enhancement methods. Our framework can\nextend beyond knowledge-intensive tasks to more complex scenarios. Our code is\navailable at https://github.com/yueshengbin/SMART."
                },
                "authors": [
                    {
                        "name": "Shengbin Yue"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09893v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09893v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10903v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10903v4",
                "updated": "2024-08-27T02:58:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    58,
                    39,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-20T14:47:38Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    47,
                    38,
                    1,
                    233,
                    0
                ],
                "title": "BEYOND DIALOGUE: A Profile-Dialogue Alignment Framework Towards General\n  Role-Playing Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEYOND DIALOGUE: A Profile-Dialogue Alignment Framework Towards General\n  Role-Playing Language Model"
                },
                "summary": "The rapid advancement of large language models (LLMs) has revolutionized\nrole-playing, enabling the development of general role-playing models. However,\ncurrent role-playing training has two significant issues: (I) Using a\npredefined role profile to prompt dialogue training for specific scenarios\nusually leads to inconsistencies and even conflicts between the dialogue and\nthe profile, resulting in training biases. (II) The model learns to imitate the\nrole based solely on the profile, neglecting profile-dialogue alignment at the\nsentence level. In this work, we propose a simple yet effective framework\ncalled BEYOND DIALOGUE, designed to overcome these hurdles. This framework\ninnovatively introduces \"beyond dialogue\" tasks to align dialogue with profile\ntraits based on each specific scenario, thereby eliminating biases during\ntraining. Furthermore, by adopting an innovative prompting mechanism that\ngenerates reasoning outcomes for training, the framework allows the model to\nachieve fine-grained alignment between profile and dialogue at the sentence\nlevel. The aforementioned methods are fully automated and low-cost.\nAdditionally, the integration of automated dialogue and objective evaluation\nmethods forms a comprehensive framework, paving the way for general\nrole-playing. Experimental results demonstrate that our model excels in\nadhering to and reflecting various dimensions of role profiles, outperforming\nmost proprietary general and specialized role-playing baselines. All code and\ndatasets are available at https://github.com/yuyouyu32/BeyondDialogue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has revolutionized\nrole-playing, enabling the development of general role-playing models. However,\ncurrent role-playing training has two significant issues: (I) Using a\npredefined role profile to prompt dialogue training for specific scenarios\nusually leads to inconsistencies and even conflicts between the dialogue and\nthe profile, resulting in training biases. (II) The model learns to imitate the\nrole based solely on the profile, neglecting profile-dialogue alignment at the\nsentence level. In this work, we propose a simple yet effective framework\ncalled BEYOND DIALOGUE, designed to overcome these hurdles. This framework\ninnovatively introduces \"beyond dialogue\" tasks to align dialogue with profile\ntraits based on each specific scenario, thereby eliminating biases during\ntraining. Furthermore, by adopting an innovative prompting mechanism that\ngenerates reasoning outcomes for training, the framework allows the model to\nachieve fine-grained alignment between profile and dialogue at the sentence\nlevel. The aforementioned methods are fully automated and low-cost.\nAdditionally, the integration of automated dialogue and objective evaluation\nmethods forms a comprehensive framework, paving the way for general\nrole-playing. Experimental results demonstrate that our model excels in\nadhering to and reflecting various dimensions of role profiles, outperforming\nmost proprietary general and specialized role-playing baselines. All code and\ndatasets are available at https://github.com/yuyouyu32/BeyondDialogue."
                },
                "authors": [
                    {
                        "name": "Yeyong Yu"
                    },
                    {
                        "name": "Runsheng Yu"
                    },
                    {
                        "name": "Haojie Wei"
                    },
                    {
                        "name": "Zhanqiu Zhang"
                    },
                    {
                        "name": "Quan Qian"
                    }
                ],
                "author_detail": {
                    "name": "Quan Qian"
                },
                "author": "Quan Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10903v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10903v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03791v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03791v2",
                "updated": "2024-08-26T07:13:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    7,
                    13,
                    47,
                    0,
                    239,
                    0
                ],
                "published": "2024-07-04T09:55:04Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    9,
                    55,
                    4,
                    3,
                    186,
                    0
                ],
                "title": "M5 -- A Diverse Benchmark to Assess the Performance of Large Multimodal\n  Models Across Multilingual and Multicultural Vision-Language Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M5 -- A Diverse Benchmark to Assess the Performance of Large Multimodal\n  Models Across Multilingual and Multicultural Vision-Language Tasks"
                },
                "summary": "Since the release of ChatGPT, the field of Natural Language Processing has\nexperienced rapid advancements, particularly in Large Language Models (LLMs)\nand their multimodal counterparts, Large Multimodal Models (LMMs). Despite\ntheir impressive capabilities, LLMs often exhibit significant performance\ndisparities across different languages and cultural contexts, as demonstrated\nby various text-only benchmarks. However, current research lacks such\nbenchmarks for multimodal visio-linguistic settings. This work fills this gap\nby introducing M5, the first comprehensive benchmark designed to evaluate LMMs\non diverse vision-language tasks within a multilingual and multicultural\ncontext. M5 includes eight datasets covering five tasks and $41$ languages,\nwith a focus on underrepresented languages and culturally diverse images.\nFurthermore, we introduce two novel datasets, M5-VGR and M5-VLOD, including a\nnew Visio-Linguistic Outlier Detection task, in which all evaluated open-source\nmodels fail to significantly surpass the random baseline. Through extensive\nevaluation and analyses, we highlight substantial task-agnostic performance\ndisparities between high- and low-resource languages. Moreover, we show that\nlarger models do not necessarily outperform smaller ones in a multilingual\nsetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the release of ChatGPT, the field of Natural Language Processing has\nexperienced rapid advancements, particularly in Large Language Models (LLMs)\nand their multimodal counterparts, Large Multimodal Models (LMMs). Despite\ntheir impressive capabilities, LLMs often exhibit significant performance\ndisparities across different languages and cultural contexts, as demonstrated\nby various text-only benchmarks. However, current research lacks such\nbenchmarks for multimodal visio-linguistic settings. This work fills this gap\nby introducing M5, the first comprehensive benchmark designed to evaluate LMMs\non diverse vision-language tasks within a multilingual and multicultural\ncontext. M5 includes eight datasets covering five tasks and $41$ languages,\nwith a focus on underrepresented languages and culturally diverse images.\nFurthermore, we introduce two novel datasets, M5-VGR and M5-VLOD, including a\nnew Visio-Linguistic Outlier Detection task, in which all evaluated open-source\nmodels fail to significantly surpass the random baseline. Through extensive\nevaluation and analyses, we highlight substantial task-agnostic performance\ndisparities between high- and low-resource languages. Moreover, we show that\nlarger models do not necessarily outperform smaller ones in a multilingual\nsetting."
                },
                "authors": [
                    {
                        "name": "Florian Schneider"
                    },
                    {
                        "name": "Sunayana Sitaram"
                    }
                ],
                "author_detail": {
                    "name": "Sunayana Sitaram"
                },
                "author": "Sunayana Sitaram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03791v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03791v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14045v1",
                "updated": "2024-08-26T06:57:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    6,
                    57,
                    22,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T06:57:22Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    6,
                    57,
                    22,
                    0,
                    239,
                    0
                ],
                "title": "Beyond Detection: Leveraging Large Language Models for Cyber Attack\n  Prediction in IoT Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Detection: Leveraging Large Language Models for Cyber Attack\n  Prediction in IoT Networks"
                },
                "summary": "In recent years, numerous large-scale cyberattacks have exploited Internet of\nThings (IoT) devices, a phenomenon that is expected to escalate with the\ncontinuing proliferation of IoT technology. Despite considerable efforts in\nattack detection, intrusion detection systems remain mostly reactive,\nresponding to specific patterns or observed anomalies. This work proposes a\nproactive approach to anticipate and mitigate malicious activities before they\ncause damage. This paper proposes a novel network intrusion prediction\nframework that combines Large Language Models (LLMs) with Long Short Term\nMemory (LSTM) networks. The framework incorporates two LLMs in a feedback loop:\na fine-tuned Generative Pre-trained Transformer (GPT) model for predicting\nnetwork traffic and a fine-tuned Bidirectional Encoder Representations from\nTransformers (BERT) for evaluating the predicted traffic. The LSTM classifier\nmodel then identifies malicious packets among these predictions. Our framework,\nevaluated on the CICIoT2023 IoT attack dataset, demonstrates a significant\nimprovement in predictive capabilities, achieving an overall accuracy of 98%,\noffering a robust solution to IoT cybersecurity challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, numerous large-scale cyberattacks have exploited Internet of\nThings (IoT) devices, a phenomenon that is expected to escalate with the\ncontinuing proliferation of IoT technology. Despite considerable efforts in\nattack detection, intrusion detection systems remain mostly reactive,\nresponding to specific patterns or observed anomalies. This work proposes a\nproactive approach to anticipate and mitigate malicious activities before they\ncause damage. This paper proposes a novel network intrusion prediction\nframework that combines Large Language Models (LLMs) with Long Short Term\nMemory (LSTM) networks. The framework incorporates two LLMs in a feedback loop:\na fine-tuned Generative Pre-trained Transformer (GPT) model for predicting\nnetwork traffic and a fine-tuned Bidirectional Encoder Representations from\nTransformers (BERT) for evaluating the predicted traffic. The LSTM classifier\nmodel then identifies malicious packets among these predictions. Our framework,\nevaluated on the CICIoT2023 IoT attack dataset, demonstrates a significant\nimprovement in predictive capabilities, achieving an overall accuracy of 98%,\noffering a robust solution to IoT cybersecurity challenges."
                },
                "authors": [
                    {
                        "name": "Alaeddine Diaf"
                    },
                    {
                        "name": "Abdelaziz Amara Korba"
                    },
                    {
                        "name": "Nour Elislem Karabadji"
                    },
                    {
                        "name": "Yacine Ghamri-Doudane"
                    }
                ],
                "author_detail": {
                    "name": "Yacine Ghamri-Doudane"
                },
                "author": "Yacine Ghamri-Doudane",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14033v1",
                "updated": "2024-08-26T05:55:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    5,
                    55,
                    48,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T05:55:48Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    5,
                    55,
                    48,
                    0,
                    239,
                    0
                ],
                "title": "MLR-Copilot: Autonomous Machine Learning Research based on Large\n  Language Models Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLR-Copilot: Autonomous Machine Learning Research based on Large\n  Language Models Agents"
                },
                "summary": "Machine learning research, crucial for technological advancements and\ninnovation, often faces significant challenges due to its inherent complexity,\nslow pace of experimentation, and the necessity for specialized expertise.\nMotivated by this, we present a new systematic framework, autonomous Machine\nLearning Research with large language models (MLR-Copilot), designed to enhance\nmachine learning research productivity through the automatic generation and\nimplementation of research ideas using Large Language Model (LLM) agents. The\nframework consists of three phases: research idea generation, experiment\nimplementation, and implementation execution. First, existing research papers\nare used to generate hypotheses and experimental plans vis IdeaAgent powered by\nLLMs. Next, the implementation generation phase translates these plans into\nexecutables with ExperimentAgent. This phase leverages retrieved prototype code\nand optionally retrieves candidate models and data. Finally, the execution\nphase, also managed by ExperimentAgent, involves running experiments with\nmechanisms for human feedback and iterative debugging to enhance the likelihood\nof achieving executable research outcomes. We evaluate our framework on five\nmachine learning research tasks and the experimental results show the\nframework's potential to facilitate the research progress and innovations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning research, crucial for technological advancements and\ninnovation, often faces significant challenges due to its inherent complexity,\nslow pace of experimentation, and the necessity for specialized expertise.\nMotivated by this, we present a new systematic framework, autonomous Machine\nLearning Research with large language models (MLR-Copilot), designed to enhance\nmachine learning research productivity through the automatic generation and\nimplementation of research ideas using Large Language Model (LLM) agents. The\nframework consists of three phases: research idea generation, experiment\nimplementation, and implementation execution. First, existing research papers\nare used to generate hypotheses and experimental plans vis IdeaAgent powered by\nLLMs. Next, the implementation generation phase translates these plans into\nexecutables with ExperimentAgent. This phase leverages retrieved prototype code\nand optionally retrieves candidate models and data. Finally, the execution\nphase, also managed by ExperimentAgent, involves running experiments with\nmechanisms for human feedback and iterative debugging to enhance the likelihood\nof achieving executable research outcomes. We evaluate our framework on five\nmachine learning research tasks and the experimental results show the\nframework's potential to facilitate the research progress and innovations."
                },
                "authors": [
                    {
                        "name": "Ruochen Li"
                    },
                    {
                        "name": "Teerth Patel"
                    },
                    {
                        "name": "Qingyun Wang"
                    },
                    {
                        "name": "Xinya Du"
                    }
                ],
                "author_detail": {
                    "name": "Xinya Du"
                },
                "author": "Xinya Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14027v1",
                "updated": "2024-08-26T05:36:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    5,
                    36,
                    58,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T05:36:58Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    5,
                    36,
                    58,
                    0,
                    239,
                    0
                ],
                "title": "UAV-Enabled Integrated Sensing and Communication in Maritime Emergency\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UAV-Enabled Integrated Sensing and Communication in Maritime Emergency\n  Networks"
                },
                "summary": "With line-of-sight mode deployment and fast response, unmanned aerial vehicle\n(UAV), equipped with the cutting-edge integrated sensing and communication\n(ISAC) technique, is poised to deliver high-quality communication and sensing\nservices in maritime emergency scenarios. In practice, however, the real-time\ntransmission of ISAC signals at the UAV side cannot be realized unless the\nreliable wireless fronthaul link between the terrestrial base station and UAV\nare available. This paper proposes a multicarrier-division duplex based joint\nfronthaul-access scheme, where mutually orthogonal subcarrier sets are\nleveraged to simultaneously support four types of fronthaul/access\ntransmissions. In order to maximize the end-to-end communication rate while\nmaintaining an adequate sensing quality-of-service (QoS) in such a complex\nscheme, the UAV trajectory, subcarrier assignment and power allocation are\njointly optimized. The overall optimization process is designed in two stages.\nAs the emergency area is usually far away from the coast, the optimal initial\noperating position for the UAV is first found. Once the UAV passes the initial\noperating position, the UAV's trajectory and resource allocation are optimized\nduring the mission period to maximize the end-to-end communication rate under\nthe constraint of minimum sensing QoS. Simulation results demonstrate the\neffectiveness of the proposed scheme in dealing with the joint fronthaul-access\noptimization problem in maritime ISAC networks, offering the advantages over\nbenchmark schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With line-of-sight mode deployment and fast response, unmanned aerial vehicle\n(UAV), equipped with the cutting-edge integrated sensing and communication\n(ISAC) technique, is poised to deliver high-quality communication and sensing\nservices in maritime emergency scenarios. In practice, however, the real-time\ntransmission of ISAC signals at the UAV side cannot be realized unless the\nreliable wireless fronthaul link between the terrestrial base station and UAV\nare available. This paper proposes a multicarrier-division duplex based joint\nfronthaul-access scheme, where mutually orthogonal subcarrier sets are\nleveraged to simultaneously support four types of fronthaul/access\ntransmissions. In order to maximize the end-to-end communication rate while\nmaintaining an adequate sensing quality-of-service (QoS) in such a complex\nscheme, the UAV trajectory, subcarrier assignment and power allocation are\njointly optimized. The overall optimization process is designed in two stages.\nAs the emergency area is usually far away from the coast, the optimal initial\noperating position for the UAV is first found. Once the UAV passes the initial\noperating position, the UAV's trajectory and resource allocation are optimized\nduring the mission period to maximize the end-to-end communication rate under\nthe constraint of minimum sensing QoS. Simulation results demonstrate the\neffectiveness of the proposed scheme in dealing with the joint fronthaul-access\noptimization problem in maritime ISAC networks, offering the advantages over\nbenchmark schemes."
                },
                "authors": [
                    {
                        "name": "Bohan Li"
                    },
                    {
                        "name": "Jiahao Liu"
                    },
                    {
                        "name": "Yifeng Xiong"
                    },
                    {
                        "name": "Junsheng Mu"
                    },
                    {
                        "name": "Pei Xiao"
                    },
                    {
                        "name": "Sheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Chen"
                },
                "author": "Sheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.05561v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.05561v5",
                "updated": "2024-08-26T05:31:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    5,
                    31,
                    38,
                    0,
                    239,
                    0
                ],
                "published": "2024-01-10T22:07:21Z",
                "published_parsed": [
                    2024,
                    1,
                    10,
                    22,
                    7,
                    21,
                    2,
                    10,
                    0
                ],
                "title": "TrustLLM: Trustworthiness in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrustLLM: Trustworthiness in Large Language Models"
                },
                "summary": "Large language models (LLMs), exemplified by ChatGPT, have gained\nconsiderable attention for their excellent natural language processing\ncapabilities. Nonetheless, these LLMs present many challenges, particularly in\nthe realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs\nemerges as an important topic. This paper introduces TrustLLM, a comprehensive\nstudy of trustworthiness in LLMs, including principles for different dimensions\nof trustworthiness, established benchmark, evaluation, and analysis of\ntrustworthiness for mainstream LLMs, and discussion of open challenges and\nfuture directions. Specifically, we first propose a set of principles for\ntrustworthy LLMs that span eight different dimensions. Based on these\nprinciples, we further establish a benchmark across six dimensions including\ntruthfulness, safety, fairness, robustness, privacy, and machine ethics. We\nthen present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of\nover 30 datasets. Our findings firstly show that in general trustworthiness and\nutility (i.e., functional effectiveness) are positively related. Secondly, our\nobservations reveal that proprietary LLMs generally outperform most open-source\ncounterparts in terms of trustworthiness, raising concerns about the potential\nrisks of widely accessible open-source LLMs. However, a few open-source LLMs\ncome very close to proprietary ones. Thirdly, it is important to note that some\nLLMs may be overly calibrated towards exhibiting trustworthiness, to the extent\nthat they compromise their utility by mistakenly treating benign prompts as\nharmful and consequently not responding. Finally, we emphasize the importance\nof ensuring transparency not only in the models themselves but also in the\ntechnologies that underpin trustworthiness. Knowing the specific trustworthy\ntechnologies that have been employed is crucial for analyzing their\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), exemplified by ChatGPT, have gained\nconsiderable attention for their excellent natural language processing\ncapabilities. Nonetheless, these LLMs present many challenges, particularly in\nthe realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs\nemerges as an important topic. This paper introduces TrustLLM, a comprehensive\nstudy of trustworthiness in LLMs, including principles for different dimensions\nof trustworthiness, established benchmark, evaluation, and analysis of\ntrustworthiness for mainstream LLMs, and discussion of open challenges and\nfuture directions. Specifically, we first propose a set of principles for\ntrustworthy LLMs that span eight different dimensions. Based on these\nprinciples, we further establish a benchmark across six dimensions including\ntruthfulness, safety, fairness, robustness, privacy, and machine ethics. We\nthen present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of\nover 30 datasets. Our findings firstly show that in general trustworthiness and\nutility (i.e., functional effectiveness) are positively related. Secondly, our\nobservations reveal that proprietary LLMs generally outperform most open-source\ncounterparts in terms of trustworthiness, raising concerns about the potential\nrisks of widely accessible open-source LLMs. However, a few open-source LLMs\ncome very close to proprietary ones. Thirdly, it is important to note that some\nLLMs may be overly calibrated towards exhibiting trustworthiness, to the extent\nthat they compromise their utility by mistakenly treating benign prompts as\nharmful and consequently not responding. Finally, we emphasize the importance\nof ensuring transparency not only in the models themselves but also in the\ntechnologies that underpin trustworthiness. Knowing the specific trustworthy\ntechnologies that have been employed is crucial for analyzing their\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Haoran Wang"
                    },
                    {
                        "name": "Siyuan Wu"
                    },
                    {
                        "name": "Qihui Zhang"
                    },
                    {
                        "name": "Yuan Li"
                    },
                    {
                        "name": "Chujie Gao"
                    },
                    {
                        "name": "Yixin Huang"
                    },
                    {
                        "name": "Wenhan Lyu"
                    },
                    {
                        "name": "Yixuan Zhang"
                    },
                    {
                        "name": "Xiner Li"
                    },
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Yijue Wang"
                    },
                    {
                        "name": "Zhikun Zhang"
                    },
                    {
                        "name": "Bertie Vidgen"
                    },
                    {
                        "name": "Bhavya Kailkhura"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Chaowei Xiao"
                    },
                    {
                        "name": "Chunyuan Li"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Hongyi Wang"
                    },
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Huaxiu Yao"
                    },
                    {
                        "name": "Manolis Kellis"
                    },
                    {
                        "name": "Marinka Zitnik"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Jian Pei"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Jieyu Zhao"
                    },
                    {
                        "name": "Jiliang Tang"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Joaquin Vanschoren"
                    },
                    {
                        "name": "John Mitchell"
                    },
                    {
                        "name": "Kai Shu"
                    },
                    {
                        "name": "Kaidi Xu"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Lifang He"
                    },
                    {
                        "name": "Lifu Huang"
                    },
                    {
                        "name": "Michael Backes"
                    },
                    {
                        "name": "Neil Zhenqiang Gong"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Ran Xu"
                    },
                    {
                        "name": "Rex Ying"
                    },
                    {
                        "name": "Shuiwang Ji"
                    },
                    {
                        "name": "Suman Jana"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "William Wang"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Xun Chen"
                    },
                    {
                        "name": "Xuyu Wang"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Yanfang Ye"
                    },
                    {
                        "name": "Yinzhi Cao"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Yue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhao"
                },
                "author": "Yue Zhao",
                "arxiv_comment": "This work is still under work and we welcome your contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.05561v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.05561v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14023v1",
                "updated": "2024-08-26T05:27:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    5,
                    27,
                    14,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T05:27:14Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    5,
                    27,
                    14,
                    0,
                    239,
                    0
                ],
                "title": "Video-CCAM: Enhancing Video-Language Understanding with Causal\n  Cross-Attention Masks for Short and Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-CCAM: Enhancing Video-Language Understanding with Causal\n  Cross-Attention Masks for Short and Long Videos"
                },
                "summary": "Multi-modal large language models (MLLMs) have demonstrated considerable\npotential across various downstream tasks that require cross-domain knowledge.\nMLLMs capable of processing videos, known as Video-MLLMs, have attracted broad\ninterest in video-language understanding. However, videos, especially long\nvideos, contain more visual tokens than images, making them difficult for LLMs\nto process. Existing works either downsample visual features or extend the LLM\ncontext size, risking the loss of high-resolution information or slowing down\ninference speed. To address these limitations, we apply cross-attention layers\nin the intermediate projector between the visual encoder and the large language\nmodel (LLM). As the naive cross-attention mechanism is insensitive to temporal\norder, we further introduce causal cross-attention masks (CCAMs) within the\ncross-attention layers. This Video-MLLM, named Video-CCAM, is trained in a\nstraightforward two-stage fashion: feature alignment and visual instruction\ntuning. We develop several Video-CCAM models based on LLMs of different sizes\n(4B, 9B, and 14B). Video-CCAM proves to be a robust Video-MLLM and shows\noutstanding performance from short videos to long ones. Among standard video\nbenchmarks like MVBench and VideoChatGPT-QA, Video-CCAM shows outstanding\nperformances (1st/2nd/3rd in MVBench and TGIF-QA, 2nd/3rd/4th in MSVD-QA,\nMSRVTT-QA, and ActivityNet-QA). In benchmarks encompassing long videos,\nVideo-CCAM models can be directly adapted to long video understanding and still\nachieve exceptional scores despite being trained solely with images and\n16-frame videos. Using 96 frames (6$\\times$ the training number of frames),\nVideo-CCAM models rank 1st/2nd/3rd in VideoVista and 1st/2nd/4th in MLVU among\nall open-source Video-MLLMs, respectively. The code is publicly available in\n\\url{https://github.com/QQ-MM/Video-CCAM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal large language models (MLLMs) have demonstrated considerable\npotential across various downstream tasks that require cross-domain knowledge.\nMLLMs capable of processing videos, known as Video-MLLMs, have attracted broad\ninterest in video-language understanding. However, videos, especially long\nvideos, contain more visual tokens than images, making them difficult for LLMs\nto process. Existing works either downsample visual features or extend the LLM\ncontext size, risking the loss of high-resolution information or slowing down\ninference speed. To address these limitations, we apply cross-attention layers\nin the intermediate projector between the visual encoder and the large language\nmodel (LLM). As the naive cross-attention mechanism is insensitive to temporal\norder, we further introduce causal cross-attention masks (CCAMs) within the\ncross-attention layers. This Video-MLLM, named Video-CCAM, is trained in a\nstraightforward two-stage fashion: feature alignment and visual instruction\ntuning. We develop several Video-CCAM models based on LLMs of different sizes\n(4B, 9B, and 14B). Video-CCAM proves to be a robust Video-MLLM and shows\noutstanding performance from short videos to long ones. Among standard video\nbenchmarks like MVBench and VideoChatGPT-QA, Video-CCAM shows outstanding\nperformances (1st/2nd/3rd in MVBench and TGIF-QA, 2nd/3rd/4th in MSVD-QA,\nMSRVTT-QA, and ActivityNet-QA). In benchmarks encompassing long videos,\nVideo-CCAM models can be directly adapted to long video understanding and still\nachieve exceptional scores despite being trained solely with images and\n16-frame videos. Using 96 frames (6$\\times$ the training number of frames),\nVideo-CCAM models rank 1st/2nd/3rd in VideoVista and 1st/2nd/4th in MLVU among\nall open-source Video-MLLMs, respectively. The code is publicly available in\n\\url{https://github.com/QQ-MM/Video-CCAM}."
                },
                "authors": [
                    {
                        "name": "Jiajun Fei"
                    },
                    {
                        "name": "Dian Li"
                    },
                    {
                        "name": "Zhidong Deng"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Gang Liu"
                    },
                    {
                        "name": "Hui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hui Wang"
                },
                "author": "Hui Wang",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10668v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10668v3",
                "updated": "2024-08-26T05:27:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    5,
                    27,
                    13,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-20T09:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    11,
                    21,
                    1,
                    233,
                    0
                ],
                "title": "Probing the Safety Response Boundary of Large Language Models via Unsafe\n  Decoding Path Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing the Safety Response Boundary of Large Language Models via Unsafe\n  Decoding Path Generation"
                },
                "summary": "Large Language Models (LLMs) are implicit troublemakers. While they provide\nvaluable insights and assist in problem-solving, they can also potentially\nserve as a resource for malicious activities. Implementing safety alignment\ncould mitigate the risk of LLMs generating harmful responses. We argue that:\neven when an LLM appears to successfully block harmful queries, there may still\nbe hidden vulnerabilities that could act as ticking time bombs. To identify\nthese underlying weaknesses, we propose to use a cost value model as both a\ndetector and an attacker. Trained on external or self-generated harmful\ndatasets, the cost value model could successfully influence the original safe\nLLM to output toxic content in decoding process. For instance, LLaMA-2-chat 7B\noutputs 39.18% concrete toxic content, along with only 22.16% refusals without\nany harmful suffixes. These potential weaknesses can then be exploited via\nprompt optimization such as soft prompts on images. We name this decoding\nstrategy: Jailbreak Value Decoding (JVD), emphasizing that seemingly secure\nLLMs may not be as safe as we initially believe. They could be used to gather\nharmful data or launch covert attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are implicit troublemakers. While they provide\nvaluable insights and assist in problem-solving, they can also potentially\nserve as a resource for malicious activities. Implementing safety alignment\ncould mitigate the risk of LLMs generating harmful responses. We argue that:\neven when an LLM appears to successfully block harmful queries, there may still\nbe hidden vulnerabilities that could act as ticking time bombs. To identify\nthese underlying weaknesses, we propose to use a cost value model as both a\ndetector and an attacker. Trained on external or self-generated harmful\ndatasets, the cost value model could successfully influence the original safe\nLLM to output toxic content in decoding process. For instance, LLaMA-2-chat 7B\noutputs 39.18% concrete toxic content, along with only 22.16% refusals without\nany harmful suffixes. These potential weaknesses can then be exploited via\nprompt optimization such as soft prompts on images. We name this decoding\nstrategy: Jailbreak Value Decoding (JVD), emphasizing that seemingly secure\nLLMs may not be as safe as we initially believe. They could be used to gather\nharmful data or launch covert attacks."
                },
                "authors": [
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Bingzhe Wu"
                    },
                    {
                        "name": "Yatao Bian"
                    },
                    {
                        "name": "Yongzhe Chang"
                    },
                    {
                        "name": "Xueqian Wang"
                    },
                    {
                        "name": "Peilin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Peilin Zhao"
                },
                "author": "Peilin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10668v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10668v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14008v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14008v1",
                "updated": "2024-08-26T04:29:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    29,
                    52,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T04:29:52Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    29,
                    52,
                    0,
                    239,
                    0
                ],
                "title": "LMM-VQA: Advancing Video Quality Assessment with Large Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMM-VQA: Advancing Video Quality Assessment with Large Multimodal Models"
                },
                "summary": "The explosive growth of videos on streaming media platforms has underscored\nthe urgent need for effective video quality assessment (VQA) algorithms to\nmonitor and perceptually optimize the quality of streaming videos. However, VQA\nremains an extremely challenging task due to the diverse video content and the\ncomplex spatial and temporal distortions, thus necessitating more advanced\nmethods to address these issues. Nowadays, large multimodal models (LMMs), such\nas GPT-4V, have exhibited strong capabilities for various visual understanding\ntasks, motivating us to leverage the powerful multimodal representation ability\nof LMMs to solve the VQA task. Therefore, we propose the first Large\nMulti-Modal Video Quality Assessment (LMM-VQA) model, which introduces a novel\nspatiotemporal visual modeling strategy for quality-aware feature extraction.\nSpecifically, we first reformulate the quality regression problem into a\nquestion and answering (Q&A) task and construct Q&A prompts for VQA instruction\ntuning. Then, we design a spatiotemporal vision encoder to extract spatial and\ntemporal features to represent the quality characteristics of videos, which are\nsubsequently mapped into the language space by the spatiotemporal projector for\nmodality alignment. Finally, the aligned visual tokens and the quality-inquired\ntext tokens are aggregated as inputs for the large language model (LLM) to\ngenerate the quality score and level. Extensive experiments demonstrate that\nLMM-VQA achieves state-of-the-art performance across five VQA benchmarks,\nexhibiting an average improvement of $5\\%$ in generalization ability over\nexisting methods. Furthermore, due to the advanced design of the spatiotemporal\nencoder and projector, LMM-VQA also performs exceptionally well on general\nvideo understanding tasks, further validating its effectiveness. Our code will\nbe released at https://github.com/Sueqk/LMM-VQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The explosive growth of videos on streaming media platforms has underscored\nthe urgent need for effective video quality assessment (VQA) algorithms to\nmonitor and perceptually optimize the quality of streaming videos. However, VQA\nremains an extremely challenging task due to the diverse video content and the\ncomplex spatial and temporal distortions, thus necessitating more advanced\nmethods to address these issues. Nowadays, large multimodal models (LMMs), such\nas GPT-4V, have exhibited strong capabilities for various visual understanding\ntasks, motivating us to leverage the powerful multimodal representation ability\nof LMMs to solve the VQA task. Therefore, we propose the first Large\nMulti-Modal Video Quality Assessment (LMM-VQA) model, which introduces a novel\nspatiotemporal visual modeling strategy for quality-aware feature extraction.\nSpecifically, we first reformulate the quality regression problem into a\nquestion and answering (Q&A) task and construct Q&A prompts for VQA instruction\ntuning. Then, we design a spatiotemporal vision encoder to extract spatial and\ntemporal features to represent the quality characteristics of videos, which are\nsubsequently mapped into the language space by the spatiotemporal projector for\nmodality alignment. Finally, the aligned visual tokens and the quality-inquired\ntext tokens are aggregated as inputs for the large language model (LLM) to\ngenerate the quality score and level. Extensive experiments demonstrate that\nLMM-VQA achieves state-of-the-art performance across five VQA benchmarks,\nexhibiting an average improvement of $5\\%$ in generalization ability over\nexisting methods. Furthermore, due to the advanced design of the spatiotemporal\nencoder and projector, LMM-VQA also performs exceptionally well on general\nvideo understanding tasks, further validating its effectiveness. Our code will\nbe released at https://github.com/Sueqk/LMM-VQA."
                },
                "authors": [
                    {
                        "name": "Qihang Ge"
                    },
                    {
                        "name": "Wei Sun"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Yunhao Li"
                    },
                    {
                        "name": "Zhongpeng Ji"
                    },
                    {
                        "name": "Fengyu Sun"
                    },
                    {
                        "name": "Shangling Jui"
                    },
                    {
                        "name": "Xiongkuo Min"
                    },
                    {
                        "name": "Guangtao Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Guangtao Zhai"
                },
                "author": "Guangtao Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14008v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12529v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12529v2",
                "updated": "2024-08-26T04:28:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    28,
                    41,
                    0,
                    239,
                    0
                ],
                "published": "2024-07-17T13:11:28Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    13,
                    11,
                    28,
                    2,
                    199,
                    0
                ],
                "title": "Crafting the Path: Robust Query Rewriting for Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crafting the Path: Robust Query Rewriting for Information Retrieval"
                },
                "summary": "Query rewriting aims to generate a new query that can complement the original\nquery to improve the information retrieval system. Recent studies on query\nrewriting, such as query2doc, query2expand and querey2cot, rely on the internal\nknowledge of Large Language Models (LLMs) to generate a relevant passage to add\ninformation to the query. Nevertheless, the efficacy of these methodologies may\nmarkedly decline in instances where the requisite knowledge is not encapsulated\nwithin the model's intrinsic parameters. In this paper, we propose a novel\nstructured query rewriting method called Crafting the Path tailored for\nretrieval systems. Crafting the Path involves a three-step process that crafts\nquery-related information necessary for finding the passages to be searched in\neach step. Specifically, the Crafting the Path begins with Query Concept\nComprehension, proceeds to Query Type Identification, and finally conducts\nExpected Answer Extraction. Experimental results show that our method\noutperforms previous rewriting methods, especially in less familiar domains for\nLLMs. We demonstrate that our method is less dependent on the internal\nparameter knowledge of the model and generates queries with fewer factual\ninaccuracies. Furthermore, we observe that \\name{} demonstrates superior\nperformance in the retrieval-augmented generation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query rewriting aims to generate a new query that can complement the original\nquery to improve the information retrieval system. Recent studies on query\nrewriting, such as query2doc, query2expand and querey2cot, rely on the internal\nknowledge of Large Language Models (LLMs) to generate a relevant passage to add\ninformation to the query. Nevertheless, the efficacy of these methodologies may\nmarkedly decline in instances where the requisite knowledge is not encapsulated\nwithin the model's intrinsic parameters. In this paper, we propose a novel\nstructured query rewriting method called Crafting the Path tailored for\nretrieval systems. Crafting the Path involves a three-step process that crafts\nquery-related information necessary for finding the passages to be searched in\neach step. Specifically, the Crafting the Path begins with Query Concept\nComprehension, proceeds to Query Type Identification, and finally conducts\nExpected Answer Extraction. Experimental results show that our method\noutperforms previous rewriting methods, especially in less familiar domains for\nLLMs. We demonstrate that our method is less dependent on the internal\nparameter knowledge of the model and generates queries with fewer factual\ninaccuracies. Furthermore, we observe that \\name{} demonstrates superior\nperformance in the retrieval-augmented generation scenarios."
                },
                "authors": [
                    {
                        "name": "Ingeol Baek"
                    },
                    {
                        "name": "Jimin Lee"
                    },
                    {
                        "name": "Joonho Yang"
                    },
                    {
                        "name": "Hwanhee Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hwanhee Lee"
                },
                "author": "Hwanhee Lee",
                "arxiv_comment": "3 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12529v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12529v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12616v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12616v2",
                "updated": "2024-08-26T03:47:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    47,
                    6,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-08T16:46:14Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    16,
                    46,
                    14,
                    3,
                    221,
                    0
                ],
                "title": "Semantic Communication based on Large Language Model for Underwater\n  Image Transmission",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Communication based on Large Language Model for Underwater\n  Image Transmission"
                },
                "summary": "Underwater communication is essential for environmental monitoring, marine\nbiology research, and underwater exploration. Traditional underwater\ncommunication faces limitations like low bandwidth, high latency, and\nsusceptibility to noise, while semantic communication (SC) offers a promising\nsolution by focusing on the exchange of semantics rather than symbols or bits.\nHowever, SC encounters challenges in underwater environments, including\nsemantic information mismatch and difficulties in accurately identifying and\ntransmitting critical information that aligns with the diverse requirements of\nunderwater applications. To address these challenges, we propose a novel\nSemantic Communication (SC) framework based on Large Language Models (LLMs).\nOur framework leverages visual LLMs to perform semantic compression and\nprioritization of underwater image data according to the query from users. By\nidentifying and encoding key semantic elements within the images, the system\nselectively transmits high-priority information while applying higher\ncompression rates to less critical regions. On the receiver side, an LLM-based\nrecovery mechanism, along with Global Vision ControlNet and Key Region\nControlNet networks, aids in reconstructing the images, thereby enhancing\ncommunication efficiency and robustness. Our framework reduces the overall data\nsize to 0.8\\% of the original. Experimental results demonstrate that our method\nsignificantly outperforms existing approaches, ensuring high-quality,\nsemantically accurate image reconstruction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Underwater communication is essential for environmental monitoring, marine\nbiology research, and underwater exploration. Traditional underwater\ncommunication faces limitations like low bandwidth, high latency, and\nsusceptibility to noise, while semantic communication (SC) offers a promising\nsolution by focusing on the exchange of semantics rather than symbols or bits.\nHowever, SC encounters challenges in underwater environments, including\nsemantic information mismatch and difficulties in accurately identifying and\ntransmitting critical information that aligns with the diverse requirements of\nunderwater applications. To address these challenges, we propose a novel\nSemantic Communication (SC) framework based on Large Language Models (LLMs).\nOur framework leverages visual LLMs to perform semantic compression and\nprioritization of underwater image data according to the query from users. By\nidentifying and encoding key semantic elements within the images, the system\nselectively transmits high-priority information while applying higher\ncompression rates to less critical regions. On the receiver side, an LLM-based\nrecovery mechanism, along with Global Vision ControlNet and Key Region\nControlNet networks, aids in reconstructing the images, thereby enhancing\ncommunication efficiency and robustness. Our framework reduces the overall data\nsize to 0.8\\% of the original. Experimental results demonstrate that our method\nsignificantly outperforms existing approaches, ensuring high-quality,\nsemantically accurate image reconstruction."
                },
                "authors": [
                    {
                        "name": "Weilong Chen"
                    },
                    {
                        "name": "Wenxuan Xu"
                    },
                    {
                        "name": "Haoran Chen"
                    },
                    {
                        "name": "Xinran Zhang"
                    },
                    {
                        "name": "Zhijin Qin"
                    },
                    {
                        "name": "Yanru Zhang"
                    },
                    {
                        "name": "Zhu Han"
                    }
                ],
                "author_detail": {
                    "name": "Zhu Han"
                },
                "author": "Zhu Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12616v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12616v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10848v2",
                "updated": "2024-08-26T03:19:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    19,
                    45,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-20T13:40:25Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    40,
                    25,
                    1,
                    233,
                    0
                ],
                "title": "Perception-guided Jailbreak against Text-to-Image Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception-guided Jailbreak against Text-to-Image Models"
                },
                "summary": "In recent years, Text-to-Image (T2I) models have garnered significant\nattention due to their remarkable advancements. However, security concerns have\nemerged due to their potential to generate inappropriate or Not-Safe-For-Work\n(NSFW) images. In this paper, inspired by the observation that texts with\ndifferent semantics can lead to similar human perceptions, we propose an\nLLM-driven perception-guided jailbreak method, termed PGJ. It is a black-box\njailbreak method that requires no specific T2I model (model-free) and generates\nhighly natural attack prompts. Specifically, we propose identifying a safe\nphrase that is similar in human perception yet inconsistent in text semantics\nwith the target unsafe word and using it as a substitution. The experiments\nconducted on six open-source models and commercial online services with\nthousands of prompts have verified the effectiveness of PGJ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Text-to-Image (T2I) models have garnered significant\nattention due to their remarkable advancements. However, security concerns have\nemerged due to their potential to generate inappropriate or Not-Safe-For-Work\n(NSFW) images. In this paper, inspired by the observation that texts with\ndifferent semantics can lead to similar human perceptions, we propose an\nLLM-driven perception-guided jailbreak method, termed PGJ. It is a black-box\njailbreak method that requires no specific T2I model (model-free) and generates\nhighly natural attack prompts. Specifically, we propose identifying a safe\nphrase that is similar in human perception yet inconsistent in text semantics\nwith the target unsafe word and using it as a substitution. The experiments\nconducted on six open-source models and commercial online services with\nthousands of prompts have verified the effectiveness of PGJ."
                },
                "authors": [
                    {
                        "name": "Yihao Huang"
                    },
                    {
                        "name": "Le Liang"
                    },
                    {
                        "name": "Tianlin Li"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Run Wang"
                    },
                    {
                        "name": "Weikai Miao"
                    },
                    {
                        "name": "Geguang Pu"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13987v1",
                "updated": "2024-08-26T02:53:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    2,
                    53,
                    24,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T02:53:24Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    2,
                    53,
                    24,
                    0,
                    239,
                    0
                ],
                "title": "Focused Large Language Models are Stable Many-Shot Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Focused Large Language Models are Stable Many-Shot Learners"
                },
                "summary": "In-Context Learning (ICL) enables large language models (LLMs) to achieve\nrapid task adaptation by learning from demonstrations. With the increase in\navailable context length of LLMs, recent experiments have shown that the\nperformance of ICL does not necessarily scale well in many-shot (demonstration)\nsettings. We theoretically and experimentally confirm that the reason lies in\nmore demonstrations dispersing the model attention from the query, hindering\nits understanding of key content. Inspired by how humans learn from examples,\nwe propose a training-free method FocusICL, which conducts triviality filtering\nto avoid attention being diverted by unimportant contents at token-level and\noperates hierarchical attention to further ensure sufficient attention towards\ncurrent query at demonstration-level. We also design an efficient\nhyperparameter searching strategy for FocusICL based on model perplexity of\ndemonstrations. Comprehensive experiments validate that FocusICL achieves an\naverage performance improvement of 5.2% over vanilla ICL and scales well with\nmany-shot demonstrations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning (ICL) enables large language models (LLMs) to achieve\nrapid task adaptation by learning from demonstrations. With the increase in\navailable context length of LLMs, recent experiments have shown that the\nperformance of ICL does not necessarily scale well in many-shot (demonstration)\nsettings. We theoretically and experimentally confirm that the reason lies in\nmore demonstrations dispersing the model attention from the query, hindering\nits understanding of key content. Inspired by how humans learn from examples,\nwe propose a training-free method FocusICL, which conducts triviality filtering\nto avoid attention being diverted by unimportant contents at token-level and\noperates hierarchical attention to further ensure sufficient attention towards\ncurrent query at demonstration-level. We also design an efficient\nhyperparameter searching strategy for FocusICL based on model perplexity of\ndemonstrations. Comprehensive experiments validate that FocusICL achieves an\naverage performance improvement of 5.2% over vanilla ICL and scales well with\nmany-shot demonstrations."
                },
                "authors": [
                    {
                        "name": "Peiwen Yuan"
                    },
                    {
                        "name": "Shaoxiong Feng"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Xinglin Wang"
                    },
                    {
                        "name": "Yueqi Zhang"
                    },
                    {
                        "name": "Chuyi Tan"
                    },
                    {
                        "name": "Boyuan Pan"
                    },
                    {
                        "name": "Heda Wang"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kan Li"
                    }
                ],
                "author_detail": {
                    "name": "Kan Li"
                },
                "author": "Kan Li",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13986v1",
                "updated": "2024-08-26T02:36:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    2,
                    36,
                    55,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T02:36:55Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    2,
                    36,
                    55,
                    0,
                    239,
                    0
                ],
                "title": "AgentMove: Predicting Human Mobility Anywhere Using Large Language Model\n  based Agentic Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentMove: Predicting Human Mobility Anywhere Using Large Language Model\n  based Agentic Framework"
                },
                "summary": "Human mobility prediction plays a crucial role in various real-world\napplications. Although deep learning based models have shown promising results\nover the past decade, their reliance on extensive private mobility data for\ntraining and their inability to perform zero-shot predictions, have hindered\nfurther advancements. Recently, attempts have been made to apply large language\nmodels (LLMs) to mobility prediction task. However, their performance has been\nconstrained by the absence of a systematic design of workflow. They directly\ngenerate the final output using LLMs, which limits the potential of LLMs to\nuncover complex mobility patterns and underestimates their extensive reserve of\nglobal geospatial knowledge. In this paper, we introduce AgentMove, a\nsystematic agentic prediction framework to achieve generalized mobility\nprediction for any cities worldwide. In AgentMove, we first decompose the\nmobility prediction task into three sub-tasks and then design corresponding\nmodules to complete these subtasks, including spatial-temporal memory for\nindividual mobility pattern mining, world knowledge generator for modeling the\neffects of urban structure and collective knowledge extractor for capturing the\nshared patterns among population. Finally, we combine the results of three\nmodules and conduct a reasoning step to generate the final predictions.\nExtensive experiments on mobility data from two sources in 12 cities\ndemonstrate that AgentMove outperforms the best baseline more than 8% in\nvarious metrics and it shows robust predictions with various LLMs as base and\nalso less geographical bias across cities. Codes and data can be found in\nhttps://github.com/tsinghua-fib-lab/AgentMove.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human mobility prediction plays a crucial role in various real-world\napplications. Although deep learning based models have shown promising results\nover the past decade, their reliance on extensive private mobility data for\ntraining and their inability to perform zero-shot predictions, have hindered\nfurther advancements. Recently, attempts have been made to apply large language\nmodels (LLMs) to mobility prediction task. However, their performance has been\nconstrained by the absence of a systematic design of workflow. They directly\ngenerate the final output using LLMs, which limits the potential of LLMs to\nuncover complex mobility patterns and underestimates their extensive reserve of\nglobal geospatial knowledge. In this paper, we introduce AgentMove, a\nsystematic agentic prediction framework to achieve generalized mobility\nprediction for any cities worldwide. In AgentMove, we first decompose the\nmobility prediction task into three sub-tasks and then design corresponding\nmodules to complete these subtasks, including spatial-temporal memory for\nindividual mobility pattern mining, world knowledge generator for modeling the\neffects of urban structure and collective knowledge extractor for capturing the\nshared patterns among population. Finally, we combine the results of three\nmodules and conduct a reasoning step to generate the final predictions.\nExtensive experiments on mobility data from two sources in 12 cities\ndemonstrate that AgentMove outperforms the best baseline more than 8% in\nvarious metrics and it shows robust predictions with various LLMs as base and\nalso less geographical bias across cities. Codes and data can be found in\nhttps://github.com/tsinghua-fib-lab/AgentMove."
                },
                "authors": [
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Yuwei Du"
                    },
                    {
                        "name": "Jie Zhao"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13985v1",
                "updated": "2024-08-26T02:35:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    2,
                    35,
                    37,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T02:35:37Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    2,
                    35,
                    37,
                    0,
                    239,
                    0
                ],
                "title": "TF-Attack: Transferable and Fast Adversarial Attacks on Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TF-Attack: Transferable and Fast Adversarial Attacks on Large Language\n  Models"
                },
                "summary": "With the great advancements in large language models (LLMs), adversarial\nattacks against LLMs have recently attracted increasing attention. We found\nthat pre-existing adversarial attack methodologies exhibit limited\ntransferability and are notably inefficient, particularly when applied to LLMs.\nIn this paper, we analyze the core mechanisms of previous predominant\nadversarial attack methods, revealing that 1) the distributions of importance\nscore differ markedly among victim models, restricting the transferability; 2)\nthe sequential attack processes induces substantial time overheads. Based on\nthe above two insights, we introduce a new scheme, named TF-Attack, for\nTransferable and Fast adversarial attacks on LLMs. TF-Attack employs an\nexternal LLM as a third-party overseer rather than the victim model to identify\ncritical units within sentences. Moreover, TF-Attack introduces the concept of\nImportance Level, which allows for parallel substitutions of attacks. We\nconduct extensive experiments on 6 widely adopted benchmarks, evaluating the\nproposed method through both automatic and human metrics. Results show that our\nmethod consistently surpasses previous methods in transferability and delivers\nsignificant speed improvements, up to 20 times faster than earlier attack\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the great advancements in large language models (LLMs), adversarial\nattacks against LLMs have recently attracted increasing attention. We found\nthat pre-existing adversarial attack methodologies exhibit limited\ntransferability and are notably inefficient, particularly when applied to LLMs.\nIn this paper, we analyze the core mechanisms of previous predominant\nadversarial attack methods, revealing that 1) the distributions of importance\nscore differ markedly among victim models, restricting the transferability; 2)\nthe sequential attack processes induces substantial time overheads. Based on\nthe above two insights, we introduce a new scheme, named TF-Attack, for\nTransferable and Fast adversarial attacks on LLMs. TF-Attack employs an\nexternal LLM as a third-party overseer rather than the victim model to identify\ncritical units within sentences. Moreover, TF-Attack introduces the concept of\nImportance Level, which allows for parallel substitutions of attacks. We\nconduct extensive experiments on 6 widely adopted benchmarks, evaluating the\nproposed method through both automatic and human metrics. Results show that our\nmethod consistently surpasses previous methods in transferability and delivers\nsignificant speed improvements, up to 20 times faster than earlier attack\nstrategies."
                },
                "authors": [
                    {
                        "name": "Zelin Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Mingming Yang"
                    },
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.03328v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.03328v3",
                "updated": "2024-08-26T02:05:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    2,
                    5,
                    37,
                    0,
                    239,
                    0
                ],
                "published": "2023-10-05T05:55:06Z",
                "published_parsed": [
                    2023,
                    10,
                    5,
                    5,
                    55,
                    6,
                    3,
                    278,
                    0
                ],
                "title": "Reformulating Domain Adaptation of Large Language Models as\n  Adapt-Retrieve-Revise: A Case Study on Chinese Legal Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reformulating Domain Adaptation of Large Language Models as\n  Adapt-Retrieve-Revise: A Case Study on Chinese Legal Domain"
                },
                "summary": "While large language models (LLMs) like GPT-4 have recently demonstrated\nastonishing zero-shot capabilities in general domain tasks, they often generate\ncontent with hallucinations in specific domains such as Chinese law, hindering\ntheir application in these areas. This is typically due to the absence of\ntraining data that encompasses such a specific domain, preventing GPT-4 from\nacquiring in-domain knowledge. A pressing challenge is that it's not plausible\nto continue training LLMs of such scale on in-domain data.\n  This paper introduces a simple and effective domain adaptation framework for\nGPT-4 by reformulating generation as an \\textbf{adapt-retrieve-revise} process.\nThe initial step is to \\textbf{adapt} an affordable 7B LLM to the target domain\nby continuing learning on in-domain data. When solving a task, we leverage the\nadapted LLM to generate a draft answer given a task query. Then, the draft\nanswer will be used to \\textbf{retrieve} supporting evidence candidates from an\nexternal in-domain knowledge base. Finally, the draft answer and retrieved\nevidence are concatenated into a whole prompt to let GPT-4 assess the evidence\nand \\textbf{revise} the draft answer to generate the final answer.\n  Our proposal combines the advantages of the efficiency of adapting a smaller\n7B model with the evidence-assessing capability of GPT-4 and effectively\nprevents GPT-4 from generating hallucinatory content. In the zero-shot setting\nof four Chinese legal tasks, our method improves accuracy by 33.3\\% compared to\nthe direct generation by GPT-4. When compared to two stronger retrieval-based\nbaselines, our method outperforms them by 15.4\\% and 23.9\\%. Our code will be\nreleased",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) like GPT-4 have recently demonstrated\nastonishing zero-shot capabilities in general domain tasks, they often generate\ncontent with hallucinations in specific domains such as Chinese law, hindering\ntheir application in these areas. This is typically due to the absence of\ntraining data that encompasses such a specific domain, preventing GPT-4 from\nacquiring in-domain knowledge. A pressing challenge is that it's not plausible\nto continue training LLMs of such scale on in-domain data.\n  This paper introduces a simple and effective domain adaptation framework for\nGPT-4 by reformulating generation as an \\textbf{adapt-retrieve-revise} process.\nThe initial step is to \\textbf{adapt} an affordable 7B LLM to the target domain\nby continuing learning on in-domain data. When solving a task, we leverage the\nadapted LLM to generate a draft answer given a task query. Then, the draft\nanswer will be used to \\textbf{retrieve} supporting evidence candidates from an\nexternal in-domain knowledge base. Finally, the draft answer and retrieved\nevidence are concatenated into a whole prompt to let GPT-4 assess the evidence\nand \\textbf{revise} the draft answer to generate the final answer.\n  Our proposal combines the advantages of the efficiency of adapting a smaller\n7B model with the evidence-assessing capability of GPT-4 and effectively\nprevents GPT-4 from generating hallucinatory content. In the zero-shot setting\nof four Chinese legal tasks, our method improves accuracy by 33.3\\% compared to\nthe direct generation by GPT-4. When compared to two stronger retrieval-based\nbaselines, our method outperforms them by 15.4\\% and 23.9\\%. Our code will be\nreleased"
                },
                "authors": [
                    {
                        "name": "Zhen wan"
                    },
                    {
                        "name": "Yating Zhang"
                    },
                    {
                        "name": "Yexiang Wang"
                    },
                    {
                        "name": "Fei Cheng"
                    },
                    {
                        "name": "Sadao Kurohashi"
                    }
                ],
                "author_detail": {
                    "name": "Sadao Kurohashi"
                },
                "author": "Sadao Kurohashi",
                "arxiv_comment": "Accepted by ACL 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.03328v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.03328v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13977v1",
                "updated": "2024-08-26T01:50:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    1,
                    50,
                    29,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T01:50:29Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    1,
                    50,
                    29,
                    0,
                    239,
                    0
                ],
                "title": "Say Your Reason: Extract Contextual Rules In Situ for Context-aware\n  Service Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Say Your Reason: Extract Contextual Rules In Situ for Context-aware\n  Service Recommendation"
                },
                "summary": "This paper introduces SayRea, an interactive system that facilitates the\nextraction of contextual rules for personalized context-aware service\nrecommendations in mobile scenarios. The system monitors a user's execution of\nregistered services on their smartphones (via accessibility service) and\nproactively requests a single-sentence reason from the user. By utilizing a\nLarge Language Model (LLM), SayRea parses the reason and predicts contextual\nrelationships between the observed service and potential contexts (such as\nsetting the alarm clock deep in the evening). In this way, SayRea can\nsignificantly reduce the cognitive load on users in anticipating future needs\nand selecting contextual attributes. A 10-day field study involving 20\nparticipants showed that SayRea accumulated an average of 62.4 rules per user\nand successfully recommended 45% of service usage. The participants provided\npositive feedback on the system's usability, interpretability, and\ncontrollability. The findings highlight SayRea's effectiveness in personalized\nservice recommendations and its potential to enhance user experience in mobile\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces SayRea, an interactive system that facilitates the\nextraction of contextual rules for personalized context-aware service\nrecommendations in mobile scenarios. The system monitors a user's execution of\nregistered services on their smartphones (via accessibility service) and\nproactively requests a single-sentence reason from the user. By utilizing a\nLarge Language Model (LLM), SayRea parses the reason and predicts contextual\nrelationships between the observed service and potential contexts (such as\nsetting the alarm clock deep in the evening). In this way, SayRea can\nsignificantly reduce the cognitive load on users in anticipating future needs\nand selecting contextual attributes. A 10-day field study involving 20\nparticipants showed that SayRea accumulated an average of 62.4 rules per user\nand successfully recommended 45% of service usage. The participants provided\npositive feedback on the system's usability, interpretability, and\ncontrollability. The findings highlight SayRea's effectiveness in personalized\nservice recommendations and its potential to enhance user experience in mobile\nscenarios."
                },
                "authors": [
                    {
                        "name": "Yuxuan Li"
                    },
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Lihang Pan"
                    },
                    {
                        "name": "Chun Yu"
                    },
                    {
                        "name": "Yuanchun Shi"
                    }
                ],
                "author_detail": {
                    "name": "Yuanchun Shi"
                },
                "author": "Yuanchun Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13976v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13976v2",
                "updated": "2024-08-27T06:49:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    6,
                    49,
                    19,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-26T01:48:57Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    1,
                    48,
                    57,
                    0,
                    239,
                    0
                ],
                "title": "Sifting through the Chaff: On Utilizing Execution Feedback for Ranking\n  the Generated Code Candidates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sifting through the Chaff: On Utilizing Execution Feedback for Ranking\n  the Generated Code Candidates"
                },
                "summary": "Large Language Models (LLMs), such as GPT-4, StarCoder, and CodeLlama, are\ntransforming the way developers approach programming by automatically\ngenerating code based on given natural language descriptions. Despite\nadvancements, generating syntactically and semantically correct code remains\nchallenging, especially for complex programming tasks. Typically, individuals\ngenerate multiple candidate solutions using LLMs to increase the likelihood of\nproducing correct code. However, selecting the correct code from these\ncandidates-a process known as code ranking-remains a major challenge. Current\nresearch on code ranking can be categorized into execution-based and\nnon-execution-based methods. Execution-based methods, although effective,\nencounter notable limitations, such as scarcity of quality unit tests and\nsecurity risks. Non-execution-based methods like CodeRanker, which rely solely\non classification labels to train a code ranker, struggle to capture subtle\nerrors and provide detailed error insights. Recognizing the strengths and\nlimitations of both approaches, we propose a new method. The key insight of our\nwork is that an effective code ranker is expected to genuinely comprehend the\nunderlying causes of erroneous code, as relying solely on classification labels\nis insufficient. Inspired by this, this paper puts forward RankEF, an\ninnovative approach for code ranking that leverages execution feedback. RankEF\nemploys multi-task learning to integrate code classification with execution\nfeedback generation. This approach enables the model to understand the reasons\nbehind incorrect code, distinguishing between correct and incorrect solutions\nwithout the need to execute the code during the ranking phase. Experiments on\nthree code generation benchmarks demonstrate that RankEF significantly\noutperforms the state-of-the-art CodeRanker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT-4, StarCoder, and CodeLlama, are\ntransforming the way developers approach programming by automatically\ngenerating code based on given natural language descriptions. Despite\nadvancements, generating syntactically and semantically correct code remains\nchallenging, especially for complex programming tasks. Typically, individuals\ngenerate multiple candidate solutions using LLMs to increase the likelihood of\nproducing correct code. However, selecting the correct code from these\ncandidates-a process known as code ranking-remains a major challenge. Current\nresearch on code ranking can be categorized into execution-based and\nnon-execution-based methods. Execution-based methods, although effective,\nencounter notable limitations, such as scarcity of quality unit tests and\nsecurity risks. Non-execution-based methods like CodeRanker, which rely solely\non classification labels to train a code ranker, struggle to capture subtle\nerrors and provide detailed error insights. Recognizing the strengths and\nlimitations of both approaches, we propose a new method. The key insight of our\nwork is that an effective code ranker is expected to genuinely comprehend the\nunderlying causes of erroneous code, as relying solely on classification labels\nis insufficient. Inspired by this, this paper puts forward RankEF, an\ninnovative approach for code ranking that leverages execution feedback. RankEF\nemploys multi-task learning to integrate code classification with execution\nfeedback generation. This approach enables the model to understand the reasons\nbehind incorrect code, distinguishing between correct and incorrect solutions\nwithout the need to execute the code during the ranking phase. Experiments on\nthree code generation benchmarks demonstrate that RankEF significantly\noutperforms the state-of-the-art CodeRanker."
                },
                "authors": [
                    {
                        "name": "Zhihong Sun"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Ge Li"
                    },
                    {
                        "name": "Chen Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Chen Lyu"
                },
                "author": "Chen Lyu",
                "arxiv_comment": "Accepted by the 39th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13976v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13976v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13960v2",
                "updated": "2024-08-27T15:06:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    6,
                    17,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-25T23:48:11Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    23,
                    48,
                    11,
                    6,
                    238,
                    0
                ],
                "title": "Time Series Analysis for Education: Methods, Applications, and Future\n  Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time Series Analysis for Education: Methods, Applications, and Future\n  Directions"
                },
                "summary": "Recent advancements in the collection and analysis of sequential educational\ndata have brought time series analysis to a pivotal position in educational\nresearch, highlighting its essential role in facilitating data-driven\ndecision-making. However, there is a lack of comprehensive summaries that\nconsolidate these advancements. To the best of our knowledge, this paper is the\nfirst to provide a comprehensive review of time series analysis techniques\nspecifically within the educational context. We begin by exploring the\nlandscape of educational data analytics, categorizing various data sources and\ntypes relevant to education. We then review four prominent time series\nmethods-forecasting, classification, clustering, and anomaly\ndetection-illustrating their specific application points in educational\nsettings. Subsequently, we present a range of educational scenarios and\napplications, focusing on how these methods are employed to address diverse\neducational tasks, which highlights the practical integration of multiple time\nseries methods to solve complex educational problems. Finally, we conclude with\na discussion on future directions, including personalized learning analytics,\nmultimodal data fusion, and the role of large language models (LLMs) in\neducational time series. The contributions of this paper include a detailed\ntaxonomy of educational data, a synthesis of time series techniques with\nspecific educational applications, and a forward-looking perspective on\nemerging trends and future research opportunities in educational analysis. The\nrelated papers and resources are available and regularly updated at the project\npage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in the collection and analysis of sequential educational\ndata have brought time series analysis to a pivotal position in educational\nresearch, highlighting its essential role in facilitating data-driven\ndecision-making. However, there is a lack of comprehensive summaries that\nconsolidate these advancements. To the best of our knowledge, this paper is the\nfirst to provide a comprehensive review of time series analysis techniques\nspecifically within the educational context. We begin by exploring the\nlandscape of educational data analytics, categorizing various data sources and\ntypes relevant to education. We then review four prominent time series\nmethods-forecasting, classification, clustering, and anomaly\ndetection-illustrating their specific application points in educational\nsettings. Subsequently, we present a range of educational scenarios and\napplications, focusing on how these methods are employed to address diverse\neducational tasks, which highlights the practical integration of multiple time\nseries methods to solve complex educational problems. Finally, we conclude with\na discussion on future directions, including personalized learning analytics,\nmultimodal data fusion, and the role of large language models (LLMs) in\neducational time series. The contributions of this paper include a detailed\ntaxonomy of educational data, a synthesis of time series techniques with\nspecific educational applications, and a forward-looking perspective on\nemerging trends and future research opportunities in educational analysis. The\nrelated papers and resources are available and regularly updated at the project\npage."
                },
                "authors": [
                    {
                        "name": "Shengzhong Mao"
                    },
                    {
                        "name": "Chaoli Zhang"
                    },
                    {
                        "name": "Yichi Song"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Xiao-Jun Zeng"
                    },
                    {
                        "name": "Zenglin Xu"
                    },
                    {
                        "name": "Qingsong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Qingsong Wen"
                },
                "author": "Qingsong Wen",
                "arxiv_comment": "24 pages, 3 figures, 6 tables, project page: see\n  https://github.com/ai-for-edu/time-series-analysis-for-education",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13959v1",
                "updated": "2024-08-25T23:46:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    23,
                    46,
                    35,
                    6,
                    238,
                    0
                ],
                "published": "2024-08-25T23:46:35Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    23,
                    46,
                    35,
                    6,
                    238,
                    0
                ],
                "title": "Bidirectional Awareness Induction in Autoregressive Seq2Seq Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional Awareness Induction in Autoregressive Seq2Seq Models"
                },
                "summary": "Autoregressive Sequence-To-Sequence models are the foundation of many Deep\nLearning achievements in major research fields such as Vision and Natural\nLanguage Processing. Despite that, they still present significant limitations.\nFor instance, when errors occur in the early steps of the prediction, the whole\noutput is severely affected. Such reliance on previously predicted tokens and\nthe inherent computational unfriendliness of sequential algorithms, motivated\nresearchers to explore different architectures and methods in the search for\nbidirectional approaches. In this work, we introduce the Bidirectional\nAwareness Induction (BAI), a training method that leverages a subset of\nelements in the network, the Pivots, to perform bidirectional learning without\nbreaking the autoregressive constraints. To showcase its flexibility, we apply\nthe method to three architectures, the Transformer, ExpansionNet v2 and GPT,\nthen perform experiments over three tasks. Experimental results showcase BAI's\neffectiveness on all selected tasks and architectures. In particular, we\nobserved an increase of up to 2.4 CIDEr in Image-Captioning, 4.96 BLEU in\nNeural Machine Translation, and 1.16 ROUGE in Text Summarization compared to\nthe respective baselines. Notably, BAI not only has a positive impact on models\ntrained from scratch but on pre-trained models as well. Such an aspect,\ncombined with the absence of architectural requirements synergizes well with\nthe current trend of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Sequence-To-Sequence models are the foundation of many Deep\nLearning achievements in major research fields such as Vision and Natural\nLanguage Processing. Despite that, they still present significant limitations.\nFor instance, when errors occur in the early steps of the prediction, the whole\noutput is severely affected. Such reliance on previously predicted tokens and\nthe inherent computational unfriendliness of sequential algorithms, motivated\nresearchers to explore different architectures and methods in the search for\nbidirectional approaches. In this work, we introduce the Bidirectional\nAwareness Induction (BAI), a training method that leverages a subset of\nelements in the network, the Pivots, to perform bidirectional learning without\nbreaking the autoregressive constraints. To showcase its flexibility, we apply\nthe method to three architectures, the Transformer, ExpansionNet v2 and GPT,\nthen perform experiments over three tasks. Experimental results showcase BAI's\neffectiveness on all selected tasks and architectures. In particular, we\nobserved an increase of up to 2.4 CIDEr in Image-Captioning, 4.96 BLEU in\nNeural Machine Translation, and 1.16 ROUGE in Text Summarization compared to\nthe respective baselines. Notably, BAI not only has a positive impact on models\ntrained from scratch but on pre-trained models as well. Such an aspect,\ncombined with the absence of architectural requirements synergizes well with\nthe current trend of LLMs."
                },
                "authors": [
                    {
                        "name": "Jia Cheng Hu"
                    },
                    {
                        "name": "Roberto Cavicchioli"
                    },
                    {
                        "name": "Alessandro Capotondi"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Capotondi"
                },
                "author": "Alessandro Capotondi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13940v1",
                "updated": "2024-08-25T21:20:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    21,
                    20,
                    17,
                    6,
                    238,
                    0
                ],
                "published": "2024-08-25T21:20:17Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    21,
                    20,
                    17,
                    6,
                    238,
                    0
                ],
                "title": "CoT Rerailer: Enhancing the Reliability of Large Language Models in\n  Complex Reasoning Tasks through Error Detection and Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoT Rerailer: Enhancing the Reliability of Large Language Models in\n  Complex Reasoning Tasks through Error Detection and Correction"
                },
                "summary": "Chain-of-Thought (CoT) prompting enhances Large Language Models (LLMs)\ncomplex reasoning abilities by generating intermediate steps. However, these\nsteps can introduce hallucinations and accumulate errors. We propose the CoT\nRerailer to address these challenges, employing self-consistency and\nmulti-agent debate systems to identify and rectify errors in the reasoning\nprocess. The CoT Rerailer first selects the most logically correct Reasoning\nPath (RP) using consistency checks and critical evaluation by automated agents.\nIt then engages a multi-agent debate system to propose and validate corrections\nto ensure the generation of an error-free intermediate logical path. The\ncorrected steps are then used to generate a revised reasoning chain to further\nreduce hallucinations and enhance answer quality. We demonstrate the\neffectiveness of our approach across diverse question-answering datasets in\nvarious knowledge domains. The CoT Rerailer enhances the reliability of\nLLM-generated reasoning, contributing to more trustworthy AI driven\ndecision-making processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting enhances Large Language Models (LLMs)\ncomplex reasoning abilities by generating intermediate steps. However, these\nsteps can introduce hallucinations and accumulate errors. We propose the CoT\nRerailer to address these challenges, employing self-consistency and\nmulti-agent debate systems to identify and rectify errors in the reasoning\nprocess. The CoT Rerailer first selects the most logically correct Reasoning\nPath (RP) using consistency checks and critical evaluation by automated agents.\nIt then engages a multi-agent debate system to propose and validate corrections\nto ensure the generation of an error-free intermediate logical path. The\ncorrected steps are then used to generate a revised reasoning chain to further\nreduce hallucinations and enhance answer quality. We demonstrate the\neffectiveness of our approach across diverse question-answering datasets in\nvarious knowledge domains. The CoT Rerailer enhances the reliability of\nLLM-generated reasoning, contributing to more trustworthy AI driven\ndecision-making processes."
                },
                "authors": [
                    {
                        "name": "Guangya Wan"
                    },
                    {
                        "name": "Yuqi Wu"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Sheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Li"
                },
                "author": "Sheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13933v1",
                "updated": "2024-08-25T20:41:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    20,
                    41,
                    22,
                    6,
                    238,
                    0
                ],
                "published": "2024-08-25T20:41:22Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    20,
                    41,
                    22,
                    6,
                    238,
                    0
                ],
                "title": "MobileQuant: Mobile-friendly Quantization for On-device Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MobileQuant: Mobile-friendly Quantization for On-device Language Models"
                },
                "summary": "Large language models (LLMs) have revolutionized language processing,\ndelivering outstanding results across multiple applications. However, deploying\nLLMs on edge devices poses several challenges with respect to memory, energy,\nand compute costs, limiting their widespread use in devices such as mobile\nphones. A promising solution is to reduce the number of bits used to represent\nweights and activations. While existing works have found partial success at\nquantizing LLMs to lower bitwidths, e.g. 4-bit weights, quantizing activations\nbeyond 16 bits often leads to large computational overheads due to poor\non-device quantization support, or a considerable accuracy drop. Yet, 8-bit\nactivations are very attractive for on-device deployment as they would enable\nLLMs to fully exploit mobile-friendly hardware, e.g. Neural Processing Units\n(NPUs). In this work, we make a first attempt to facilitate the on-device\ndeployment of LLMs using integer-only quantization. We first investigate the\nlimitations of existing quantization methods for on-device deployment, with a\nspecial focus on activation quantization. We then address these limitations by\nintroducing a simple post-training quantization method, named MobileQuant, that\nextends previous weight equivalent transformation works by jointly optimizing\nthe weight transformation and activation range parameters in an end-to-end\nmanner. MobileQuant demonstrates superior capabilities over existing methods by\n1) achieving near-lossless quantization on a wide range of LLM benchmarks, 2)\nreducing latency and energy consumption by 20\\%-50\\% compared to current\non-device quantization strategies, 3) requiring limited compute budget, 4)\nbeing compatible with mobile-friendly compute units, e.g. NPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized language processing,\ndelivering outstanding results across multiple applications. However, deploying\nLLMs on edge devices poses several challenges with respect to memory, energy,\nand compute costs, limiting their widespread use in devices such as mobile\nphones. A promising solution is to reduce the number of bits used to represent\nweights and activations. While existing works have found partial success at\nquantizing LLMs to lower bitwidths, e.g. 4-bit weights, quantizing activations\nbeyond 16 bits often leads to large computational overheads due to poor\non-device quantization support, or a considerable accuracy drop. Yet, 8-bit\nactivations are very attractive for on-device deployment as they would enable\nLLMs to fully exploit mobile-friendly hardware, e.g. Neural Processing Units\n(NPUs). In this work, we make a first attempt to facilitate the on-device\ndeployment of LLMs using integer-only quantization. We first investigate the\nlimitations of existing quantization methods for on-device deployment, with a\nspecial focus on activation quantization. We then address these limitations by\nintroducing a simple post-training quantization method, named MobileQuant, that\nextends previous weight equivalent transformation works by jointly optimizing\nthe weight transformation and activation range parameters in an end-to-end\nmanner. MobileQuant demonstrates superior capabilities over existing methods by\n1) achieving near-lossless quantization on a wide range of LLM benchmarks, 2)\nreducing latency and energy consumption by 20\\%-50\\% compared to current\non-device quantization strategies, 3) requiring limited compute budget, 4)\nbeing compatible with mobile-friendly compute units, e.g. NPU."
                },
                "authors": [
                    {
                        "name": "Fuwen Tan"
                    },
                    {
                        "name": "Royson Lee"
                    },
                    {
                        "name": "Łukasz Dudziak"
                    },
                    {
                        "name": "Shell Xu Hu"
                    },
                    {
                        "name": "Sourav Bhattacharya"
                    },
                    {
                        "name": "Timothy Hospedales"
                    },
                    {
                        "name": "Georgios Tzimiropoulos"
                    },
                    {
                        "name": "Brais Martinez"
                    }
                ],
                "author_detail": {
                    "name": "Brais Martinez"
                },
                "author": "Brais Martinez",
                "arxiv_comment": "Code and models available: https://github.com/saic-fi/MobileQuant",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13918v1",
                "updated": "2024-08-25T19:03:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    19,
                    3,
                    46,
                    6,
                    238,
                    0
                ],
                "published": "2024-08-25T19:03:46Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    19,
                    3,
                    46,
                    6,
                    238,
                    0
                ],
                "title": "Geo-Llama: Leveraging LLMs for Human Mobility Trajectory Generation with\n  Spatiotemporal Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geo-Llama: Leveraging LLMs for Human Mobility Trajectory Generation with\n  Spatiotemporal Constraints"
                },
                "summary": "Simulating human mobility data is essential for various application domains,\nincluding transportation, urban planning, and epidemic control, since real data\nare often inaccessible to researchers due to expensive costs and privacy\nissues. Several existing deep generative solutions propose learning from real\ntrajectories to generate synthetic ones. Despite the progress, most of them\nsuffer from training stability issues and scale poorly with growing data size.\nMore importantly, they generally lack control mechanisms to steer the generated\ntrajectories based on spatiotemporal constraints such as fixing specific\nvisits. To address such limitations, we formally define the controlled\ntrajectory generation problem with spatiotemporal constraints and propose\nGeo-Llama. This novel LLM-inspired framework enforces explicit visit\nconstraints in a contextually coherent way. It fine-tunes pre-trained LLMs on\ntrajectories with a visit-wise permutation strategy where each visit\ncorresponds to a time and location. This enables the model to capture the\nspatiotemporal patterns regardless of visit orders and allows flexible and\nin-context constraint integration through prompts during generation. Extensive\nexperiments on real-world and synthetic datasets validate the effectiveness of\nGeo-Llama, demonstrating its versatility and robustness in handling a broad\nrange of constraints to generate more realistic trajectories compared to\nexisting methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating human mobility data is essential for various application domains,\nincluding transportation, urban planning, and epidemic control, since real data\nare often inaccessible to researchers due to expensive costs and privacy\nissues. Several existing deep generative solutions propose learning from real\ntrajectories to generate synthetic ones. Despite the progress, most of them\nsuffer from training stability issues and scale poorly with growing data size.\nMore importantly, they generally lack control mechanisms to steer the generated\ntrajectories based on spatiotemporal constraints such as fixing specific\nvisits. To address such limitations, we formally define the controlled\ntrajectory generation problem with spatiotemporal constraints and propose\nGeo-Llama. This novel LLM-inspired framework enforces explicit visit\nconstraints in a contextually coherent way. It fine-tunes pre-trained LLMs on\ntrajectories with a visit-wise permutation strategy where each visit\ncorresponds to a time and location. This enables the model to capture the\nspatiotemporal patterns regardless of visit orders and allows flexible and\nin-context constraint integration through prompts during generation. Extensive\nexperiments on real-world and synthetic datasets validate the effectiveness of\nGeo-Llama, demonstrating its versatility and robustness in handling a broad\nrange of constraints to generate more realistic trajectories compared to\nexisting methods."
                },
                "authors": [
                    {
                        "name": "Siyu Li"
                    },
                    {
                        "name": "Toan Tran"
                    },
                    {
                        "name": "Haowen Lin"
                    },
                    {
                        "name": "John Khrumm"
                    },
                    {
                        "name": "Cyrus Shahabi"
                    },
                    {
                        "name": "Li Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiong"
                },
                "author": "Li Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13915v1",
                "updated": "2024-08-25T18:47:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    18,
                    47,
                    55,
                    6,
                    238,
                    0
                ],
                "published": "2024-08-25T18:47:55Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    18,
                    47,
                    55,
                    6,
                    238,
                    0
                ],
                "title": "LLMs are Superior Feedback Providers: Bootstrapping Reasoning for Lie\n  Detection with Self-Generated Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are Superior Feedback Providers: Bootstrapping Reasoning for Lie\n  Detection with Self-Generated Feedback"
                },
                "summary": "Large Language Models (LLMs) excel at generating human-like dialogues and\ncomprehending text. However, understanding the subtleties of complex exchanges\nin language remains a challenge. We propose a bootstrapping framework that\nleverages self-generated feedback to enhance LLM reasoning capabilities for lie\ndetection. The framework consists of three stages: suggestion, feedback\ncollection, and modification. In the suggestion stage, a cost-effective\nlanguage model generates initial predictions based on game state and dialogue.\nThe feedback-collection stage involves a language model providing feedback on\nthese predictions. In the modification stage, a more advanced language model\nrefines the initial predictions using the auto-generated feedback. We\ninvestigate the application of the proposed framework for detecting betrayal\nand deception in Diplomacy games, and compare it with feedback from\nprofessional human players. The LLM-generated feedback exhibits superior\nquality and significantly enhances the performance of the model. Our approach\nachieves a 39% improvement over the zero-shot baseline in lying-F1 without the\nneed for any training data, rivaling state-of-the-art supervised learning\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at generating human-like dialogues and\ncomprehending text. However, understanding the subtleties of complex exchanges\nin language remains a challenge. We propose a bootstrapping framework that\nleverages self-generated feedback to enhance LLM reasoning capabilities for lie\ndetection. The framework consists of three stages: suggestion, feedback\ncollection, and modification. In the suggestion stage, a cost-effective\nlanguage model generates initial predictions based on game state and dialogue.\nThe feedback-collection stage involves a language model providing feedback on\nthese predictions. In the modification stage, a more advanced language model\nrefines the initial predictions using the auto-generated feedback. We\ninvestigate the application of the proposed framework for detecting betrayal\nand deception in Diplomacy games, and compare it with feedback from\nprofessional human players. The LLM-generated feedback exhibits superior\nquality and significantly enhances the performance of the model. Our approach\nachieves a 39% improvement over the zero-shot baseline in lying-F1 without the\nneed for any training data, rivaling state-of-the-art supervised learning\nresults."
                },
                "authors": [
                    {
                        "name": "Tanushree Banerjee"
                    },
                    {
                        "name": "Richard Zhu"
                    },
                    {
                        "name": "Runzhe Yang"
                    },
                    {
                        "name": "Karthik Narasimhan"
                    }
                ],
                "author_detail": {
                    "name": "Karthik Narasimhan"
                },
                "author": "Karthik Narasimhan",
                "arxiv_comment": "19 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05109v2",
                "updated": "2024-08-25T17:22:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    17,
                    22,
                    29,
                    6,
                    238,
                    0
                ],
                "published": "2024-05-08T15:05:55Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    15,
                    5,
                    55,
                    2,
                    129,
                    0
                ],
                "title": "QFMTS: Generating Query-Focused Summaries over Multi-Table Inputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QFMTS: Generating Query-Focused Summaries over Multi-Table Inputs"
                },
                "summary": "Table summarization is a crucial task aimed at condensing information from\ntabular data into concise and comprehensible textual summaries. However,\nexisting approaches often fall short of adequately meeting users' information\nand quality requirements and tend to overlook the complexities of real-world\nqueries. In this paper, we propose a novel method to address these limitations\nby introducing query-focused multi-table summarization. Our approach, which\ncomprises a table serialization module, a summarization controller, and a large\nlanguage model (LLM), utilizes textual queries and multiple tables to generate\nquery-dependent table summaries tailored to users' information needs. To\nfacilitate research in this area, we present a comprehensive dataset\nspecifically tailored for this task, consisting of 4909 query-summary pairs,\neach associated with multiple tables. Through extensive experiments using our\ncurated dataset, we demonstrate the effectiveness of our proposed method\ncompared to baseline approaches. Our findings offer insights into the\nchallenges of complex table reasoning for precise summarization, contributing\nto the advancement of research in query-focused multi-table summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Table summarization is a crucial task aimed at condensing information from\ntabular data into concise and comprehensible textual summaries. However,\nexisting approaches often fall short of adequately meeting users' information\nand quality requirements and tend to overlook the complexities of real-world\nqueries. In this paper, we propose a novel method to address these limitations\nby introducing query-focused multi-table summarization. Our approach, which\ncomprises a table serialization module, a summarization controller, and a large\nlanguage model (LLM), utilizes textual queries and multiple tables to generate\nquery-dependent table summaries tailored to users' information needs. To\nfacilitate research in this area, we present a comprehensive dataset\nspecifically tailored for this task, consisting of 4909 query-summary pairs,\neach associated with multiple tables. Through extensive experiments using our\ncurated dataset, we demonstrate the effectiveness of our proposed method\ncompared to baseline approaches. Our findings offer insights into the\nchallenges of complex table reasoning for precise summarization, contributing\nto the advancement of research in query-focused multi-table summarization."
                },
                "authors": [
                    {
                        "name": "Weijia Zhang"
                    },
                    {
                        "name": "Vaishali Pal"
                    },
                    {
                        "name": "Jia-Hong Huang"
                    },
                    {
                        "name": "Evangelos Kanoulas"
                    },
                    {
                        "name": "Maarten de Rijke"
                    }
                ],
                "author_detail": {
                    "name": "Maarten de Rijke"
                },
                "author": "Maarten de Rijke",
                "arxiv_comment": "Accepted by the 27th European Conference on Artificial Intelligence\n  (ECAI-2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13890v1",
                "updated": "2024-08-25T16:43:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    16,
                    43,
                    47,
                    6,
                    238,
                    0
                ],
                "published": "2024-08-25T16:43:47Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    16,
                    43,
                    47,
                    6,
                    238,
                    0
                ],
                "title": "Making Large Language Models Better Planners with Reasoning-Decision\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Large Language Models Better Planners with Reasoning-Decision\n  Alignment"
                },
                "summary": "Data-driven approaches for autonomous driving (AD) have been widely adopted\nin the past decade but are confronted with dataset bias and uninterpretability.\nInspired by the knowledge-driven nature of human driving, recent approaches\nexplore the potential of large language models (LLMs) to improve understanding\nand decision-making in traffic scenarios. They find that the pretrain-finetune\nparadigm of LLMs on downstream data with the Chain-of-Thought (CoT) reasoning\nprocess can enhance explainability and scene understanding. However, such a\npopular strategy proves to suffer from the notorious problems of misalignment\nbetween the crafted CoTs against the consequent decision-making, which remains\nuntouched by previous LLM-based AD methods. To address this problem, we\nmotivate an end-to-end decision-making model based on multimodality-augmented\nLLM, which simultaneously executes CoT reasoning and carries out planning\nresults. Furthermore, we propose a reasoning-decision alignment constraint\nbetween the paired CoTs and planning results, imposing the correspondence\nbetween reasoning and decision-making. Moreover, we redesign the CoTs to enable\nthe model to comprehend complex scenarios and enhance decision-making\nperformance. We dub our proposed large language planners with\nreasoning-decision alignment as RDA-Driver. Experimental evaluations on the\nnuScenes and DriveLM-nuScenes benchmarks demonstrate the effectiveness of our\nRDA-Driver in enhancing the performance of end-to-end AD systems. Specifically,\nour RDA-Driver achieves state-of-the-art planning performance on the nuScenes\ndataset with 0.80 L2 error and 0.32 collision rate, and also achieves leading\nresults on challenging DriveLM-nuScenes benchmarks with 0.82 L2 error and 0.38\ncollision rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven approaches for autonomous driving (AD) have been widely adopted\nin the past decade but are confronted with dataset bias and uninterpretability.\nInspired by the knowledge-driven nature of human driving, recent approaches\nexplore the potential of large language models (LLMs) to improve understanding\nand decision-making in traffic scenarios. They find that the pretrain-finetune\nparadigm of LLMs on downstream data with the Chain-of-Thought (CoT) reasoning\nprocess can enhance explainability and scene understanding. However, such a\npopular strategy proves to suffer from the notorious problems of misalignment\nbetween the crafted CoTs against the consequent decision-making, which remains\nuntouched by previous LLM-based AD methods. To address this problem, we\nmotivate an end-to-end decision-making model based on multimodality-augmented\nLLM, which simultaneously executes CoT reasoning and carries out planning\nresults. Furthermore, we propose a reasoning-decision alignment constraint\nbetween the paired CoTs and planning results, imposing the correspondence\nbetween reasoning and decision-making. Moreover, we redesign the CoTs to enable\nthe model to comprehend complex scenarios and enhance decision-making\nperformance. We dub our proposed large language planners with\nreasoning-decision alignment as RDA-Driver. Experimental evaluations on the\nnuScenes and DriveLM-nuScenes benchmarks demonstrate the effectiveness of our\nRDA-Driver in enhancing the performance of end-to-end AD systems. Specifically,\nour RDA-Driver achieves state-of-the-art planning performance on the nuScenes\ndataset with 0.80 L2 error and 0.32 collision rate, and also achieves leading\nresults on challenging DriveLM-nuScenes benchmarks with 0.82 L2 error and 0.38\ncollision rate."
                },
                "authors": [
                    {
                        "name": "Zhijian Huang"
                    },
                    {
                        "name": "Tao Tang"
                    },
                    {
                        "name": "Shaoxiang Chen"
                    },
                    {
                        "name": "Sihao Lin"
                    },
                    {
                        "name": "Zequn Jie"
                    },
                    {
                        "name": "Lin Ma"
                    },
                    {
                        "name": "Guangrun Wang"
                    },
                    {
                        "name": "Xiaodan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Liang"
                },
                "author": "Xiaodan Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13889v1",
                "updated": "2024-08-25T16:43:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    16,
                    43,
                    19,
                    6,
                    238,
                    0
                ],
                "published": "2024-08-25T16:43:19Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    16,
                    43,
                    19,
                    6,
                    238,
                    0
                ],
                "title": "LLM with Relation Classifier for Document-Level Relation Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM with Relation Classifier for Document-Level Relation Extraction"
                },
                "summary": "Large language models (LLMs) create a new paradigm for natural language\nprocessing. Despite their advancement, LLM-based methods still lag behind\ntraditional approaches in document-level relation extraction (DocRE), a\ncritical task for understanding complex entity relations. This paper\ninvestigates the causes of this performance gap, identifying the dispersion of\nattention by LLMs due to entity pairs without relations as a primary factor. We\nthen introduce a novel classifier-LLM approach to DocRE. The proposed approach\nbegins with a classifier specifically designed to select entity pair candidates\nexhibiting potential relations and thereby feeds them to LLM for the final\nrelation extraction. This method ensures that during inference, the LLM's focus\nis directed primarily at entity pairs with relations. Experiments on DocRE\nbenchmarks reveal that our method significantly outperforms recent LLM-based\nDocRE models and achieves competitive performance with several leading\ntraditional DocRE models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) create a new paradigm for natural language\nprocessing. Despite their advancement, LLM-based methods still lag behind\ntraditional approaches in document-level relation extraction (DocRE), a\ncritical task for understanding complex entity relations. This paper\ninvestigates the causes of this performance gap, identifying the dispersion of\nattention by LLMs due to entity pairs without relations as a primary factor. We\nthen introduce a novel classifier-LLM approach to DocRE. The proposed approach\nbegins with a classifier specifically designed to select entity pair candidates\nexhibiting potential relations and thereby feeds them to LLM for the final\nrelation extraction. This method ensures that during inference, the LLM's focus\nis directed primarily at entity pairs with relations. Experiments on DocRE\nbenchmarks reveal that our method significantly outperforms recent LLM-based\nDocRE models and achieves competitive performance with several leading\ntraditional DocRE models."
                },
                "authors": [
                    {
                        "name": "Xingzuo Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Yunfei Long"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13863v1",
                "updated": "2024-08-25T15:27:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    15,
                    27,
                    21,
                    6,
                    238,
                    0
                ],
                "published": "2024-08-25T15:27:21Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    15,
                    27,
                    21,
                    6,
                    238,
                    0
                ],
                "title": "CodeGraph: Enhancing Graph Reasoning of LLMs with Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeGraph: Enhancing Graph Reasoning of LLMs with Code"
                },
                "summary": "With the increasing popularity of large language models (LLMs), reasoning on\nbasic graph algorithm problems is an essential intermediate step in assessing\ntheir abilities to process and infer complex graph reasoning tasks. Existing\nmethods usually convert graph-structured data to textual descriptions and then\nuse LLMs for reasoning and computation. However, LLMs often produce computation\nerrors on arithmetic parts in basic graph algorithm problems, such as counting\nnumber of edges. In addition, they struggle to control or understand the output\nof the reasoning process, raising concerns about whether LLMs are simply\nguessing. In this paper, we introduce CodeGraph, a method that encodes graph\nproblem solutions as code. The methods solve new graph problems by learning\nfrom exemplars, generating programs, and executing them via a program\ninterpreter. Using the few-shot setting, we evaluate CodeGraph with the base\nLLM being GPT-3.5 Turbo, Llama3-70B Instruct, Mixtral-8x22B Instruct, and\nMixtral-8x7B Instruct. Experimental results on six tasks with six graph\nencoding methods in the GraphQA dataset demonstrate that CodeGraph can boost\nperformance on graph reasoning tasks inside LLMs by 1.3% to 58.6%, depending on\nthe task. Compared to the existing methods, CodeGraph demonstrates strong\nperformance on arithmetic problems in graph tasks and offers a more\ncontrollable and interpretable approach to the reasoning process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing popularity of large language models (LLMs), reasoning on\nbasic graph algorithm problems is an essential intermediate step in assessing\ntheir abilities to process and infer complex graph reasoning tasks. Existing\nmethods usually convert graph-structured data to textual descriptions and then\nuse LLMs for reasoning and computation. However, LLMs often produce computation\nerrors on arithmetic parts in basic graph algorithm problems, such as counting\nnumber of edges. In addition, they struggle to control or understand the output\nof the reasoning process, raising concerns about whether LLMs are simply\nguessing. In this paper, we introduce CodeGraph, a method that encodes graph\nproblem solutions as code. The methods solve new graph problems by learning\nfrom exemplars, generating programs, and executing them via a program\ninterpreter. Using the few-shot setting, we evaluate CodeGraph with the base\nLLM being GPT-3.5 Turbo, Llama3-70B Instruct, Mixtral-8x22B Instruct, and\nMixtral-8x7B Instruct. Experimental results on six tasks with six graph\nencoding methods in the GraphQA dataset demonstrate that CodeGraph can boost\nperformance on graph reasoning tasks inside LLMs by 1.3% to 58.6%, depending on\nthe task. Compared to the existing methods, CodeGraph demonstrates strong\nperformance on arithmetic problems in graph tasks and offers a more\ncontrollable and interpretable approach to the reasoning process."
                },
                "authors": [
                    {
                        "name": "Qiaolong Cai"
                    },
                    {
                        "name": "Zhaowei Wang"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "James Kwok"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "In Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13858v1",
                "updated": "2024-08-25T15:05:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    15,
                    5,
                    32,
                    6,
                    238,
                    0
                ],
                "published": "2024-08-25T15:05:32Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    15,
                    5,
                    32,
                    6,
                    238,
                    0
                ],
                "title": "Draw Like an Artist: Complex Scene Generation with Diffusion Model via\n  Composition, Painting, and Retouching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Draw Like an Artist: Complex Scene Generation with Diffusion Model via\n  Composition, Painting, and Retouching"
                },
                "summary": "Recent advances in text-to-image diffusion models have demonstrated\nimpressive capabilities in image quality. However, complex scene generation\nremains relatively unexplored, and even the definition of `complex scene'\nitself remains unclear. In this paper, we address this gap by providing a\nprecise definition of complex scenes and introducing a set of Complex\nDecomposition Criteria (CDC) based on this definition. Inspired by the artists\npainting process, we propose a training-free diffusion framework called Complex\nDiffusion (CxD), which divides the process into three stages: composition,\npainting, and retouching. Our method leverages the powerful chain-of-thought\ncapabilities of large language models (LLMs) to decompose complex prompts based\non CDC and to manage composition and layout. We then develop an attention\nmodulation method that guides simple prompts to specific regions to complete\nthe complex scene painting. Finally, we inject the detailed output of the LLM\ninto a retouching model to enhance the image details, thus implementing the\nretouching stage. Extensive experiments demonstrate that our method outperforms\nprevious SOTA approaches, significantly improving the generation of\nhigh-quality, semantically consistent, and visually diverse images for complex\nscenes, even with intricate prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in text-to-image diffusion models have demonstrated\nimpressive capabilities in image quality. However, complex scene generation\nremains relatively unexplored, and even the definition of `complex scene'\nitself remains unclear. In this paper, we address this gap by providing a\nprecise definition of complex scenes and introducing a set of Complex\nDecomposition Criteria (CDC) based on this definition. Inspired by the artists\npainting process, we propose a training-free diffusion framework called Complex\nDiffusion (CxD), which divides the process into three stages: composition,\npainting, and retouching. Our method leverages the powerful chain-of-thought\ncapabilities of large language models (LLMs) to decompose complex prompts based\non CDC and to manage composition and layout. We then develop an attention\nmodulation method that guides simple prompts to specific regions to complete\nthe complex scene painting. Finally, we inject the detailed output of the LLM\ninto a retouching model to enhance the image details, thus implementing the\nretouching stage. Extensive experiments demonstrate that our method outperforms\nprevious SOTA approaches, significantly improving the generation of\nhigh-quality, semantically consistent, and visually diverse images for complex\nscenes, even with intricate prompts."
                },
                "authors": [
                    {
                        "name": "Minghao Liu"
                    },
                    {
                        "name": "Le Zhang"
                    },
                    {
                        "name": "Yingjie Tian"
                    },
                    {
                        "name": "Xiaochao Qu"
                    },
                    {
                        "name": "Luoqi Liu"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13849v1",
                "updated": "2024-08-25T14:38:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    14,
                    38,
                    13,
                    6,
                    238,
                    0
                ],
                "published": "2024-08-25T14:38:13Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    14,
                    38,
                    13,
                    6,
                    238,
                    0
                ],
                "title": "Sample-Independent Federated Learning Backdoor Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sample-Independent Federated Learning Backdoor Attack"
                },
                "summary": "In federated learning, backdoor attacks embed triggers in the adversarial\nclient's data to inject a backdoor into the model. To evade detection through\nsample analysis, non-sample-modifying backdoor attack methods based on dropout\nhave been developed. However, these methods struggle to covertly utilize\ndropout in evaluation mode, thus hindering their deployment in real-world\nscenarios. To address these, this paper introduces GhostB, a novel approach to\nfederated learning backdoor attacks that neither alters samples nor relies on\ndropout. This method employs the behavior of neurons producing specific values\nas triggers. By mapping these neuronal values to categories specified by the\nadversary, the backdoor is implanted and activated when particular feature\nvalues are detected at designated neurons. Our experiments conducted on TIMIT,\nLibriSpeech, and VoxCeleb2 databases in both Closed Set Identification (CSI)\nand Open Set Identification (OSI) scenarios demonstrate that GhostB achieves a\n100% success rate upon activation, with this rate maintained across experiments\ninvolving 1 to 50 ghost neurons. This paper investigates how the dispersion of\nneurons and their depth within hidden layers affect the success rate, revealing\nthat increased dispersion and positioning of neurons can significantly decrease\neffectiveness, potentially rendering the attack unsuccessful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In federated learning, backdoor attacks embed triggers in the adversarial\nclient's data to inject a backdoor into the model. To evade detection through\nsample analysis, non-sample-modifying backdoor attack methods based on dropout\nhave been developed. However, these methods struggle to covertly utilize\ndropout in evaluation mode, thus hindering their deployment in real-world\nscenarios. To address these, this paper introduces GhostB, a novel approach to\nfederated learning backdoor attacks that neither alters samples nor relies on\ndropout. This method employs the behavior of neurons producing specific values\nas triggers. By mapping these neuronal values to categories specified by the\nadversary, the backdoor is implanted and activated when particular feature\nvalues are detected at designated neurons. Our experiments conducted on TIMIT,\nLibriSpeech, and VoxCeleb2 databases in both Closed Set Identification (CSI)\nand Open Set Identification (OSI) scenarios demonstrate that GhostB achieves a\n100% success rate upon activation, with this rate maintained across experiments\ninvolving 1 to 50 ghost neurons. This paper investigates how the dispersion of\nneurons and their depth within hidden layers affect the success rate, revealing\nthat increased dispersion and positioning of neurons can significantly decrease\neffectiveness, potentially rendering the attack unsuccessful."
                },
                "authors": [
                    {
                        "name": "Weida Xu"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Sicong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Sicong Zhang"
                },
                "author": "Sicong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13630v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13630v2",
                "updated": "2024-08-25T14:37:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    14,
                    37,
                    35,
                    6,
                    238,
                    0
                ],
                "published": "2024-02-21T09:06:31Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    9,
                    6,
                    31,
                    2,
                    52,
                    0
                ],
                "title": "UniGraph: Learning a Unified Cross-Domain Foundation Model for\n  Text-Attributed Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniGraph: Learning a Unified Cross-Domain Foundation Model for\n  Text-Attributed Graphs"
                },
                "summary": "Foundation models like ChatGPT and GPT-4 have revolutionized artificial\nintelligence, exhibiting remarkable abilities to generalize across a wide array\nof tasks and applications beyond their initial training objectives. However,\ngraph learning has predominantly focused on single-graph models, tailored to\nspecific tasks or datasets, lacking the ability to transfer learned knowledge\nto different domains. This limitation stems from the inherent complexity and\ndiversity of graph structures, along with the different feature and label\nspaces specific to graph data. In this paper, we recognize text as an effective\nunifying medium and employ Text-Attributed Graphs (TAGs) to leverage this\npotential. We present our UniGraph framework, designed to learn a foundation\nmodel for TAGs, which is capable of generalizing to unseen graphs and tasks\nacross diverse domains. Unlike single-graph models that use pre-computed node\nfeatures of varying dimensions as input, our approach leverages textual\nfeatures for unifying node representations, even for graphs such as molecular\ngraphs that do not naturally have textual features. We propose a novel cascaded\narchitecture of Language Models (LMs) and Graph Neural Networks (GNNs) as\nbackbone networks. Additionally, we propose the first pre-training algorithm\nspecifically designed for large-scale self-supervised learning on TAGs, based\non Masked Graph Modeling. We introduce graph instruction tuning using Large\nLanguage Models (LLMs) to enable zero-shot prediction ability. Our\ncomprehensive experiments across various graph learning tasks and domains\ndemonstrate the model's effectiveness in self-supervised representation\nlearning on unseen graphs, few-shot in-context transfer, and zero-shot\ntransfer, even surpassing or matching the performance of GNNs that have\nundergone supervised training on target datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models like ChatGPT and GPT-4 have revolutionized artificial\nintelligence, exhibiting remarkable abilities to generalize across a wide array\nof tasks and applications beyond their initial training objectives. However,\ngraph learning has predominantly focused on single-graph models, tailored to\nspecific tasks or datasets, lacking the ability to transfer learned knowledge\nto different domains. This limitation stems from the inherent complexity and\ndiversity of graph structures, along with the different feature and label\nspaces specific to graph data. In this paper, we recognize text as an effective\nunifying medium and employ Text-Attributed Graphs (TAGs) to leverage this\npotential. We present our UniGraph framework, designed to learn a foundation\nmodel for TAGs, which is capable of generalizing to unseen graphs and tasks\nacross diverse domains. Unlike single-graph models that use pre-computed node\nfeatures of varying dimensions as input, our approach leverages textual\nfeatures for unifying node representations, even for graphs such as molecular\ngraphs that do not naturally have textual features. We propose a novel cascaded\narchitecture of Language Models (LMs) and Graph Neural Networks (GNNs) as\nbackbone networks. Additionally, we propose the first pre-training algorithm\nspecifically designed for large-scale self-supervised learning on TAGs, based\non Masked Graph Modeling. We introduce graph instruction tuning using Large\nLanguage Models (LLMs) to enable zero-shot prediction ability. Our\ncomprehensive experiments across various graph learning tasks and domains\ndemonstrate the model's effectiveness in self-supervised representation\nlearning on unseen graphs, few-shot in-context transfer, and zero-shot\ntransfer, even surpassing or matching the performance of GNNs that have\nundergone supervised training on target datasets."
                },
                "authors": [
                    {
                        "name": "Yufei He"
                    },
                    {
                        "name": "Yuan Sui"
                    },
                    {
                        "name": "Xiaoxin He"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13630v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13630v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02481v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02481v3",
                "updated": "2024-08-25T14:21:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    14,
                    21,
                    29,
                    6,
                    238,
                    0
                ],
                "published": "2024-06-04T16:49:06Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    16,
                    49,
                    6,
                    1,
                    156,
                    0
                ],
                "title": "Large Language Models as Carriers of Hidden Messages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Carriers of Hidden Messages"
                },
                "summary": "With the help of simple fine-tuning, one can artificially embed hidden text\ninto large language models (LLMs). This text is revealed only when triggered by\na specific query to the LLM. Two primary applications are LLM fingerprinting\nand steganography. In the context of LLM fingerprinting, a unique text\nidentifier (fingerprint) is embedded within the model to verify licensing\ncompliance. In the context of steganography, the LLM serves as a carrier for\nhidden messages that can be disclosed through a chosen trigger question.\n  Our work demonstrates that embedding hidden text in the LLM via fine-tuning,\nthough seemingly secure due to the vast number of potential triggers (any\nsequence of characters or tokens could serve as a trigger), is susceptible to\nextraction through analysis of the LLM's output decoding process. We propose an\nextraction attack called Unconditional Token Forcing (UTF). It is premised on\nthe hypothesis that iteratively feeding each token from the LLM's vocabulary\ninto the model should reveal output sequences with abnormally high token\nprobabilities, indicating potential hidden text candidates. We also present a\ndefense method to hide text in such a way that it is resistant to both UTF and\nattacks based on sampling decoding methods, which we named Unconditional Token\nForcing Confusion (UTFC). To the best of our knowledge, there is no attack\nmethod that can extract text hidden with UTFC. UTFC has both benign\napplications (improving LLM fingerprinting) and malign applications (using LLMs\nto create covert communication channels).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the help of simple fine-tuning, one can artificially embed hidden text\ninto large language models (LLMs). This text is revealed only when triggered by\na specific query to the LLM. Two primary applications are LLM fingerprinting\nand steganography. In the context of LLM fingerprinting, a unique text\nidentifier (fingerprint) is embedded within the model to verify licensing\ncompliance. In the context of steganography, the LLM serves as a carrier for\nhidden messages that can be disclosed through a chosen trigger question.\n  Our work demonstrates that embedding hidden text in the LLM via fine-tuning,\nthough seemingly secure due to the vast number of potential triggers (any\nsequence of characters or tokens could serve as a trigger), is susceptible to\nextraction through analysis of the LLM's output decoding process. We propose an\nextraction attack called Unconditional Token Forcing (UTF). It is premised on\nthe hypothesis that iteratively feeding each token from the LLM's vocabulary\ninto the model should reveal output sequences with abnormally high token\nprobabilities, indicating potential hidden text candidates. We also present a\ndefense method to hide text in such a way that it is resistant to both UTF and\nattacks based on sampling decoding methods, which we named Unconditional Token\nForcing Confusion (UTFC). To the best of our knowledge, there is no attack\nmethod that can extract text hidden with UTFC. UTFC has both benign\napplications (improving LLM fingerprinting) and malign applications (using LLMs\nto create covert communication channels)."
                },
                "authors": [
                    {
                        "name": "Jakub Hoscilowicz"
                    },
                    {
                        "name": "Pawel Popiolek"
                    },
                    {
                        "name": "Jan Rudkowski"
                    },
                    {
                        "name": "Jedrzej Bieniasz"
                    },
                    {
                        "name": "Artur Janicki"
                    }
                ],
                "author_detail": {
                    "name": "Artur Janicki"
                },
                "author": "Artur Janicki",
                "arxiv_comment": "Work in progress. Code is available at\n  https://github.com/j-hoscilowic/zurek-stegano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02481v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02481v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13833v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13833v1",
                "updated": "2024-08-25T13:36:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    13,
                    36,
                    22,
                    6,
                    238,
                    0
                ],
                "published": "2024-08-25T13:36:22Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    13,
                    36,
                    22,
                    6,
                    238,
                    0
                ],
                "title": "Biomedical Large Languages Models Seem not to be Superior to Generalist\n  Models on Unseen Medical Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomedical Large Languages Models Seem not to be Superior to Generalist\n  Models on Unseen Medical Data"
                },
                "summary": "Large language models (LLMs) have shown potential in biomedical applications,\nleading to efforts to fine-tune them on domain-specific data. However, the\neffectiveness of this approach remains unclear. This study evaluates the\nperformance of biomedically fine-tuned LLMs against their general-purpose\ncounterparts on a variety of clinical tasks. We evaluated their performance on\nclinical case challenges from the New England Journal of Medicine (NEJM) and\nthe Journal of the American Medical Association (JAMA) and on several clinical\ntasks (e.g., information extraction, document summarization, and clinical\ncoding). Using benchmarks specifically chosen to be likely outside the\nfine-tuning datasets of biomedical models, we found that biomedical LLMs mostly\nperform inferior to their general-purpose counterparts, especially on tasks not\nfocused on medical knowledge. While larger models showed similar performance on\ncase tasks (e.g., OpenBioLLM-70B: 66.4% vs. Llama-3-70B-Instruct: 65% on JAMA\ncases), smaller biomedical models showed more pronounced underperformance\n(e.g., OpenBioLLM-8B: 30% vs. Llama-3-8B-Instruct: 64.3% on NEJM cases).\nSimilar trends were observed across the CLUE (Clinical Language Understanding\nEvaluation) benchmark tasks, with general-purpose models often performing\nbetter on text generation, question answering, and coding tasks. Our results\nsuggest that fine-tuning LLMs to biomedical data may not provide the expected\nbenefits and may potentially lead to reduced performance, challenging\nprevailing assumptions about domain-specific adaptation of LLMs and\nhighlighting the need for more rigorous evaluation frameworks in healthcare AI.\nAlternative approaches, such as retrieval-augmented generation, may be more\neffective in enhancing the biomedical capabilities of LLMs without compromising\ntheir general knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown potential in biomedical applications,\nleading to efforts to fine-tune them on domain-specific data. However, the\neffectiveness of this approach remains unclear. This study evaluates the\nperformance of biomedically fine-tuned LLMs against their general-purpose\ncounterparts on a variety of clinical tasks. We evaluated their performance on\nclinical case challenges from the New England Journal of Medicine (NEJM) and\nthe Journal of the American Medical Association (JAMA) and on several clinical\ntasks (e.g., information extraction, document summarization, and clinical\ncoding). Using benchmarks specifically chosen to be likely outside the\nfine-tuning datasets of biomedical models, we found that biomedical LLMs mostly\nperform inferior to their general-purpose counterparts, especially on tasks not\nfocused on medical knowledge. While larger models showed similar performance on\ncase tasks (e.g., OpenBioLLM-70B: 66.4% vs. Llama-3-70B-Instruct: 65% on JAMA\ncases), smaller biomedical models showed more pronounced underperformance\n(e.g., OpenBioLLM-8B: 30% vs. Llama-3-8B-Instruct: 64.3% on NEJM cases).\nSimilar trends were observed across the CLUE (Clinical Language Understanding\nEvaluation) benchmark tasks, with general-purpose models often performing\nbetter on text generation, question answering, and coding tasks. Our results\nsuggest that fine-tuning LLMs to biomedical data may not provide the expected\nbenefits and may potentially lead to reduced performance, challenging\nprevailing assumptions about domain-specific adaptation of LLMs and\nhighlighting the need for more rigorous evaluation frameworks in healthcare AI.\nAlternative approaches, such as retrieval-augmented generation, may be more\neffective in enhancing the biomedical capabilities of LLMs without compromising\ntheir general knowledge."
                },
                "authors": [
                    {
                        "name": "Felix J. Dorfner"
                    },
                    {
                        "name": "Amin Dada"
                    },
                    {
                        "name": "Felix Busch"
                    },
                    {
                        "name": "Marcus R. Makowski"
                    },
                    {
                        "name": "Tianyu Han"
                    },
                    {
                        "name": "Daniel Truhn"
                    },
                    {
                        "name": "Jens Kleesiek"
                    },
                    {
                        "name": "Madhumita Sushil"
                    },
                    {
                        "name": "Jacqueline Lammert"
                    },
                    {
                        "name": "Lisa C. Adams"
                    },
                    {
                        "name": "Keno K. Bressem"
                    }
                ],
                "author_detail": {
                    "name": "Keno K. Bressem"
                },
                "author": "Keno K. Bressem",
                "arxiv_comment": "10 pages, 3 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13833v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13833v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13546v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13546v2",
                "updated": "2024-08-25T11:23:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    11,
                    23,
                    50,
                    6,
                    238,
                    0
                ],
                "published": "2024-02-21T05:56:52Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    5,
                    56,
                    52,
                    2,
                    52,
                    0
                ],
                "title": "LLMs Meet Long Video: Advancing Long Video Question Answering with An\n  Interactive Visual Adapter in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Meet Long Video: Advancing Long Video Question Answering with An\n  Interactive Visual Adapter in LLMs"
                },
                "summary": "Long video understanding is a significant and ongoing challenge in the\nintersection of multimedia and artificial intelligence. Employing large\nlanguage models (LLMs) for comprehending video becomes an emerging and\npromising method. However, this approach incurs high computational costs due to\nthe extensive array of video tokens, experiences reduced visual clarity as a\nconsequence of token aggregation, and confronts challenges arising from\nirrelevant visual tokens while answering video-related questions. To alleviate\nthese issues, we present an Interactive Visual Adapter (IVA) within LLMs,\ndesigned to enhance interaction with fine-grained visual elements.\nSpecifically, we first transform long videos into temporal video tokens via\nleveraging a visual encoder alongside a pretrained causal transformer, then\nfeed them into LLMs with the video instructions. Subsequently, we integrated\nIVA, which contains a lightweight temporal frame selector and a spatial feature\ninteractor, within the internal blocks of LLMs to capture instruction-aware and\nfine-grained visual signals. Consequently, the proposed video-LLM facilitates a\ncomprehensive understanding of long video content through appropriate long\nvideo modeling and precise visual interactions. We conducted extensive\nexperiments on nine video understanding benchmarks and experimental results\nshow that our interactive visual adapter significantly improves the performance\nof video LLMs on long video QA tasks. Ablation studies further verify the\neffectiveness of IVA in understanding long and short video.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long video understanding is a significant and ongoing challenge in the\nintersection of multimedia and artificial intelligence. Employing large\nlanguage models (LLMs) for comprehending video becomes an emerging and\npromising method. However, this approach incurs high computational costs due to\nthe extensive array of video tokens, experiences reduced visual clarity as a\nconsequence of token aggregation, and confronts challenges arising from\nirrelevant visual tokens while answering video-related questions. To alleviate\nthese issues, we present an Interactive Visual Adapter (IVA) within LLMs,\ndesigned to enhance interaction with fine-grained visual elements.\nSpecifically, we first transform long videos into temporal video tokens via\nleveraging a visual encoder alongside a pretrained causal transformer, then\nfeed them into LLMs with the video instructions. Subsequently, we integrated\nIVA, which contains a lightweight temporal frame selector and a spatial feature\ninteractor, within the internal blocks of LLMs to capture instruction-aware and\nfine-grained visual signals. Consequently, the proposed video-LLM facilitates a\ncomprehensive understanding of long video content through appropriate long\nvideo modeling and precise visual interactions. We conducted extensive\nexperiments on nine video understanding benchmarks and experimental results\nshow that our interactive visual adapter significantly improves the performance\nof video LLMs on long video QA tasks. Ablation studies further verify the\neffectiveness of IVA in understanding long and short video."
                },
                "authors": [
                    {
                        "name": "Yunxin Li"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Baotain Hu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "12 pages; working in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13546v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13546v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05746v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05746v4",
                "updated": "2024-08-25T11:19:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    11,
                    19,
                    33,
                    6,
                    238,
                    0
                ],
                "published": "2023-10-09T14:22:09Z",
                "published_parsed": [
                    2023,
                    10,
                    9,
                    14,
                    22,
                    9,
                    0,
                    282,
                    0
                ],
                "title": "Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and\n  Execution of LLM Agents in an Auction Arena",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and\n  Execution of LLM Agents in an Auction Arena"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) showcase advanced\nreasoning, yet NLP evaluations often depend on static benchmarks. Evaluating\nthis necessitates environments that test strategic reasoning in dynamic,\ncompetitive scenarios requiring long-term planning. We introduce AucArena, a\nnovel evaluation suite that simulates auctions, a setting chosen for being\nhighly unpredictable and involving many skills related to resource and risk\nmanagement, while also being easy to evaluate. We conduct controlled\nexperiments using state-of-the-art LLMs to power bidding agents to benchmark\ntheir planning and execution skills. Our research demonstrates that LLMs, such\nas GPT-4, possess key skills for auction participation, such as budget\nmanagement and goal adherence, which improve with adaptive strategies. This\nhighlights LLMs' potential in modeling complex social interactions in\ncompetitive contexts. However, variability in LLM performance and occasional\noutperformance by simpler methods indicate opportunities for further\nadvancements in LLM design and the value of our simulation environment for\nongoing testing and refinement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) showcase advanced\nreasoning, yet NLP evaluations often depend on static benchmarks. Evaluating\nthis necessitates environments that test strategic reasoning in dynamic,\ncompetitive scenarios requiring long-term planning. We introduce AucArena, a\nnovel evaluation suite that simulates auctions, a setting chosen for being\nhighly unpredictable and involving many skills related to resource and risk\nmanagement, while also being easy to evaluate. We conduct controlled\nexperiments using state-of-the-art LLMs to power bidding agents to benchmark\ntheir planning and execution skills. Our research demonstrates that LLMs, such\nas GPT-4, possess key skills for auction participation, such as budget\nmanagement and goal adherence, which improve with adaptive strategies. This\nhighlights LLMs' potential in modeling complex social interactions in\ncompetitive contexts. However, variability in LLM performance and occasional\noutperformance by simpler methods indicate opportunities for further\nadvancements in LLM design and the value of our simulation environment for\nongoing testing and refinement."
                },
                "authors": [
                    {
                        "name": "Jiangjie Chen"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Rong Ye"
                    },
                    {
                        "name": "Bodhisattwa Prasad Majumder"
                    },
                    {
                        "name": "Kyle Richardson"
                    }
                ],
                "author_detail": {
                    "name": "Kyle Richardson"
                },
                "author": "Kyle Richardson",
                "arxiv_comment": "Project page: https://auction-arena.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.05746v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05746v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13808v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13808v1",
                "updated": "2024-08-25T11:09:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    11,
                    9,
                    15,
                    6,
                    238,
                    0
                ],
                "published": "2024-08-25T11:09:15Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    11,
                    9,
                    15,
                    6,
                    238,
                    0
                ],
                "title": "Towards Reliable Medical Question Answering: Techniques and Challenges\n  in Mitigating Hallucinations in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reliable Medical Question Answering: Techniques and Challenges\n  in Mitigating Hallucinations in Language Models"
                },
                "summary": "The rapid advancement of large language models (LLMs) has significantly\nimpacted various domains, including healthcare and biomedicine. However, the\nphenomenon of hallucination, where LLMs generate outputs that deviate from\nfactual accuracy or context, poses a critical challenge, especially in\nhigh-stakes domains. This paper conducts a scoping study of existing techniques\nfor mitigating hallucinations in knowledge-based task in general and especially\nfor medical domains. Key methods covered in the paper include\nRetrieval-Augmented Generation (RAG)-based techniques, iterative feedback\nloops, supervised fine-tuning, and prompt engineering. These techniques, while\npromising in general contexts, require further adaptation and optimization for\nthe medical domain due to its unique demands for up-to-date, specialized\nknowledge and strict adherence to medical guidelines. Addressing these\nchallenges is crucial for developing trustworthy AI systems that enhance\nclinical decision-making and patient safety as well as accuracy of biomedical\nscientific research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has significantly\nimpacted various domains, including healthcare and biomedicine. However, the\nphenomenon of hallucination, where LLMs generate outputs that deviate from\nfactual accuracy or context, poses a critical challenge, especially in\nhigh-stakes domains. This paper conducts a scoping study of existing techniques\nfor mitigating hallucinations in knowledge-based task in general and especially\nfor medical domains. Key methods covered in the paper include\nRetrieval-Augmented Generation (RAG)-based techniques, iterative feedback\nloops, supervised fine-tuning, and prompt engineering. These techniques, while\npromising in general contexts, require further adaptation and optimization for\nthe medical domain due to its unique demands for up-to-date, specialized\nknowledge and strict adherence to medical guidelines. Addressing these\nchallenges is crucial for developing trustworthy AI systems that enhance\nclinical decision-making and patient safety as well as accuracy of biomedical\nscientific research."
                },
                "authors": [
                    {
                        "name": "Duy Khoa Pham"
                    },
                    {
                        "name": "Bao Quoc Vo"
                    }
                ],
                "author_detail": {
                    "name": "Bao Quoc Vo"
                },
                "author": "Bao Quoc Vo",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13808v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13808v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.18743v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.18743v4",
                "updated": "2024-08-25T09:58:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    9,
                    58,
                    57,
                    6,
                    238,
                    0
                ],
                "published": "2023-11-30T17:41:30Z",
                "published_parsed": [
                    2023,
                    11,
                    30,
                    17,
                    41,
                    30,
                    3,
                    334,
                    0
                ],
                "title": "AlignBench: Benchmarking Chinese Alignment of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignBench: Benchmarking Chinese Alignment of Large Language Models"
                },
                "summary": "Alignment has become a critical step for instruction-tuned Large Language\nModels (LLMs) to become helpful assistants. However, the effective evaluation\nof alignment for emerging Chinese LLMs is still largely unexplored. To fill in\nthis gap, we introduce AlignBench, a comprehensive multi-dimensional benchmark\nfor evaluating LLMs' alignment in Chinese. We design a human-in-the-loop data\ncuration pipeline, containing eight main categories, 683 real-scenario rooted\nqueries and corresponding human verified references. To ensure the correctness\nof references, each knowledge-intensive query is accompanied with evidences\ncollected from reliable web sources (including URLs and quotations) by our\nannotators. For automatic evaluation, our benchmark employs a rule-calibrated\nmulti-dimensional LLM-as-Judge~\\cite{zheng2023judging} approach with\nChain-of-Thought to generate explanations and final ratings, ensuring high\nreliability and interpretability. All evaluation code, data, and LLM\ngenerations are available at \\url{https://github.com/THUDM/AlignBench}. Since\nits release, AlignBench has been adopted by top (Chinese) LLMs for evaluating\ntheir alignment capabilities in Chinese, including ChatGLM, Qwen, DeepSeek, Yi,\nBaichuan, and Abab.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment has become a critical step for instruction-tuned Large Language\nModels (LLMs) to become helpful assistants. However, the effective evaluation\nof alignment for emerging Chinese LLMs is still largely unexplored. To fill in\nthis gap, we introduce AlignBench, a comprehensive multi-dimensional benchmark\nfor evaluating LLMs' alignment in Chinese. We design a human-in-the-loop data\ncuration pipeline, containing eight main categories, 683 real-scenario rooted\nqueries and corresponding human verified references. To ensure the correctness\nof references, each knowledge-intensive query is accompanied with evidences\ncollected from reliable web sources (including URLs and quotations) by our\nannotators. For automatic evaluation, our benchmark employs a rule-calibrated\nmulti-dimensional LLM-as-Judge~\\cite{zheng2023judging} approach with\nChain-of-Thought to generate explanations and final ratings, ensuring high\nreliability and interpretability. All evaluation code, data, and LLM\ngenerations are available at \\url{https://github.com/THUDM/AlignBench}. Since\nits release, AlignBench has been adopted by top (Chinese) LLMs for evaluating\ntheir alignment capabilities in Chinese, including ChatGLM, Qwen, DeepSeek, Yi,\nBaichuan, and Abab."
                },
                "authors": [
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Xuanyu Lei"
                    },
                    {
                        "name": "Shengyuan Wang"
                    },
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Zhuoer Feng"
                    },
                    {
                        "name": "Bosi Wen"
                    },
                    {
                        "name": "Jiale Cheng"
                    },
                    {
                        "name": "Pei Ke"
                    },
                    {
                        "name": "Yifan Xu"
                    },
                    {
                        "name": "Weng Lam Tam"
                    },
                    {
                        "name": "Xiaohan Zhang"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Xiaotao Gu"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Minlie Huang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "arxiv_comment": "Accepted to ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.18743v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.18743v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13781v1",
                "updated": "2024-08-25T09:22:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    9,
                    22,
                    7,
                    6,
                    238,
                    0
                ],
                "published": "2024-08-25T09:22:07Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    9,
                    22,
                    7,
                    6,
                    238,
                    0
                ],
                "title": "Demo: Generative Open xG Network Simulation with Multi-Agent LLM and\n  ns-3 (GenOnet)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demo: Generative Open xG Network Simulation with Multi-Agent LLM and\n  ns-3 (GenOnet)"
                },
                "summary": "The move toward Sixth-Generation (6G) networks relies on open interfaces and\nprotocols for seamless interoperability across devices, vendors, and\ntechnologies. In this context, open 6G development involves multiple\ndisciplines and requires advanced simulation approaches for testing. In this\ndemo paper, we propose a generative simulation approach based on a multi-agent\nLarge Language Model (LLM) and Network Simulator 3 (ns-3), called Generative\nOpen xG Network Simulation (GenOnet), to effectively generate, debug, execute,\nand interpret simulated Open Fifth-Generation (5G) environments. The first\nversion of GenOnet application represents a specialized adaptation of the\nOpenAI GPT models. It incorporates supplementary tools, agents, 5G standards,\nand seamless integration with ns-3 simulation capabilities, supporting both C++\nvariants and Python implementations. This release complies with the latest Open\nRadio Access Network (O-RAN) and 3GPP standards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The move toward Sixth-Generation (6G) networks relies on open interfaces and\nprotocols for seamless interoperability across devices, vendors, and\ntechnologies. In this context, open 6G development involves multiple\ndisciplines and requires advanced simulation approaches for testing. In this\ndemo paper, we propose a generative simulation approach based on a multi-agent\nLarge Language Model (LLM) and Network Simulator 3 (ns-3), called Generative\nOpen xG Network Simulation (GenOnet), to effectively generate, debug, execute,\nand interpret simulated Open Fifth-Generation (5G) environments. The first\nversion of GenOnet application represents a specialized adaptation of the\nOpenAI GPT models. It incorporates supplementary tools, agents, 5G standards,\nand seamless integration with ns-3 simulation capabilities, supporting both C++\nvariants and Python implementations. This release complies with the latest Open\nRadio Access Network (O-RAN) and 3GPP standards."
                },
                "authors": [
                    {
                        "name": "Farhad Rezazadeh"
                    },
                    {
                        "name": "Amir Ashtari Gargari"
                    },
                    {
                        "name": "Sandra Lagén"
                    },
                    {
                        "name": "Josep Mangues"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Lingjia Liu"
                    }
                ],
                "author_detail": {
                    "name": "Lingjia Liu"
                },
                "author": "Lingjia Liu",
                "arxiv_comment": "3 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.01639v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.01639v4",
                "updated": "2024-08-25T08:42:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    8,
                    42,
                    39,
                    6,
                    238,
                    0
                ],
                "published": "2023-12-04T05:41:02Z",
                "published_parsed": [
                    2023,
                    12,
                    4,
                    5,
                    41,
                    2,
                    0,
                    338,
                    0
                ],
                "title": "On the Effectiveness of Large Language Models in Domain-Specific Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Effectiveness of Large Language Models in Domain-Specific Code\n  Generation"
                },
                "summary": "Large language models (LLMs) such as ChatGPT have shown remarkable\ncapabilities in code generation. Despite significant achievements, they rely on\nenormous training data to acquire a broad spectrum of open-domain knowledge.\nBesides, their evaluation revolves around open-domain benchmarks like\nHumanEval, which primarily consist of programming contests. Therefore, it is\nhard to fully characterize the intricacies and challenges associated with\nparticular domains (e.g., web, game, and math). In this paper, we conduct an\nin-depth study of the LLMs in domain-specific code generation. Our results\ndemonstrate that LLMs exhibit sub-optimal performance in generating\ndomain-specific code, due to their limited proficiency in utilizing\ndomain-specific libraries. We further observe that incorporating API knowledge\nas prompts can empower LLMs to generate more professional code. Based on these\nfindings, we further investigate how to effectively incorporate API knowledge\ninto the code generation process. We experiment with three strategies for\nincorporating domain knowledge, namely, external knowledge inquirer,\nchain-of-thought prompting, and chain-of-thought fine-tuning. We refer to these\nstrategies as a new code generation approach called DomCoder. Experimental\nresults show that all strategies of DomCoder lead to improvement in the\neffectiveness of domain-specific code generation under certain settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) such as ChatGPT have shown remarkable\ncapabilities in code generation. Despite significant achievements, they rely on\nenormous training data to acquire a broad spectrum of open-domain knowledge.\nBesides, their evaluation revolves around open-domain benchmarks like\nHumanEval, which primarily consist of programming contests. Therefore, it is\nhard to fully characterize the intricacies and challenges associated with\nparticular domains (e.g., web, game, and math). In this paper, we conduct an\nin-depth study of the LLMs in domain-specific code generation. Our results\ndemonstrate that LLMs exhibit sub-optimal performance in generating\ndomain-specific code, due to their limited proficiency in utilizing\ndomain-specific libraries. We further observe that incorporating API knowledge\nas prompts can empower LLMs to generate more professional code. Based on these\nfindings, we further investigate how to effectively incorporate API knowledge\ninto the code generation process. We experiment with three strategies for\nincorporating domain knowledge, namely, external knowledge inquirer,\nchain-of-thought prompting, and chain-of-thought fine-tuning. We refer to these\nstrategies as a new code generation approach called DomCoder. Experimental\nresults show that all strategies of DomCoder lead to improvement in the\neffectiveness of domain-specific code generation under certain settings."
                },
                "authors": [
                    {
                        "name": "Yalan Lin"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Yuhan Hu"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Chengcheng Wan"
                    },
                    {
                        "name": "Zhao Wei"
                    },
                    {
                        "name": "Yong Xu"
                    },
                    {
                        "name": "Juhong Wang"
                    },
                    {
                        "name": "Xiaodong Gu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodong Gu"
                },
                "author": "Xiaodong Gu",
                "arxiv_comment": "Accepted by the ACM Transactions on Software Engineering and\n  Methodology (TOSEM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.01639v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.01639v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13745v1",
                "updated": "2024-08-25T07:10:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    7,
                    10,
                    36,
                    6,
                    238,
                    0
                ],
                "published": "2024-08-25T07:10:36Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    7,
                    10,
                    36,
                    6,
                    238,
                    0
                ],
                "title": "DOCE: Finding the Sweet Spot for Execution-Based Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOCE: Finding the Sweet Spot for Execution-Based Code Generation"
                },
                "summary": "Recently, a diverse set of decoding and reranking procedures have been shown\neffective for LLM-based code generation. However, a comprehensive framework\nthat links and experimentally compares these methods is missing. We address\nthis by proposing Decoding Objectives for Code Execution, a comprehensive\nframework that includes candidate generation, $n$-best reranking, minimum Bayes\nrisk (MBR) decoding, and self-debugging as the core components. We then study\nthe contributions of these components through execution-based evaluation\nmetrics. Our findings highlight the importance of execution-based methods and\nthe difference gap between execution-based and execution-free methods.\nFurthermore, we assess the impact of filtering based on trial unit tests, a\nsimple and effective strategy that has been often overlooked in prior works. We\nalso propose self-debugging on multiple candidates, obtaining state-of-the-art\nperformance on reranking for code generation. We expect our framework to\nprovide a solid guideline for future research on code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, a diverse set of decoding and reranking procedures have been shown\neffective for LLM-based code generation. However, a comprehensive framework\nthat links and experimentally compares these methods is missing. We address\nthis by proposing Decoding Objectives for Code Execution, a comprehensive\nframework that includes candidate generation, $n$-best reranking, minimum Bayes\nrisk (MBR) decoding, and self-debugging as the core components. We then study\nthe contributions of these components through execution-based evaluation\nmetrics. Our findings highlight the importance of execution-based methods and\nthe difference gap between execution-based and execution-free methods.\nFurthermore, we assess the impact of filtering based on trial unit tests, a\nsimple and effective strategy that has been often overlooked in prior works. We\nalso propose self-debugging on multiple candidates, obtaining state-of-the-art\nperformance on reranking for code generation. We expect our framework to\nprovide a solid guideline for future research on code generation."
                },
                "authors": [
                    {
                        "name": "Haau-Sing Li"
                    },
                    {
                        "name": "Patrick Fernandes"
                    },
                    {
                        "name": "Iryna Gurevych"
                    },
                    {
                        "name": "André F. T. Martins"
                    }
                ],
                "author_detail": {
                    "name": "André F. T. Martins"
                },
                "author": "André F. T. Martins",
                "arxiv_comment": "10 pages (32 including appendix), 5 figures, 25 tables. arXiv admin\n  note: text overlap with arXiv:2304.05128 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13740v2",
                "updated": "2024-08-27T08:05:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    8,
                    5,
                    45,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-25T07:01:37Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    7,
                    1,
                    37,
                    6,
                    238,
                    0
                ],
                "title": "PIE: Parkour with Implicit-Explicit Learning Framework for Legged Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIE: Parkour with Implicit-Explicit Learning Framework for Legged Robots"
                },
                "summary": "Parkour presents a highly challenging task for legged robots, requiring them\nto traverse various terrains with agile and smooth locomotion. This\nnecessitates comprehensive understanding of both the robot's own state and the\nsurrounding terrain, despite the inherent unreliability of robot perception and\nactuation. Current state-of-the-art methods either rely on complex pre-trained\nhigh-level terrain reconstruction modules or limit the maximum potential of\nrobot parkour to avoid failure due to inaccurate perception. In this paper, we\npropose a one-stage end-to-end learning-based parkour framework: Parkour with\nImplicit-Explicit learning framework for legged robots (PIE) that leverages\ndual-level implicit-explicit estimation. With this mechanism, even a low-cost\nquadruped robot equipped with an unreliable egocentric depth camera can achieve\nexceptional performance on challenging parkour terrains using a relatively\nsimple training process and reward function. While the training process is\nconducted entirely in simulation, our real-world validation demonstrates\nsuccessful zero-shot deployment of our framework, showcasing superior parkour\nperformance on harsh terrains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parkour presents a highly challenging task for legged robots, requiring them\nto traverse various terrains with agile and smooth locomotion. This\nnecessitates comprehensive understanding of both the robot's own state and the\nsurrounding terrain, despite the inherent unreliability of robot perception and\nactuation. Current state-of-the-art methods either rely on complex pre-trained\nhigh-level terrain reconstruction modules or limit the maximum potential of\nrobot parkour to avoid failure due to inaccurate perception. In this paper, we\npropose a one-stage end-to-end learning-based parkour framework: Parkour with\nImplicit-Explicit learning framework for legged robots (PIE) that leverages\ndual-level implicit-explicit estimation. With this mechanism, even a low-cost\nquadruped robot equipped with an unreliable egocentric depth camera can achieve\nexceptional performance on challenging parkour terrains using a relatively\nsimple training process and reward function. While the training process is\nconducted entirely in simulation, our real-world validation demonstrates\nsuccessful zero-shot deployment of our framework, showcasing superior parkour\nperformance on harsh terrains."
                },
                "authors": [
                    {
                        "name": "Shixin Luo"
                    },
                    {
                        "name": "Songbo Li"
                    },
                    {
                        "name": "Ruiqi Yu"
                    },
                    {
                        "name": "Zhicheng Wang"
                    },
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Qiuguo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Qiuguo Zhu"
                },
                "author": "Qiuguo Zhu",
                "arxiv_comment": "Accepted for IEEE Robotics and Automation Letters (RA-L)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08448v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08448v3",
                "updated": "2024-08-27T09:04:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    4,
                    35,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-15T22:57:39Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    22,
                    57,
                    39,
                    3,
                    228,
                    0
                ],
                "title": "Exploring Cross-model Neuronal Correlations in the Context of Predicting\n  Model Performance and Generalizability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Cross-model Neuronal Correlations in the Context of Predicting\n  Model Performance and Generalizability"
                },
                "summary": "As Artificial Intelligence (AI) models are increasingly integrated into\ncritical systems, the need for a robust framework to establish the\ntrustworthiness of AI is increasingly paramount. While collaborative efforts\nhave established conceptual foundations for such a framework, there remains a\nsignificant gap in developing concrete, technically robust methods for\nassessing AI model quality and performance. A critical drawback in the\ntraditional methods for assessing the validity and generalizability of models\nis their dependence on internal developer datasets, rendering it challenging to\nindependently assess and verify their performance claims. This paper introduces\na novel approach for assessing a newly trained model's performance based on\nanother known model by calculating correlation between neural networks. The\nproposed method evaluates correlations by determining if, for each neuron in\none network, there exists a neuron in the other network that produces similar\noutput. This approach has implications for memory efficiency, allowing for the\nuse of smaller networks when high correlation exists between networks of\ndifferent sizes. Additionally, the method provides insights into robustness,\nsuggesting that if two highly correlated networks are compared and one\ndemonstrates robustness when operating in production environments, the other is\nlikely to exhibit similar robustness. This contribution advances the technical\ntoolkit for responsible AI, supporting more comprehensive and nuanced\nevaluations of AI models to ensure their safe and effective deployment. Code is\navailable at https://github.com/aheldis/Cross-model-correlation.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Artificial Intelligence (AI) models are increasingly integrated into\ncritical systems, the need for a robust framework to establish the\ntrustworthiness of AI is increasingly paramount. While collaborative efforts\nhave established conceptual foundations for such a framework, there remains a\nsignificant gap in developing concrete, technically robust methods for\nassessing AI model quality and performance. A critical drawback in the\ntraditional methods for assessing the validity and generalizability of models\nis their dependence on internal developer datasets, rendering it challenging to\nindependently assess and verify their performance claims. This paper introduces\na novel approach for assessing a newly trained model's performance based on\nanother known model by calculating correlation between neural networks. The\nproposed method evaluates correlations by determining if, for each neuron in\none network, there exists a neuron in the other network that produces similar\noutput. This approach has implications for memory efficiency, allowing for the\nuse of smaller networks when high correlation exists between networks of\ndifferent sizes. Additionally, the method provides insights into robustness,\nsuggesting that if two highly correlated networks are compared and one\ndemonstrates robustness when operating in production environments, the other is\nlikely to exhibit similar robustness. This contribution advances the technical\ntoolkit for responsible AI, supporting more comprehensive and nuanced\nevaluations of AI models to ensure their safe and effective deployment. Code is\navailable at https://github.com/aheldis/Cross-model-correlation.git."
                },
                "authors": [
                    {
                        "name": "Haniyeh Ehsani Oskouie"
                    },
                    {
                        "name": "Lionel Levine"
                    },
                    {
                        "name": "Majid Sarrafzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Majid Sarrafzadeh"
                },
                "author": "Majid Sarrafzadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08448v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08448v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13738v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13738v1",
                "updated": "2024-08-25T06:49:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    6,
                    49,
                    3,
                    6,
                    238,
                    0
                ],
                "published": "2024-08-25T06:49:03Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    6,
                    49,
                    3,
                    6,
                    238,
                    0
                ],
                "title": "Poor-Supervised Evaluation for SuperLLM via Mutual Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Poor-Supervised Evaluation for SuperLLM via Mutual Consistency"
                },
                "summary": "The guidance from capability evaluations has greatly propelled the progress\nof both human society and Artificial Intelligence. However, as LLMs evolve, it\nbecomes challenging to construct evaluation benchmarks for them with accurate\nlabels on hard tasks that approach the boundaries of human capabilities. To\ncredibly conduct evaluation without accurate labels (denoted as poor-supervised\nevaluation), we propose the PoEM framework. We first prove that the capability\nof a model can be equivalently assessed by the consistency between it and\ncertain reference model, when their prediction distributions are independent\nand the sample size is infinite. To alleviate the insufficiencies of the\nconditions in reality, we further introduce an algorithm that treats humans\n(when available) and the models under evaluation as reference models,\nalternately conducting model weights calibration and filtering during E-step\nand M-step. Comprehensive experiments across 3 types of tasks with 16\nmainstream LLMs have shown that PoEM under poor supervision can achieve an\naverage of 0.98 Pearson correlation coefficient with supervised evaluation\nresults, demonstrating good effectiveness, efficiency and generalizability.\nMore generally, PoEM has advanced the evaluation paradigm evolution from\nhuman-centric to human&model-centric by treating both of them as reference\nmodels, mitigating the limitations of human evaluation in the era of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The guidance from capability evaluations has greatly propelled the progress\nof both human society and Artificial Intelligence. However, as LLMs evolve, it\nbecomes challenging to construct evaluation benchmarks for them with accurate\nlabels on hard tasks that approach the boundaries of human capabilities. To\ncredibly conduct evaluation without accurate labels (denoted as poor-supervised\nevaluation), we propose the PoEM framework. We first prove that the capability\nof a model can be equivalently assessed by the consistency between it and\ncertain reference model, when their prediction distributions are independent\nand the sample size is infinite. To alleviate the insufficiencies of the\nconditions in reality, we further introduce an algorithm that treats humans\n(when available) and the models under evaluation as reference models,\nalternately conducting model weights calibration and filtering during E-step\nand M-step. Comprehensive experiments across 3 types of tasks with 16\nmainstream LLMs have shown that PoEM under poor supervision can achieve an\naverage of 0.98 Pearson correlation coefficient with supervised evaluation\nresults, demonstrating good effectiveness, efficiency and generalizability.\nMore generally, PoEM has advanced the evaluation paradigm evolution from\nhuman-centric to human&model-centric by treating both of them as reference\nmodels, mitigating the limitations of human evaluation in the era of LLMs."
                },
                "authors": [
                    {
                        "name": "Peiwen Yuan"
                    },
                    {
                        "name": "Shaoxiong Feng"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Xinglin Wang"
                    },
                    {
                        "name": "Boyuan Pan"
                    },
                    {
                        "name": "Heda Wang"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kan Li"
                    }
                ],
                "author_detail": {
                    "name": "Kan Li"
                },
                "author": "Kan Li",
                "arxiv_comment": "ACL findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13738v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13738v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13727v1",
                "updated": "2024-08-25T05:34:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    5,
                    34,
                    24,
                    6,
                    238,
                    0
                ],
                "published": "2024-08-25T05:34:24Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    5,
                    34,
                    24,
                    6,
                    238,
                    0
                ],
                "title": "LogParser-LLM: Advancing Efficient Log Parsing with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogParser-LLM: Advancing Efficient Log Parsing with Large Language\n  Models"
                },
                "summary": "Logs are ubiquitous digital footprints, playing an indispensable role in\nsystem diagnostics, security analysis, and performance optimization. The\nextraction of actionable insights from logs is critically dependent on the log\nparsing process, which converts raw logs into structured formats for downstream\nanalysis. Yet, the complexities of contemporary systems and the dynamic nature\nof logs pose significant challenges to existing automatic parsing techniques.\nThe emergence of Large Language Models (LLM) offers new horizons. With their\nexpansive knowledge and contextual prowess, LLMs have been transformative\nacross diverse applications. Building on this, we introduce LogParser-LLM, a\nnovel log parser integrated with LLM capabilities. This union seamlessly blends\nsemantic insights with statistical nuances, obviating the need for\nhyper-parameter tuning and labeled training data, while ensuring rapid\nadaptability through online parsing. Further deepening our exploration, we\naddress the intricate challenge of parsing granularity, proposing a new metric\nand integrating human interactions to allow users to calibrate granularity to\ntheir specific needs. Our method's efficacy is empirically demonstrated through\nevaluations on the Loghub-2k and the large-scale LogPub benchmark. In\nevaluations on the LogPub benchmark, involving an average of 3.6 million logs\nper dataset across 14 datasets, our LogParser-LLM requires only 272.5 LLM\ninvocations on average, achieving a 90.6% F1 score for grouping accuracy and an\n81.1% for parsing accuracy. These results demonstrate the method's high\nefficiency and accuracy, outperforming current state-of-the-art log parsers,\nincluding pattern-based, neural network-based, and existing LLM-enhanced\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logs are ubiquitous digital footprints, playing an indispensable role in\nsystem diagnostics, security analysis, and performance optimization. The\nextraction of actionable insights from logs is critically dependent on the log\nparsing process, which converts raw logs into structured formats for downstream\nanalysis. Yet, the complexities of contemporary systems and the dynamic nature\nof logs pose significant challenges to existing automatic parsing techniques.\nThe emergence of Large Language Models (LLM) offers new horizons. With their\nexpansive knowledge and contextual prowess, LLMs have been transformative\nacross diverse applications. Building on this, we introduce LogParser-LLM, a\nnovel log parser integrated with LLM capabilities. This union seamlessly blends\nsemantic insights with statistical nuances, obviating the need for\nhyper-parameter tuning and labeled training data, while ensuring rapid\nadaptability through online parsing. Further deepening our exploration, we\naddress the intricate challenge of parsing granularity, proposing a new metric\nand integrating human interactions to allow users to calibrate granularity to\ntheir specific needs. Our method's efficacy is empirically demonstrated through\nevaluations on the Loghub-2k and the large-scale LogPub benchmark. In\nevaluations on the LogPub benchmark, involving an average of 3.6 million logs\nper dataset across 14 datasets, our LogParser-LLM requires only 272.5 LLM\ninvocations on average, achieving a 90.6% F1 score for grouping accuracy and an\n81.1% for parsing accuracy. These results demonstrate the method's high\nefficiency and accuracy, outperforming current state-of-the-art log parsers,\nincluding pattern-based, neural network-based, and existing LLM-enhanced\napproaches."
                },
                "authors": [
                    {
                        "name": "Aoxiao Zhong"
                    },
                    {
                        "name": "Dengyao Mo"
                    },
                    {
                        "name": "Guiyang Liu"
                    },
                    {
                        "name": "Jinbu Liu"
                    },
                    {
                        "name": "Qingda Lu"
                    },
                    {
                        "name": "Qi Zhou"
                    },
                    {
                        "name": "Jiesheng Wu"
                    },
                    {
                        "name": "Quanzheng Li"
                    },
                    {
                        "name": "Qingsong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Qingsong Wen"
                },
                "author": "Qingsong Wen",
                "arxiv_comment": "Accepted by ACM KDD 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03679v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03679v4",
                "updated": "2024-08-25T03:53:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    3,
                    53,
                    10,
                    6,
                    238,
                    0
                ],
                "published": "2024-06-06T01:49:29Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    1,
                    49,
                    29,
                    3,
                    158,
                    0
                ],
                "title": "On the Effects of Data Scale on Computer Control Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Effects of Data Scale on Computer Control Agents"
                },
                "summary": "Autonomous agents that control computer interfaces to accomplish human tasks\nare emerging. Leveraging LLMs to power such agents has been of special\ninterest, but unless fine-tuned on human-collected task demonstrations,\nperformance is still relatively low. In this work we study whether fine-tuning\nalone is a viable approach for building real-world computer control agents. In\nparticularly, we investigate how performance measured on both high and\nlow-level tasks in domain and out of domain scales as more training data is\ncollected. To this end we collect and release a new dataset, AndroidControl,\nconsisting of 15,283 demonstrations of everyday tasks with Android apps.\nCompared to existing datasets, each AndroidControl task instance includes both\nhigh and low-level human-generated instructions, allowing us to explore the\nlevel of task complexity an agent can handle. Moreover, AndroidControl is the\nmost diverse computer control dataset to date, including 15,283 unique tasks\nover 833 Android apps, thus allowing us to conduct in-depth analysis of the\nmodel performance in and out of the domain of the training data. Using the\ndataset, we find that when tested in domain fine-tuned models outperform zero\nand few-shot baselines and scale in such a way that robust performance might\nfeasibly be obtained simply by collecting more data. Out of domain, performance\nscales significantly more slowly and suggests that in particular for high-level\ntasks, fine-tuning on more data alone may be insufficient for achieving robust\nout-of-domain performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous agents that control computer interfaces to accomplish human tasks\nare emerging. Leveraging LLMs to power such agents has been of special\ninterest, but unless fine-tuned on human-collected task demonstrations,\nperformance is still relatively low. In this work we study whether fine-tuning\nalone is a viable approach for building real-world computer control agents. In\nparticularly, we investigate how performance measured on both high and\nlow-level tasks in domain and out of domain scales as more training data is\ncollected. To this end we collect and release a new dataset, AndroidControl,\nconsisting of 15,283 demonstrations of everyday tasks with Android apps.\nCompared to existing datasets, each AndroidControl task instance includes both\nhigh and low-level human-generated instructions, allowing us to explore the\nlevel of task complexity an agent can handle. Moreover, AndroidControl is the\nmost diverse computer control dataset to date, including 15,283 unique tasks\nover 833 Android apps, thus allowing us to conduct in-depth analysis of the\nmodel performance in and out of the domain of the training data. Using the\ndataset, we find that when tested in domain fine-tuned models outperform zero\nand few-shot baselines and scale in such a way that robust performance might\nfeasibly be obtained simply by collecting more data. Out of domain, performance\nscales significantly more slowly and suggests that in particular for high-level\ntasks, fine-tuning on more data alone may be insufficient for achieving robust\nout-of-domain performance."
                },
                "authors": [
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "William Bishop"
                    },
                    {
                        "name": "Alice Li"
                    },
                    {
                        "name": "Chris Rawles"
                    },
                    {
                        "name": "Folawiyo Campbell-Ajala"
                    },
                    {
                        "name": "Divya Tyamagundlu"
                    },
                    {
                        "name": "Oriana Riva"
                    }
                ],
                "author_detail": {
                    "name": "Oriana Riva"
                },
                "author": "Oriana Riva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03679v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03679v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15295v2",
                "updated": "2024-08-25T03:25:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    3,
                    25,
                    16,
                    6,
                    238,
                    0
                ],
                "published": "2024-01-27T04:49:37Z",
                "published_parsed": [
                    2024,
                    1,
                    27,
                    4,
                    49,
                    37,
                    5,
                    27,
                    0
                ],
                "title": "Shortcuts Everywhere and Nowhere: Exploring Multi-Trigger Backdoor\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shortcuts Everywhere and Nowhere: Exploring Multi-Trigger Backdoor\n  Attacks"
                },
                "summary": "Backdoor attacks have become a significant threat to the pre-training and\ndeployment of deep neural networks (DNNs). Although numerous methods for\ndetecting and mitigating backdoor attacks have been proposed, most rely on\nidentifying and eliminating the ``shortcut\" created by the backdoor, which\nlinks a specific source class to a target class. However, these approaches can\nbe easily circumvented by designing multiple backdoor triggers that create\nshortcuts everywhere and therefore nowhere specific. In this study, we explore\nthe concept of Multi-Trigger Backdoor Attacks (MTBAs), where multiple\nadversaries leverage different types of triggers to poison the same dataset. By\nproposing and investigating three types of multi-trigger attacks including\n\\textit{parallel}, \\textit{sequential}, and \\textit{hybrid} attacks, we\ndemonstrate that 1) multiple triggers can coexist, overwrite, or cross-activate\none another, and 2) MTBAs easily break the prevalent shortcut assumption\nunderlying most existing backdoor detection/removal methods, rendering them\nineffective. Given the security risk posed by MTBAs, we have created a\nmulti-trigger backdoor poisoning dataset to facilitate future research on\ndetecting and mitigating these attacks, and we also discuss potential defense\nstrategies against MTBAs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backdoor attacks have become a significant threat to the pre-training and\ndeployment of deep neural networks (DNNs). Although numerous methods for\ndetecting and mitigating backdoor attacks have been proposed, most rely on\nidentifying and eliminating the ``shortcut\" created by the backdoor, which\nlinks a specific source class to a target class. However, these approaches can\nbe easily circumvented by designing multiple backdoor triggers that create\nshortcuts everywhere and therefore nowhere specific. In this study, we explore\nthe concept of Multi-Trigger Backdoor Attacks (MTBAs), where multiple\nadversaries leverage different types of triggers to poison the same dataset. By\nproposing and investigating three types of multi-trigger attacks including\n\\textit{parallel}, \\textit{sequential}, and \\textit{hybrid} attacks, we\ndemonstrate that 1) multiple triggers can coexist, overwrite, or cross-activate\none another, and 2) MTBAs easily break the prevalent shortcut assumption\nunderlying most existing backdoor detection/removal methods, rendering them\nineffective. Given the security risk posed by MTBAs, we have created a\nmulti-trigger backdoor poisoning dataset to facilitate future research on\ndetecting and mitigating these attacks, and we also discuss potential defense\nstrategies against MTBAs."
                },
                "authors": [
                    {
                        "name": "Yige Li"
                    },
                    {
                        "name": "Jiabo He"
                    },
                    {
                        "name": "Hanxun Huang"
                    },
                    {
                        "name": "Jun Sun"
                    },
                    {
                        "name": "Xingjun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xingjun Ma"
                },
                "author": "Xingjun Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13704v1",
                "updated": "2024-08-25T02:01:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    2,
                    1,
                    38,
                    6,
                    238,
                    0
                ],
                "published": "2024-08-25T02:01:38Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    2,
                    1,
                    38,
                    6,
                    238,
                    0
                ],
                "title": "DHP Benchmark: Are LLMs Good NLG Evaluators?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DHP Benchmark: Are LLMs Good NLG Evaluators?"
                },
                "summary": "Large Language Models (LLMs) are increasingly serving as evaluators in\nNatural Language Generation (NLG) tasks. However, the capabilities of LLMs in\nscoring NLG quality remain inadequately explored. Current studies depend on\nhuman assessments and simple metrics that fail to capture the discernment of\nLLMs across diverse NLG tasks. To address this gap, we propose the Discernment\nof Hierarchical Perturbation (DHP) benchmarking framework, which provides\nquantitative discernment scores for LLMs utilizing hierarchically perturbed\ntext data and statistical tests to measure the NLG evaluation capabilities of\nLLMs systematically. We have re-established six evaluation datasets for this\nbenchmark, covering four NLG tasks: Summarization, Story Completion, Question\nAnswering, and Translation. Our comprehensive benchmarking of five major LLM\nseries provides critical insight into their strengths and limitations as NLG\nevaluators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly serving as evaluators in\nNatural Language Generation (NLG) tasks. However, the capabilities of LLMs in\nscoring NLG quality remain inadequately explored. Current studies depend on\nhuman assessments and simple metrics that fail to capture the discernment of\nLLMs across diverse NLG tasks. To address this gap, we propose the Discernment\nof Hierarchical Perturbation (DHP) benchmarking framework, which provides\nquantitative discernment scores for LLMs utilizing hierarchically perturbed\ntext data and statistical tests to measure the NLG evaluation capabilities of\nLLMs systematically. We have re-established six evaluation datasets for this\nbenchmark, covering four NLG tasks: Summarization, Story Completion, Question\nAnswering, and Translation. Our comprehensive benchmarking of five major LLM\nseries provides critical insight into their strengths and limitations as NLG\nevaluators."
                },
                "authors": [
                    {
                        "name": "Yicheng Wang"
                    },
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Yu-Neng Chuang"
                    },
                    {
                        "name": "Zhuoer Wang"
                    },
                    {
                        "name": "Yingchi Liu"
                    },
                    {
                        "name": "Mark Cusick"
                    },
                    {
                        "name": "Param Kulkarni"
                    },
                    {
                        "name": "Zhengping Ji"
                    },
                    {
                        "name": "Yasser Ibrahim"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03827v2",
                "updated": "2024-08-25T01:38:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    1,
                    38,
                    45,
                    6,
                    238,
                    0
                ],
                "published": "2024-06-06T08:03:05Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    8,
                    3,
                    5,
                    3,
                    158,
                    0
                ],
                "title": "Chaos with Keywords: Exposing Large Language Models Sycophantic\n  Hallucination to Misleading Keywords and Evaluating Defense Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chaos with Keywords: Exposing Large Language Models Sycophantic\n  Hallucination to Misleading Keywords and Evaluating Defense Strategies"
                },
                "summary": "This study explores the sycophantic tendencies of Large Language Models\n(LLMs), where these models tend to provide answers that match what users want\nto hear, even if they are not entirely correct. The motivation behind this\nexploration stems from the common behavior observed in individuals searching\nthe internet for facts with partial or misleading knowledge. Similar to using\nweb search engines, users may recall fragments of misleading keywords and\nsubmit them to an LLM, hoping for a comprehensive response. Our empirical\nanalysis of several LLMs shows the potential danger of these models amplifying\nmisinformation when presented with misleading keywords. Additionally, we\nthoroughly assess four existing hallucination mitigation strategies to reduce\nLLMs sycophantic behavior. Our experiments demonstrate the effectiveness of\nthese strategies for generating factually correct statements. Furthermore, our\nanalyses delve into knowledge-probing experiments on factual keywords and\ndifferent categories of sycophancy mitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the sycophantic tendencies of Large Language Models\n(LLMs), where these models tend to provide answers that match what users want\nto hear, even if they are not entirely correct. The motivation behind this\nexploration stems from the common behavior observed in individuals searching\nthe internet for facts with partial or misleading knowledge. Similar to using\nweb search engines, users may recall fragments of misleading keywords and\nsubmit them to an LLM, hoping for a comprehensive response. Our empirical\nanalysis of several LLMs shows the potential danger of these models amplifying\nmisinformation when presented with misleading keywords. Additionally, we\nthoroughly assess four existing hallucination mitigation strategies to reduce\nLLMs sycophantic behavior. Our experiments demonstrate the effectiveness of\nthese strategies for generating factually correct statements. Furthermore, our\nanalyses delve into knowledge-probing experiments on factual keywords and\ndifferent categories of sycophancy mitigation."
                },
                "authors": [
                    {
                        "name": "Aswin RRV"
                    },
                    {
                        "name": "Nemika Tyagi"
                    },
                    {
                        "name": "Md Nayem Uddin"
                    },
                    {
                        "name": "Neeraj Varshney"
                    },
                    {
                        "name": "Chitta Baral"
                    }
                ],
                "author_detail": {
                    "name": "Chitta Baral"
                },
                "author": "Chitta Baral",
                "arxiv_comment": "Findings of ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13696v1",
                "updated": "2024-08-25T01:13:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    25,
                    1,
                    13,
                    0,
                    6,
                    238,
                    0
                ],
                "published": "2024-08-25T01:13:00Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    1,
                    13,
                    0,
                    6,
                    238,
                    0
                ],
                "title": "Revisiting DNN Training for Intermittently Powered Energy Harvesting\n  Micro Computers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting DNN Training for Intermittently Powered Energy Harvesting\n  Micro Computers"
                },
                "summary": "The deployment of Deep Neural Networks in energy-constrained environments,\nsuch as Energy Harvesting Wireless Sensor Networks, presents unique challenges,\nprimarily due to the intermittent nature of power availability. To address\nthese challenges, this study introduces and evaluates a novel training\nmethodology tailored for DNNs operating within such contexts. In particular, we\npropose a dynamic dropout technique that adapts to both the architecture of the\ndevice and the variability in energy availability inherent in energy harvesting\nscenarios. Our proposed approach leverages a device model that incorporates\nspecific parameters of the network architecture and the energy harvesting\nprofile to optimize dropout rates dynamically during the training phase. By\nmodulating the network's training process based on predicted energy\navailability, our method not only conserves energy but also ensures sustained\nlearning and inference capabilities under power constraints. Our preliminary\nresults demonstrate that this strategy provides 6 to 22 percent accuracy\nimprovements compared to the state of the art with less than 5 percent\nadditional compute. This paper details the development of the device model,\ndescribes the integration of energy profiles with intermittency aware dropout\nand quantization algorithms, and presents a comprehensive evaluation of the\nproposed approach using real-world energy harvesting data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Deep Neural Networks in energy-constrained environments,\nsuch as Energy Harvesting Wireless Sensor Networks, presents unique challenges,\nprimarily due to the intermittent nature of power availability. To address\nthese challenges, this study introduces and evaluates a novel training\nmethodology tailored for DNNs operating within such contexts. In particular, we\npropose a dynamic dropout technique that adapts to both the architecture of the\ndevice and the variability in energy availability inherent in energy harvesting\nscenarios. Our proposed approach leverages a device model that incorporates\nspecific parameters of the network architecture and the energy harvesting\nprofile to optimize dropout rates dynamically during the training phase. By\nmodulating the network's training process based on predicted energy\navailability, our method not only conserves energy but also ensures sustained\nlearning and inference capabilities under power constraints. Our preliminary\nresults demonstrate that this strategy provides 6 to 22 percent accuracy\nimprovements compared to the state of the art with less than 5 percent\nadditional compute. This paper details the development of the device model,\ndescribes the integration of energy profiles with intermittency aware dropout\nand quantization algorithms, and presents a comprehensive evaluation of the\nproposed approach using real-world energy harvesting data."
                },
                "authors": [
                    {
                        "name": "Cyan Subhra Mishra"
                    },
                    {
                        "name": "Deeksha Chaudhary"
                    },
                    {
                        "name": "Jack Sampson"
                    },
                    {
                        "name": "Mahmut Taylan Knademir"
                    },
                    {
                        "name": "Chita Das"
                    }
                ],
                "author_detail": {
                    "name": "Chita Das"
                },
                "author": "Chita Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09224v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09224v2",
                "updated": "2024-08-24T20:44:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    24,
                    20,
                    44,
                    30,
                    5,
                    237,
                    0
                ],
                "published": "2024-08-17T15:06:43Z",
                "published_parsed": [
                    2024,
                    8,
                    17,
                    15,
                    6,
                    43,
                    5,
                    230,
                    0
                ],
                "title": "Neuro-Symbolic AI for Military Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuro-Symbolic AI for Military Applications"
                },
                "summary": "Artificial Intelligence (AI) plays a significant role in enhancing the\ncapabilities of defense systems, revolutionizing strategic decision-making, and\nshaping the future landscape of military operations. Neuro-Symbolic AI is an\nemerging approach that leverages and augments the strengths of neural networks\nand symbolic reasoning. These systems have the potential to be more impactful\nand flexible than traditional AI systems, making them well-suited for military\napplications. This paper comprehensively explores the diverse dimensions and\ncapabilities of Neuro-Symbolic AI, aiming to shed light on its potential\napplications in military contexts. We investigate its capacity to improve\ndecision-making, automate complex intelligence analysis, and strengthen\nautonomous systems. We further explore its potential to solve complex tasks in\nvarious domains, in addition to its applications in military contexts. Through\nthis exploration, we address ethical, strategic, and technical considerations\ncrucial to the development and deployment of Neuro-Symbolic AI in military and\ncivilian applications. Contributing to the growing body of research, this study\nrepresents a comprehensive exploration of the extensive possibilities offered\nby Neuro-Symbolic AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) plays a significant role in enhancing the\ncapabilities of defense systems, revolutionizing strategic decision-making, and\nshaping the future landscape of military operations. Neuro-Symbolic AI is an\nemerging approach that leverages and augments the strengths of neural networks\nand symbolic reasoning. These systems have the potential to be more impactful\nand flexible than traditional AI systems, making them well-suited for military\napplications. This paper comprehensively explores the diverse dimensions and\ncapabilities of Neuro-Symbolic AI, aiming to shed light on its potential\napplications in military contexts. We investigate its capacity to improve\ndecision-making, automate complex intelligence analysis, and strengthen\nautonomous systems. We further explore its potential to solve complex tasks in\nvarious domains, in addition to its applications in military contexts. Through\nthis exploration, we address ethical, strategic, and technical considerations\ncrucial to the development and deployment of Neuro-Symbolic AI in military and\ncivilian applications. Contributing to the growing body of research, this study\nrepresents a comprehensive exploration of the extensive possibilities offered\nby Neuro-Symbolic AI."
                },
                "authors": [
                    {
                        "name": "Desta Haileselassie Hagos"
                    },
                    {
                        "name": "Danda B. Rawat"
                    }
                ],
                "author_detail": {
                    "name": "Danda B. Rawat"
                },
                "author": "Danda B. Rawat",
                "arxiv_comment": "Accepted at IEEE Transactions on Artificial Intelligence (TAI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09224v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09224v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09172v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09172v3",
                "updated": "2024-08-24T20:26:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    24,
                    20,
                    26,
                    43,
                    5,
                    237,
                    0
                ],
                "published": "2024-08-17T11:33:23Z",
                "published_parsed": [
                    2024,
                    8,
                    17,
                    11,
                    33,
                    23,
                    5,
                    230,
                    0
                ],
                "title": "Unc-TTP: A Method for Classifying LLM Uncertainty to Improve In-Context\n  Example Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unc-TTP: A Method for Classifying LLM Uncertainty to Improve In-Context\n  Example Selection"
                },
                "summary": "Nowadays, Large Language Models (LLMs) have demonstrated exceptional\nperformance across various downstream tasks. However, it is challenging for\nusers to discern whether the responses are generated with certainty or are\nfabricated to meet user expectations. Estimating the uncertainty of LLMs is\nparticularly challenging due to their vast scale and the lack of white-box\naccess. In this work, we propose a novel Uncertainty Tripartite Testing\nParadigm (Unc-TTP) to classify LLM uncertainty, via evaluating the consistency\nof LLM outputs when incorporating label interference into the sampling-based\napproach. Based on Unc-TTP outputs, we aggregate instances into certain and\nuncertain categories. Further, we conduct a detailed analysis of the\nuncertainty properties of LLMs and show Unc-TTP's superiority over the existing\nsampling-based methods. In addition, we leverage the obtained uncertainty\ninformation to guide in-context example selection, demonstrating that Unc-TTP\nobviously outperforms retrieval-based and sampling-based approaches in\nselecting more informative examples. Our work paves a new way to classify the\nuncertainty of both open- and closed-source LLMs, and introduces a practical\napproach to exploit this uncertainty to improve LLMs performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, Large Language Models (LLMs) have demonstrated exceptional\nperformance across various downstream tasks. However, it is challenging for\nusers to discern whether the responses are generated with certainty or are\nfabricated to meet user expectations. Estimating the uncertainty of LLMs is\nparticularly challenging due to their vast scale and the lack of white-box\naccess. In this work, we propose a novel Uncertainty Tripartite Testing\nParadigm (Unc-TTP) to classify LLM uncertainty, via evaluating the consistency\nof LLM outputs when incorporating label interference into the sampling-based\napproach. Based on Unc-TTP outputs, we aggregate instances into certain and\nuncertain categories. Further, we conduct a detailed analysis of the\nuncertainty properties of LLMs and show Unc-TTP's superiority over the existing\nsampling-based methods. In addition, we leverage the obtained uncertainty\ninformation to guide in-context example selection, demonstrating that Unc-TTP\nobviously outperforms retrieval-based and sampling-based approaches in\nselecting more informative examples. Our work paves a new way to classify the\nuncertainty of both open- and closed-source LLMs, and introduces a practical\napproach to exploit this uncertainty to improve LLMs performance."
                },
                "authors": [
                    {
                        "name": "Hsiu-Yuan Huang"
                    },
                    {
                        "name": "Zichen Wu"
                    },
                    {
                        "name": "Yutong Yang"
                    },
                    {
                        "name": "Junzhao Zhang"
                    },
                    {
                        "name": "Yunfang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yunfang Wu"
                },
                "author": "Yunfang Wu",
                "arxiv_comment": "The model diagram in Figure 1 on page 3 of the paper has significant\n  ambiguities. It may lead readers to mistakenly believe that the experiments\n  were conducted in a multi-turn dialogue format. Therefore, we request the\n  withdrawal of this submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09172v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09172v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13661v1",
                "updated": "2024-08-24T19:24:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    24,
                    19,
                    24,
                    44,
                    5,
                    237,
                    0
                ],
                "published": "2024-08-24T19:24:44Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    19,
                    24,
                    44,
                    5,
                    237,
                    0
                ],
                "title": "Hierarchical Network Fusion for Multi-Modal Electron Micrograph\n  Representation Learning with Foundational Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Network Fusion for Multi-Modal Electron Micrograph\n  Representation Learning with Foundational Large Language Models"
                },
                "summary": "Characterizing materials with electron micrographs is a crucial task in\nfields such as semiconductors and quantum materials. The complex hierarchical\nstructure of micrographs often poses challenges for traditional classification\nmethods. In this study, we propose an innovative backbone architecture for\nanalyzing electron micrographs. We create multi-modal representations of the\nmicrographs by tokenizing them into patch sequences and, additionally,\nrepresenting them as vision graphs, commonly referred to as patch attributed\ngraphs. We introduce the Hierarchical Network Fusion (HNF), a multi-layered\nnetwork structure architecture that facilitates information exchange between\nthe multi-modal representations and knowledge integration across different\npatch resolutions. Furthermore, we leverage large language models (LLMs) to\ngenerate detailed technical descriptions of nanomaterials as auxiliary\ninformation to assist in the downstream task. We utilize a cross-modal\nattention mechanism for knowledge fusion across cross-domain\nrepresentations(both image-based and linguistic insights) to predict the\nnanomaterial category. This multi-faceted approach promises a more\ncomprehensive and accurate representation and classification of micrographs for\nnanomaterial identification. Our framework outperforms traditional methods,\novercoming challenges posed by distributional shifts, and facilitating\nhigh-throughput screening.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing materials with electron micrographs is a crucial task in\nfields such as semiconductors and quantum materials. The complex hierarchical\nstructure of micrographs often poses challenges for traditional classification\nmethods. In this study, we propose an innovative backbone architecture for\nanalyzing electron micrographs. We create multi-modal representations of the\nmicrographs by tokenizing them into patch sequences and, additionally,\nrepresenting them as vision graphs, commonly referred to as patch attributed\ngraphs. We introduce the Hierarchical Network Fusion (HNF), a multi-layered\nnetwork structure architecture that facilitates information exchange between\nthe multi-modal representations and knowledge integration across different\npatch resolutions. Furthermore, we leverage large language models (LLMs) to\ngenerate detailed technical descriptions of nanomaterials as auxiliary\ninformation to assist in the downstream task. We utilize a cross-modal\nattention mechanism for knowledge fusion across cross-domain\nrepresentations(both image-based and linguistic insights) to predict the\nnanomaterial category. This multi-faceted approach promises a more\ncomprehensive and accurate representation and classification of micrographs for\nnanomaterial identification. Our framework outperforms traditional methods,\novercoming challenges posed by distributional shifts, and facilitating\nhigh-throughput screening."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Geethan Sannidhi"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "arxiv_comment": "Our paper is published at the workshop on Robustness of Few-shot and\n  Zero-shot Learning in Foundation Models at NeurIPS 2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13654v1",
                "updated": "2024-08-24T19:11:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    24,
                    19,
                    11,
                    54,
                    5,
                    237,
                    0
                ],
                "published": "2024-08-24T19:11:54Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    19,
                    11,
                    54,
                    5,
                    237,
                    0
                ],
                "title": "Symbolic Working Memory Enhances Language Models for Complex Rule\n  Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic Working Memory Enhances Language Models for Complex Rule\n  Application"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable reasoning performance but\nstruggle with multi-step deductive reasoning involving a series of rule\napplication steps, especially when rules are presented non-sequentially. Our\npreliminary analysis shows that while LLMs excel in single-step rule\napplication, their performance drops significantly in multi-step scenarios due\nto the challenge in rule grounding. It requires anchoring the applicable rule\nand supporting facts at each step, amidst multiple input rules, facts, and\ninferred facts. To address this, we propose augmenting LLMs with external\nworking memory and introduce a neurosymbolic framework for rule application.\nThe memory stores facts and rules in both natural language and symbolic forms,\nenabling precise tracking. Utilizing this memory, our framework iteratively\nperforms symbolic rule grounding and LLM-based rule implementation. The former\nmatches predicates and variables of symbolic rules and facts to ground\napplicable rules at each step. Experiments indicate our framework's\neffectiveness in rule application and its robustness across various steps and\nsettings~\\footnote{Code and data are available at\n\\url{https://github.com/SiyuanWangw/RuleApplication}.}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable reasoning performance but\nstruggle with multi-step deductive reasoning involving a series of rule\napplication steps, especially when rules are presented non-sequentially. Our\npreliminary analysis shows that while LLMs excel in single-step rule\napplication, their performance drops significantly in multi-step scenarios due\nto the challenge in rule grounding. It requires anchoring the applicable rule\nand supporting facts at each step, amidst multiple input rules, facts, and\ninferred facts. To address this, we propose augmenting LLMs with external\nworking memory and introduce a neurosymbolic framework for rule application.\nThe memory stores facts and rules in both natural language and symbolic forms,\nenabling precise tracking. Utilizing this memory, our framework iteratively\nperforms symbolic rule grounding and LLM-based rule implementation. The former\nmatches predicates and variables of symbolic rules and facts to ground\napplicable rules at each step. Experiments indicate our framework's\neffectiveness in rule application and its robustness across various steps and\nsettings~\\footnote{Code and data are available at\n\\url{https://github.com/SiyuanWangw/RuleApplication}.}."
                },
                "authors": [
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Xiang Ren"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Ren"
                },
                "author": "Xiang Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.03636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.03636v2",
                "updated": "2024-08-24T17:03:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    24,
                    17,
                    3,
                    11,
                    5,
                    237,
                    0
                ],
                "published": "2024-03-06T11:48:08Z",
                "published_parsed": [
                    2024,
                    3,
                    6,
                    11,
                    48,
                    8,
                    2,
                    66,
                    0
                ],
                "title": "SheetAgent: Towards A Generalist Agent for Spreadsheet Reasoning and\n  Manipulation via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SheetAgent: Towards A Generalist Agent for Spreadsheet Reasoning and\n  Manipulation via Large Language Models"
                },
                "summary": "Spreadsheet manipulation is widely existing in most daily works and\nsignificantly improves working efficiency. Large language model (LLM) has been\nrecently attempted for automatic spreadsheet manipulation but has not yet been\ninvestigated in complicated and realistic tasks where reasoning challenges\nexist (e.g., long horizon manipulation with multi-step reasoning and ambiguous\nrequirements). To bridge the gap with the real-world requirements, we introduce\n$\\textbf{SheetRM}$, a benchmark featuring long-horizon and multi-category tasks\nwith reasoning-dependent manipulation caused by real-life challenges. To\nmitigate the above challenges, we further propose $\\textbf{SheetAgent}$, a\nnovel autonomous agent that utilizes the power of LLMs. SheetAgent consists of\nthree collaborative modules: $\\textit{Planner}$, $\\textit{Informer}$, and\n$\\textit{Retriever}$, achieving both advanced reasoning and accurate\nmanipulation over spreadsheets without human interaction through iterative task\nreasoning and reflection. Extensive experiments demonstrate that SheetAgent\ndelivers 20-30% pass rate improvements on multiple benchmarks over baselines,\nachieving enhanced precision in spreadsheet manipulation and demonstrating\nsuperior table reasoning abilities. More details and visualizations are\navailable at https://sheetagent.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spreadsheet manipulation is widely existing in most daily works and\nsignificantly improves working efficiency. Large language model (LLM) has been\nrecently attempted for automatic spreadsheet manipulation but has not yet been\ninvestigated in complicated and realistic tasks where reasoning challenges\nexist (e.g., long horizon manipulation with multi-step reasoning and ambiguous\nrequirements). To bridge the gap with the real-world requirements, we introduce\n$\\textbf{SheetRM}$, a benchmark featuring long-horizon and multi-category tasks\nwith reasoning-dependent manipulation caused by real-life challenges. To\nmitigate the above challenges, we further propose $\\textbf{SheetAgent}$, a\nnovel autonomous agent that utilizes the power of LLMs. SheetAgent consists of\nthree collaborative modules: $\\textit{Planner}$, $\\textit{Informer}$, and\n$\\textit{Retriever}$, achieving both advanced reasoning and accurate\nmanipulation over spreadsheets without human interaction through iterative task\nreasoning and reflection. Extensive experiments demonstrate that SheetAgent\ndelivers 20-30% pass rate improvements on multiple benchmarks over baselines,\nachieving enhanced precision in spreadsheet manipulation and demonstrating\nsuperior table reasoning abilities. More details and visualizations are\navailable at https://sheetagent.github.io."
                },
                "authors": [
                    {
                        "name": "Yibin Chen"
                    },
                    {
                        "name": "Yifu Yuan"
                    },
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Yan Zheng"
                    },
                    {
                        "name": "Jinyi Liu"
                    },
                    {
                        "name": "Fei Ni"
                    },
                    {
                        "name": "Jianye Hao"
                    }
                ],
                "author_detail": {
                    "name": "Jianye Hao"
                },
                "author": "Jianye Hao",
                "arxiv_comment": "Paper of new version. Accepted by Large Language Models and Cognition\n  @ ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.03636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.03636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13624v1",
                "updated": "2024-08-24T16:35:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    24,
                    16,
                    35,
                    0,
                    5,
                    237,
                    0
                ],
                "published": "2024-08-24T16:35:00Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    16,
                    35,
                    0,
                    5,
                    237,
                    0
                ],
                "title": "No Dataset Needed for Downstream Knowledge Benchmarking: Response\n  Dispersion Inversely Correlates with Accuracy on Domain-specific QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Dataset Needed for Downstream Knowledge Benchmarking: Response\n  Dispersion Inversely Correlates with Accuracy on Domain-specific QA"
                },
                "summary": "This research seeks to obviate the need for creating QA datasets and grading\n(chatbot) LLM responses when comparing LLMs' knowledge in specific topic\ndomains. This is done in an entirely end-user centric way without need for\naccess to any inner workings of the LLM, so long as it can be prompted and\ngiven a random seed to create different generations to the same prompt. The\npaper does this by, for a given topic domain, defining the \"response\ndispersion\" of an LLM by repeatedly asking an LLM the same opinion question\nabout that topic domain. Namely, the response dispersion is the count of\nsingular values needed to explain 95% of the variance in the embedding matrix\nof the LLM's responses. It is found that the response dispersion is inversely\ncorrelated with accuracy on relevant QA evaluations (average spearman rank\ncorrelation stronger than -.59). A use-case analysis shows that when comparing\ntwo different LLMs on the same topic domain, comparing their response\ndispersion is a suitable replacement for comparing their QA accuracy between\n74% and 89% of the time, the range depending on certain reasonable\naccuracy-difference tolerances that may be acceptable to an end-user in\nexchange for the labor being saved using response dispersion instead of QA\naccuracy for comparison. Two response embeddings are studied for creating the\nembedding matrix in this study, one is from OpenAI's APIs and one is a novel\nembedding, here named reference sentence similarity embeddings, that can be\ncomputed locally and performs very nearly as well in calculating response\ndispersion. Also in this research, a pre-existing dataset called the IRC-Wiki\nTrivia dataset, originally developed for trivia games, has been re-purposed,\ncurated, and the curation, called IRC-WikiTriviaQA, is made available for the\npurpose of this research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research seeks to obviate the need for creating QA datasets and grading\n(chatbot) LLM responses when comparing LLMs' knowledge in specific topic\ndomains. This is done in an entirely end-user centric way without need for\naccess to any inner workings of the LLM, so long as it can be prompted and\ngiven a random seed to create different generations to the same prompt. The\npaper does this by, for a given topic domain, defining the \"response\ndispersion\" of an LLM by repeatedly asking an LLM the same opinion question\nabout that topic domain. Namely, the response dispersion is the count of\nsingular values needed to explain 95% of the variance in the embedding matrix\nof the LLM's responses. It is found that the response dispersion is inversely\ncorrelated with accuracy on relevant QA evaluations (average spearman rank\ncorrelation stronger than -.59). A use-case analysis shows that when comparing\ntwo different LLMs on the same topic domain, comparing their response\ndispersion is a suitable replacement for comparing their QA accuracy between\n74% and 89% of the time, the range depending on certain reasonable\naccuracy-difference tolerances that may be acceptable to an end-user in\nexchange for the labor being saved using response dispersion instead of QA\naccuracy for comparison. Two response embeddings are studied for creating the\nembedding matrix in this study, one is from OpenAI's APIs and one is a novel\nembedding, here named reference sentence similarity embeddings, that can be\ncomputed locally and performs very nearly as well in calculating response\ndispersion. Also in this research, a pre-existing dataset called the IRC-Wiki\nTrivia dataset, originally developed for trivia games, has been re-purposed,\ncurated, and the curation, called IRC-WikiTriviaQA, is made available for the\npurpose of this research."
                },
                "authors": [
                    {
                        "name": "Robert L Simione II"
                    }
                ],
                "author_detail": {
                    "name": "Robert L Simione II"
                },
                "author": "Robert L Simione II",
                "arxiv_comment": "16 pages, 3 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13621v1",
                "updated": "2024-08-24T16:28:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    24,
                    16,
                    28,
                    0,
                    5,
                    237,
                    0
                ],
                "published": "2024-08-24T16:28:00Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    16,
                    28,
                    0,
                    5,
                    237,
                    0
                ],
                "title": "Preliminary Investigations of a Multi-Faceted Robust and Synergistic\n  Approach in Semiconductor Electron Micrograph Analysis: Integrating Vision\n  Transformers with Large Language and Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preliminary Investigations of a Multi-Faceted Robust and Synergistic\n  Approach in Semiconductor Electron Micrograph Analysis: Integrating Vision\n  Transformers with Large Language and Multimodal Models"
                },
                "summary": "Characterizing materials using electron micrographs is crucial in areas such\nas semiconductors and quantum materials. Traditional classification methods\nfalter due to the intricatestructures of these micrographs. This study\nintroduces an innovative architecture that leverages the generative\ncapabilities of zero-shot prompting in Large Language Models (LLMs) such as\nGPT-4(language only), the predictive ability of few-shot (in-context) learning\nin Large Multimodal Models (LMMs) such as GPT-4(V)ision, and fuses knowledge\nacross image based and linguistic insights for accurate nanomaterial category\nprediction. This comprehensive approach aims to provide a robust solution for\nthe automated nanomaterial identification task in semiconductor manufacturing,\nblending performance, efficiency, and interpretability. Our method surpasses\nconventional approaches, offering precise nanomaterial identification and\nfacilitating high-throughput screening.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing materials using electron micrographs is crucial in areas such\nas semiconductors and quantum materials. Traditional classification methods\nfalter due to the intricatestructures of these micrographs. This study\nintroduces an innovative architecture that leverages the generative\ncapabilities of zero-shot prompting in Large Language Models (LLMs) such as\nGPT-4(language only), the predictive ability of few-shot (in-context) learning\nin Large Multimodal Models (LMMs) such as GPT-4(V)ision, and fuses knowledge\nacross image based and linguistic insights for accurate nanomaterial category\nprediction. This comprehensive approach aims to provide a robust solution for\nthe automated nanomaterial identification task in semiconductor manufacturing,\nblending performance, efficiency, and interpretability. Our method surpasses\nconventional approaches, offering precise nanomaterial identification and\nfacilitating high-throughput screening."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Geethan Sannidhi"
                    },
                    {
                        "name": "Sreeja Gangasani"
                    },
                    {
                        "name": "Chidaksh Ravuru"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "arxiv_comment": "Published at Deployable AI (DAI) Workshop at AAAI-2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]