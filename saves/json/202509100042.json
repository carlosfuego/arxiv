[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.06949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06949v1",
                "updated": "2025-09-08T17:58:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:58:06Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models"
                },
                "summary": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL"
                },
                "authors": [
                    {
                        "name": "Yinjie Wang"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ke Shen"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03377v2",
                "updated": "2025-09-08T17:22:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    22,
                    17,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-03T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    53,
                    45,
                    2,
                    246,
                    0
                ],
                "title": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing"
                },
                "summary": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v4",
                "updated": "2025-09-08T13:34:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    34,
                    54,
                    0,
                    251,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AGI/AMD-Hybrid-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AGI/AMD-Hybrid-Models."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06579v1",
                "updated": "2025-09-08T11:49:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    49,
                    51,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T11:49:51Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    49,
                    51,
                    0,
                    251,
                    0
                ],
                "title": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View\n  Synthesis"
                },
                "summary": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html."
                },
                "authors": [
                    {
                        "name": "Xin Kong"
                    },
                    {
                        "name": "Daniel Watson"
                    },
                    {
                        "name": "Yannick Strümpler"
                    },
                    {
                        "name": "Michael Niemeyer"
                    },
                    {
                        "name": "Federico Tombari"
                    }
                ],
                "author_detail": {
                    "name": "Federico Tombari"
                },
                "author": "Federico Tombari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06493v1",
                "updated": "2025-09-08T09:54:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T09:54:18Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers"
                },
                "summary": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search."
                },
                "authors": [
                    {
                        "name": "Ran Xin"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Yanchen Nie"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Xia Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xia Xiao"
                },
                "author": "Xia Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09822v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09822v4",
                "updated": "2025-09-08T09:09:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    9,
                    36,
                    0,
                    251,
                    0
                ],
                "published": "2025-08-13T13:54:51Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "title": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining"
                },
                "summary": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/"
                },
                "authors": [
                    {
                        "name": "Zijian Song"
                    },
                    {
                        "name": "Sihan Qin"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Guangrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangrun Wang"
                },
                "author": "Guangrun Wang",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09822v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09822v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06444v1",
                "updated": "2025-09-08T08:44:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    44,
                    24,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T08:44:24Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    44,
                    24,
                    0,
                    251,
                    0
                ],
                "title": "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for\n  Heterogeneous and Privacy-Sensitive Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for\n  Heterogeneous and Privacy-Sensitive Data"
                },
                "summary": "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments."
                },
                "authors": [
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Hainan Zhang"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Hong-Wei Zheng"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06436v1",
                "updated": "2025-09-08T08:34:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T08:34:02Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "title": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning"
                },
                "summary": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Xiaofei Xu"
                    },
                    {
                        "name": "Ke Deng"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Lin Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lin Tian"
                },
                "author": "Lin Tian",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06261v1",
                "updated": "2025-09-08T00:57:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    0,
                    57,
                    50,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T00:57:50Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    0,
                    57,
                    50,
                    0,
                    251,
                    0
                ],
                "title": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving"
                },
                "summary": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems."
                },
                "authors": [
                    {
                        "name": "Kyungmin Bin"
                    },
                    {
                        "name": "Seungbeom Choi"
                    },
                    {
                        "name": "Jimyoung Son"
                    },
                    {
                        "name": "Jieun Choi"
                    },
                    {
                        "name": "Daseul Bae"
                    },
                    {
                        "name": "Daehyeon Baek"
                    },
                    {
                        "name": "Kihyo Moon"
                    },
                    {
                        "name": "Minsung Jang"
                    },
                    {
                        "name": "Hyojung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hyojung Lee"
                },
                "author": "Hyojung Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06047v1",
                "updated": "2025-09-07T13:15:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    7,
                    13,
                    15,
                    17,
                    6,
                    250,
                    0
                ],
                "published": "2025-09-07T13:15:17Z",
                "published_parsed": [
                    2025,
                    9,
                    7,
                    13,
                    15,
                    17,
                    6,
                    250,
                    0
                ],
                "title": "A facile vector substrate platform via BaTiO3 membrane transfer enables\n  high quality solution processed epitaxial PZT on silicon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A facile vector substrate platform via BaTiO3 membrane transfer enables\n  high quality solution processed epitaxial PZT on silicon"
                },
                "summary": "The direct integration of high-performance ferroelectric oxides with silicon\nremains challenging due to lattice mismatch, thermal incompatibility, and the\nneed for high-temperature epitaxial growth. Here, a hybrid integration approach\nis demonstrated in which crystalline BaTiO3 (BTO) membranes are first\ntransferred onto Pt coated Si substrates and subsequently used as vector\nsubstrates (VS) for the growth of epitaxial (001) Pb(Zr0.52Ti0.48)O3 (PZT) thin\nfilms via chemical solution deposition (CSD). A KI and HCl based etchant\nenables rapid and complete dissolution of the SrVO3 sacrificial layer in about\n30 minutes, reducing the release time from days to minutes compared with\nconventional water based approaches to dissolve AVO3 and AMoO3 (A is Ca, Sr,\nBa). The BTO VS imposes dominant (00l) out of plane orientation and in plane\ncube on cube epitaxy in the overlying PZT. Devices exhibit remnant polarization\n10 to 12 micro coulomb/cm2 and coercive field of 100 kV/cm, with stable\nswitching to 10^8 cycles on the VS. From piezoelectric butterfly loops, we\nextract effective d33 of 70 pm/V for PZT on VS, and 54 pm/V for PZT grown on\nconventional Pt Si substrates. This approach demonstrates a scalable and cost\neffective route for integrating functional ferroelectric materials onto silicon\nand offers a promising platform for future CMOS compatible oxide electronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The direct integration of high-performance ferroelectric oxides with silicon\nremains challenging due to lattice mismatch, thermal incompatibility, and the\nneed for high-temperature epitaxial growth. Here, a hybrid integration approach\nis demonstrated in which crystalline BaTiO3 (BTO) membranes are first\ntransferred onto Pt coated Si substrates and subsequently used as vector\nsubstrates (VS) for the growth of epitaxial (001) Pb(Zr0.52Ti0.48)O3 (PZT) thin\nfilms via chemical solution deposition (CSD). A KI and HCl based etchant\nenables rapid and complete dissolution of the SrVO3 sacrificial layer in about\n30 minutes, reducing the release time from days to minutes compared with\nconventional water based approaches to dissolve AVO3 and AMoO3 (A is Ca, Sr,\nBa). The BTO VS imposes dominant (00l) out of plane orientation and in plane\ncube on cube epitaxy in the overlying PZT. Devices exhibit remnant polarization\n10 to 12 micro coulomb/cm2 and coercive field of 100 kV/cm, with stable\nswitching to 10^8 cycles on the VS. From piezoelectric butterfly loops, we\nextract effective d33 of 70 pm/V for PZT on VS, and 54 pm/V for PZT grown on\nconventional Pt Si substrates. This approach demonstrates a scalable and cost\neffective route for integrating functional ferroelectric materials onto silicon\nand offers a promising platform for future CMOS compatible oxide electronics."
                },
                "authors": [
                    {
                        "name": "Asraful Haque"
                    },
                    {
                        "name": "Antony Jeyaseelan"
                    },
                    {
                        "name": "Shubham Kumar Parate"
                    },
                    {
                        "name": "Srinivasan Raghavan"
                    },
                    {
                        "name": "Pavan Nukala"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Nukala"
                },
                "arxiv_affiliation": "Centre for Nanoscience and Engineering, Indian Institute of Science, Bengaluru, India",
                "author": "Pavan Nukala",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13863v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13863v2",
                "updated": "2025-09-06T05:58:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    6,
                    5,
                    58,
                    51,
                    5,
                    249,
                    0
                ],
                "published": "2025-08-19T14:30:41Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    30,
                    41,
                    1,
                    231,
                    0
                ],
                "title": "Tight Cache Contention Analysis for WCET Estimation on Multicore Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tight Cache Contention Analysis for WCET Estimation on Multicore Systems"
                },
                "summary": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Shenlin Cai"
                    },
                    {
                        "name": "Yaowei Liang"
                    },
                    {
                        "name": "Chen Jie"
                    },
                    {
                        "name": "Yinjie Fang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Guoquan Zhang"
                    },
                    {
                        "name": "Yaoyao Gu"
                    },
                    {
                        "name": "Xiang Xiao"
                    },
                    {
                        "name": "Wei Qin"
                    },
                    {
                        "name": "Xiangzhen Ouyang"
                    },
                    {
                        "name": "Wanli Chang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Chang"
                },
                "author": "Wanli Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13863v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13863v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05207v1",
                "updated": "2025-09-05T16:10:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    20,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T16:10:20Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    20,
                    4,
                    248,
                    0
                ],
                "title": "RapidGNN: Energy and Communication-Efficient Distributed Training on\n  Large-Scale Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RapidGNN: Energy and Communication-Efficient Distributed Training on\n  Large-Scale Graph Neural Networks"
                },
                "summary": "Graph Neural Networks (GNNs) have become popular across a diverse set of\ntasks in exploring structural relationships between entities. However, due to\nthe highly connected structure of the datasets, distributed training of GNNs on\nlarge-scale graphs poses significant challenges. Traditional sampling-based\napproaches mitigate the computational loads, yet the communication overhead\nremains a challenge. This paper presents RapidGNN, a distributed GNN training\nframework with deterministic sampling-based scheduling to enable efficient\ncache construction and prefetching of remote features. Evaluation on benchmark\ngraph datasets demonstrates RapidGNN's effectiveness across different scales\nand topologies. RapidGNN improves end-to-end training throughput by 2.46x to\n3.00x on average over baseline methods across the benchmark datasets, while\ncutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further\ndemonstrates near-linear scalability with an increasing number of computing\nunits efficiently. Furthermore, it achieves increased energy efficiency over\nthe baseline methods for both CPU and GPU by 44% and 32%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have become popular across a diverse set of\ntasks in exploring structural relationships between entities. However, due to\nthe highly connected structure of the datasets, distributed training of GNNs on\nlarge-scale graphs poses significant challenges. Traditional sampling-based\napproaches mitigate the computational loads, yet the communication overhead\nremains a challenge. This paper presents RapidGNN, a distributed GNN training\nframework with deterministic sampling-based scheduling to enable efficient\ncache construction and prefetching of remote features. Evaluation on benchmark\ngraph datasets demonstrates RapidGNN's effectiveness across different scales\nand topologies. RapidGNN improves end-to-end training throughput by 2.46x to\n3.00x on average over baseline methods across the benchmark datasets, while\ncutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further\ndemonstrates near-linear scalability with an increasing number of computing\nunits efficiently. Furthermore, it achieves increased energy efficiency over\nthe baseline methods for both CPU and GPU by 44% and 32%, respectively."
                },
                "authors": [
                    {
                        "name": "Arefin Niam"
                    },
                    {
                        "name": "Tevfik Kosar"
                    },
                    {
                        "name": "M S Q Zulkar Nine"
                    }
                ],
                "author_detail": {
                    "name": "M S Q Zulkar Nine"
                },
                "author": "M S Q Zulkar Nine",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2505.10806",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05165v1",
                "updated": "2025-09-05T14:58:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T14:58:24Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "title": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens"
                },
                "summary": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment."
                },
                "authors": [
                    {
                        "name": "Dmitry Akulov"
                    },
                    {
                        "name": "Mohamed Sana"
                    },
                    {
                        "name": "Antonio De Domenico"
                    },
                    {
                        "name": "Tareq Si Salem"
                    },
                    {
                        "name": "Nicola Piovesan"
                    },
                    {
                        "name": "Fadhel Ayed"
                    }
                ],
                "author_detail": {
                    "name": "Fadhel Ayed"
                },
                "author": "Fadhel Ayed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09758v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09758v2",
                "updated": "2025-09-05T10:39:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    39,
                    3,
                    4,
                    248,
                    0
                ],
                "published": "2025-06-11T14:03:13Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "title": "Mainframe-Style Channel Controllers for Modern Disaggregated Memory\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainframe-Style Channel Controllers for Modern Disaggregated Memory\n  Systems"
                },
                "summary": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs."
                },
                "authors": [
                    {
                        "name": "Zikai Liu"
                    },
                    {
                        "name": "Jasmin Schult"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "arxiv_doi": "10.1145/3725783.3764403",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725783.3764403",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.09758v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09758v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Camera-ready authors' version for APSys'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04377v1",
                "updated": "2025-09-04T16:40:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T16:40:01Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "title": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference"
                },
                "summary": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks."
                },
                "authors": [
                    {
                        "name": "Krishna Teja Chitty-Venkata"
                    },
                    {
                        "name": "Jie Ye"
                    },
                    {
                        "name": "Xian-He Sun"
                    },
                    {
                        "name": "Anthony Kougkas"
                    },
                    {
                        "name": "Murali Emani"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Nicolae"
                },
                "author": "Bogdan Nicolae",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12084v2",
                "updated": "2025-09-04T15:21:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    21,
                    11,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-21T12:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "title": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis"
                },
                "summary": "This study presents a comprehensive multi-level analysis of the NVIDIA Hopper\nGPU architecture, focusing on its performance characteristics and novel\nfeatures. We benchmark Hopper's memory subsystem, highlighting improvements in\nthe L2 partitioned cache and global memory access compared to Ampere and Ada\nLovelace. The evaluation of Hopper's fourth-generation tensor cores reveals the\nbenefits of FP8 precision and asynchronous wgmma instructions for matrix\noperations. Additionally, we investigate the performance of DPX instructions\nfor dynamic programming, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. Through multi-level evaluation, we discover that the Hopper\narchitecture demonstrates significant acceleration potential in real-world\napplications. For instance, the asynchronous programming model supported by TMA\nachieves a 1.5x speedup in matrix multiplication, FP8 delivers nearly double\nthe performance of FP16, and DPX instructions accelerate a computational\nbiology algorithm by at least 4.75x. Our findings provide actionable insights\nfor optimizing compute-intensive workloads, from AI training to bioinformatics,\non Hopper GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a comprehensive multi-level analysis of the NVIDIA Hopper\nGPU architecture, focusing on its performance characteristics and novel\nfeatures. We benchmark Hopper's memory subsystem, highlighting improvements in\nthe L2 partitioned cache and global memory access compared to Ampere and Ada\nLovelace. The evaluation of Hopper's fourth-generation tensor cores reveals the\nbenefits of FP8 precision and asynchronous wgmma instructions for matrix\noperations. Additionally, we investigate the performance of DPX instructions\nfor dynamic programming, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. Through multi-level evaluation, we discover that the Hopper\narchitecture demonstrates significant acceleration potential in real-world\napplications. For instance, the asynchronous programming model supported by TMA\nachieves a 1.5x speedup in matrix multiplication, FP8 delivers nearly double\nthe performance of FP16, and DPX instructions accelerate a computational\nbiology algorithm by at least 4.75x. Our findings provide actionable insights\nfor optimizing compute-intensive workloads, from AI training to bioinformatics,\non Hopper GPUs."
                },
                "authors": [
                    {
                        "name": "Weile Luo"
                    },
                    {
                        "name": "Ruibo Fan"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Hongyuan Liu"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2402.13499",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v2",
                "updated": "2025-09-04T13:14:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    14,
                    33,
                    3,
                    247,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04185v1",
                "updated": "2025-09-04T13:02:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T13:02:39Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "title": "Set Block Decoding is a Language Model Inference Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Set Block Decoding is a Language Model Inference Accelerator"
                },
                "summary": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training."
                },
                "authors": [
                    {
                        "name": "Itai Gat"
                    },
                    {
                        "name": "Heli Ben-Hamu"
                    },
                    {
                        "name": "Marton Havasi"
                    },
                    {
                        "name": "Daniel Haziza"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Gabriel Synnaeve"
                    },
                    {
                        "name": "David Lopez-Paz"
                    },
                    {
                        "name": "Brian Karrer"
                    },
                    {
                        "name": "Yaron Lipman"
                    }
                ],
                "author_detail": {
                    "name": "Yaron Lipman"
                },
                "author": "Yaron Lipman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04180v1",
                "updated": "2025-09-04T12:54:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    54,
                    32,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T12:54:32Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    54,
                    32,
                    3,
                    247,
                    0
                ],
                "title": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision"
                },
                "summary": "AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}."
                },
                "authors": [
                    {
                        "name": "Safouane El Ghazouali"
                    },
                    {
                        "name": "Umberto Michelucci"
                    }
                ],
                "author_detail": {
                    "name": "Umberto Michelucci"
                },
                "author": "Umberto Michelucci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19740v2",
                "updated": "2025-09-04T09:08:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    9,
                    8,
                    29,
                    3,
                    247,
                    0
                ],
                "published": "2025-08-27T10:11:27Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval"
                },
                "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04010v1",
                "updated": "2025-09-04T08:41:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    41,
                    6,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T08:41:06Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    41,
                    6,
                    3,
                    247,
                    0
                ],
                "title": "Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and\n  Lessons Learned",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and\n  Lessons Learned"
                },
                "summary": "The PQDSS standardization process requires cryptographic primitives to be\nfree from vulnerabilities, including timing and cache side-channels. Resistance\nto timing leakage is therefore an essential property, and achieving this\ntypically relies on software implementations that follow constant-time\nprinciples. Moreover, ensuring that all implementations are constant-time is\ncrucial for fair performance comparisons, as secure implementations often incur\nadditional overhead. Such analysis also helps identify scheme proposals that\nare inherently difficult to implement in constant time. Because constant-time\nproperties can be broken during compilation, it is often necessary to analyze\nthe compiled binary directly. Since manual binary analysis is extremely\nchallenging, automated analysis becomes highly important. Although several\ntools exist to assist with such analysis, they often have usability limitations\nand are difficult to set up correctly. To support the developers besides the\nNIST committee in verifying candidates, we developed a toolchain that automates\nconfiguration, execution, and result analysis for several widely used\nconstant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify\nconstant-time policy compliance at the binary level, and dudect and RTLF to\ndetect side-channel vulnerabilities through statistical analysis of execution\ntime behavior. We demonstrate its effectiveness and practicability by\nevaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26\nissues in total to the respective developers, and 5 of them have already been\nfixed. We also discuss our different findings, as well as the benefits of\nshortcomings of the different tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The PQDSS standardization process requires cryptographic primitives to be\nfree from vulnerabilities, including timing and cache side-channels. Resistance\nto timing leakage is therefore an essential property, and achieving this\ntypically relies on software implementations that follow constant-time\nprinciples. Moreover, ensuring that all implementations are constant-time is\ncrucial for fair performance comparisons, as secure implementations often incur\nadditional overhead. Such analysis also helps identify scheme proposals that\nare inherently difficult to implement in constant time. Because constant-time\nproperties can be broken during compilation, it is often necessary to analyze\nthe compiled binary directly. Since manual binary analysis is extremely\nchallenging, automated analysis becomes highly important. Although several\ntools exist to assist with such analysis, they often have usability limitations\nand are difficult to set up correctly. To support the developers besides the\nNIST committee in verifying candidates, we developed a toolchain that automates\nconfiguration, execution, and result analysis for several widely used\nconstant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify\nconstant-time policy compliance at the binary level, and dudect and RTLF to\ndetect side-channel vulnerabilities through statistical analysis of execution\ntime behavior. We demonstrate its effectiveness and practicability by\nevaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26\nissues in total to the respective developers, and 5 of them have already been\nfixed. We also discuss our different findings, as well as the benefits of\nshortcomings of the different tools."
                },
                "authors": [
                    {
                        "name": "Olivier Adjonyo"
                    },
                    {
                        "name": "Sebastien Bardin"
                    },
                    {
                        "name": "Emanuele Bellini"
                    },
                    {
                        "name": "Gilbert Ndollane Dione"
                    },
                    {
                        "name": "Mahmudul Faisal Al Ameen"
                    },
                    {
                        "name": "Robert Merget"
                    },
                    {
                        "name": "Frederic Recoules"
                    },
                    {
                        "name": "Yanis Sellami"
                    }
                ],
                "author_detail": {
                    "name": "Yanis Sellami"
                },
                "author": "Yanis Sellami",
                "arxiv_comment": "20 pages, 1 figure, to be published and presented at Sixth PQC\n  Standardization Conference by NIST, partially supported by the \"France 2030\"\n  government investment plan managed by the French National Research Agency,\n  under the reference ANR-22-PECY-0005",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12689v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12689v3",
                "updated": "2025-09-04T06:20:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    6,
                    20,
                    55,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-22T07:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "IC-Cache: Efficient Large Language Model Serving via In-context Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IC-Cache: Efficient Large Language Model Serving via In-context Caching"
                },
                "summary": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 70% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge transfer among requests. However, naively caching and reusing past\nresponses leads to a big quality drop. In this paper, we introduce IC-Cache, a\ncaching system that enables live LLM capability augmentation to improve serving\nefficiency: by leveraging historical request-response pairs from larger models\nas in-context examples, IC-Cache empowers small LLMs to imitate and even exceed\nthe compositional abilities (e.g., reasoning) of their larger counterparts,\nenabling selective offloading of requests to reduce cost and latency. Achieving\nthis live augmentation at scale introduces intricate trade-offs between\nresponse quality, latency, and system throughput. For a new request, IC-Cache\nefficiently selects similar, high-utility examples to prepend them to the new\nrequest's input. At scale, it adaptively routes requests across LLMs of varying\ncapabilities, accounting for response quality and serving loads. IC-Cache\nemploys a cost-aware cache replay mechanism that refines example quality\noffline to maximize online cache utility and efficiency. Evaluations on\nmillions of realistic requests demonstrate that IC-Cache improves LLM serving\nthroughput by 1.4-5.9x and reduces latency by 28-71% without hurting response\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 70% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge transfer among requests. However, naively caching and reusing past\nresponses leads to a big quality drop. In this paper, we introduce IC-Cache, a\ncaching system that enables live LLM capability augmentation to improve serving\nefficiency: by leveraging historical request-response pairs from larger models\nas in-context examples, IC-Cache empowers small LLMs to imitate and even exceed\nthe compositional abilities (e.g., reasoning) of their larger counterparts,\nenabling selective offloading of requests to reduce cost and latency. Achieving\nthis live augmentation at scale introduces intricate trade-offs between\nresponse quality, latency, and system throughput. For a new request, IC-Cache\nefficiently selects similar, high-utility examples to prepend them to the new\nrequest's input. At scale, it adaptively routes requests across LLMs of varying\ncapabilities, accounting for response quality and serving loads. IC-Cache\nemploys a cost-aware cache replay mechanism that refines example quality\noffline to maximize online cache utility and efficiency. Evaluations on\nmillions of realistic requests demonstrate that IC-Cache improves LLM serving\nthroughput by 1.4-5.9x and reduces latency by 28-71% without hurting response\nquality."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Nikhil Sarda"
                    },
                    {
                        "name": "Lillian Tsai"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Yanqi Zhou"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Henry M. Levy"
                    },
                    {
                        "name": "David Culler"
                    }
                ],
                "author_detail": {
                    "name": "David Culler"
                },
                "author": "David Culler",
                "arxiv_doi": "10.1145/3731569.3764829",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731569.3764829",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.12689v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12689v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01228v2",
                "updated": "2025-09-03T20:54:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    20,
                    54,
                    57,
                    2,
                    246,
                    0
                ],
                "published": "2024-10-02T04:12:13Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    4,
                    12,
                    13,
                    2,
                    276,
                    0
                ],
                "title": "ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline\n  Co-Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline\n  Co-Serving"
                },
                "summary": "Large language model (LLM) serving demands low latency and high throughput,\nbut high load variability makes it challenging to achieve high GPU utilization.\nIn this paper, we identify a synergetic but overlooked opportunity to co-serve\nlatency-critical online requests alongside latency-tolerant offline tasks such\nas model benchmarking. While promising, existing serving systems fail to\nco-serve them efficiently, as their coarse-grained resource management at the\nrequest or iteration level cannot harvest millisecond-level GPU idle cycles\nwithout introducing interference that violates online latency objectives.\nConServe is a new LLM co-serving system that achieves high throughput and\nstrong online latency guarantees by managing resources at finer granularities.\nConServe introduces three techniques: (1) a latency-aware token-level scheduler\nthat precisely sizes offline batches and tokens to fit within online latency\nobjectives; (2) sub-iteration, layer-wise preemption that allows offline tasks\nto yield to online load spikes; and (3) incremental KV cache management that\nenables preempting and resuming offline requests at near-zero cost. Evaluations\nwith Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe\ndelivers an average of 2.2$\\times$ higher throughput and reduces online serving\ntail latency by 2.9$\\times$ on average compared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving demands low latency and high throughput,\nbut high load variability makes it challenging to achieve high GPU utilization.\nIn this paper, we identify a synergetic but overlooked opportunity to co-serve\nlatency-critical online requests alongside latency-tolerant offline tasks such\nas model benchmarking. While promising, existing serving systems fail to\nco-serve them efficiently, as their coarse-grained resource management at the\nrequest or iteration level cannot harvest millisecond-level GPU idle cycles\nwithout introducing interference that violates online latency objectives.\nConServe is a new LLM co-serving system that achieves high throughput and\nstrong online latency guarantees by managing resources at finer granularities.\nConServe introduces three techniques: (1) a latency-aware token-level scheduler\nthat precisely sizes offline batches and tokens to fit within online latency\nobjectives; (2) sub-iteration, layer-wise preemption that allows offline tasks\nto yield to online load spikes; and (3) incremental KV cache management that\nenables preempting and resuming offline requests at near-zero cost. Evaluations\nwith Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe\ndelivers an average of 2.2$\\times$ higher throughput and reduces online serving\ntail latency by 2.9$\\times$ on average compared to state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Yifan Qiao"
                    },
                    {
                        "name": "Shu Anzai"
                    },
                    {
                        "name": "Shan Yu"
                    },
                    {
                        "name": "Haoran Ma"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Miryung Kim"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jiarong Xing"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Harry Xu"
                    }
                ],
                "author_detail": {
                    "name": "Harry Xu"
                },
                "author": "Harry Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03394v1",
                "updated": "2025-09-03T15:15:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    44,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T15:15:44Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    44,
                    2,
                    246,
                    0
                ],
                "title": "CloudFormer: An Attention-based Performance Prediction for Public Clouds\n  with Unknown Workload",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CloudFormer: An Attention-based Performance Prediction for Public Clouds\n  with Unknown Workload"
                },
                "summary": "Cloud platforms are increasingly relied upon to host diverse,\nresource-intensive workloads due to their scalability, flexibility, and\ncost-efficiency. In multi-tenant cloud environments, virtual machines are\nconsolidated on shared physical servers to improve resource utilization. While\nvirtualization guarantees resource partitioning for CPU, memory, and storage,\nit cannot ensure performance isolation. Competition for shared resources such\nas last-level cache, memory bandwidth, and network interfaces often leads to\nsevere performance degradation. Existing management techniques, including VM\nscheduling and resource provisioning, require accurate performance prediction\nto mitigate interference. However, this remains challenging in public clouds\ndue to the black-box nature of VMs and the highly dynamic nature of workloads.\nTo address these limitations, we propose CloudFormer, a dual-branch\nTransformer-based model designed to predict VM performance degradation in\nblack-box environments. CloudFormer jointly models temporal dynamics and\nsystem-level interactions, leveraging 206 system metrics at one-second\nresolution across both static and dynamic scenarios. This design enables the\nmodel to capture transient interference effects and adapt to varying workload\nconditions without scenario-specific tuning. Complementing the methodology, we\nprovide a fine-grained dataset that significantly expands the temporal\nresolution and metric diversity compared to existing benchmarks. Experimental\nresults demonstrate that CloudFormer consistently outperforms state-of-the-art\nbaselines across multiple evaluation metrics, achieving robust generalization\nacross diverse and previously unseen workloads. Notably, CloudFormer attains a\nmean absolute error (MAE) of just 7.8%, representing a substantial improvement\nin predictive accuracy and outperforming existing methods at least by 28%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms are increasingly relied upon to host diverse,\nresource-intensive workloads due to their scalability, flexibility, and\ncost-efficiency. In multi-tenant cloud environments, virtual machines are\nconsolidated on shared physical servers to improve resource utilization. While\nvirtualization guarantees resource partitioning for CPU, memory, and storage,\nit cannot ensure performance isolation. Competition for shared resources such\nas last-level cache, memory bandwidth, and network interfaces often leads to\nsevere performance degradation. Existing management techniques, including VM\nscheduling and resource provisioning, require accurate performance prediction\nto mitigate interference. However, this remains challenging in public clouds\ndue to the black-box nature of VMs and the highly dynamic nature of workloads.\nTo address these limitations, we propose CloudFormer, a dual-branch\nTransformer-based model designed to predict VM performance degradation in\nblack-box environments. CloudFormer jointly models temporal dynamics and\nsystem-level interactions, leveraging 206 system metrics at one-second\nresolution across both static and dynamic scenarios. This design enables the\nmodel to capture transient interference effects and adapt to varying workload\nconditions without scenario-specific tuning. Complementing the methodology, we\nprovide a fine-grained dataset that significantly expands the temporal\nresolution and metric diversity compared to existing benchmarks. Experimental\nresults demonstrate that CloudFormer consistently outperforms state-of-the-art\nbaselines across multiple evaluation metrics, achieving robust generalization\nacross diverse and previously unseen workloads. Notably, CloudFormer attains a\nmean absolute error (MAE) of just 7.8%, representing a substantial improvement\nin predictive accuracy and outperforming existing methods at least by 28%."
                },
                "authors": [
                    {
                        "name": "Amirhossein Shahbazinia"
                    },
                    {
                        "name": "Darong Huang"
                    },
                    {
                        "name": "Luis Costero"
                    },
                    {
                        "name": "David Atienza"
                    }
                ],
                "author_detail": {
                    "name": "David Atienza"
                },
                "author": "David Atienza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00079v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00079v4",
                "updated": "2025-09-03T14:56:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    56,
                    29,
                    2,
                    246,
                    0
                ],
                "published": "2024-06-24T02:05:32Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    2,
                    5,
                    32,
                    0,
                    176,
                    0
                ],
                "title": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving"
                },
                "summary": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests."
                },
                "authors": [
                    {
                        "name": "Ruoyu Qin"
                    },
                    {
                        "name": "Zheming Li"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yongwei Wu"
                    },
                    {
                        "name": "Weimin Zheng"
                    },
                    {
                        "name": "Xinran Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xinran Xu"
                },
                "author": "Xinran Xu",
                "arxiv_comment": "23 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00079v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00079v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04416v2",
                "updated": "2025-09-03T14:28:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    28,
                    23,
                    2,
                    246,
                    0
                ],
                "published": "2025-07-06T15:08:49Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "title": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling"
                },
                "summary": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT."
                },
                "authors": [
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Anunay Yadav"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03560v1",
                "updated": "2025-09-03T11:23:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    23,
                    35,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T11:23:35Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    23,
                    35,
                    2,
                    246,
                    0
                ],
                "title": "A Cegar-centric Bounded Reachability Analysis for Compositional Affine\n  Hybrid Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Cegar-centric Bounded Reachability Analysis for Compositional Affine\n  Hybrid Systems"
                },
                "summary": "Reachability analysis of compositional hybrid systems, where individual\ncomponents are modeled as hybrid automata, poses unique challenges. In addition\nto preserving the compositional semantics while computing system behaviors,\nalgorithms have to cater to the explosion in the number of locations in the\nparallel product automaton. In this paper, we propose a bounded reachability\nanalysis algorithm for compositional hybrid systems with piecewise affine\ndynamics, based on the principle of counterexample guided abstraction\nrefinement (CEGAR). In particular, the algorithm searches for a counterexample\nin the discrete abstraction of the composition model, without explicitly\ncomputing a product automaton. When a counterexample is discovered in the\nabstraction, its validity is verified by a refinement of the state-space guided\nby the abstract counterexample. The state-space refinement is through a\nsymbolic reachability analysis, particularly using a state-of-the-art algorithm\nwith support functions as the continuous state representation. In addition, the\nalgorithm mixes different semantics of composition with the objective of\nimproved efficiency. Step compositional semantics is followed while exploring\nthe abstract (discrete) state-space, while shallow compositional semantics is\nfollowed during state-space refinement with symbolic reachability analysis.\nOptimizations such as caching the results of the symbolic reachability\nanalysis, which can be later reused, have been proposed. We implement this\nalgorithm in the tool SAT-Reach and demonstrate the scalability benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reachability analysis of compositional hybrid systems, where individual\ncomponents are modeled as hybrid automata, poses unique challenges. In addition\nto preserving the compositional semantics while computing system behaviors,\nalgorithms have to cater to the explosion in the number of locations in the\nparallel product automaton. In this paper, we propose a bounded reachability\nanalysis algorithm for compositional hybrid systems with piecewise affine\ndynamics, based on the principle of counterexample guided abstraction\nrefinement (CEGAR). In particular, the algorithm searches for a counterexample\nin the discrete abstraction of the composition model, without explicitly\ncomputing a product automaton. When a counterexample is discovered in the\nabstraction, its validity is verified by a refinement of the state-space guided\nby the abstract counterexample. The state-space refinement is through a\nsymbolic reachability analysis, particularly using a state-of-the-art algorithm\nwith support functions as the continuous state representation. In addition, the\nalgorithm mixes different semantics of composition with the objective of\nimproved efficiency. Step compositional semantics is followed while exploring\nthe abstract (discrete) state-space, while shallow compositional semantics is\nfollowed during state-space refinement with symbolic reachability analysis.\nOptimizations such as caching the results of the symbolic reachability\nanalysis, which can be later reused, have been proposed. We implement this\nalgorithm in the tool SAT-Reach and demonstrate the scalability benefits."
                },
                "authors": [
                    {
                        "name": "Atanu Kundu"
                    },
                    {
                        "name": "Pratyay Sarkar"
                    },
                    {
                        "name": "Rajarshi Ray"
                    }
                ],
                "author_detail": {
                    "name": "Rajarshi Ray"
                },
                "author": "Rajarshi Ray",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03136v1",
                "updated": "2025-09-03T08:38:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T08:38:40Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "title": "Adaptive KV-Cache Compression without Manually Setting Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive KV-Cache Compression without Manually Setting Budget"
                },
                "summary": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable."
                },
                "authors": [
                    {
                        "name": "Chenxia Tang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20353v2",
                "updated": "2025-09-03T06:56:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    6,
                    56,
                    21,
                    2,
                    246,
                    0
                ],
                "published": "2025-05-26T05:58:49Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    58,
                    49,
                    0,
                    146,
                    0
                ],
                "title": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation"
                },
                "summary": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Ben Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v2",
                "updated": "2025-09-02T18:10:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    18,
                    10,
                    0,
                    1,
                    245,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    },
                    {
                        "name": "Yuchen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Liu"
                },
                "author": "Yuchen Liu",
                "arxiv_comment": "Under Major Revision in IEEE Network",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02532v1",
                "updated": "2025-09-02T17:35:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    17,
                    35,
                    42,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T17:35:42Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    17,
                    35,
                    42,
                    1,
                    245,
                    0
                ],
                "title": "A Novel Coded Caching Scheme for Partially Cooperative Device-to-Device\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Coded Caching Scheme for Partially Cooperative Device-to-Device\n  Networks"
                },
                "summary": "Device-to-device (D2D) communication is one of the most promising techniques\nfor future wireless cellular communication systems. This paper considers coded\ncaching in a partially cooperative wireless D2D network, where only a subset of\nusers transmit during delivery, while all users request files. The\nnon-transmitting users are referred to as selfish users. All existing schemes\nthat do not require knowledge of the identity of selfish users before content\nplacement are limited to the high-memory regime, particularly when the number\nof selfish users is large. We propose a novel coded caching scheme for a\npartially cooperative D2D network that operates in all feasible memory regimes,\nregardless of the number of selfish users. We also derive a lower bound on the\ntransmission load of a partially cooperative D2D coded caching scheme. Using\nthis bound, the proposed scheme is shown to be optimal in the high-memory\nregime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Device-to-device (D2D) communication is one of the most promising techniques\nfor future wireless cellular communication systems. This paper considers coded\ncaching in a partially cooperative wireless D2D network, where only a subset of\nusers transmit during delivery, while all users request files. The\nnon-transmitting users are referred to as selfish users. All existing schemes\nthat do not require knowledge of the identity of selfish users before content\nplacement are limited to the high-memory regime, particularly when the number\nof selfish users is large. We propose a novel coded caching scheme for a\npartially cooperative D2D network that operates in all feasible memory regimes,\nregardless of the number of selfish users. We also derive a lower bound on the\ntransmission load of a partially cooperative D2D coded caching scheme. Using\nthis bound, the proposed scheme is shown to be optimal in the high-memory\nregime."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "7 pages and 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v5",
                "updated": "2025-09-02T16:39:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    39,
                    56,
                    1,
                    245,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation"
                },
                "summary": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. However, existing Ethernet-based solutions, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilization due to both increasing traffic demands and\nthe expanding scale of datacenter topologies, which also exacerbate network\nfailures. To address these limitations, we propose REPS, a lightweight\ndecentralized per-packet adaptive load balancing algorithm designed to optimize\nnetwork utilization while ensuring rapid recovery from link failures. REPS\nadapts to network conditions by caching good-performing paths. In case of a\nnetwork failure, REPS re-routes traffic away from it in less than 100\nmicroseconds. REPS is designed to be deployed with next-generation out-of-order\ntransports, such as Ultra Ethernet, and uses less than 25 bytes of\nper-connection state regardless of the topology size. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. However, existing Ethernet-based solutions, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilization due to both increasing traffic demands and\nthe expanding scale of datacenter topologies, which also exacerbate network\nfailures. To address these limitations, we propose REPS, a lightweight\ndecentralized per-packet adaptive load balancing algorithm designed to optimize\nnetwork utilization while ensuring rapid recovery from link failures. REPS\nadapts to network conditions by caching good-performing paths. In case of a\nnetwork failure, REPS re-routes traffic away from it in less than 100\nmicroseconds. REPS is designed to be deployed with next-generation out-of-order\ntransports, such as Ultra Ethernet, and uses less than 25 bytes of\nper-connection state regardless of the topology size. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02480v1",
                "updated": "2025-09-02T16:30:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    30,
                    49,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T16:30:49Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    30,
                    49,
                    1,
                    245,
                    0
                ],
                "title": "MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to\n  Break the GPU Memory Wall",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to\n  Break the GPU Memory Wall"
                },
                "summary": "Training LLMs larger than the aggregated memory of multiple GPUs is\nincreasingly necessary due to the faster growth of LLM sizes compared to GPU\nmemory. To this end, multi-tier host memory or disk offloading techniques are\nproposed by state of art. Despite advanced asynchronous multi-tier read/write\nstrategies, such offloading strategies result in significant I/O overheads in\nthe critical path of training, resulting in slower iterations. To this end, we\npropose MLP-Offload, a novel multi-level, multi-path offloading engine\nspecifically designed for optimizing LLM training on resource-constrained\nsetups by mitigating I/O bottlenecks. We make several key observations that\ndrive the design of MLP-Offload, such as I/O overheads during the update\ndominate the iteration time; I/O bandwidth of the third-level remote storage\ntier remains unutilized; and, contention due to concurrent offloading amplifies\nI/O bottlenecks. Driven by these insights, we design and implement MLP-Offload\nto offload the optimizer states across multiple tiers in a cache-efficient and\nconcurrency-controlled fashion to mitigate I/O bottlenecks during the backward\nand update phases. Evaluations on models up to 280B parameters shows that\nMLP-Offload achieves 2.5$\\times$ faster iterations compared to the\nstate-of-the-art LLM training runtimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLMs larger than the aggregated memory of multiple GPUs is\nincreasingly necessary due to the faster growth of LLM sizes compared to GPU\nmemory. To this end, multi-tier host memory or disk offloading techniques are\nproposed by state of art. Despite advanced asynchronous multi-tier read/write\nstrategies, such offloading strategies result in significant I/O overheads in\nthe critical path of training, resulting in slower iterations. To this end, we\npropose MLP-Offload, a novel multi-level, multi-path offloading engine\nspecifically designed for optimizing LLM training on resource-constrained\nsetups by mitigating I/O bottlenecks. We make several key observations that\ndrive the design of MLP-Offload, such as I/O overheads during the update\ndominate the iteration time; I/O bandwidth of the third-level remote storage\ntier remains unutilized; and, contention due to concurrent offloading amplifies\nI/O bottlenecks. Driven by these insights, we design and implement MLP-Offload\nto offload the optimizer states across multiple tiers in a cache-efficient and\nconcurrency-controlled fashion to mitigate I/O bottlenecks during the backward\nand update phases. Evaluations on models up to 280B parameters shows that\nMLP-Offload achieves 2.5$\\times$ faster iterations compared to the\nstate-of-the-art LLM training runtimes."
                },
                "authors": [
                    {
                        "name": "Avinash Maurya"
                    },
                    {
                        "name": "M. Mustafa Rafique"
                    },
                    {
                        "name": "Franck Cappello"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Nicolae"
                },
                "author": "Bogdan Nicolae",
                "arxiv_doi": "10.1145/3712285.3759864",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3712285.3759864",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SC'25: The International Conference for High Performance Computing,\n  Networking, Storage and Analysis",
                "arxiv_journal_ref": "SC'25: The International Conference for High Performance\n  Computing, Networking, Storage and Analysis, 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.0; E.2; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02408v1",
                "updated": "2025-09-02T15:19:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    15,
                    19,
                    6,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T15:19:06Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    15,
                    19,
                    6,
                    1,
                    245,
                    0
                ],
                "title": "Cache Management for Mixture-of-Experts LLMs -- extended version",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Management for Mixture-of-Experts LLMs -- extended version"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks. One of the main challenges towards the successful\ndeployment of LLMs is memory management, since they typically involve billions\nof parameters. To this end, architectures based on Mixture-of-Experts have been\nproposed, which aim to reduce the size of the parameters that are activated\nwhen producing a token. This raises the equally critical issue of efficiently\nmanaging the limited cache of the system, in that frequently used experts\nshould be stored in the fast cache rather than in the slower secondary memory.\n  In this work, we introduce and study a new paging problem that models expert\nmanagement optimization. Our formulation captures both the layered architecture\nof LLMs and the requirement that experts are cached efficiently. We first\npresent lower bounds on the competitive ratio of both deterministic and\nrandomized algorithms, which show that under mild assumptions, LRU-like\npolicies have good theoretical competitive performance. We then propose a\nlayer-based extension of LRU that is tailored to the problem at hand.\n  Extensive simulations on both synthetic datasets and actual traces of MoE\nusage show that our algorithm outperforms policies for the classic paging\nproblem, such as the standard LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks. One of the main challenges towards the successful\ndeployment of LLMs is memory management, since they typically involve billions\nof parameters. To this end, architectures based on Mixture-of-Experts have been\nproposed, which aim to reduce the size of the parameters that are activated\nwhen producing a token. This raises the equally critical issue of efficiently\nmanaging the limited cache of the system, in that frequently used experts\nshould be stored in the fast cache rather than in the slower secondary memory.\n  In this work, we introduce and study a new paging problem that models expert\nmanagement optimization. Our formulation captures both the layered architecture\nof LLMs and the requirement that experts are cached efficiently. We first\npresent lower bounds on the competitive ratio of both deterministic and\nrandomized algorithms, which show that under mild assumptions, LRU-like\npolicies have good theoretical competitive performance. We then propose a\nlayer-based extension of LRU that is tailored to the problem at hand.\n  Extensive simulations on both synthetic datasets and actual traces of MoE\nusage show that our algorithm outperforms policies for the classic paging\nproblem, such as the standard LRU."
                },
                "authors": [
                    {
                        "name": "Spyros Angelopoulos"
                    },
                    {
                        "name": "Loris Marchal"
                    },
                    {
                        "name": "Adrien Obrecht"
                    },
                    {
                        "name": "Bertrand Simon"
                    }
                ],
                "author_detail": {
                    "name": "Bertrand Simon"
                },
                "author": "Bertrand Simon",
                "arxiv_doi": "10.1007/978-3-031-99872-0_2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-99872-0_2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v2",
                "updated": "2025-09-02T13:09:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    13,
                    9,
                    37,
                    1,
                    245,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing reliance on expensive vector\ndatabase lookups. To scale efficiently, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically skewed MedRAG workload reduces database calls by 78.9% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our work highlights that approximate caching\nis a viable and effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing reliance on expensive vector\ndatabase lookups. To scale efficiently, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically skewed MedRAG workload reduces database calls by 78.9% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our work highlights that approximate caching\nis a viable and effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Zhang Ji"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    }
                ],
                "author_detail": {
                    "name": "Martijn de Vos"
                },
                "author": "Martijn de Vos",
                "arxiv_doi": "10.1145/3721146.3721941",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721941",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02232v1",
                "updated": "2025-09-02T11:58:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    58,
                    6,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T11:58:06Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    58,
                    6,
                    1,
                    245,
                    0
                ],
                "title": "Efficient Geometry Compression and Communication for 3D Gaussian\n  Splatting Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Geometry Compression and Communication for 3D Gaussian\n  Splatting Point Clouds"
                },
                "summary": "Storage and transmission challenges in dynamic 3D scene representation based\non the i3DV platform, With increasing scene complexity, the explosive growth of\n3D Gaussian data volume causes excessive storage space occupancy. To address\nthis issue, we propose adopting the AVS PCRM reference software for efficient\ncompression of Gaussian point cloud geometry data. The strategy deeply\nintegrates the advanced encoding capabilities of AVS PCRM into the i3DV\nplatform, forming technical complementarity with the original rate-distortion\noptimization mechanism based on binary hash tables. On one hand, the hash table\nefficiently caches inter-frame Gaussian point transformation relationships,\nwhich allows for high-fidelity transmission within a 40 Mbps bandwidth\nconstraint. On the other hand, AVS PCRM performs precise compression on\ngeometry data. Experimental results demonstrate that the joint framework\nmaintains the advantages of fast rendering and high-quality synthesis in 3D\nGaussian technology while achieving significant 10\\%-25\\% bitrate savings on\nuniversal test sets. It provides a superior rate-distortion tradeoff solution\nfor the storage, transmission, and interaction of 3D volumetric video.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Storage and transmission challenges in dynamic 3D scene representation based\non the i3DV platform, With increasing scene complexity, the explosive growth of\n3D Gaussian data volume causes excessive storage space occupancy. To address\nthis issue, we propose adopting the AVS PCRM reference software for efficient\ncompression of Gaussian point cloud geometry data. The strategy deeply\nintegrates the advanced encoding capabilities of AVS PCRM into the i3DV\nplatform, forming technical complementarity with the original rate-distortion\noptimization mechanism based on binary hash tables. On one hand, the hash table\nefficiently caches inter-frame Gaussian point transformation relationships,\nwhich allows for high-fidelity transmission within a 40 Mbps bandwidth\nconstraint. On the other hand, AVS PCRM performs precise compression on\ngeometry data. Experimental results demonstrate that the joint framework\nmaintains the advantages of fast rendering and high-quality synthesis in 3D\nGaussian technology while achieving significant 10\\%-25\\% bitrate savings on\nuniversal test sets. It provides a superior rate-distortion tradeoff solution\nfor the storage, transmission, and interaction of 3D volumetric video."
                },
                "authors": [
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Yanting Li"
                    },
                    {
                        "name": "Luyang Tang"
                    },
                    {
                        "name": "Wei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Gao"
                },
                "author": "Wei Gao",
                "arxiv_doi": "10.1145/3680207.3765659",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3680207.3765659",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages,5 figures",
                "arxiv_journal_ref": "ACM MOBICOM 2025",
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15212v2",
                "updated": "2025-09-02T11:29:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    29,
                    34,
                    1,
                    245,
                    0
                ],
                "published": "2025-08-21T03:48:28Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    48,
                    28,
                    3,
                    233,
                    0
                ],
                "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning"
                },
                "summary": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Guanchen Li"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Emad Barsoum"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02121v1",
                "updated": "2025-09-02T09:17:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    9,
                    17,
                    40,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T09:17:40Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    9,
                    17,
                    40,
                    1,
                    245,
                    0
                ],
                "title": "Batch Query Processing and Optimization for Agentic Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch Query Processing and Optimization for Agentic Workflows"
                },
                "summary": "Large Language Models (LLMs) in agentic workflows combine multi-step\nreasoning, tool use, and collaboration across multiple specialized agents.\nExisting LLM serving engines optimize individual calls in isolation, while\nmulti-agent frameworks focus on orchestration without system-level performance\nplanning. As a result, repeated prompts, overlapping contexts, and concurrent\nexecutions create substantial redundancy and poor GPU utilization, especially\nin batch analytics scenarios. We introduce Halo, a system that brings batch\nquery processing and optimization into agentic LLM workflows. Halo represents\neach workflow as a structured query plan DAG and constructs a consolidated\ngraph for batched queries that exposes shared computation. Guided by a cost\nmodel that jointly considers prefill and decode costs, cache reuse, and GPU\nplacement, Halo performs plan-level optimization to minimize redundant\nexecution. Its runtime integrates adaptive batching, KV-cache sharing and\nmigration, along with compute-communication overlap to maximize hardware\nefficiency. Evaluation across six benchmarks shows that Halo achieves up to\n18.6x speedup for batch inference and 4.7x throughput improvement under online\nserving, scaling to workloads of tens of thousands of queries and complex\ngraphs. These gains are achieved without compromising output quality. By\nunifying query optimization with LLM serving, Halo enables efficient agentic\nworkflows in data analytics and decision-making applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) in agentic workflows combine multi-step\nreasoning, tool use, and collaboration across multiple specialized agents.\nExisting LLM serving engines optimize individual calls in isolation, while\nmulti-agent frameworks focus on orchestration without system-level performance\nplanning. As a result, repeated prompts, overlapping contexts, and concurrent\nexecutions create substantial redundancy and poor GPU utilization, especially\nin batch analytics scenarios. We introduce Halo, a system that brings batch\nquery processing and optimization into agentic LLM workflows. Halo represents\neach workflow as a structured query plan DAG and constructs a consolidated\ngraph for batched queries that exposes shared computation. Guided by a cost\nmodel that jointly considers prefill and decode costs, cache reuse, and GPU\nplacement, Halo performs plan-level optimization to minimize redundant\nexecution. Its runtime integrates adaptive batching, KV-cache sharing and\nmigration, along with compute-communication overlap to maximize hardware\nefficiency. Evaluation across six benchmarks shows that Halo achieves up to\n18.6x speedup for batch inference and 4.7x throughput improvement under online\nserving, scaling to workloads of tens of thousands of queries and complex\ngraphs. These gains are achieved without compromising output quality. By\nunifying query optimization with LLM serving, Halo enables efficient agentic\nworkflows in data analytics and decision-making applications."
                },
                "authors": [
                    {
                        "name": "Junyi Shen"
                    },
                    {
                        "name": "Noppanat Wadlom"
                    },
                    {
                        "name": "Yao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Lu"
                },
                "author": "Yao Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02004v1",
                "updated": "2025-09-02T06:40:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    6,
                    40,
                    45,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T06:40:45Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    6,
                    40,
                    45,
                    1,
                    245,
                    0
                ],
                "title": "Augmented Shuffle Differential Privacy Protocols for Large-Domain\n  Categorical and Key-Value Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Shuffle Differential Privacy Protocols for Large-Domain\n  Categorical and Key-Value Data"
                },
                "summary": "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols."
                },
                "authors": [
                    {
                        "name": "Takao Murakami"
                    },
                    {
                        "name": "Yuichi Sei"
                    },
                    {
                        "name": "Reo Eriguchi"
                    }
                ],
                "author_detail": {
                    "name": "Reo Eriguchi"
                },
                "author": "Reo Eriguchi",
                "arxiv_comment": "Full version of the paper accepted at NDSS 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01742v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01742v2",
                "updated": "2025-09-09T00:15:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    0,
                    15,
                    5,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-01T19:49:21Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    19,
                    49,
                    21,
                    0,
                    244,
                    0
                ],
                "title": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators"
                },
                "summary": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations:\n(i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache\nto accelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations:\n(i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache\nto accelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook."
                },
                "authors": [
                    {
                        "name": "Yitong Guo"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Haobin Hiroki Chen"
                    },
                    {
                        "name": "Yukui Luo"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Chenghong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chenghong Wang"
                },
                "author": "Chenghong Wang",
                "arxiv_comment": "Accepted by CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01742v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01742v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01395v1",
                "updated": "2025-09-01T11:41:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    41,
                    10,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T11:41:10Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    41,
                    10,
                    0,
                    244,
                    0
                ],
                "title": "LLMs cannot spot math errors, even when allowed to peek into the\n  solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs cannot spot math errors, even when allowed to peek into the\n  solution"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance."
                },
                "authors": [
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "Accepted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15779v2",
                "updated": "2025-09-01T07:26:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    7,
                    26,
                    57,
                    0,
                    244,
                    0
                ],
                "published": "2025-02-17T08:12:34Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    12,
                    34,
                    0,
                    48,
                    0
                ],
                "title": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer"
                },
                "summary": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP."
                },
                "authors": [
                    {
                        "name": "Euntae Choi"
                    },
                    {
                        "name": "Sumin Song"
                    },
                    {
                        "name": "Woosang Lim"
                    },
                    {
                        "name": "Sungjoo Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Sungjoo Yoo"
                },
                "author": "Sungjoo Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v3",
                "updated": "2025-09-01T03:51:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    51,
                    9,
                    0,
                    244,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01092v1",
                "updated": "2025-09-01T03:31:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T03:31:44Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "title": "REFRAG: Rethinking RAG based Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REFRAG: Rethinking RAG based Decoding"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes."
                },
                "authors": [
                    {
                        "name": "Xiaoqiang Lin"
                    },
                    {
                        "name": "Aritra Ghosh"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Anshumali Shrivastava"
                    },
                    {
                        "name": "Vijai Mohan"
                    }
                ],
                "author_detail": {
                    "name": "Vijai Mohan"
                },
                "author": "Vijai Mohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01085v1",
                "updated": "2025-09-01T03:16:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    16,
                    52,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T03:16:52Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    16,
                    52,
                    0,
                    244,
                    0
                ],
                "title": "Bidirectional Sparse Attention for Faster Video Diffusion Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional Sparse Attention for Faster Video Diffusion Training"
                },
                "summary": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention."
                },
                "authors": [
                    {
                        "name": "Chenlu Zhan"
                    },
                    {
                        "name": "Wen Li"
                    },
                    {
                        "name": "Chuyu Shen"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Suhui Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06133v2",
                "updated": "2025-08-31T15:09:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    15,
                    9,
                    36,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-08T08:54:21Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "title": "LLM Serving Optimization with Variable Prefill and Decode Lengths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Serving Optimization with Variable Prefill and Decode Lengths"
                },
                "summary": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency."
                },
                "authors": [
                    {
                        "name": "Meixuan Wang"
                    },
                    {
                        "name": "Yinyu Ye"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00883v1",
                "updated": "2025-08-31T14:51:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    51,
                    19,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-31T14:51:19Z",
                "published_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    51,
                    19,
                    6,
                    243,
                    0
                ],
                "title": "Accelerating Latency-Critical Applications with AI-Powered\n  Semi-Automatic Fine-Grained Parallelization on SMT Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Latency-Critical Applications with AI-Powered\n  Semi-Automatic Fine-Grained Parallelization on SMT Processors"
                },
                "summary": "Latency-critical applications tend to show low utilization of functional\nunits due to frequent cache misses and mispredictions during speculative\nexecution in high-performance superscalar processors. However, due to\nsignificant impact on single-thread performance, Simultaneous Multithreading\n(SMT) technology is rarely used with heavy threads of latency-critical\napplications. In this paper, we explore utilization of SMT technology to\nsupport fine-grained parallelization of latency-critical applications.\nFollowing the advancements in the development of Large Language Models (LLMs),\nwe introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we\nextend AI Coding Agent in Cursor IDE with additional tools connected through\nModel Context Protocol, enabling end-to-end AI Agent for parallelization.\nAdditional connected tools enable LLM-guided hotspot detection, collection of\ndynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance\nsimulation to estimate performance gains. We apply Aira with Relic parallel\nframework for fine-grained task parallelism on SMT cores to parallelize\nlatency-critical benchmarks representing real-world applications used in\nindustry. We show 17% geomean performance gain from parallelization of\nlatency-critical benchmarks using Aira with Relic framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency-critical applications tend to show low utilization of functional\nunits due to frequent cache misses and mispredictions during speculative\nexecution in high-performance superscalar processors. However, due to\nsignificant impact on single-thread performance, Simultaneous Multithreading\n(SMT) technology is rarely used with heavy threads of latency-critical\napplications. In this paper, we explore utilization of SMT technology to\nsupport fine-grained parallelization of latency-critical applications.\nFollowing the advancements in the development of Large Language Models (LLMs),\nwe introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we\nextend AI Coding Agent in Cursor IDE with additional tools connected through\nModel Context Protocol, enabling end-to-end AI Agent for parallelization.\nAdditional connected tools enable LLM-guided hotspot detection, collection of\ndynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance\nsimulation to estimate performance gains. We apply Aira with Relic parallel\nframework for fine-grained task parallelism on SMT cores to parallelize\nlatency-critical benchmarks representing real-world applications used in\nindustry. We show 17% geomean performance gain from parallelization of\nlatency-critical benchmarks using Aira with Relic framework."
                },
                "authors": [
                    {
                        "name": "Denis Los"
                    },
                    {
                        "name": "Igor Petushkov"
                    }
                ],
                "author_detail": {
                    "name": "Igor Petushkov"
                },
                "author": "Igor Petushkov",
                "arxiv_journal_ref": "International Journal of Open Information Technologies, vol. 13,\n  no. 9, pp. 129-134, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10431v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10431v3",
                "updated": "2025-08-31T05:43:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    5,
                    43,
                    55,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-14T08:04:15Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    4,
                    15,
                    3,
                    226,
                    0
                ],
                "title": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches"
                },
                "summary": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE."
                },
                "authors": [
                    {
                        "name": "Chris Cao"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "arxiv_comment": "This version includes updated analysis of RCO Bugs (one additional\n  bug identified). Appendix added with code snippets for bug fixes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10431v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10431v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00625v1",
                "updated": "2025-08-30T22:47:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    22,
                    47,
                    15,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T22:47:15Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    22,
                    47,
                    15,
                    5,
                    242,
                    0
                ],
                "title": "NetGent: Agent-Based Automation of Network Application Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NetGent: Agent-Based Automation of Network Application Workflows"
                },
                "summary": "We present NetGent, an AI-agent framework for automating complex application\nworkflows to generate realistic network traffic datasets. Developing\ngeneralizable ML models for networking requires data collection from network\nenvironments with traffic that results from a diverse set of real-world web\napplications. However, using existing browser automation tools that are\ndiverse, repeatable, realistic, and efficient remains fragile and costly.\nNetGent addresses this challenge by allowing users to specify workflows as\nnatural-language rules that define state-dependent actions. These abstract\nspecifications are compiled into nondeterministic finite automata (NFAs), which\na state synthesis component translates into reusable, executable code. This\ndesign enables deterministic replay, reduces redundant LLM calls through state\ncaching, and adapts quickly when application interfaces change. In experiments,\nNetGent automated more than 50+ workflows spanning video-on-demand streaming,\nlive video streaming, video conferencing, social media, and web scraping,\nproducing realistic traffic traces while remaining robust to UI variability. By\ncombining the flexibility of language-based agents with the reliability of\ncompiled execution, NetGent provides a scalable foundation for generating the\ndiverse, repeatable datasets needed to advance ML in networking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present NetGent, an AI-agent framework for automating complex application\nworkflows to generate realistic network traffic datasets. Developing\ngeneralizable ML models for networking requires data collection from network\nenvironments with traffic that results from a diverse set of real-world web\napplications. However, using existing browser automation tools that are\ndiverse, repeatable, realistic, and efficient remains fragile and costly.\nNetGent addresses this challenge by allowing users to specify workflows as\nnatural-language rules that define state-dependent actions. These abstract\nspecifications are compiled into nondeterministic finite automata (NFAs), which\na state synthesis component translates into reusable, executable code. This\ndesign enables deterministic replay, reduces redundant LLM calls through state\ncaching, and adapts quickly when application interfaces change. In experiments,\nNetGent automated more than 50+ workflows spanning video-on-demand streaming,\nlive video streaming, video conferencing, social media, and web scraping,\nproducing realistic traffic traces while remaining robust to UI variability. By\ncombining the flexibility of language-based agents with the reliability of\ncompiled execution, NetGent provides a scalable foundation for generating the\ndiverse, repeatable datasets needed to advance ML in networking."
                },
                "authors": [
                    {
                        "name": "Jaber Daneshamooz"
                    },
                    {
                        "name": "Eugene Vuong"
                    },
                    {
                        "name": "Laasya Koduru"
                    },
                    {
                        "name": "Sanjay Chandrasekaran"
                    },
                    {
                        "name": "Arpit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Arpit Gupta"
                },
                "author": "Arpit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00579v1",
                "updated": "2025-08-30T18:25:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    18,
                    25,
                    19,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T18:25:19Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    18,
                    25,
                    19,
                    5,
                    242,
                    0
                ],
                "title": "KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for\n  KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for\n  KV Cache"
                },
                "summary": "Transformer-based large language models (LLMs) demonstrate impressive\npotential in various practical applications. However, long context inference\nposes a significant challenge due to the enormous memory requirements of the\nkey-value (KV) cache, which can scale to multiple gigabytes as sequence length\nand batch size increase. In this paper, we present KVComp, a generic and\nefficient KV cache management framework optimized for long-text generation that\nsynergistically works with both latency-critical and throughput-critical\ninference systems. KVComp employs novel lossy compression techniques\nspecifically designed for KV cache data characteristics, featuring careful\nco-design of compression algorithms and system architecture. Our approach\nmaintains compatibility with the growing nature of KV cache while preserving\nhigh computational efficiency. Experimental results show that KVComp achieves\non average 47\\% and up to 83\\% higher memory reduction rate compared to\nexisting methods with little/no model accuracy degradation. Furthermore, KVComp\nachieves extremely high execution throughput, effectively reducing\ndecompression overhead and, in some cases, even accelerating the matrix-vector\nmultiplication operation and outperform cuBLAS-based attention kernels with\nless data movement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) demonstrate impressive\npotential in various practical applications. However, long context inference\nposes a significant challenge due to the enormous memory requirements of the\nkey-value (KV) cache, which can scale to multiple gigabytes as sequence length\nand batch size increase. In this paper, we present KVComp, a generic and\nefficient KV cache management framework optimized for long-text generation that\nsynergistically works with both latency-critical and throughput-critical\ninference systems. KVComp employs novel lossy compression techniques\nspecifically designed for KV cache data characteristics, featuring careful\nco-design of compression algorithms and system architecture. Our approach\nmaintains compatibility with the growing nature of KV cache while preserving\nhigh computational efficiency. Experimental results show that KVComp achieves\non average 47\\% and up to 83\\% higher memory reduction rate compared to\nexisting methods with little/no model accuracy degradation. Furthermore, KVComp\nachieves extremely high execution throughput, effectively reducing\ndecompression overhead and, in some cases, even accelerating the matrix-vector\nmultiplication operation and outperform cuBLAS-based attention kernels with\nless data movement."
                },
                "authors": [
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Taolue Yang"
                    },
                    {
                        "name": "Youyuan Liu"
                    },
                    {
                        "name": "Chengming Zhang"
                    },
                    {
                        "name": "Xubin He"
                    },
                    {
                        "name": "Sian Jin"
                    }
                ],
                "author_detail": {
                    "name": "Sian Jin"
                },
                "author": "Sian Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.13777v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.13777v2",
                "updated": "2025-08-30T14:49:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    14,
                    49,
                    34,
                    5,
                    242,
                    0
                ],
                "published": "2023-10-20T19:22:58Z",
                "published_parsed": [
                    2023,
                    10,
                    20,
                    19,
                    22,
                    58,
                    4,
                    293,
                    0
                ],
                "title": "Discrete and Continuous Caching Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete and Continuous Caching Games"
                },
                "summary": "We investigate a discrete search game called the Multiple Caching Game where\nthe searcher's aim is to find all of a set of $d$ treasures hidden in $n$\nlocations. Allowed queries are sets of locations of size $k$, and the searcher\nwins if in all $d$ queries, at least one treasure is hidden in one of the $k$\npicked locations. P\\'alv\\\"olgyi showed that the value of the game is at most\n$\\frac{k^d}{\\binom{n+d-1}{d}}$, with equality for large enough $n$. We\nconjecture the exact cases of equality. We also investigate variants of the\ngame and show an example where their values are different, answering a question\nof P\\'alv\\\"olgyi.\n  This game is closely related to a continuous variant, Alpern's Caching Game,\nbased on which we define other continous variants of the multiple caching game\nand examine their values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate a discrete search game called the Multiple Caching Game where\nthe searcher's aim is to find all of a set of $d$ treasures hidden in $n$\nlocations. Allowed queries are sets of locations of size $k$, and the searcher\nwins if in all $d$ queries, at least one treasure is hidden in one of the $k$\npicked locations. P\\'alv\\\"olgyi showed that the value of the game is at most\n$\\frac{k^d}{\\binom{n+d-1}{d}}$, with equality for large enough $n$. We\nconjecture the exact cases of equality. We also investigate variants of the\ngame and show an example where their values are different, answering a question\nof P\\'alv\\\"olgyi.\n  This game is closely related to a continuous variant, Alpern's Caching Game,\nbased on which we define other continous variants of the multiple caching game\nand examine their values."
                },
                "authors": [
                    {
                        "name": "Áron Jánosik"
                    },
                    {
                        "name": "Csenge Miklós"
                    },
                    {
                        "name": "Dániel G. Simon"
                    },
                    {
                        "name": "Kristóf Zólomy"
                    }
                ],
                "author_detail": {
                    "name": "Kristóf Zólomy"
                },
                "author": "Kristóf Zólomy",
                "arxiv_doi": "10.1142/S0219198925500057",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1142/S0219198925500057",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.13777v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.13777v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "International Game Theory Review 27 (3), 2025",
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91A05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v3",
                "updated": "2025-08-30T09:35:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    9,
                    35,
                    22,
                    5,
                    242,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "DiffKV: Differentiated Memory Management for Large Language Models with\n  Parallel KV Compaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffKV: Differentiated Memory Management for Large Language Models with\n  Parallel KV Compaction"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable capabilities but face\nsubstantial serving costs due to their high memory demands, with the key-value\n(KV) cache being a primary bottleneck. State-of-the-art KV cache compression\ntechniques, such as quantization and pruning, apply uniform treatment to both\nkeys and values, and discard unimportant tokens entirely, overlooking the\nfine-grained distinctions in the significance of individual KV cache\ncomponents. To address such limitations, we introduce \\textit{DiffKV}, a novel\nframework for efficient KV cache compression that exploits three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. These levels of\ndifferentiation introduce irregular memory usage patterns across different\nrequests and attention heads, posing significant scalability challenges for\nmemory management. To address these challenges, DiffKV proposes an on-GPU\nmemory manager that compacts fragmented free memory list into contiguous\nregions in parallel, effectively translating sparsity in the KV cache into\nperformance gains. We evaluate DiffKV on several mainstream LLMs, including the\nemerging thinking models that generate extended chains of thought. DiffKV is\nable to compress the KV cache by $2.7\\times$ to $5.7\\times$ with near-lossless\naccuracy on complex workloads requiring sophisticated reasoning and\nlong-generation capabilities, and enhances throughput by $1.9\\times$ to\n$5.4\\times$. Source codes of DiffKV are available at\nhttps://github.com/zyqCSL/DiffKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable capabilities but face\nsubstantial serving costs due to their high memory demands, with the key-value\n(KV) cache being a primary bottleneck. State-of-the-art KV cache compression\ntechniques, such as quantization and pruning, apply uniform treatment to both\nkeys and values, and discard unimportant tokens entirely, overlooking the\nfine-grained distinctions in the significance of individual KV cache\ncomponents. To address such limitations, we introduce \\textit{DiffKV}, a novel\nframework for efficient KV cache compression that exploits three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. These levels of\ndifferentiation introduce irregular memory usage patterns across different\nrequests and attention heads, posing significant scalability challenges for\nmemory management. To address these challenges, DiffKV proposes an on-GPU\nmemory manager that compacts fragmented free memory list into contiguous\nregions in parallel, effectively translating sparsity in the KV cache into\nperformance gains. We evaluate DiffKV on several mainstream LLMs, including the\nemerging thinking models that generate extended chains of thought. DiffKV is\nable to compress the KV cache by $2.7\\times$ to $5.7\\times$ with near-lossless\naccuracy on complex workloads requiring sophisticated reasoning and\nlong-generation capabilities, and enhances throughput by $1.9\\times$ to\n$5.4\\times$. Source codes of DiffKV are available at\nhttps://github.com/zyqCSL/DiffKV."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "SOSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00419v1",
                "updated": "2025-08-30T08:57:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    8,
                    57,
                    53,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T08:57:53Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    8,
                    57,
                    53,
                    5,
                    242,
                    0
                ],
                "title": "LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging\n  and KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging\n  and KV Cache Compression"
                },
                "summary": "In this paper, we introduce LightVLM, a simple but effective method that can\nbe seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly\naccelerate the inference process in a training-free manner. We divide the\ninference procedure of VLMs into two stages, i.e., encoding and decoding, and\npropose to simultaneously accelerate VLMs in both stages to largely improve\nmodel efficiency. During encoding, we propose pyramid token merging to reduce\ntokens of different LLM layers in a hierarchical manner by finally only keeping\na few dominant tokens to achieve high efficiency. During decoding, aimed at\nreducing the high latency of outputting long sequences, we propose KV Cache\ncompression to remove unnecessary caches to increase the network throughput.\nExperimental results show that LightVLM successfully retains 100% performance\nwhen only preserving 35% image tokens, and maintains around 98% performance\nwhen keeping only 3% image tokens. LightVLM could 2.02$\\times$ the network\nthroughput and reduce the prefilling time by 3.65$\\times$. LightVLM also makes\nlarge VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to\ninfer faster than significantly smaller models (e.g., InternVL2.5 8B),\nhopefully facilitating the real-world deployment. When generating long text\nsequences (e.g., 4096 tokens), LightVLM could reduce the inference time by\n3.21$\\times$, largely outperforming existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce LightVLM, a simple but effective method that can\nbe seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly\naccelerate the inference process in a training-free manner. We divide the\ninference procedure of VLMs into two stages, i.e., encoding and decoding, and\npropose to simultaneously accelerate VLMs in both stages to largely improve\nmodel efficiency. During encoding, we propose pyramid token merging to reduce\ntokens of different LLM layers in a hierarchical manner by finally only keeping\na few dominant tokens to achieve high efficiency. During decoding, aimed at\nreducing the high latency of outputting long sequences, we propose KV Cache\ncompression to remove unnecessary caches to increase the network throughput.\nExperimental results show that LightVLM successfully retains 100% performance\nwhen only preserving 35% image tokens, and maintains around 98% performance\nwhen keeping only 3% image tokens. LightVLM could 2.02$\\times$ the network\nthroughput and reduce the prefilling time by 3.65$\\times$. LightVLM also makes\nlarge VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to\ninfer faster than significantly smaller models (e.g., InternVL2.5 8B),\nhopefully facilitating the real-world deployment. When generating long text\nsequences (e.g., 4096 tokens), LightVLM could reduce the inference time by\n3.21$\\times$, largely outperforming existing methods."
                },
                "authors": [
                    {
                        "name": "Lianyu Hu"
                    },
                    {
                        "name": "Fanhua Shang"
                    },
                    {
                        "name": "Wei Feng"
                    },
                    {
                        "name": "Liang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wan"
                },
                "author": "Liang Wan",
                "arxiv_comment": "EMNLP2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00388v1",
                "updated": "2025-08-30T06:56:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    6,
                    56,
                    28,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T06:56:28Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    6,
                    56,
                    28,
                    5,
                    242,
                    0
                ],
                "title": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV\n  Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV\n  Cache Eviction"
                },
                "summary": "Efficient Key-Value (KV) cache management is essential for processing long\ntext sequences in large language models (LLMs), where memory constraints often\nlimit performance. Conventional KV eviction strategies, such as top-k selection\nbased on attention scores, depend on static heuristics that fail to capture the\nevolving implicit dependencies among tokens during inference. To overcome this,\nwe propose GraphKV, a graph-based framework that redefines token selection for\nKV cache compression. In GraphKV, tokens are modeled as nodes with importance\nscores, and edges represent their similarity relationships. Through a\ndecay-signal-propagation mechanism, token importance is dynamically updated by\npropagating information across the graph, enabling adaptive retention of the\nmost contextually significant tokens. GraphKV can be seamlessly utilized in\nexisting KV cache eviction methods such as SnapKV and PyramidKV in a\nplug-and-play manner. Codes will be released on Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Key-Value (KV) cache management is essential for processing long\ntext sequences in large language models (LLMs), where memory constraints often\nlimit performance. Conventional KV eviction strategies, such as top-k selection\nbased on attention scores, depend on static heuristics that fail to capture the\nevolving implicit dependencies among tokens during inference. To overcome this,\nwe propose GraphKV, a graph-based framework that redefines token selection for\nKV cache compression. In GraphKV, tokens are modeled as nodes with importance\nscores, and edges represent their similarity relationships. Through a\ndecay-signal-propagation mechanism, token importance is dynamically updated by\npropagating information across the graph, enabling adaptive retention of the\nmost contextually significant tokens. GraphKV can be seamlessly utilized in\nexisting KV cache eviction methods such as SnapKV and PyramidKV in a\nplug-and-play manner. Codes will be released on Github."
                },
                "authors": [
                    {
                        "name": "Xuelin Li"
                    },
                    {
                        "name": "Xiangqi Jin"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11435v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11435v2",
                "updated": "2025-08-29T20:39:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    20,
                    39,
                    21,
                    4,
                    241,
                    0
                ],
                "published": "2025-04-15T17:51:39Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    51,
                    39,
                    1,
                    105,
                    0
                ],
                "title": "Robust Containment Queries over Collections of Trimmed NURBS Surfaces\n  via Generalized Winding Numbers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Containment Queries over Collections of Trimmed NURBS Surfaces\n  via Generalized Winding Numbers"
                },
                "summary": "We propose a containment query that is robust to the watertightness of\nregions bound by trimmed NURBS surfaces, as this property is difficult to\nguarantee for in-the-wild CAD models. Containment is determined through the\ngeneralized winding number (GWN), a mathematical construction that is\nindifferent to the arrangement of surfaces in the shape. Applying contemporary\ntechniques for the 3D GWN to trimmed NURBS surfaces requires some form of\ngeometric discretization, introducing computational inefficiency to the\nalgorithm and even risking containment misclassifications near the surface. In\ncontrast, our proposed method uses a novel reformulation of the relevant\nsurface integral based on Stokes' theorem, which operates on the boundary and\ntrimming curves as provided through rapidly converging adaptive quadrature.\nBatches of queries are further accelerated by memoizing (i.e.\\ caching and\nreusing) quadrature node positions and tangents as they are evaluated. We\ndemonstrate that our GWN method is robust to complex trimming geometry in a CAD\nmodel, and is accurate up to arbitrary precision at arbitrary distances from\nthe surface. The derived containment query is therefore robust to model\nnon-watertightness while respecting all curved features of the input shape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a containment query that is robust to the watertightness of\nregions bound by trimmed NURBS surfaces, as this property is difficult to\nguarantee for in-the-wild CAD models. Containment is determined through the\ngeneralized winding number (GWN), a mathematical construction that is\nindifferent to the arrangement of surfaces in the shape. Applying contemporary\ntechniques for the 3D GWN to trimmed NURBS surfaces requires some form of\ngeometric discretization, introducing computational inefficiency to the\nalgorithm and even risking containment misclassifications near the surface. In\ncontrast, our proposed method uses a novel reformulation of the relevant\nsurface integral based on Stokes' theorem, which operates on the boundary and\ntrimming curves as provided through rapidly converging adaptive quadrature.\nBatches of queries are further accelerated by memoizing (i.e.\\ caching and\nreusing) quadrature node positions and tangents as they are evaluated. We\ndemonstrate that our GWN method is robust to complex trimming geometry in a CAD\nmodel, and is accurate up to arbitrary precision at arbitrary distances from\nthe surface. The derived containment query is therefore robust to model\nnon-watertightness while respecting all curved features of the input shape."
                },
                "authors": [
                    {
                        "name": "Jacob Spainhour"
                    },
                    {
                        "name": "Kenneth Weiss"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth Weiss"
                },
                "author": "Kenneth Weiss",
                "arxiv_comment": "18 Pages, 16 Figures, 1 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11435v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11435v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68U05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.3.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00202v1",
                "updated": "2025-08-29T19:23:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    23,
                    35,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T19:23:35Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    23,
                    35,
                    4,
                    241,
                    0
                ],
                "title": "From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer\n  Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer\n  Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive\n  Inference"
                },
                "summary": "Although the Transformer has become the cornerstone of modern AI, its\nautoregressive inference suffers from a linearly growing KV Cache and a\ncomputational complexity of O(N^2 d), severely hindering its ability to process\nultra-long sequences. To overcome this limitation, this paper introduces the\nTConstFormer architecture, building upon our previous work, TLinFormer.\nTConstFormer employs an innovative periodic state update mechanism to achieve a\ntruly constant-size O(1) KV Cache. The computational complexity of this\nmechanism is also O(1) in an amortized sense: it performs purely constant-time\ncomputations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single\nlinear-time global information synchronization only on the $k$-th step.\nTheoretical calculations and experimental results demonstrate that TConstFormer\nexhibits an overwhelming advantage over baseline models in terms of speed,\nmemory efficiency, and overall performance on long-text inference tasks. This\nbreakthrough paves the way for efficient and robust streaming language model\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although the Transformer has become the cornerstone of modern AI, its\nautoregressive inference suffers from a linearly growing KV Cache and a\ncomputational complexity of O(N^2 d), severely hindering its ability to process\nultra-long sequences. To overcome this limitation, this paper introduces the\nTConstFormer architecture, building upon our previous work, TLinFormer.\nTConstFormer employs an innovative periodic state update mechanism to achieve a\ntruly constant-size O(1) KV Cache. The computational complexity of this\nmechanism is also O(1) in an amortized sense: it performs purely constant-time\ncomputations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single\nlinear-time global information synchronization only on the $k$-th step.\nTheoretical calculations and experimental results demonstrate that TConstFormer\nexhibits an overwhelming advantage over baseline models in terms of speed,\nmemory efficiency, and overall performance on long-text inference tasks. This\nbreakthrough paves the way for efficient and robust streaming language model\napplications."
                },
                "authors": [
                    {
                        "name": "Zhongpan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongpan Tang"
                },
                "author": "Zhongpan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00195v1",
                "updated": "2025-08-29T19:12:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    12,
                    4,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T19:12:04Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    12,
                    4,
                    4,
                    241,
                    0
                ],
                "title": "Democratizing Agentic AI with Fast Test-Time Scaling on the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Democratizing Agentic AI with Fast Test-Time Scaling on the Edge"
                },
                "summary": "Deploying agentic AI on edge devices is crucial for privacy and\nresponsiveness, but memory constraints typically relegate these systems to\nsmaller Large Language Models (LLMs) with inferior reasoning capabilities.\nTest-Time Scaling (TTS) can bridge this reasoning gap by dedicating more\ncompute during inference, but existing methods incur prohibitive overhead on\nedge hardware. To overcome this, we introduce FlashTTS, a serving system that\nmakes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces\nthree synergistic optimizations: (i) Speculative Beam Extension to mitigate\nsystem stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model\nMemory Allocation to dynamically balance memory between generation and\nverification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache\nreuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on\na single consumer GPU (24 GB) to match the accuracy and latency of large cloud\nmodels. Our evaluation demonstrates that FlashTTS achieves an average 2.2x\nhigher goodput and reduces latency by 38%-68% compared to a vLLM baseline,\npaving the way for democratized, high-performance agentic AI on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying agentic AI on edge devices is crucial for privacy and\nresponsiveness, but memory constraints typically relegate these systems to\nsmaller Large Language Models (LLMs) with inferior reasoning capabilities.\nTest-Time Scaling (TTS) can bridge this reasoning gap by dedicating more\ncompute during inference, but existing methods incur prohibitive overhead on\nedge hardware. To overcome this, we introduce FlashTTS, a serving system that\nmakes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces\nthree synergistic optimizations: (i) Speculative Beam Extension to mitigate\nsystem stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model\nMemory Allocation to dynamically balance memory between generation and\nverification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache\nreuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on\na single consumer GPU (24 GB) to match the accuracy and latency of large cloud\nmodels. Our evaluation demonstrates that FlashTTS achieves an average 2.2x\nhigher goodput and reduces latency by 38%-68% compared to a vLLM baseline,\npaving the way for democratized, high-performance agentic AI on edge devices."
                },
                "authors": [
                    {
                        "name": "Hao Mark Chen"
                    },
                    {
                        "name": "Zhiwen Mo"
                    },
                    {
                        "name": "Guanxi Lu"
                    },
                    {
                        "name": "Shuang Liang"
                    },
                    {
                        "name": "Lingxiao Ma"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "author": "Hongxiang Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16217v2",
                "updated": "2025-08-29T18:45:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    18,
                    45,
                    22,
                    4,
                    241,
                    0
                ],
                "published": "2025-07-22T04:21:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Towards Compute-Optimal Many-Shot In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Compute-Optimal Many-Shot In-Context Learning"
                },
                "summary": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL."
                },
                "authors": [
                    {
                        "name": "Shahriar Golchin"
                    },
                    {
                        "name": "Yanfei Chen"
                    },
                    {
                        "name": "Rujun Han"
                    },
                    {
                        "name": "Manan Gandhi"
                    },
                    {
                        "name": "Tianli Yu"
                    },
                    {
                        "name": "Swaroop Mishra"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Chen-Yu Lee"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "arxiv_comment": "Final version; accepted at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05930v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05930v2",
                "updated": "2025-08-29T09:58:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    58,
                    17,
                    4,
                    241,
                    0
                ],
                "published": "2025-06-06T09:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    9,
                    55,
                    59,
                    4,
                    157,
                    0
                ],
                "title": "Neural Visibility Cache for Real-Time Light Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Visibility Cache for Real-Time Light Sampling"
                },
                "summary": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR)."
                },
                "authors": [
                    {
                        "name": "Jakub Bokšanský"
                    },
                    {
                        "name": "Daniel Meister"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Meister"
                },
                "author": "Daniel Meister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05930v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05930v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15683v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15683v2",
                "updated": "2025-08-29T07:40:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    7,
                    40,
                    34,
                    4,
                    241,
                    0
                ],
                "published": "2025-05-21T15:58:08Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "title": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models"
                },
                "summary": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability."
                },
                "authors": [
                    {
                        "name": "Zishuai Zhang"
                    },
                    {
                        "name": "Hainan zhang"
                    },
                    {
                        "name": "Weihua Li"
                    },
                    {
                        "name": "Qinnan zhang"
                    },
                    {
                        "name": "jin Dong"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15683v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15683v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04467v1",
                "updated": "2025-08-29T02:29:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    29,
                    52,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T02:29:52Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    29,
                    52,
                    4,
                    241,
                    0
                ],
                "title": "Enhancing LLM Efficiency: Targeted Pruning for Prefill-Decode\n  Disaggregation in Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Efficiency: Targeted Pruning for Prefill-Decode\n  Disaggregation in Inference"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the default settings, our method\nachieves a 20.56% inference speedup and a 4.95 times reduction in data\ntransmission bandwidth consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the default settings, our method\nachieves a 20.56% inference speedup and a 4.95 times reduction in data\ntransmission bandwidth consumption."
                },
                "authors": [
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Mengsi Lyu"
                    },
                    {
                        "name": "Yulong Ao"
                    },
                    {
                        "name": "Yonghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yonghua Lin"
                },
                "author": "Yonghua Lin",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20865v1",
                "updated": "2025-08-28T14:58:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    47,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:58:47Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    47,
                    3,
                    240,
                    0
                ],
                "title": "Deep Multiple Quantization Network on Long Behavior Sequence for\n  Click-Through Rate Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Multiple Quantization Network on Long Behavior Sequence for\n  Click-Through Rate Prediction"
                },
                "summary": "In Click-Through Rate (CTR) prediction, the long behavior sequence,\ncomprising the user's long period of historical interactions with items has a\nvital influence on assessing the user's interest in the candidate item.\nExisting approaches strike efficiency and effectiveness through a two-stage\nparadigm: first retrieving hundreds of candidate-related items and then\nextracting interest intensity vector through target attention. However, we\nargue that the discrepancy in target attention's relevance distribution between\nthe retrieved items and the full long behavior sequence inevitably leads to a\nperformance decline. To alleviate the discrepancy, we propose the Deep Multiple\nQuantization Network (DMQN) to process long behavior sequence end-to-end\nthrough compressing the long behavior sequence. Firstly, the entire spectrum of\nlong behavior sequence will be quantized into multiple codeword sequences based\non multiple independent codebooks. Hierarchical Sequential Transduction Unit is\nincorporated to facilitate the interaction of reduced codeword sequences. Then,\nattention between the candidate and multiple codeword sequences will output the\ninterest vector. To enable online serving, intermediate representations of the\ncodeword sequences are cached, significantly reducing latency. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMQN. The A/B test in our advertising system shows that DMQN\nimproves CTR by 3.5% and RPM by 2.0%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Click-Through Rate (CTR) prediction, the long behavior sequence,\ncomprising the user's long period of historical interactions with items has a\nvital influence on assessing the user's interest in the candidate item.\nExisting approaches strike efficiency and effectiveness through a two-stage\nparadigm: first retrieving hundreds of candidate-related items and then\nextracting interest intensity vector through target attention. However, we\nargue that the discrepancy in target attention's relevance distribution between\nthe retrieved items and the full long behavior sequence inevitably leads to a\nperformance decline. To alleviate the discrepancy, we propose the Deep Multiple\nQuantization Network (DMQN) to process long behavior sequence end-to-end\nthrough compressing the long behavior sequence. Firstly, the entire spectrum of\nlong behavior sequence will be quantized into multiple codeword sequences based\non multiple independent codebooks. Hierarchical Sequential Transduction Unit is\nincorporated to facilitate the interaction of reduced codeword sequences. Then,\nattention between the candidate and multiple codeword sequences will output the\ninterest vector. To enable online serving, intermediate representations of the\ncodeword sequences are cached, significantly reducing latency. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMQN. The A/B test in our advertising system shows that DMQN\nimproves CTR by 3.5% and RPM by 2.0%."
                },
                "authors": [
                    {
                        "name": "Zhuoxing Wei"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Qingchen Xie"
                    }
                ],
                "author_detail": {
                    "name": "Qingchen Xie"
                },
                "author": "Qingchen Xie",
                "arxiv_doi": "10.1145/3726302.3730177",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730177",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.20865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, 1 figures, SIGIR 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18250v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18250v2",
                "updated": "2025-08-28T08:49:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    49,
                    24,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-25T17:41:13Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    13,
                    0,
                    237,
                    0
                ],
                "title": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study"
                },
                "summary": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM."
                },
                "authors": [
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Fernando García-Redondo"
                    },
                    {
                        "name": "Arvind Sharma"
                    },
                    {
                        "name": "Van Dai Nguyen"
                    },
                    {
                        "name": "Andrea Fantini"
                    },
                    {
                        "name": "Philippe Matagne"
                    },
                    {
                        "name": "Siddharth Rao"
                    },
                    {
                        "name": "Subhali Subhechha"
                    },
                    {
                        "name": "Lynn Verschueren"
                    },
                    {
                        "name": "Mohammed Aftab Baig"
                    },
                    {
                        "name": "Marie Garcia Bardon"
                    },
                    {
                        "name": "Geert Hellings"
                    }
                ],
                "author_detail": {
                    "name": "Geert Hellings"
                },
                "author": "Geert Hellings",
                "arxiv_comment": "Manuscript submitted to IEEE Trans. Elec. Dev. Work enabled in part\n  by NanoIC pilot line; acquisition and operation jointly funded by Chips Joint\n  Undertaking, through EU's Digital Europe (101183266) and Horizon Europe\n  programs (101183277), as well as by the participating states\n  (Belgium-Flanders, France, Germany, Finland, Ireland, Romania)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18250v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20524v1",
                "updated": "2025-08-28T08:05:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    5,
                    42,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T08:05:42Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    5,
                    42,
                    3,
                    240,
                    0
                ],
                "title": "Matrixed-Spectrum Decomposition Accelerated Linear Boltzmann Transport\n  Equation Solver for Fast Scatter Correction in Multi-Spectral CT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrixed-Spectrum Decomposition Accelerated Linear Boltzmann Transport\n  Equation Solver for Fast Scatter Correction in Multi-Spectral CT"
                },
                "summary": "X-ray scatter has been a serious concern in computed tomography (CT), leading\nto image artifacts and distortion of CT values. The linear Boltzmann transport\nequation (LBTE) is recognized as a fast and accurate approach for scatter\nestimation. However, for multi-spectral CT, it is cumbersome to compute\nmultiple scattering components for different spectra separately when applying\nLBTE-based scatter correction. In this work, we propose a Matrixed-Spectrum\nDecomposition accelerated LBTE solver (MSD-LBTE) that can be used to compute\nX-ray scatter distributions from CT acquisitions at two or more different\nspectra simultaneously, in a unified framework with no sacrifice in accuracy\nand nearly no increase in computation in theory. First, a matrixed-spectrum\nsolver of LBTE is obtained by introducing an additional label dimension to\nexpand the phase space. Then, we propose a ``spectrum basis'' for LBTE and a\nprinciple of selection of basis using the QR decomposition, along with the\nabove solver to construct the MSD-LBTE. Based on MSD-LBTE, a unified scatter\ncorrection method can be established for multi-spectral CT. We validate the\neffectiveness and accuracy of our method by comparing it with the Monte Carlo\nmethod, including the computational time. We also evaluate the scatter\ncorrection performance using two different phantoms for fast-kV switching based\ndual-energy CT, and using an elliptical phantom in a numerical simulation for\nkV-modulation enabled CT scans, validating that our proposed method can\nsignificantly reduce the computational cost at multiple spectra and effectively\nreduce scatter artifact in reconstructed CT images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-ray scatter has been a serious concern in computed tomography (CT), leading\nto image artifacts and distortion of CT values. The linear Boltzmann transport\nequation (LBTE) is recognized as a fast and accurate approach for scatter\nestimation. However, for multi-spectral CT, it is cumbersome to compute\nmultiple scattering components for different spectra separately when applying\nLBTE-based scatter correction. In this work, we propose a Matrixed-Spectrum\nDecomposition accelerated LBTE solver (MSD-LBTE) that can be used to compute\nX-ray scatter distributions from CT acquisitions at two or more different\nspectra simultaneously, in a unified framework with no sacrifice in accuracy\nand nearly no increase in computation in theory. First, a matrixed-spectrum\nsolver of LBTE is obtained by introducing an additional label dimension to\nexpand the phase space. Then, we propose a ``spectrum basis'' for LBTE and a\nprinciple of selection of basis using the QR decomposition, along with the\nabove solver to construct the MSD-LBTE. Based on MSD-LBTE, a unified scatter\ncorrection method can be established for multi-spectral CT. We validate the\neffectiveness and accuracy of our method by comparing it with the Monte Carlo\nmethod, including the computational time. We also evaluate the scatter\ncorrection performance using two different phantoms for fast-kV switching based\ndual-energy CT, and using an elliptical phantom in a numerical simulation for\nkV-modulation enabled CT scans, validating that our proposed method can\nsignificantly reduce the computational cost at multiple spectra and effectively\nreduce scatter artifact in reconstructed CT images."
                },
                "authors": [
                    {
                        "name": "Guoxi Zhu"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Zhiqiang Chen"
                    },
                    {
                        "name": "Hewei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Hewei Gao"
                },
                "author": "Hewei Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20433v1",
                "updated": "2025-08-28T05:22:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    5,
                    22,
                    25,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T05:22:25Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    5,
                    22,
                    25,
                    3,
                    240,
                    0
                ],
                "title": "MegaCacheX: Towards Cost-Effective Hierarchical Collaborative Content\n  Caching in Emerging Mega-Constellations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaCacheX: Towards Cost-Effective Hierarchical Collaborative Content\n  Caching in Emerging Mega-Constellations"
                },
                "summary": "Significant latency in global content delivery primarily arises from\ninsufficient terrestrial infrastructure. Deploying space-based content delivery\nnetworks within emerging mega-constellations provides an effective means to\nbridge the digital divide. However, space-based caching faces constraints from\nphysical-layer dynamics, including dynamic topologies, time-varying\ninter-satellite link conditions, and limited onboard energy. In addition,\nexisting mechanisms often lack fine-grained content categorization and global\noptimization. This paper proposes MegaCacheX, a cost-effective hierarchical\nframework for collaborative content distribution that achieves\n\"Earth-independence\" by providing cloud services directly from space.\nSpecifically, data centers in Sun-synchronous orbit act as primary content\nsources, while caching nodes in mega-constellations and ground stations\ncollaboratively form a distributed edge layer. MegaCacheX optimizes caching\nstrategies by integrating content popularity, regional user distribution, and\nsatellite trajectory predictions. Multi-tier caching nodes serve as service\nanchors, enabling seamless content delivery with low latency. A prototype\nimplemented on a microservices-based, containerized testbed demonstrates that\nMegaCacheX reduces global content access latency by about 36% compared to\nbaseline approaches, while maintaining cost efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant latency in global content delivery primarily arises from\ninsufficient terrestrial infrastructure. Deploying space-based content delivery\nnetworks within emerging mega-constellations provides an effective means to\nbridge the digital divide. However, space-based caching faces constraints from\nphysical-layer dynamics, including dynamic topologies, time-varying\ninter-satellite link conditions, and limited onboard energy. In addition,\nexisting mechanisms often lack fine-grained content categorization and global\noptimization. This paper proposes MegaCacheX, a cost-effective hierarchical\nframework for collaborative content distribution that achieves\n\"Earth-independence\" by providing cloud services directly from space.\nSpecifically, data centers in Sun-synchronous orbit act as primary content\nsources, while caching nodes in mega-constellations and ground stations\ncollaboratively form a distributed edge layer. MegaCacheX optimizes caching\nstrategies by integrating content popularity, regional user distribution, and\nsatellite trajectory predictions. Multi-tier caching nodes serve as service\nanchors, enabling seamless content delivery with low latency. A prototype\nimplemented on a microservices-based, containerized testbed demonstrates that\nMegaCacheX reduces global content access latency by about 36% compared to\nbaseline approaches, while maintaining cost efficiency."
                },
                "authors": [
                    {
                        "name": "Haoyang Shi"
                    },
                    {
                        "name": "Xing Zhang"
                    },
                    {
                        "name": "Sitong Li"
                    },
                    {
                        "name": "Minghang Li"
                    },
                    {
                        "name": "Xinming Lu"
                    },
                    {
                        "name": "Shaoxiang Xu"
                    },
                    {
                        "name": "Guoquan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guoquan Wang"
                },
                "author": "Guoquan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20424v1",
                "updated": "2025-08-28T04:46:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    46,
                    44,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T04:46:44Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    46,
                    44,
                    3,
                    240,
                    0
                ],
                "title": "Breaking Diffusion with Cache: Exploiting Approximate Caches in\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Diffusion with Cache: Exploiting Approximate Caches in\n  Diffusion Models"
                },
                "summary": "Diffusion models are a powerful class of generative models that produce\ncontent, such as images, from user prompts, but they are computationally\nintensive. To mitigate this cost, recent academic and industry work has adopted\napproximate caching, which reuses intermediate states from similar prompts in a\ncache. While efficient, this optimization introduces new security risks by\nbreaking isolation among users. This work aims to comprehensively assess new\nsecurity vulnerabilities arising from approximate caching. First, we\ndemonstrate a remote covert channel established with the cache, where a sender\ninjects prompts with special keywords into the cache and a receiver can recover\nthat even after days, to exchange information. Second, we introduce a prompt\nstealing attack using the cache, where an attacker can recover existing cached\nprompts based on cache hit prompts. Finally, we introduce a poisoning attack\nthat embeds the attacker's logos into the previously stolen prompt, to render\nthem in future user prompts that hit the cache. These attacks are all performed\nremotely through the serving system, which indicates severe security\nvulnerabilities in approximate caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models are a powerful class of generative models that produce\ncontent, such as images, from user prompts, but they are computationally\nintensive. To mitigate this cost, recent academic and industry work has adopted\napproximate caching, which reuses intermediate states from similar prompts in a\ncache. While efficient, this optimization introduces new security risks by\nbreaking isolation among users. This work aims to comprehensively assess new\nsecurity vulnerabilities arising from approximate caching. First, we\ndemonstrate a remote covert channel established with the cache, where a sender\ninjects prompts with special keywords into the cache and a receiver can recover\nthat even after days, to exchange information. Second, we introduce a prompt\nstealing attack using the cache, where an attacker can recover existing cached\nprompts based on cache hit prompts. Finally, we introduce a poisoning attack\nthat embeds the attacker's logos into the previously stolen prompt, to render\nthem in future user prompts that hit the cache. These attacks are all performed\nremotely through the serving system, which indicates severe security\nvulnerabilities in approximate caching."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Shuncheng Jie"
                    },
                    {
                        "name": "Sihang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sihang Liu"
                },
                "author": "Sihang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20407v1",
                "updated": "2025-08-28T04:10:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    10,
                    19,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T04:10:19Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    10,
                    19,
                    3,
                    240,
                    0
                ],
                "title": "Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full\n  Context-Aware Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full\n  Context-Aware Linear Attention"
                },
                "summary": "The Transformer architecture has become a cornerstone of modern artificial\nintelligence, but its core self-attention mechanism suffers from a complexity\nbottleneck that scales quadratically with sequence length, severely limiting\nits application in long-sequence tasks. To address this challenge, existing\nlinear attention methods typically sacrifice model performance by relying on\ndata-agnostic kernel approximations or restrictive context selection. This\npaper returns to the first principles of connectionism, starting from the\ntopological structure of information flow, to introduce a novel linear\nattention architecture-\\textbf{TLinFormer}. By reconfiguring neuron connection\npatterns, TLinFormer achieves strict linear complexity while computing exact\nattention scores and ensuring information flow remains aware of the full\nhistorical context. This design aims to bridge the performance gap prevalent\nbetween existing efficient attention methods and standard attention. Through a\nseries of experiments, we systematically evaluate the performance of TLinFormer\nagainst a standard Transformer baseline on long-sequence inference tasks. The\nresults demonstrate that TLinFormer exhibits overwhelming advantages in key\nmetrics such as \\textbf{inference latency}, \\textbf{KV cache efficiency},\n\\textbf{memory footprint}, and \\textbf{overall speedup}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Transformer architecture has become a cornerstone of modern artificial\nintelligence, but its core self-attention mechanism suffers from a complexity\nbottleneck that scales quadratically with sequence length, severely limiting\nits application in long-sequence tasks. To address this challenge, existing\nlinear attention methods typically sacrifice model performance by relying on\ndata-agnostic kernel approximations or restrictive context selection. This\npaper returns to the first principles of connectionism, starting from the\ntopological structure of information flow, to introduce a novel linear\nattention architecture-\\textbf{TLinFormer}. By reconfiguring neuron connection\npatterns, TLinFormer achieves strict linear complexity while computing exact\nattention scores and ensuring information flow remains aware of the full\nhistorical context. This design aims to bridge the performance gap prevalent\nbetween existing efficient attention methods and standard attention. Through a\nseries of experiments, we systematically evaluate the performance of TLinFormer\nagainst a standard Transformer baseline on long-sequence inference tasks. The\nresults demonstrate that TLinFormer exhibits overwhelming advantages in key\nmetrics such as \\textbf{inference latency}, \\textbf{KV cache efficiency},\n\\textbf{memory footprint}, and \\textbf{overall speedup}."
                },
                "authors": [
                    {
                        "name": "Zhongpan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongpan Tang"
                },
                "author": "Zhongpan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v5",
                "updated": "2025-08-28T03:57:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    3,
                    57,
                    52,
                    3,
                    240,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from (1) the distribution variance in the LLM activations and (2) the\nsensitivity difference among various kinds of layers. To address these issues,\nwe propose a training-free approach called Activation-aware Singular Value\nDecomposition (ASVD). Specifically, ASVD manages activation outliers by\ntransforming the weight matrix based on the activation distribution. This\ntransformation allows the outliers in the activation matrix to be absorbed into\nthe transformed weight matrix, thereby enhancing decomposition accuracy.\nAdditionally, we propose an efficient iterative calibration process to optimize\nlayer-specific decomposition by addressing the varying sensitivity of different\nLLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the\nsuccess of the low-rank decomposition of projection matrices in the\nself-attention module, we further introduce ASVD to compress the KV cache. By\nreducing the channel dimension of KV activations, memory requirements for KV\ncache can be largely reduced. ASVD can further achieve 50% KV cache reductions\nwithout performance drop in a training-free manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from (1) the distribution variance in the LLM activations and (2) the\nsensitivity difference among various kinds of layers. To address these issues,\nwe propose a training-free approach called Activation-aware Singular Value\nDecomposition (ASVD). Specifically, ASVD manages activation outliers by\ntransforming the weight matrix based on the activation distribution. This\ntransformation allows the outliers in the activation matrix to be absorbed into\nthe transformed weight matrix, thereby enhancing decomposition accuracy.\nAdditionally, we propose an efficient iterative calibration process to optimize\nlayer-specific decomposition by addressing the varying sensitivity of different\nLLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the\nsuccess of the low-rank decomposition of projection matrices in the\nself-attention module, we further introduce ASVD to compress the KV cache. By\nreducing the channel dimension of KV activations, memory requirements for KV\ncache can be largely reduced. ASVD can further achieve 50% KV cache reductions\nwithout performance drop in a training-free manner."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09888v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09888v2",
                "updated": "2025-08-28T01:40:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    1,
                    40,
                    30,
                    3,
                    240,
                    0
                ],
                "published": "2025-02-14T03:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "title": "Climber: Toward Efficient Scaling Laws for Large Recommendation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Climber: Toward Efficient Scaling Laws for Large Recommendation Models"
                },
                "summary": "Transformer-based generative models have achieved remarkable success across\ndomains with various scaling law manifestations. However, our extensive\nexperiments reveal persistent challenges when applying Transformer to\nrecommendation systems: (1) Transformer scaling is not ideal with increased\ncomputational resources, due to structural incompatibilities with\nrecommendation-specific features such as multi-source data heterogeneity; (2)\ncritical online inference latency constraints (tens of milliseconds) that\nintensify with longer user behavior sequences and growing computational\ndemands. We propose Climber, an efficient recommendation framework comprising\ntwo synergistic components: the model architecture for efficient scaling and\nthe co-designed acceleration techniques. Our proposed model adopts two core\ninnovations: (1) multi-scale sequence extraction that achieves a time\ncomplexity reduction by a constant factor, enabling more efficient scaling with\nsequence length; (2) dynamic temperature modulation adapting attention\ndistributions to the multi-scenario and multi-behavior patterns. Complemented\nby acceleration techniques, Climber achieves a 5.15$\\times$ throughput gain\nwithout performance degradation by adopting a \"single user, multiple item\"\nbatched processing and memory-efficient Key-Value caching. Comprehensive\noffline experiments on multiple datasets validate that Climber exhibits a more\nideal scaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19\\% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based generative models have achieved remarkable success across\ndomains with various scaling law manifestations. However, our extensive\nexperiments reveal persistent challenges when applying Transformer to\nrecommendation systems: (1) Transformer scaling is not ideal with increased\ncomputational resources, due to structural incompatibilities with\nrecommendation-specific features such as multi-source data heterogeneity; (2)\ncritical online inference latency constraints (tens of milliseconds) that\nintensify with longer user behavior sequences and growing computational\ndemands. We propose Climber, an efficient recommendation framework comprising\ntwo synergistic components: the model architecture for efficient scaling and\nthe co-designed acceleration techniques. Our proposed model adopts two core\ninnovations: (1) multi-scale sequence extraction that achieves a time\ncomplexity reduction by a constant factor, enabling more efficient scaling with\nsequence length; (2) dynamic temperature modulation adapting attention\ndistributions to the multi-scenario and multi-behavior patterns. Complemented\nby acceleration techniques, Climber achieves a 5.15$\\times$ throughput gain\nwithout performance degradation by adopting a \"single user, multiple item\"\nbatched processing and memory-efficient Key-Value caching. Comprehensive\noffline experiments on multiple datasets validate that Climber exhibits a more\nideal scaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19\\% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily."
                },
                "authors": [
                    {
                        "name": "Songpei Xu"
                    },
                    {
                        "name": "Shijia Wang"
                    },
                    {
                        "name": "Da Guo"
                    },
                    {
                        "name": "Xianwen Guo"
                    },
                    {
                        "name": "Qiang Xiao"
                    },
                    {
                        "name": "Bin Huang"
                    },
                    {
                        "name": "Guanlin Wu"
                    },
                    {
                        "name": "Chuanjiang Luo"
                    }
                ],
                "author_detail": {
                    "name": "Chuanjiang Luo"
                },
                "author": "Chuanjiang Luo",
                "arxiv_doi": "10.1145/3746252.3761561",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761561",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09888v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09888v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00105v1",
                "updated": "2025-08-28T00:46:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    0,
                    46,
                    51,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T00:46:51Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    0,
                    46,
                    51,
                    3,
                    240,
                    0
                ],
                "title": "AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and\n  High-Quality Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and\n  High-Quality Language Model Serving"
                },
                "summary": "Large language model (LLM) applications often reuse previously processed\ncontext, such as chat history and documents, which introduces significant\nredundant computation. Existing LLM serving systems address such redundant\ncomputation by storing the KV caches of processed context and loading the\ncorresponding KV cache when a new request reuses the context. Further, as these\nLLM applications scale, the total size of KV caches becomes excessively large\nand requires both DRAM and SSD for full storage.\n  However, prior work that stores KV caches in DRAM and SSD suffers from high\nloading delays, as most KV cache hits come from SSD, which is slow to load. To\nincrease the KV cache hit rate on DRAM, we identify lossy KV cache compression\nas a promising approach. We design a lossy compression system that decides the\ncompression algorithm, compression rate and device placement for each KV cache\nentry to maximise DRAM hits and minimise loading delay without significantly\ndegrading generation quality. Compared to various static compression baselines\nacross three tasks, our system AdaptCache achieves 1.43--2.4 x delay savings at\nthe same quality and 6--55% quality improvements at the same delay.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) applications often reuse previously processed\ncontext, such as chat history and documents, which introduces significant\nredundant computation. Existing LLM serving systems address such redundant\ncomputation by storing the KV caches of processed context and loading the\ncorresponding KV cache when a new request reuses the context. Further, as these\nLLM applications scale, the total size of KV caches becomes excessively large\nand requires both DRAM and SSD for full storage.\n  However, prior work that stores KV caches in DRAM and SSD suffers from high\nloading delays, as most KV cache hits come from SSD, which is slow to load. To\nincrease the KV cache hit rate on DRAM, we identify lossy KV cache compression\nas a promising approach. We design a lossy compression system that decides the\ncompression algorithm, compression rate and device placement for each KV cache\nentry to maximise DRAM hits and minimise loading delay without significantly\ndegrading generation quality. Compared to various static compression baselines\nacross three tasks, our system AdaptCache achieves 1.43--2.4 x delay savings at\nthe same quality and 6--55% quality improvements at the same delay."
                },
                "authors": [
                    {
                        "name": "Shaoting Feng"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Samuel Shen"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Ganesh Ananthanarayanan"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20272v1",
                "updated": "2025-08-27T21:05:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    21,
                    5,
                    5,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T21:05:05Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    21,
                    5,
                    5,
                    2,
                    239,
                    0
                ],
                "title": "DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource\n  Allocation and Markov Decision Process in Named Data Networking (NDN)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource\n  Allocation and Markov Decision Process in Named Data Networking (NDN)"
                },
                "summary": "Named Data Networking (NDN) represents a transformative shift in network\narchitecture, prioritizing content names over host addresses to enhance data\ndissemination. Efficient queue and resource management are critical to NDN\nperformance, especially under dynamic and high-traffic conditions. This paper\nintroduces DRR-MDPF, a novel hybrid strategy that integrates the Markov\nDecision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)\nalgorithm. MDPF enables routers to intelligently predict optimal forwarding\ndecisions based on key metrics such as bandwidth, delay, and the number of\nunsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation\namong competing data flows. The proposed method models each router as a\nlearning agent capable of adjusting its strategies through continuous feedback\nand probabilistic updates. Simulation results using ndnSIM demonstrate that\nDRR-MDPF significantly outperforms state-of-the-art strategies including SAF,\nRFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest\nSatisfaction Rate (ISR), packet drop rate, content retrieval time, and load\nbalancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and\nheavy traffic, offering enhanced adaptability and lower computational\ncomplexity due to its single-path routing design. Furthermore, its multi-metric\ndecision-making capability enables more accurate interface selection, leading\nto optimized network performance. Overall, DRR-MDPF serves as an intelligent,\nadaptive, and scalable queue management solution for NDN, effectively\naddressing core challenges such as resource allocation, congestion control, and\nroute optimization in dynamic networking environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Data Networking (NDN) represents a transformative shift in network\narchitecture, prioritizing content names over host addresses to enhance data\ndissemination. Efficient queue and resource management are critical to NDN\nperformance, especially under dynamic and high-traffic conditions. This paper\nintroduces DRR-MDPF, a novel hybrid strategy that integrates the Markov\nDecision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)\nalgorithm. MDPF enables routers to intelligently predict optimal forwarding\ndecisions based on key metrics such as bandwidth, delay, and the number of\nunsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation\namong competing data flows. The proposed method models each router as a\nlearning agent capable of adjusting its strategies through continuous feedback\nand probabilistic updates. Simulation results using ndnSIM demonstrate that\nDRR-MDPF significantly outperforms state-of-the-art strategies including SAF,\nRFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest\nSatisfaction Rate (ISR), packet drop rate, content retrieval time, and load\nbalancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and\nheavy traffic, offering enhanced adaptability and lower computational\ncomplexity due to its single-path routing design. Furthermore, its multi-metric\ndecision-making capability enables more accurate interface selection, leading\nto optimized network performance. Overall, DRR-MDPF serves as an intelligent,\nadaptive, and scalable queue management solution for NDN, effectively\naddressing core challenges such as resource allocation, congestion control, and\nroute optimization in dynamic networking environments."
                },
                "authors": [
                    {
                        "name": "Fatemeh Roshanzadeh"
                    },
                    {
                        "name": "Hamid Barati"
                    },
                    {
                        "name": "Ali Barati"
                    }
                ],
                "author_detail": {
                    "name": "Ali Barati"
                },
                "author": "Ali Barati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20253v1",
                "updated": "2025-08-27T20:18:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    20,
                    18,
                    37,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T20:18:37Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    20,
                    18,
                    37,
                    2,
                    239,
                    0
                ],
                "title": "SpeedMalloc: Improving Multi-threaded Applications via a Lightweight\n  Core for Memory Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeedMalloc: Improving Multi-threaded Applications via a Lightweight\n  Core for Memory Allocation"
                },
                "summary": "Memory allocation, though constituting only a small portion of the executed\ncode, can have a \"butterfly effect\" on overall program performance, leading to\nsignificant and far-reaching impacts. Despite accounting for just approximately\n5% of total instructions, memory allocation can result in up to a 2.7x\nperformance variation depending on the allocator used. This effect arises from\nthe complexity of memory allocation in modern multi-threaded multi-core\nsystems, where allocator metadata becomes intertwined with user data, leading\nto cache pollution or increased cross-thread synchronization overhead.\nOffloading memory allocators to accelerators, e.g., Mallacc and Memento, is a\npotential direction to improve the allocator performance and mitigate cache\npollution. However, these accelerators currently have limited support for\nmulti-threaded applications, and synchronization between cores and accelerators\nremains a significant challenge.\n  We present SpeedMalloc, using a lightweight support-core to process memory\nallocation tasks in multi-threaded applications. The support-core is a\nlightweight programmable processor with efficient cross-core data\nsynchronization and houses all allocator metadata in its own caches. This\ndesign minimizes cache conflicts with user data and eliminates the need for\ncross-core metadata synchronization. In addition, using a general-purpose core\ninstead of domain-specific accelerators makes SpeedMalloc capable of adopting\nnew allocator designs. We compare SpeedMalloc with state-of-the-art software\nand hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and\nMemento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on\nmultithreaded workloads over these five allocators, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory allocation, though constituting only a small portion of the executed\ncode, can have a \"butterfly effect\" on overall program performance, leading to\nsignificant and far-reaching impacts. Despite accounting for just approximately\n5% of total instructions, memory allocation can result in up to a 2.7x\nperformance variation depending on the allocator used. This effect arises from\nthe complexity of memory allocation in modern multi-threaded multi-core\nsystems, where allocator metadata becomes intertwined with user data, leading\nto cache pollution or increased cross-thread synchronization overhead.\nOffloading memory allocators to accelerators, e.g., Mallacc and Memento, is a\npotential direction to improve the allocator performance and mitigate cache\npollution. However, these accelerators currently have limited support for\nmulti-threaded applications, and synchronization between cores and accelerators\nremains a significant challenge.\n  We present SpeedMalloc, using a lightweight support-core to process memory\nallocation tasks in multi-threaded applications. The support-core is a\nlightweight programmable processor with efficient cross-core data\nsynchronization and houses all allocator metadata in its own caches. This\ndesign minimizes cache conflicts with user data and eliminates the need for\ncross-core metadata synchronization. In addition, using a general-purpose core\ninstead of domain-specific accelerators makes SpeedMalloc capable of adopting\nnew allocator designs. We compare SpeedMalloc with state-of-the-art software\nand hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and\nMemento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on\nmultithreaded workloads over these five allocators, respectively."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Qinzhe Wu"
                    },
                    {
                        "name": "Krishna Kavi"
                    },
                    {
                        "name": "Gayatri Mehta"
                    },
                    {
                        "name": "Jonathan C. Beard"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    },
                    {
                        "name": "Lizy K. John"
                    }
                ],
                "author_detail": {
                    "name": "Lizy K. John"
                },
                "author": "Lizy K. John",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00100v1",
                "updated": "2025-08-27T17:45:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    17,
                    45,
                    16,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T17:45:16Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    17,
                    45,
                    16,
                    2,
                    239,
                    0
                ],
                "title": "MODE: Mixture of Document Experts for RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MODE: Mixture of Document Experts for RAG"
                },
                "summary": "Retrieval-Augmented Generation (RAG) often relies on large vector databases\nand cross-encoders tuned for large-scale corpora, which can be excessive for\nsmall, domain-specific collections. We present MODE (Mixture of Document\nExperts), a lightweight alternative that replaces fine-grained nearest-neighbor\nsearch with cluster-and-route retrieval. Documents are embedded, grouped into\nsemantically coherent clusters, and represented by cached centroids. At query\ntime, we route to the top centroid(s) and retrieve context only within those\nclusters, eliminating external vector-database infrastructure and reranking\nwhile keeping latency low. On HotpotQA and SQuAD corpora with 100-500 chunks,\nMODE matches or exceeds a dense-retrieval baseline in answer quality while\nreducing end-to-end retrieval time. Ablations show that cluster granularity and\nmulti-cluster routing control the recall/precision trade-off, and that tighter\nclusters improve downstream accuracy. MODE offers a practical recipe for small\nand medium corpora where simplicity, speed, and topical focus matter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) often relies on large vector databases\nand cross-encoders tuned for large-scale corpora, which can be excessive for\nsmall, domain-specific collections. We present MODE (Mixture of Document\nExperts), a lightweight alternative that replaces fine-grained nearest-neighbor\nsearch with cluster-and-route retrieval. Documents are embedded, grouped into\nsemantically coherent clusters, and represented by cached centroids. At query\ntime, we route to the top centroid(s) and retrieve context only within those\nclusters, eliminating external vector-database infrastructure and reranking\nwhile keeping latency low. On HotpotQA and SQuAD corpora with 100-500 chunks,\nMODE matches or exceeds a dense-retrieval baseline in answer quality while\nreducing end-to-end retrieval time. Ablations show that cluster granularity and\nmulti-cluster routing control the recall/precision trade-off, and that tighter\nclusters improve downstream accuracy. MODE offers a practical recipe for small\nand medium corpora where simplicity, speed, and topical focus matter."
                },
                "authors": [
                    {
                        "name": "Rahul Anand"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Anand"
                },
                "author": "Rahul Anand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13575v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13575v3",
                "updated": "2025-08-27T16:34:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    16,
                    34,
                    47,
                    2,
                    239,
                    0
                ],
                "published": "2025-07-17T23:37:19Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "title": "Apple Intelligence Foundation Language Models: Tech Report 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apple Intelligence Foundation Language Models: Tech Report 2025"
                },
                "summary": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute."
                },
                "authors": [
                    {
                        "name": "Ethan Li"
                    },
                    {
                        "name": "Anders Boesen Lindbo Larsen"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Xiyou Zhou"
                    },
                    {
                        "name": "Jun Qin"
                    },
                    {
                        "name": "Dian Ang Yap"
                    },
                    {
                        "name": "Narendran Raghavan"
                    },
                    {
                        "name": "Xuankai Chang"
                    },
                    {
                        "name": "Margit Bowler"
                    },
                    {
                        "name": "Eray Yildiz"
                    },
                    {
                        "name": "John Peebles"
                    },
                    {
                        "name": "Hannah Gillis Coleman"
                    },
                    {
                        "name": "Matteo Ronchi"
                    },
                    {
                        "name": "Peter Gray"
                    },
                    {
                        "name": "Keen You"
                    },
                    {
                        "name": "Anthony Spalvieri-Kruse"
                    },
                    {
                        "name": "Ruoming Pang"
                    },
                    {
                        "name": "Reed Li"
                    },
                    {
                        "name": "Yuli Yang"
                    },
                    {
                        "name": "Emad Soroush"
                    },
                    {
                        "name": "Zhiyun Lu"
                    },
                    {
                        "name": "Crystal Xiao"
                    },
                    {
                        "name": "Rong Situ"
                    },
                    {
                        "name": "Jordan Huffaker"
                    },
                    {
                        "name": "David Griffiths"
                    },
                    {
                        "name": "Zaid Ahmed"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Daniel Parilla"
                    },
                    {
                        "name": "Asaf Liberman"
                    },
                    {
                        "name": "Jennifer Mallalieu"
                    },
                    {
                        "name": "Parsa Mazaheri"
                    },
                    {
                        "name": "Qibin Chen"
                    },
                    {
                        "name": "Manjot Bilkhu"
                    },
                    {
                        "name": "Aonan Zhang"
                    },
                    {
                        "name": "Eric Wang"
                    },
                    {
                        "name": "Dave Nelson"
                    },
                    {
                        "name": "Michael FitzMaurice"
                    },
                    {
                        "name": "Thomas Voice"
                    },
                    {
                        "name": "Jeremy Liu"
                    },
                    {
                        "name": "Josh Shaffer"
                    },
                    {
                        "name": "Shiwen Zhao"
                    },
                    {
                        "name": "Prasanth Yadla"
                    },
                    {
                        "name": "Farzin Rasteh"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Arsalan Farooq"
                    },
                    {
                        "name": "Jeremy Snow"
                    },
                    {
                        "name": "Stephen Murphy"
                    },
                    {
                        "name": "Tao Lei"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "George Horrell"
                    },
                    {
                        "name": "Sam Dodge"
                    },
                    {
                        "name": "Lindsay Hislop"
                    },
                    {
                        "name": "Sumeet Singh"
                    },
                    {
                        "name": "Alex Dombrowski"
                    },
                    {
                        "name": "Aiswarya Raghavan"
                    },
                    {
                        "name": "Sasha Sirovica"
                    },
                    {
                        "name": "Mandana Saebi"
                    },
                    {
                        "name": "Faye Lao"
                    },
                    {
                        "name": "Max Lam"
                    },
                    {
                        "name": "TJ Lu"
                    },
                    {
                        "name": "Zhaoyang Xu"
                    },
                    {
                        "name": "Karanjeet Singh"
                    },
                    {
                        "name": "Marc Kirchner"
                    },
                    {
                        "name": "David Mizrahi"
                    },
                    {
                        "name": "Rajat Arora"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Henry Mason"
                    },
                    {
                        "name": "Lawrence Zhou"
                    },
                    {
                        "name": "Yi Hua"
                    },
                    {
                        "name": "Ankur Jain"
                    },
                    {
                        "name": "Felix Bai"
                    },
                    {
                        "name": "Joseph Astrauskas"
                    },
                    {
                        "name": "Floris Weers"
                    },
                    {
                        "name": "Josh Gardner"
                    },
                    {
                        "name": "Mira Chiang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Tony Sun"
                    },
                    {
                        "name": "Quentin Keunebroek"
                    },
                    {
                        "name": "Matthew Hopkins"
                    },
                    {
                        "name": "Bugu Wu"
                    },
                    {
                        "name": "Tao Jia"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Nanzhu Wang"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Ruixuan Hou"
                    },
                    {
                        "name": "Rene Rauch"
                    },
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Jonathan Janke"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Cha Chen"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Feng Nan"
                    },
                    {
                        "name": "Josh Elman"
                    },
                    {
                        "name": "Dong Yin"
                    },
                    {
                        "name": "Yusuf Goren"
                    },
                    {
                        "name": "Jeff Lai"
                    },
                    {
                        "name": "Yiran Fei"
                    },
                    {
                        "name": "Syd Evans"
                    },
                    {
                        "name": "Muyang Yu"
                    },
                    {
                        "name": "Guoli Yin"
                    },
                    {
                        "name": "Yi Qin"
                    },
                    {
                        "name": "Erin Feldman"
                    },
                    {
                        "name": "Isha Garg"
                    },
                    {
                        "name": "Aparna Rajamani"
                    },
                    {
                        "name": "Karla Vega"
                    },
                    {
                        "name": "Walker Cheng"
                    },
                    {
                        "name": "TJ Collins"
                    },
                    {
                        "name": "Hans Han"
                    },
                    {
                        "name": "Raul Rea Menacho"
                    },
                    {
                        "name": "Simon Yeung"
                    },
                    {
                        "name": "Sophy Lee"
                    },
                    {
                        "name": "Phani Mutyala"
                    },
                    {
                        "name": "Ying-Chang Cheng"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Sprite Chu"
                    },
                    {
                        "name": "Justin Lazarow"
                    },
                    {
                        "name": "Alessandro Pappalardo"
                    },
                    {
                        "name": "Federico Scozzafava"
                    },
                    {
                        "name": "Jing Lu"
                    },
                    {
                        "name": "Erik Daxberger"
                    },
                    {
                        "name": "Laurent Duchesne"
                    },
                    {
                        "name": "Jen Liu"
                    },
                    {
                        "name": "David Güera"
                    },
                    {
                        "name": "Stefano Ligas"
                    },
                    {
                        "name": "Mary Beth Kery"
                    },
                    {
                        "name": "Brent Ramerth"
                    },
                    {
                        "name": "Ciro Sannino"
                    },
                    {
                        "name": "Marcin Eichner"
                    },
                    {
                        "name": "Haoshuo Huang"
                    },
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Moritz Schwarzer-Becker"
                    },
                    {
                        "name": "David Riazati"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Bailin Wang"
                    },
                    {
                        "name": "Jack Cackler"
                    },
                    {
                        "name": "Yang Lu"
                    },
                    {
                        "name": "Ransen Niu"
                    },
                    {
                        "name": "John Dennison"
                    },
                    {
                        "name": "Guillaume Klein"
                    },
                    {
                        "name": "Jeffrey Bigham"
                    },
                    {
                        "name": "Deepak Gopinath"
                    },
                    {
                        "name": "Navid Shiee"
                    },
                    {
                        "name": "Darren Botten"
                    },
                    {
                        "name": "Guillaume Tartavel"
                    },
                    {
                        "name": "Alex Guillen Garcia"
                    },
                    {
                        "name": "Sam Xu"
                    },
                    {
                        "name": "Victoria MönchJuan Haladjian"
                    },
                    {
                        "name": "Zi-Yi Dou"
                    },
                    {
                        "name": "Matthias Paulik"
                    },
                    {
                        "name": "Adolfo Lopez Mendez"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Hong-You Chen"
                    },
                    {
                        "name": "Chao Jia"
                    },
                    {
                        "name": "Dhaval Doshi"
                    },
                    {
                        "name": "Zhengdong Zhang"
                    },
                    {
                        "name": "Raunak Manjani"
                    },
                    {
                        "name": "Aaron Franklin"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "David Chen"
                    },
                    {
                        "name": "Artsiom Peshko"
                    },
                    {
                        "name": "Nandhitha Raghuram"
                    },
                    {
                        "name": "Hans Hao"
                    },
                    {
                        "name": "Jiulong Shan"
                    },
                    {
                        "name": "Kavya Nerella"
                    },
                    {
                        "name": "Ramsey Tantawi"
                    },
                    {
                        "name": "Vivek Kumar"
                    },
                    {
                        "name": "Saiwen Wang"
                    },
                    {
                        "name": "Brycen Wershing"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    },
                    {
                        "name": "Dhruti Shah"
                    },
                    {
                        "name": "Ob Adaranijo"
                    },
                    {
                        "name": "Xin Zheng"
                    },
                    {
                        "name": "Tait Madsen"
                    },
                    {
                        "name": "Hadas Kotek"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Yin Xia"
                    },
                    {
                        "name": "Hanli Li"
                    },
                    {
                        "name": "Suma Jayaram"
                    },
                    {
                        "name": "Yanchao Sun"
                    },
                    {
                        "name": "Ahmed Fakhry"
                    },
                    {
                        "name": "Vasileios Saveris"
                    },
                    {
                        "name": "Dustin Withers"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Alp Aygar"
                    },
                    {
                        "name": "Andres Romero Mier Y Teran"
                    },
                    {
                        "name": "Kaiwei Huang"
                    },
                    {
                        "name": "Mark Lee"
                    },
                    {
                        "name": "Xiujun Li"
                    },
                    {
                        "name": "Yuhong Li"
                    },
                    {
                        "name": "Tyler Johnson"
                    },
                    {
                        "name": "Jay Tang"
                    },
                    {
                        "name": "Joseph Yitan Cheng"
                    },
                    {
                        "name": "Futang Peng"
                    },
                    {
                        "name": "Andrew Walkingshaw"
                    },
                    {
                        "name": "Lucas Guibert"
                    },
                    {
                        "name": "Abhishek Sharma"
                    },
                    {
                        "name": "Cheng Shen"
                    },
                    {
                        "name": "Piotr Maj"
                    },
                    {
                        "name": "Yasutaka Tanaka"
                    },
                    {
                        "name": "You-Cyuan Jhang"
                    },
                    {
                        "name": "Vivian Ma"
                    },
                    {
                        "name": "Tommi Vehvilainen"
                    },
                    {
                        "name": "Kelvin Zou"
                    },
                    {
                        "name": "Jeff Nichols"
                    },
                    {
                        "name": "Matthew Lei"
                    },
                    {
                        "name": "David Qiu"
                    },
                    {
                        "name": "Yihao Qian"
                    },
                    {
                        "name": "Gokul Santhanam"
                    },
                    {
                        "name": "Wentao Wu"
                    },
                    {
                        "name": "Yena Han"
                    },
                    {
                        "name": "Dominik Moritz"
                    },
                    {
                        "name": "Haijing Fu"
                    },
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Vivek Rathod"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Louis D'hauwe"
                    },
                    {
                        "name": "Qin Ba"
                    },
                    {
                        "name": "Haitian Sun"
                    },
                    {
                        "name": "Haoran Yan"
                    },
                    {
                        "name": "Philipp Dufter"
                    },
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Emma Wang"
                    },
                    {
                        "name": "Keyu He"
                    },
                    {
                        "name": "Rahul Nair"
                    },
                    {
                        "name": "Sanskruti Shah"
                    },
                    {
                        "name": "Jiarui Lu"
                    },
                    {
                        "name": "Patrick Sonnenberg"
                    },
                    {
                        "name": "Jeremy Warner"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "Bowen Pan"
                    },
                    {
                        "name": "Ziyi Zhong"
                    },
                    {
                        "name": "Joe Zhou"
                    },
                    {
                        "name": "Sam Davarnia"
                    },
                    {
                        "name": "Olli Saarikivi"
                    },
                    {
                        "name": "Irina Belousova"
                    },
                    {
                        "name": "Rachel Burger"
                    },
                    {
                        "name": "Shang-Chen Wu"
                    },
                    {
                        "name": "Di Feng"
                    },
                    {
                        "name": "Bas Straathof"
                    },
                    {
                        "name": "James Chou"
                    },
                    {
                        "name": "Yuanyang Zhang"
                    },
                    {
                        "name": "Marco Zuliani"
                    },
                    {
                        "name": "Eduardo Jimenez"
                    },
                    {
                        "name": "Abhishek Sundararajan"
                    },
                    {
                        "name": "Xianzhi Du"
                    },
                    {
                        "name": "Chang Lan"
                    },
                    {
                        "name": "Nilesh Shahdadpuri"
                    },
                    {
                        "name": "Peter Grasch"
                    },
                    {
                        "name": "Sergiu Sima"
                    },
                    {
                        "name": "Josh Newnham"
                    },
                    {
                        "name": "Varsha Paidi"
                    },
                    {
                        "name": "Jianyu Wang"
                    },
                    {
                        "name": "Kaelen Haag"
                    },
                    {
                        "name": "Alex Braunstein"
                    },
                    {
                        "name": "Daniele Molinari"
                    },
                    {
                        "name": "Richard Wei"
                    },
                    {
                        "name": "Brenda Yang"
                    },
                    {
                        "name": "Nicholas Lusskin"
                    },
                    {
                        "name": "Joanna Arreaza-Taylor"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Nicholas Seidl"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "Jiaming Hu"
                    },
                    {
                        "name": "Yiping Ma"
                    },
                    {
                        "name": "Mengyu Li"
                    },
                    {
                        "name": "Kieran Liu"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Sachin Ravi"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Kevin Smith"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Binazir Karimzadeh"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Jinhao Lei"
                    },
                    {
                        "name": "Wei Fang"
                    },
                    {
                        "name": "Alec Doane"
                    },
                    {
                        "name": "Sam Wiseman"
                    },
                    {
                        "name": "Ismael Fernandez"
                    },
                    {
                        "name": "Jane Li"
                    },
                    {
                        "name": "Andrew Hansen"
                    },
                    {
                        "name": "Javier Movellan"
                    },
                    {
                        "name": "Christopher Neubauer"
                    },
                    {
                        "name": "Hanzhi Zhou"
                    },
                    {
                        "name": "Chris Chaney"
                    },
                    {
                        "name": "Nazir Kamaldin"
                    },
                    {
                        "name": "Valentin Wolf"
                    },
                    {
                        "name": "Fernando Bermúdez-Medina"
                    },
                    {
                        "name": "Joris Pelemans"
                    },
                    {
                        "name": "Peter Fu"
                    },
                    {
                        "name": "Howard Xing"
                    },
                    {
                        "name": "Xiang Kong"
                    },
                    {
                        "name": "Wayne Shan"
                    },
                    {
                        "name": "Gabriel Jacoby-Cooper"
                    },
                    {
                        "name": "Dongcai Shen"
                    },
                    {
                        "name": "Tom Gunter"
                    },
                    {
                        "name": "Guillaume Seguin"
                    },
                    {
                        "name": "Fangping Shi"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Areeba Kamal"
                    },
                    {
                        "name": "Dan Masi"
                    },
                    {
                        "name": "Saptarshi Guha"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Jenna Thibodeau"
                    },
                    {
                        "name": "Changyuan Zhang"
                    },
                    {
                        "name": "Rebecca Callahan"
                    },
                    {
                        "name": "Charles Maalouf"
                    },
                    {
                        "name": "Wilson Tsao"
                    },
                    {
                        "name": "Boyue Li"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Naomy Sabo"
                    },
                    {
                        "name": "Cheng Leong"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Anupama Mann Anupama"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Kenneth Jung"
                    },
                    {
                        "name": "Zhifeng Chen"
                    },
                    {
                        "name": "Mohana Prasad Sathya Moorthy"
                    },
                    {
                        "name": "Yifei He"
                    },
                    {
                        "name": "Erik Hornberger"
                    },
                    {
                        "name": "Devi Krishna"
                    },
                    {
                        "name": "Senyu Tong"
                    },
                    {
                        "name": "Michael"
                    },
                    {
                        "name": "Lee"
                    },
                    {
                        "name": "David Haldimann"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Chris Bartels"
                    },
                    {
                        "name": "Sushma Rao"
                    },
                    {
                        "name": "Nathalie Tran"
                    },
                    {
                        "name": "Simon Lehnerer"
                    },
                    {
                        "name": "Co Giang"
                    },
                    {
                        "name": "Patrick Dong"
                    },
                    {
                        "name": "Junting Pan"
                    },
                    {
                        "name": "Biyao Wang"
                    },
                    {
                        "name": "Dongxu Li"
                    },
                    {
                        "name": "Mehrdad Farajtabar"
                    },
                    {
                        "name": "Dongseong Hwang"
                    },
                    {
                        "name": "Grace Duanmu"
                    },
                    {
                        "name": "Eshan Verma"
                    },
                    {
                        "name": "Sujeeth Reddy"
                    },
                    {
                        "name": "Qi Shan"
                    },
                    {
                        "name": "Hongbin Gao"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Pragnya Sridhar"
                    },
                    {
                        "name": "Forrest Huang"
                    },
                    {
                        "name": "Yingbo Wang"
                    },
                    {
                        "name": "Nikhil Bhendawade"
                    },
                    {
                        "name": "Diane Zhu"
                    },
                    {
                        "name": "Sai Aitharaju"
                    },
                    {
                        "name": "Fred Hohman"
                    },
                    {
                        "name": "Lauren Gardiner"
                    },
                    {
                        "name": "Chung-Cheng Chiu"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Alper Kokmen"
                    },
                    {
                        "name": "Frank Chu"
                    },
                    {
                        "name": "Ke Ye"
                    },
                    {
                        "name": "Kaan Elgin"
                    },
                    {
                        "name": "Oron Levy"
                    },
                    {
                        "name": "John Park"
                    },
                    {
                        "name": "Donald Zhang"
                    },
                    {
                        "name": "Eldon Schoop"
                    },
                    {
                        "name": "Nina Wenzel"
                    },
                    {
                        "name": "Michael Booker"
                    },
                    {
                        "name": "Hyunjik Kim"
                    },
                    {
                        "name": "Chinguun Erdenebileg"
                    },
                    {
                        "name": "Nan Dun"
                    },
                    {
                        "name": "Eric Liang Yang"
                    },
                    {
                        "name": "Priyal Chhatrapati"
                    },
                    {
                        "name": "Vishaal Mahtani"
                    },
                    {
                        "name": "Haiming Gang"
                    },
                    {
                        "name": "Kohen Chia"
                    },
                    {
                        "name": "Deepa Seshadri"
                    },
                    {
                        "name": "Donghan Yu"
                    },
                    {
                        "name": "Yan Meng"
                    },
                    {
                        "name": "Kelsey Peterson"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Yongqiang Wang"
                    },
                    {
                        "name": "Carina Peng"
                    },
                    {
                        "name": "Doug Kang"
                    },
                    {
                        "name": "Anuva Agarwal"
                    },
                    {
                        "name": "Albert Antony"
                    },
                    {
                        "name": "Juan Lao Tebar"
                    },
                    {
                        "name": "Albin Madappally Jose"
                    },
                    {
                        "name": "Regan Poston"
                    },
                    {
                        "name": "Andy De Wang"
                    },
                    {
                        "name": "Gerard Casamayor"
                    },
                    {
                        "name": "Elmira Amirloo"
                    },
                    {
                        "name": "Violet Yao"
                    },
                    {
                        "name": "Wojciech Kryscinski"
                    },
                    {
                        "name": "Kun Duan"
                    },
                    {
                        "name": "Lezhi L"
                    }
                ],
                "author_detail": {
                    "name": "Lezhi L"
                },
                "arxiv_affiliation": "Taoyi",
                "author": "Lezhi L",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13575v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13575v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09570v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09570v2",
                "updated": "2025-08-27T12:13:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    12,
                    13,
                    45,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-13T07:40:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    40,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Re-thinking Memory-Bound Limitations in CGRAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-thinking Memory-Bound Limitations in CGRAs"
                },
                "summary": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns."
                },
                "authors": [
                    {
                        "name": "Xiangfeng Liu"
                    },
                    {
                        "name": "Zhe Jiang"
                    },
                    {
                        "name": "Anzhen Zhu"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Mingsong Lyu"
                    },
                    {
                        "name": "Qingxu Deng"
                    },
                    {
                        "name": "Nan Guan"
                    }
                ],
                "author_detail": {
                    "name": "Nan Guan"
                },
                "author": "Nan Guan",
                "arxiv_doi": "10.1145/3760386",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3760386",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.09570v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09570v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 18 figures, CODES+ISSS 2025",
                "arxiv_journal_ref": "ACM Transactions on Embedded Computing Systems 2025",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.0; B.6.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21091v1",
                "updated": "2025-08-27T10:37:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    37,
                    24,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T10:37:24Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    37,
                    24,
                    2,
                    239,
                    0
                ],
                "title": "ERTACache: Error Rectification and Timesteps Adjustment for Efficient\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERTACache: Error Rectification and Timesteps Adjustment for Efficient\n  Diffusion"
                },
                "summary": "Diffusion models suffer from substantial computational overhead due to their\ninherently iterative inference process. While feature caching offers a\npromising acceleration strategy by reusing intermediate outputs across\ntimesteps, naive reuse often incurs noticeable quality degradation. In this\nwork, we formally analyze the cumulative error introduced by caching and\ndecompose it into two principal components: feature shift error, caused by\ninaccuracies in cached outputs, and step amplification error, which arises from\nerror propagation under fixed timestep schedules. To address these issues, we\npropose ERTACache, a principled caching framework that jointly rectifies both\nerror types. Our method employs an offline residual profiling stage to identify\nreusable steps, dynamically adjusts integration intervals via a\ntrajectory-aware correction coefficient, and analytically approximates\ncache-induced errors through a closed-form residual linearization model.\nTogether, these components enable accurate and efficient sampling under\naggressive cache reuse. Extensive experiments across standard image and video\ngeneration benchmarks show that ERTACache achieves up to 2x inference speedup\nwhile consistently preserving or even improving visual quality. Notably, on the\nstate-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x\nacceleration with minimal VBench degradation, effectively maintaining baseline\nfidelity while significantly improving efficiency. The code is available at\nhttps://github.com/bytedance/ERTACache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models suffer from substantial computational overhead due to their\ninherently iterative inference process. While feature caching offers a\npromising acceleration strategy by reusing intermediate outputs across\ntimesteps, naive reuse often incurs noticeable quality degradation. In this\nwork, we formally analyze the cumulative error introduced by caching and\ndecompose it into two principal components: feature shift error, caused by\ninaccuracies in cached outputs, and step amplification error, which arises from\nerror propagation under fixed timestep schedules. To address these issues, we\npropose ERTACache, a principled caching framework that jointly rectifies both\nerror types. Our method employs an offline residual profiling stage to identify\nreusable steps, dynamically adjusts integration intervals via a\ntrajectory-aware correction coefficient, and analytically approximates\ncache-induced errors through a closed-form residual linearization model.\nTogether, these components enable accurate and efficient sampling under\naggressive cache reuse. Extensive experiments across standard image and video\ngeneration benchmarks show that ERTACache achieves up to 2x inference speedup\nwhile consistently preserving or even improving visual quality. Notably, on the\nstate-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x\nacceleration with minimal VBench degradation, effectively maintaining baseline\nfidelity while significantly improving efficiency. The code is available at\nhttps://github.com/bytedance/ERTACache."
                },
                "authors": [
                    {
                        "name": "Xurui Peng"
                    },
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Chenqian Yan"
                    },
                    {
                        "name": "Rui Ma"
                    },
                    {
                        "name": "Fangmin Chen"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Zhihua Wu"
                    },
                    {
                        "name": "Songwei Liu"
                    },
                    {
                        "name": "Mingbao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Mingbao Lin"
                },
                "author": "Mingbao Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19670v1",
                "updated": "2025-08-27T08:30:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    8,
                    30,
                    33,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T08:30:33Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    8,
                    30,
                    33,
                    2,
                    239,
                    0
                ],
                "title": "Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed\n  Criticality Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed\n  Criticality Systems"
                },
                "summary": "As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate\nheterogeneous computing platforms, combining general-purpose processors with\nspecialized accelerators such as AI engines, GPUs, and high-speed networking\ninterfaces. This heterogeneity introduces challenges, as these accelerators and\nDMA-capable devices act as independent bus masters, directly accessing memory.\nConsequently, ensuring both security and timing predictability in such\nenvironments becomes critical. To address these concerns, the Input-Output\nMemory Management Unit (IOMMU) plays a key role in mediating and regulating\nmemory access, preventing unauthorized transactions while enforcing isolation\nand access control policies. While prior work has explored IOMMU-related\nside-channel vulnerabilities from a security standpoint, its role in\nperformance interference remains largely unexplored. Moreover, many of the same\narchitectural properties that enable side-channel leakage, such as shared TLBs,\ncaching effects, and translation overheads, can also introduce timing\nunpredictability. In this work, we analyze the contention effects within IOMMU\nstructures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how\ntheir shared nature introduce unpredictable delays. Our findings reveal that\nIOMMU-induced interference primarily affects small memory transactions, where\ntranslation overheads significantly impact execution time. Additionally, we\nhypothesize that contention effects arising from IOTLBs exhibit similar\nbehavior across architectures due to shared caching principles, such as\nprefetching and hierarchical TLB structures. Notably, our experiments show that\nIOMMU interference can delay DMA transactions by up to 1.79x for lower-size\ntransfers on the Arm SMMUv2 implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate\nheterogeneous computing platforms, combining general-purpose processors with\nspecialized accelerators such as AI engines, GPUs, and high-speed networking\ninterfaces. This heterogeneity introduces challenges, as these accelerators and\nDMA-capable devices act as independent bus masters, directly accessing memory.\nConsequently, ensuring both security and timing predictability in such\nenvironments becomes critical. To address these concerns, the Input-Output\nMemory Management Unit (IOMMU) plays a key role in mediating and regulating\nmemory access, preventing unauthorized transactions while enforcing isolation\nand access control policies. While prior work has explored IOMMU-related\nside-channel vulnerabilities from a security standpoint, its role in\nperformance interference remains largely unexplored. Moreover, many of the same\narchitectural properties that enable side-channel leakage, such as shared TLBs,\ncaching effects, and translation overheads, can also introduce timing\nunpredictability. In this work, we analyze the contention effects within IOMMU\nstructures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how\ntheir shared nature introduce unpredictable delays. Our findings reveal that\nIOMMU-induced interference primarily affects small memory transactions, where\ntranslation overheads significantly impact execution time. Additionally, we\nhypothesize that contention effects arising from IOTLBs exhibit similar\nbehavior across architectures due to shared caching principles, such as\nprefetching and hierarchical TLB structures. Notably, our experiments show that\nIOMMU interference can delay DMA transactions by up to 1.79x for lower-size\ntransfers on the Arm SMMUv2 implementation."
                },
                "authors": [
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Jose Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v4",
                "updated": "2025-08-27T04:58:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    4,
                    58,
                    58,
                    2,
                    239,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19247v1",
                "updated": "2025-08-26T17:59:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    59,
                    47,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T17:59:47Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    59,
                    47,
                    1,
                    238,
                    0
                ],
                "title": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space"
                },
                "summary": "3D local editing of specified regions is crucial for game industry and robot\ninteraction. Recent methods typically edit rendered multi-view images and then\nreconstruct 3D models, but they face challenges in precisely preserving\nunedited regions and overall coherence. Inspired by structured 3D generative\nmodels, we propose VoxHammer, a novel training-free approach that performs\nprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammer\nfirst predicts its inversion trajectory and obtains its inverted latents and\nkey-value tokens at each timestep. Subsequently, in the denoising and editing\nphase, we replace the denoising features of preserved regions with the\ncorresponding inverted latents and cached key-value tokens. By retaining these\ncontextual features, this approach ensures consistent reconstruction of\npreserved areas and coherent integration of edited parts. To evaluate the\nconsistency of preserved regions, we constructed Edit3D-Bench, a\nhuman-annotated dataset comprising hundreds of samples, each with carefully\nlabeled 3D editing regions. Experiments demonstrate that VoxHammer\nsignificantly outperforms existing methods in terms of both 3D consistency of\npreserved regions and overall quality. Our method holds promise for\nsynthesizing high-quality edited paired data, thereby laying the data\nfoundation for in-context 3D generation. See our project page at\nhttps://huanngzh.github.io/VoxHammer-Page/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D local editing of specified regions is crucial for game industry and robot\ninteraction. Recent methods typically edit rendered multi-view images and then\nreconstruct 3D models, but they face challenges in precisely preserving\nunedited regions and overall coherence. Inspired by structured 3D generative\nmodels, we propose VoxHammer, a novel training-free approach that performs\nprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammer\nfirst predicts its inversion trajectory and obtains its inverted latents and\nkey-value tokens at each timestep. Subsequently, in the denoising and editing\nphase, we replace the denoising features of preserved regions with the\ncorresponding inverted latents and cached key-value tokens. By retaining these\ncontextual features, this approach ensures consistent reconstruction of\npreserved areas and coherent integration of edited parts. To evaluate the\nconsistency of preserved regions, we constructed Edit3D-Bench, a\nhuman-annotated dataset comprising hundreds of samples, each with carefully\nlabeled 3D editing regions. Experiments demonstrate that VoxHammer\nsignificantly outperforms existing methods in terms of both 3D consistency of\npreserved regions and overall quality. Our method holds promise for\nsynthesizing high-quality edited paired data, thereby laying the data\nfoundation for in-context 3D generation. See our project page at\nhttps://huanngzh.github.io/VoxHammer-Page/."
                },
                "authors": [
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Zehuan Huang"
                    },
                    {
                        "name": "Haoran Feng"
                    },
                    {
                        "name": "Gengxiong Zhuang"
                    },
                    {
                        "name": "Rui Chen"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Lu Sheng"
                    }
                ],
                "author_detail": {
                    "name": "Lu Sheng"
                },
                "author": "Lu Sheng",
                "arxiv_comment": "Project page: https://huanngzh.github.io/VoxHammer-Page/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18983v1",
                "updated": "2025-08-26T12:32:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    32,
                    9,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T12:32:09Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    32,
                    9,
                    1,
                    238,
                    0
                ],
                "title": "Enabling MoE on the Edge via Importance-Driven Expert Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling MoE on the Edge via Importance-Driven Expert Scheduling"
                },
                "summary": "The Mixture of Experts (MoE) architecture has emerged as a key technique for\nscaling Large Language Models by activating only a subset of experts per query.\nDeploying MoE on consumer-grade edge hardware, however, is constrained by\nlimited device memory, making dynamic expert offloading essential. Unlike prior\nwork that treats offloading purely as a scheduling problem, we leverage expert\nimportance to guide decisions, substituting low-importance activated experts\nwith functionally similar ones already cached in GPU memory, thereby preserving\naccuracy. As a result, this design reduces memory usage and data transfer,\nwhile largely eliminating PCIe overhead. In addition, we introduce a scheduling\npolicy that maximizes the reuse ratio of GPU-cached experts, further boosting\nefficiency. Extensive evaluations show that our approach delivers 48% lower\ndecoding latency with over 60% expert cache hit rate, while maintaining nearly\nlossless accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture has emerged as a key technique for\nscaling Large Language Models by activating only a subset of experts per query.\nDeploying MoE on consumer-grade edge hardware, however, is constrained by\nlimited device memory, making dynamic expert offloading essential. Unlike prior\nwork that treats offloading purely as a scheduling problem, we leverage expert\nimportance to guide decisions, substituting low-importance activated experts\nwith functionally similar ones already cached in GPU memory, thereby preserving\naccuracy. As a result, this design reduces memory usage and data transfer,\nwhile largely eliminating PCIe overhead. In addition, we introduce a scheduling\npolicy that maximizes the reuse ratio of GPU-cached experts, further boosting\nefficiency. Extensive evaluations show that our approach delivers 48% lower\ndecoding latency with over 60% expert cache hit rate, while maintaining nearly\nlossless accuracy."
                },
                "authors": [
                    {
                        "name": "Guoying Zhu"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Haipeng Dai"
                    },
                    {
                        "name": "Xuechen Liu"
                    },
                    {
                        "name": "Weijun Wang"
                    },
                    {
                        "name": "Keran Li"
                    },
                    {
                        "name": "Jun xiao"
                    },
                    {
                        "name": "Ligeng Chen"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18736v1",
                "updated": "2025-08-26T07:09:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    7,
                    9,
                    9,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T07:09:09Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    7,
                    9,
                    9,
                    1,
                    238,
                    0
                ],
                "title": "Rethinking Caching for LLM Serving Systems: Beyond Traditional\n  Heuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Caching for LLM Serving Systems: Beyond Traditional\n  Heuristics"
                },
                "summary": "Serving Large Language Models (LLMs) at scale requires meeting strict Service\nLevel Objectives (SLOs) under severe computational and memory constraints.\nNevertheless, traditional caching strategies fall short: exact-matching and\nprefix caches neglect query semantics, while state-of-the-art semantic caches\nremain confined to traditional intuitions, offering little conceptual\ndeparture. Building on this, we present SISO, a semantic caching system that\nredefines efficiency for LLM serving. SISO introduces centroid-based caching to\nmaximize coverage with minimal memory, locality-aware replacement to preserve\nhigh-value entries, and dynamic thresholding to balance accuracy and latency\nunder varying workloads. Across diverse datasets, SISO delivers up to\n1.71$\\times$ higher hit ratios and consistently stronger SLO attainment\ncompared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) at scale requires meeting strict Service\nLevel Objectives (SLOs) under severe computational and memory constraints.\nNevertheless, traditional caching strategies fall short: exact-matching and\nprefix caches neglect query semantics, while state-of-the-art semantic caches\nremain confined to traditional intuitions, offering little conceptual\ndeparture. Building on this, we present SISO, a semantic caching system that\nredefines efficiency for LLM serving. SISO introduces centroid-based caching to\nmaximize coverage with minimal memory, locality-aware replacement to preserve\nhigh-value entries, and dynamic thresholding to balance accuracy and latency\nunder varying workloads. Across diverse datasets, SISO delivers up to\n1.71$\\times$ higher hit ratios and consistently stronger SLO attainment\ncompared to state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Jungwoo Kim"
                    },
                    {
                        "name": "Minsang Kim"
                    },
                    {
                        "name": "Jaeheon Lee"
                    },
                    {
                        "name": "Chanwoo Moon"
                    },
                    {
                        "name": "Heejin Kim"
                    },
                    {
                        "name": "Taeho Hwang"
                    },
                    {
                        "name": "Woosuk Chung"
                    },
                    {
                        "name": "Yeseong Kim"
                    },
                    {
                        "name": "Sungjin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sungjin Lee"
                },
                "author": "Sungjin Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08045v2",
                "updated": "2025-08-26T01:55:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    1,
                    55,
                    27,
                    1,
                    238,
                    0
                ],
                "published": "2025-07-10T01:51:17Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "title": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing"
                },
                "summary": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Junyi Wen"
                    },
                    {
                        "name": "Junyuan Liang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Ting Cai"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19365v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19365v3",
                "updated": "2025-08-26T01:45:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    1,
                    45,
                    34,
                    1,
                    238,
                    0
                ],
                "published": "2025-04-27T22:05:14Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "title": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration"
                },
                "summary": "GPUs are critical for compute-intensive applications, yet emerging workloads\nsuch as recommender systems, graph analytics, and data analytics often exceed\nGPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as\nexternal memory, and the GPU-centric approach enables GPU threads to directly\nissue NVMe requests, further avoiding CPU intervention. However, current\nGPU-centric approaches adopt synchronous I/O, forcing threads to stall during\nlong communication delays.\n  We propose AGILE, a lightweight asynchronous GPU-centric I/O library that\neliminates deadlock risks and integrates a flexible HBM-based software cache.\nAGILE overlaps computation and I/O, improving performance by up to 1.88$\\times$\nacross workloads with diverse computation-to-communication ratios. Compared to\nBaM on DLRM, AGILE achieves up to 1.75$\\times$ speedup through efficient design\nand overlapping; on graph applications, AGILE reduces software cache overhead\nby up to 3.12$\\times$ and NVMe I/O overhead by up to 2.85$\\times$; AGILE also\nlowers per-thread register usage by up to 1.32$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPUs are critical for compute-intensive applications, yet emerging workloads\nsuch as recommender systems, graph analytics, and data analytics often exceed\nGPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as\nexternal memory, and the GPU-centric approach enables GPU threads to directly\nissue NVMe requests, further avoiding CPU intervention. However, current\nGPU-centric approaches adopt synchronous I/O, forcing threads to stall during\nlong communication delays.\n  We propose AGILE, a lightweight asynchronous GPU-centric I/O library that\neliminates deadlock risks and integrates a flexible HBM-based software cache.\nAGILE overlaps computation and I/O, improving performance by up to 1.88$\\times$\nacross workloads with diverse computation-to-communication ratios. Compared to\nBaM on DLRM, AGILE achieves up to 1.75$\\times$ speedup through efficient design\nand overlapping; on graph applications, AGILE reduces software cache overhead\nby up to 3.12$\\times$ and NVMe I/O overhead by up to 2.85$\\times$; AGILE also\nlowers per-thread register usage by up to 1.32$\\times$."
                },
                "authors": [
                    {
                        "name": "Zhuoping Yang"
                    },
                    {
                        "name": "Jinming Zhuang"
                    },
                    {
                        "name": "Xingzhen Chen"
                    },
                    {
                        "name": "Alex K. Jones"
                    },
                    {
                        "name": "Peipei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Peipei Zhou"
                },
                "author": "Peipei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19365v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19365v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18572v1",
                "updated": "2025-08-26T00:09:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    0,
                    9,
                    3,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T00:09:03Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    0,
                    9,
                    3,
                    1,
                    238,
                    0
                ],
                "title": "Strata: Hierarchical Context Caching for Long Context Language Model\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strata: Hierarchical Context Caching for Long Context Language Model\n  Serving"
                },
                "summary": "Large Language Models (LLMs) with expanding context windows face significant\nperformance hurdles. While caching key-value (KV) states is critical for\navoiding redundant computation, the storage footprint of long-context caches\nquickly exceeds GPU memory capacity, forcing production systems to adopt\nhierarchical caching across memory hierarchies. However, transferring large\ncached contexts back to the GPU introduces severe performance bottlenecks:\nfragmented I/O from paged layouts prevents full bandwidth utilization, and\nexisting schedulers fail to account for cache-loading delays, leaving systems\nloading-bound rather than compute-bound. We present Strata, a hierarchical\ncontext caching framework designed for efficient long context LLM serving.\nStrata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling\nGPU and CPU memory layouts and employs cache-aware request scheduling to\nbalance compute with I/O latency and overlapping unavoidable stalls with\ncomplementary tasks. Built on SGLang and deployed in production, Strata\nachieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache\nand 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without\ndegrading short-context performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with expanding context windows face significant\nperformance hurdles. While caching key-value (KV) states is critical for\navoiding redundant computation, the storage footprint of long-context caches\nquickly exceeds GPU memory capacity, forcing production systems to adopt\nhierarchical caching across memory hierarchies. However, transferring large\ncached contexts back to the GPU introduces severe performance bottlenecks:\nfragmented I/O from paged layouts prevents full bandwidth utilization, and\nexisting schedulers fail to account for cache-loading delays, leaving systems\nloading-bound rather than compute-bound. We present Strata, a hierarchical\ncontext caching framework designed for efficient long context LLM serving.\nStrata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling\nGPU and CPU memory layouts and employs cache-aware request scheduling to\nbalance compute with I/O latency and overlapping unavoidable stalls with\ncomplementary tasks. Built on SGLang and deployed in production, Strata\nachieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache\nand 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without\ndegrading short-context performance."
                },
                "authors": [
                    {
                        "name": "Zhiqiang Xie"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Vikram Sharma Mailthody"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Michael Garland"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "13 pages, 14 figures, under peer review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18540v1",
                "updated": "2025-08-25T22:21:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    22,
                    21,
                    4,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T22:21:04Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    22,
                    21,
                    4,
                    0,
                    237,
                    0
                ],
                "title": "Real-time 3D Visualization of Radiance Fields on Light Field Displays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time 3D Visualization of Radiance Fields on Light Field Displays"
                },
                "summary": "Radiance fields have revolutionized photo-realistic 3D scene visualization by\nenabling high-fidelity reconstruction of complex environments, making them an\nideal match for light field displays. However, integrating these technologies\npresents significant computational challenges, as light field displays require\nmultiple high-resolution renderings from slightly shifted viewpoints, while\nradiance fields rely on computationally intensive volume rendering. In this\npaper, we propose a unified and efficient framework for real-time radiance\nfield rendering on light field displays. Our method supports a wide range of\nradiance field representations, including NeRFs, 3D Gaussian Splatting, and\nSparse Voxels, within a shared architecture based on a single-pass plane\nsweeping strategy and caching of shared, non-directional components. The\nframework generalizes across different scene formats without retraining, and\navoids redundant computation across views. We further demonstrate a real-time\ninteractive application on a Looking Glass display, achieving 200+ FPS at 512p\nacross 45 views, enabling seamless, immersive 3D interaction. On standard\nbenchmarks, our method achieves up to 22x speedup compared to independently\nrendering each view, while preserving image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radiance fields have revolutionized photo-realistic 3D scene visualization by\nenabling high-fidelity reconstruction of complex environments, making them an\nideal match for light field displays. However, integrating these technologies\npresents significant computational challenges, as light field displays require\nmultiple high-resolution renderings from slightly shifted viewpoints, while\nradiance fields rely on computationally intensive volume rendering. In this\npaper, we propose a unified and efficient framework for real-time radiance\nfield rendering on light field displays. Our method supports a wide range of\nradiance field representations, including NeRFs, 3D Gaussian Splatting, and\nSparse Voxels, within a shared architecture based on a single-pass plane\nsweeping strategy and caching of shared, non-directional components. The\nframework generalizes across different scene formats without retraining, and\navoids redundant computation across views. We further demonstrate a real-time\ninteractive application on a Looking Glass display, achieving 200+ FPS at 512p\nacross 45 views, enabling seamless, immersive 3D interaction. On standard\nbenchmarks, our method achieves up to 22x speedup compared to independently\nrendering each view, while preserving image quality."
                },
                "authors": [
                    {
                        "name": "Jonghyun Kim"
                    },
                    {
                        "name": "Cheng Sun"
                    },
                    {
                        "name": "Michael Stengel"
                    },
                    {
                        "name": "Matthew Chan"
                    },
                    {
                        "name": "Andrew Russell"
                    },
                    {
                        "name": "Jaehyun Jung"
                    },
                    {
                        "name": "Wil Braithwaite"
                    },
                    {
                        "name": "Shalini De Mello"
                    },
                    {
                        "name": "David Luebke"
                    }
                ],
                "author_detail": {
                    "name": "David Luebke"
                },
                "author": "David Luebke",
                "arxiv_comment": "10 pages, 14 figures. J. Kim, C. Sun, and M. Stengel contributed\n  equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18494v1",
                "updated": "2025-08-25T21:07:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    21,
                    7,
                    52,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T21:07:52Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    21,
                    7,
                    52,
                    0,
                    237,
                    0
                ],
                "title": "DiskJoin: Large-scale Vector Similarity Join with SSD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiskJoin: Large-scale Vector Similarity Join with SSD"
                },
                "summary": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x."
                },
                "authors": [
                    {
                        "name": "Yanqi Chen"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Alexandra Meliou"
                    },
                    {
                        "name": "Eric Lo"
                    }
                ],
                "author_detail": {
                    "name": "Eric Lo"
                },
                "author": "Eric Lo",
                "arxiv_comment": "Accepted at SIGMOD 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v3",
                "updated": "2025-08-25T15:48:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    48,
                    28,
                    0,
                    237,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "CIKM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17892v1",
                "updated": "2025-08-25T10:59:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T10:59:02Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "title": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance\ncomparable to or better than the full context in the long context scenarios.\nWithout additional post training or operator development, ILRe can process a\nsingle $1M$ tokens request in less than half a minute (speedup $\\approx\n180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with model\nLlama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance\ncomparable to or better than the full context in the long context scenarios.\nWithout additional post training or operator development, ILRe can process a\nsingle $1M$ tokens request in less than half a minute (speedup $\\approx\n180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with model\nLlama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "Mandi Liu"
                    },
                    {
                        "name": "Jiangzhou Ji"
                    },
                    {
                        "name": "Huaijun Li"
                    },
                    {
                        "name": "Haobo Yang"
                    },
                    {
                        "name": "Yaohan He"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17756v1",
                "updated": "2025-08-25T07:49:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    7,
                    49,
                    17,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T07:49:17Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    7,
                    49,
                    17,
                    0,
                    237,
                    0
                ],
                "title": "SuperGen: An Efficient Ultra-high-resolution Video Generation System\n  with Sketching and Tiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperGen: An Efficient Ultra-high-resolution Video Generation System\n  with Sketching and Tiling"
                },
                "summary": "Diffusion models have recently achieved remarkable success in generative\ntasks (e.g., image and video generation), and the demand for high-quality\ncontent (e.g., 2K/4K videos) is rapidly increasing across various domains.\nHowever, generating ultra-high-resolution videos on existing\nstandard-resolution (e.g., 720p) platforms remains challenging due to the\nexcessive re-training requirements and prohibitively high computational and\nmemory costs. To this end, we introduce SuperGen, an efficient tile-based\nframework for ultra-high-resolution video generation. SuperGen features a novel\ntraining-free algorithmic innovation with tiling to successfully support a wide\nrange of resolutions without additional training efforts while significantly\nreducing both memory footprint and computational complexity. Moreover, SuperGen\nincorporates a tile-tailored, adaptive, region-aware caching strategy that\naccelerates video generation by exploiting redundancy across denoising steps\nand spatial regions. SuperGen also integrates cache-guided,\ncommunication-minimized tile parallelism for enhanced throughput and minimized\nlatency. Evaluations demonstrate that SuperGen harvests the maximum performance\ngains while achieving high output quality across various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have recently achieved remarkable success in generative\ntasks (e.g., image and video generation), and the demand for high-quality\ncontent (e.g., 2K/4K videos) is rapidly increasing across various domains.\nHowever, generating ultra-high-resolution videos on existing\nstandard-resolution (e.g., 720p) platforms remains challenging due to the\nexcessive re-training requirements and prohibitively high computational and\nmemory costs. To this end, we introduce SuperGen, an efficient tile-based\nframework for ultra-high-resolution video generation. SuperGen features a novel\ntraining-free algorithmic innovation with tiling to successfully support a wide\nrange of resolutions without additional training efforts while significantly\nreducing both memory footprint and computational complexity. Moreover, SuperGen\nincorporates a tile-tailored, adaptive, region-aware caching strategy that\naccelerates video generation by exploiting redundancy across denoising steps\nand spatial regions. SuperGen also integrates cache-guided,\ncommunication-minimized tile parallelism for enhanced throughput and minimized\nlatency. Evaluations demonstrate that SuperGen harvests the maximum performance\ngains while achieving high output quality across various benchmarks."
                },
                "authors": [
                    {
                        "name": "Fanjiang Ye"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yi Mu"
                    },
                    {
                        "name": "Jucheng Shen"
                    },
                    {
                        "name": "Renjie Li"
                    },
                    {
                        "name": "Kaijian Wang"
                    },
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Triston Cao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "T. S. Eugene Ng"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16212v2",
                "updated": "2025-08-25T03:07:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    7,
                    2,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-22T08:36:58Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    36,
                    58,
                    4,
                    234,
                    0
                ],
                "title": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free\n  Cache Reuse for Diffusion Transformer Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free\n  Cache Reuse for Diffusion Transformer Models"
                },
                "summary": "Diffusion models have emerged as a powerful paradigm for generative tasks\nsuch as image synthesis and video generation, with Transformer architectures\nfurther enhancing performance. However, the high computational cost of\ndiffusion Transformers-stemming from a large number of sampling steps and\ncomplex per-step computations-presents significant challenges for real-time\ndeployment. In this paper, we introduce OmniCache, a training-free acceleration\nmethod that exploits the global redundancy inherent in the denoising process.\nUnlike existing methods that determine caching strategies based on inter-step\nsimilarities and tend to prioritize reusing later sampling steps, our approach\noriginates from the sampling perspective of DIT models. We systematically\nanalyze the model's sampling trajectories and strategically distribute cache\nreuse across the entire sampling process. This global perspective enables more\neffective utilization of cached computations throughout the diffusion\ntrajectory, rather than concentrating reuse within limited segments of the\nsampling procedure. In addition, during cache reuse, we dynamically estimate\nthe corresponding noise and filter it out to reduce its impact on the sampling\ndirection. Extensive experiments demonstrate that our approach accelerates the\nsampling process while maintaining competitive generative quality, offering a\npromising and practical solution for efficient deployment of diffusion-based\ngenerative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a powerful paradigm for generative tasks\nsuch as image synthesis and video generation, with Transformer architectures\nfurther enhancing performance. However, the high computational cost of\ndiffusion Transformers-stemming from a large number of sampling steps and\ncomplex per-step computations-presents significant challenges for real-time\ndeployment. In this paper, we introduce OmniCache, a training-free acceleration\nmethod that exploits the global redundancy inherent in the denoising process.\nUnlike existing methods that determine caching strategies based on inter-step\nsimilarities and tend to prioritize reusing later sampling steps, our approach\noriginates from the sampling perspective of DIT models. We systematically\nanalyze the model's sampling trajectories and strategically distribute cache\nreuse across the entire sampling process. This global perspective enables more\neffective utilization of cached computations throughout the diffusion\ntrajectory, rather than concentrating reuse within limited segments of the\nsampling procedure. In addition, during cache reuse, we dynamically estimate\nthe corresponding noise and filter it out to reduce its impact on the sampling\ndirection. Extensive experiments demonstrate that our approach accelerates the\nsampling process while maintaining competitive generative quality, offering a\npromising and practical solution for efficient deployment of diffusion-based\ngenerative models."
                },
                "authors": [
                    {
                        "name": "Huanpeng Chu"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Guanyu Fen"
                    },
                    {
                        "name": "Yutao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yutao Zhang"
                },
                "author": "Yutao Zhang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17624v1",
                "updated": "2025-08-25T03:05:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    5,
                    16,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T03:05:16Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    5,
                    16,
                    0,
                    237,
                    0
                ],
                "title": "ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters\n  at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters\n  at Scale"
                },
                "summary": "Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large\nlanguage models to enhance their task-specific performance by selectively\ntuning the top-activated experts for the task. Serving these fine-tuned models\nat scale is challenging: deploying merged models in isolation is prohibitively\nresource-hungry, while existing multi-adapter serving systems with LoRA-style\nadditive updates are incompatible with ESFT's expert-oriented paradigm. We\npresent ExpertWeave, a system that serves multiple ESFT adapters concurrently\nover a single shared MoE base model, drastically reducing the memory footprint\nand improving resource utilization. To seamlessly integrate into existing\ninference pipelines for MoE models with non-intrusive modifications and minimal\nlatency overhead, ExpertWeave introduces a virtual-memory-assisted expert\nweight manager that co-locates base-model and adapter experts without incurring\nmemory overhead from fragmentation, and a fused kernel for batched rerouting to\nenable lightweight redirection of tokens to the appropriate experts at runtime.\nOur evaluations show that ExpertWeave can simultaneously serve multiple\nadapters of a 16B MoE model on a single accelerator where the baseline runs out\nof memory, or provides up to 94x more KV cache capacity and achieves up to 18%\nhigher throughput while using comparable resources, all without compromising\nmodel accuracy. ExpertWeave maintains low overhead even when scaling to 20\nadapters, with a 4-11% latency increase compared with serving the base model\nalone. Source code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large\nlanguage models to enhance their task-specific performance by selectively\ntuning the top-activated experts for the task. Serving these fine-tuned models\nat scale is challenging: deploying merged models in isolation is prohibitively\nresource-hungry, while existing multi-adapter serving systems with LoRA-style\nadditive updates are incompatible with ESFT's expert-oriented paradigm. We\npresent ExpertWeave, a system that serves multiple ESFT adapters concurrently\nover a single shared MoE base model, drastically reducing the memory footprint\nand improving resource utilization. To seamlessly integrate into existing\ninference pipelines for MoE models with non-intrusive modifications and minimal\nlatency overhead, ExpertWeave introduces a virtual-memory-assisted expert\nweight manager that co-locates base-model and adapter experts without incurring\nmemory overhead from fragmentation, and a fused kernel for batched rerouting to\nenable lightweight redirection of tokens to the appropriate experts at runtime.\nOur evaluations show that ExpertWeave can simultaneously serve multiple\nadapters of a 16B MoE model on a single accelerator where the baseline runs out\nof memory, or provides up to 94x more KV cache capacity and achieves up to 18%\nhigher throughput while using comparable resources, all without compromising\nmodel accuracy. ExpertWeave maintains low overhead even when scaling to 20\nadapters, with a 4-11% latency increase compared with serving the base model\nalone. Source code will be released soon."
                },
                "authors": [
                    {
                        "name": "Ge Shi"
                    },
                    {
                        "name": "Hanieh Sadri"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00052v1",
                "updated": "2025-08-25T02:58:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    2,
                    58,
                    39,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T02:58:39Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    2,
                    58,
                    39,
                    0,
                    237,
                    0
                ],
                "title": "Lightning Fast Caching-based Parallel Denoising Prediction for\n  Accelerating Talking Head Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightning Fast Caching-based Parallel Denoising Prediction for\n  Accelerating Talking Head Generation"
                },
                "summary": "Diffusion-based talking head models generate high-quality, photorealistic\nvideos but suffer from slow inference, limiting practical applications.\nExisting acceleration methods for general diffusion models fail to exploit the\ntemporal and spatial redundancies unique to talking head generation. In this\npaper, we propose a task-specific framework addressing these inefficiencies\nthrough two key innovations. First, we introduce Lightning-fast Caching-based\nParallel denoising prediction (LightningCP), caching static features to bypass\nmost model layers in inference time. We also enable parallel prediction using\ncached features and estimated noisy latents as inputs, efficiently bypassing\nsequential sampling. Second, we propose Decoupled Foreground Attention (DFA) to\nfurther accelerate attention computations, exploiting the spatial decoupling in\ntalking head videos to restrict attention to dynamic foreground regions.\nAdditionally, we remove reference features in certain layers to bring extra\nspeedup. Extensive experiments demonstrate that our framework significantly\nimproves inference speed while preserving video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based talking head models generate high-quality, photorealistic\nvideos but suffer from slow inference, limiting practical applications.\nExisting acceleration methods for general diffusion models fail to exploit the\ntemporal and spatial redundancies unique to talking head generation. In this\npaper, we propose a task-specific framework addressing these inefficiencies\nthrough two key innovations. First, we introduce Lightning-fast Caching-based\nParallel denoising prediction (LightningCP), caching static features to bypass\nmost model layers in inference time. We also enable parallel prediction using\ncached features and estimated noisy latents as inputs, efficiently bypassing\nsequential sampling. Second, we propose Decoupled Foreground Attention (DFA) to\nfurther accelerate attention computations, exploiting the spatial decoupling in\ntalking head videos to restrict attention to dynamic foreground regions.\nAdditionally, we remove reference features in certain layers to bring extra\nspeedup. Extensive experiments demonstrate that our framework significantly\nimproves inference speed while preserving video quality."
                },
                "authors": [
                    {
                        "name": "Jianzhi Long"
                    },
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rongcheng Tu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15881v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15881v2",
                "updated": "2025-08-25T02:24:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    2,
                    24,
                    20,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-21T15:25:40Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    15,
                    25,
                    40,
                    3,
                    233,
                    0
                ],
                "title": "TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated\n  Prefill and Decode Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated\n  Prefill and Decode Inference"
                },
                "summary": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses\nkey-value states into a low-rank latent vector, caching only this vector to\nreduce memory. In tensor parallelism (TP), however, attention heads are\ncomputed across multiple devices, and each device must load the full cache,\neroding the advantage of MLA over Grouped Query Attention (GQA). We propose\nTensor-Parallel Latent Attention (TPLA): a scheme that partitions both the\nlatent representation and each head's input dimension across devices, performs\nattention independently per shard, and then combines results with an\nall-reduce. TPLA preserves the benefits of a compressed KV cache while\nunlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in\nTPLA still leverages the full latent representation, maintaining stronger\nrepresentational capacity. TPLA is drop-in compatible with models pre-trained\nusing MLA: it supports MLA-style prefilling and enables efficient\ntensor-parallel decoding without retraining. Applying simple orthogonal\ntransforms -- e.g., the Hadamard transform or PCA -- before TP slicing further\nmitigates cross-shard interference, yielding minimal accuracy degradation. By\nreducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x\nand 1.93x speedups, respectively, at a 32K-token context length while\nmaintaining performance on commonsense and LongBench benchmarks. TPLA can be\nimplemented with FlashAttention-3, enabling practical end-to-end acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses\nkey-value states into a low-rank latent vector, caching only this vector to\nreduce memory. In tensor parallelism (TP), however, attention heads are\ncomputed across multiple devices, and each device must load the full cache,\neroding the advantage of MLA over Grouped Query Attention (GQA). We propose\nTensor-Parallel Latent Attention (TPLA): a scheme that partitions both the\nlatent representation and each head's input dimension across devices, performs\nattention independently per shard, and then combines results with an\nall-reduce. TPLA preserves the benefits of a compressed KV cache while\nunlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in\nTPLA still leverages the full latent representation, maintaining stronger\nrepresentational capacity. TPLA is drop-in compatible with models pre-trained\nusing MLA: it supports MLA-style prefilling and enables efficient\ntensor-parallel decoding without retraining. Applying simple orthogonal\ntransforms -- e.g., the Hadamard transform or PCA -- before TP slicing further\nmitigates cross-shard interference, yielding minimal accuracy degradation. By\nreducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x\nand 1.93x speedups, respectively, at a 32K-token context length while\nmaintaining performance on commonsense and LongBench benchmarks. TPLA can be\nimplemented with FlashAttention-3, enabling practical end-to-end acceleration."
                },
                "authors": [
                    {
                        "name": "Xiaojuan Tang"
                    },
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15881v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15881v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17593v1",
                "updated": "2025-08-25T01:33:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    1,
                    33,
                    18,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T01:33:18Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    1,
                    33,
                    18,
                    0,
                    237,
                    0
                ],
                "title": "Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD\n  NPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD\n  NPUs"
                },
                "summary": "Transformer-based deep learning models are increasingly deployed on energy,\nand DRAM bandwidth constrained devices such as laptops and gaming consoles,\nwhich presents significant challenges in meeting the latency requirements of\nthe models. The industry is turning to neural processing units (NPUs) for\nsuperior performance-per-watt (perf/watt); however, efficiently mapping dynamic\nattention layers to the NPUs remains a challenging task. For optimizing\nperf/watt, AMD XDNA NPUs employ software managed caches and share system memory\nwith host. This requires substantial engineering effort to unlock efficient\ntiling, buffer allocation, and data movement to extract the maximum efficiency\nfrom the device. This paper introduces Zen-Attention, a framework that\noptimizes DRAM bandwidth utilization in the attention layer of models by\nsystematically exploring the complex design space of layer folding, tiling, and\ndata-movement on the interconnect, and the tensor layouts to come up with an\noptimal solution. Our evaluation includes comparative analysis of end-to-end\nmodel latency and specific attention latency in each model. We demonstrate how\nthe framework enhances mapping capabilities by varying input dimensions, which\nrequire padding and masking in the attention block. For representative\ntransformer models, the Zen-Attention Framework achieves up to 4x improvement\nin the latency of the attention block and up to 32% improvement in end-to-end\nnetwork latency compared to the baseline Unfolded- approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based deep learning models are increasingly deployed on energy,\nand DRAM bandwidth constrained devices such as laptops and gaming consoles,\nwhich presents significant challenges in meeting the latency requirements of\nthe models. The industry is turning to neural processing units (NPUs) for\nsuperior performance-per-watt (perf/watt); however, efficiently mapping dynamic\nattention layers to the NPUs remains a challenging task. For optimizing\nperf/watt, AMD XDNA NPUs employ software managed caches and share system memory\nwith host. This requires substantial engineering effort to unlock efficient\ntiling, buffer allocation, and data movement to extract the maximum efficiency\nfrom the device. This paper introduces Zen-Attention, a framework that\noptimizes DRAM bandwidth utilization in the attention layer of models by\nsystematically exploring the complex design space of layer folding, tiling, and\ndata-movement on the interconnect, and the tensor layouts to come up with an\noptimal solution. Our evaluation includes comparative analysis of end-to-end\nmodel latency and specific attention latency in each model. We demonstrate how\nthe framework enhances mapping capabilities by varying input dimensions, which\nrequire padding and masking in the attention block. For representative\ntransformer models, the Zen-Attention Framework achieves up to 4x improvement\nin the latency of the attention block and up to 32% improvement in end-to-end\nnetwork latency compared to the baseline Unfolded- approaches."
                },
                "authors": [
                    {
                        "name": "Aadesh Deshmukh"
                    },
                    {
                        "name": "Venkata Yaswanth Raparti"
                    },
                    {
                        "name": "Samuel Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Hsu"
                },
                "author": "Samuel Hsu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09040v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09040v3",
                "updated": "2025-08-25T00:15:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    0,
                    15,
                    27,
                    0,
                    237,
                    0
                ],
                "published": "2025-05-14T00:41:44Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    41,
                    44,
                    2,
                    134,
                    0
                ],
                "title": "RT-Cache: Training-Free Retrieval for Real-Time Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RT-Cache: Training-Free Retrieval for Real-Time Manipulation"
                },
                "summary": "Real robots are expected to repeat the same behavior in new environments with\nvery little new data, yet modern controllers either incur heavy per-step\ninference or require deployment-time fine-tuning. We propose RT-Cache, a\ntraining-free retrieval-as-control pipeline that caches diverse image action\ntrajectories in a unified vector memory and, at test time, embeds the current\nframe to retrieve and replay multi-step snippets, replacing per-step model\ncalls. A hierarchical search keeps lookups sub-second at million scale,\nshifting cost from compute to storage and enabling real-time control on modest\nGPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher\nsuccess and lower completion time than strong retrieval baselines\n(approximately x2 higher success and ~30% faster in our settings), and a\nsingle-episode anchoring study shows immediate adaptation to a more complex,\ncontact-rich task without fine-tuning. RT-Cache turns experience into an\nappend-only memory, offering a simple, scalable path to few-shot deployment\ntoday and a foundation for multimodal keys and optional integration with\nhigh-level policies. Project page: https://rt-cache.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real robots are expected to repeat the same behavior in new environments with\nvery little new data, yet modern controllers either incur heavy per-step\ninference or require deployment-time fine-tuning. We propose RT-Cache, a\ntraining-free retrieval-as-control pipeline that caches diverse image action\ntrajectories in a unified vector memory and, at test time, embeds the current\nframe to retrieve and replay multi-step snippets, replacing per-step model\ncalls. A hierarchical search keeps lookups sub-second at million scale,\nshifting cost from compute to storage and enabling real-time control on modest\nGPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher\nsuccess and lower completion time than strong retrieval baselines\n(approximately x2 higher success and ~30% faster in our settings), and a\nsingle-episode anchoring study shows immediate adaptation to a more complex,\ncontact-rich task without fine-tuning. RT-Cache turns experience into an\nappend-only memory, offering a simple, scalable path to few-shot deployment\ntoday and a foundation for multimodal keys and optional integration with\nhigh-level policies. Project page: https://rt-cache.github.io/."
                },
                "authors": [
                    {
                        "name": "Owen Kwon"
                    },
                    {
                        "name": "Abraham George"
                    },
                    {
                        "name": "Alison Bartsch"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "arxiv_comment": "8 pages, 6 figures. 2025 IEEE-RAS 24th International Conference on\n  Humanoid Robots",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09040v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09040v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v3",
                "updated": "2025-08-24T22:09:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    22,
                    9,
                    57,
                    6,
                    236,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs"
                },
                "summary": "Long-range tasks demand reasoning over long inputs. However, existing\nsolutions are limited, e.g., long-context models require large compute budgets,\nparameter-efficient fine-tuning (PEFT) needs training data, and\nretrieval-augmented generation (RAG) entails complex task-specific designs.\nThough in-context approaches overcome many of these issues, methods with\nshort-context LLMs are inefficient, trading context for processing more tokens.\nWe introduce PRISM, a highly token-efficient in-context method based on\nstructured schemas that outperforms baselines on diverse tasks with 4x shorter\ncontexts. This approach produces concise outputs and efficiently leverages\nkey-value (KV) caches to reduce costs by up to 54%. PRISM scales down to tiny\ncontexts without increasing costs or sacrificing quality, and generalizes to\nnew tasks with minimal effort by generating schemas from task descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks demand reasoning over long inputs. However, existing\nsolutions are limited, e.g., long-context models require large compute budgets,\nparameter-efficient fine-tuning (PEFT) needs training data, and\nretrieval-augmented generation (RAG) entails complex task-specific designs.\nThough in-context approaches overcome many of these issues, methods with\nshort-context LLMs are inefficient, trading context for processing more tokens.\nWe introduce PRISM, a highly token-efficient in-context method based on\nstructured schemas that outperforms baselines on diverse tasks with 4x shorter\ncontexts. This approach produces concise outputs and efficiently leverages\nkey-value (KV) caches to reduce costs by up to 54%. PRISM scales down to tiny\ncontexts without increasing costs or sacrificing quality, and generalizes to\nnew tasks with minimal effort by generating schemas from task descriptions."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "Published as a conference paper at EMNLP 2025. 28 pages, 7 figures, 5\n  tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17518v1",
                "updated": "2025-08-24T20:51:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    20,
                    51,
                    6,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T20:51:06Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    20,
                    51,
                    6,
                    6,
                    236,
                    0
                ],
                "title": "Evaluating Compiler Optimization Impacts on zkVM Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Compiler Optimization Impacts on zkVM Performance"
                },
                "summary": "Zero-knowledge proofs (ZKPs) are the cornerstone of programmable\ncryptography. They enable (1) privacy-preserving and verifiable computation\nacross blockchains, and (2) an expanding range of off-chain applications such\nas credential schemes. Zero-knowledge virtual machines (zkVMs) lower the\nbarrier by turning ZKPs into a drop-in backend for standard compilation\npipelines. This lets developers write proof-generating programs in conventional\nlanguages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits.\nHowever, these VMs inherit compiler infrastructures tuned for traditional\narchitectures rather than for proof systems. In particular, standard compiler\noptimizations assume features that are absent in zkVMs, including cache\nlocality, branch prediction, or instruction-level parallelism. Therefore, their\nimpact on proof generation is questionable.\n  We present the first systematic study of the impact of compiler optimizations\non zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an\nunoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero\nand SP1). While standard LLVM optimization levels do improve zkVM performance\n(over 40\\%), their impact is far smaller than on traditional CPUs, since their\ndecisions rely on hardware features rather than proof constraints. Guided by a\nfine-grained pass-level analysis, we~\\emph{slightly} refine a small set of LLVM\npasses to be zkVM-aware, improving zkVM execution time by up to 45\\% (average\n+4.6\\% on RISC Zero, +1\\% on SP1) and achieving consistent proving-time gains.\nOur work highlights the potential of compiler-level optimizations for zkVM\nperformance and opens new direction for zkVM-specific passes, backends, and\nsuperoptimizers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-knowledge proofs (ZKPs) are the cornerstone of programmable\ncryptography. They enable (1) privacy-preserving and verifiable computation\nacross blockchains, and (2) an expanding range of off-chain applications such\nas credential schemes. Zero-knowledge virtual machines (zkVMs) lower the\nbarrier by turning ZKPs into a drop-in backend for standard compilation\npipelines. This lets developers write proof-generating programs in conventional\nlanguages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits.\nHowever, these VMs inherit compiler infrastructures tuned for traditional\narchitectures rather than for proof systems. In particular, standard compiler\noptimizations assume features that are absent in zkVMs, including cache\nlocality, branch prediction, or instruction-level parallelism. Therefore, their\nimpact on proof generation is questionable.\n  We present the first systematic study of the impact of compiler optimizations\non zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an\nunoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero\nand SP1). While standard LLVM optimization levels do improve zkVM performance\n(over 40\\%), their impact is far smaller than on traditional CPUs, since their\ndecisions rely on hardware features rather than proof constraints. Guided by a\nfine-grained pass-level analysis, we~\\emph{slightly} refine a small set of LLVM\npasses to be zkVM-aware, improving zkVM execution time by up to 45\\% (average\n+4.6\\% on RISC Zero, +1\\% on SP1) and achieving consistent proving-time gains.\nOur work highlights the potential of compiler-level optimizations for zkVM\nperformance and opens new direction for zkVM-specific passes, backends, and\nsuperoptimizers."
                },
                "authors": [
                    {
                        "name": "Thomas Gassmann"
                    },
                    {
                        "name": "Stefanos Chaliasos"
                    },
                    {
                        "name": "Thodoris Sotiropoulos"
                    },
                    {
                        "name": "Zhendong Su"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Su"
                },
                "author": "Zhendong Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17496v1",
                "updated": "2025-08-24T19:28:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    19,
                    28,
                    22,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T19:28:22Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    19,
                    28,
                    22,
                    6,
                    236,
                    0
                ],
                "title": "Practical Insertion-Only Convex Hull",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical Insertion-Only Convex Hull"
                },
                "summary": "Convex hull data structures are fundamental in computational geometry. We\nstudy insertion-only data structures, supporting various containment and\nintersection queries. When $P$ is sorted by $x$- or $y$-coordinate, convex\nhulls can be constructed in linear time using classical algorithms such as\nGraham scan. We investigate a variety of methods tailored to the insertion-only\nsetting. We explore a broad selection of trade-offs involving robustness,\nmemory access patterns, and space usage, providing an extensive evaluation of\nboth existing and novel techniques. Logarithmic-time methods rely on\npointer-based tree structures, which suffer in practice due to poor memory\nlocality. Motivated by this, we develop a vector-based solution inspired by\nOvermars' logarithmic method. Our structure has worse asymptotic bounds,\nsupporting queries in $O(\\log^2 n)$ time, but stores data in $O(\\log n)$\ncontiguous vectors, greatly improving cache performance.\n  Through empirical evaluation on real-world and synthetic data sets, we\nuncover surprising trends. Let $h$ denote the size of the convex hull. We show\nthat a na\\\"ive $O(h)$ insertion-only algorithm based on Graham scan\nconsistently outperforms both theoretical and practical state-of-the-art\nmethods under realistic workloads, even on data sets with rather large convex\nhulls. While tree-based methods with $O(\\log h)$ update times offer solid\ntheoretical guarantees, they are never optimal in practice. In contrast, our\nvector-based logarithmic method, despite its theoretically inferior bounds, is\nhighly competitive across all tested scenarios. It is optimal whenever the\nconvex hull becomes large.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convex hull data structures are fundamental in computational geometry. We\nstudy insertion-only data structures, supporting various containment and\nintersection queries. When $P$ is sorted by $x$- or $y$-coordinate, convex\nhulls can be constructed in linear time using classical algorithms such as\nGraham scan. We investigate a variety of methods tailored to the insertion-only\nsetting. We explore a broad selection of trade-offs involving robustness,\nmemory access patterns, and space usage, providing an extensive evaluation of\nboth existing and novel techniques. Logarithmic-time methods rely on\npointer-based tree structures, which suffer in practice due to poor memory\nlocality. Motivated by this, we develop a vector-based solution inspired by\nOvermars' logarithmic method. Our structure has worse asymptotic bounds,\nsupporting queries in $O(\\log^2 n)$ time, but stores data in $O(\\log n)$\ncontiguous vectors, greatly improving cache performance.\n  Through empirical evaluation on real-world and synthetic data sets, we\nuncover surprising trends. Let $h$ denote the size of the convex hull. We show\nthat a na\\\"ive $O(h)$ insertion-only algorithm based on Graham scan\nconsistently outperforms both theoretical and practical state-of-the-art\nmethods under realistic workloads, even on data sets with rather large convex\nhulls. While tree-based methods with $O(\\log h)$ update times offer solid\ntheoretical guarantees, they are never optimal in practice. In contrast, our\nvector-based logarithmic method, despite its theoretically inferior bounds, is\nhighly competitive across all tested scenarios. It is optimal whenever the\nconvex hull becomes large."
                },
                "authors": [
                    {
                        "name": "Ivor van der Hoog"
                    },
                    {
                        "name": "Henrik Reinstädtler"
                    },
                    {
                        "name": "Eva Rotenberg"
                    }
                ],
                "author_detail": {
                    "name": "Eva Rotenberg"
                },
                "author": "Eva Rotenberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17445v1",
                "updated": "2025-08-24T16:52:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    52,
                    37,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T16:52:37Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    52,
                    37,
                    6,
                    236,
                    0
                ],
                "title": "TreePO: Bridging the Gap of Policy Optimization and Efficacy and\n  Inference Efficiency with Heuristic Tree-based Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreePO: Bridging the Gap of Policy Optimization and Efficacy and\n  Inference Efficiency with Heuristic Tree-based Modeling"
                },
                "summary": "Recent advancements in aligning large language models via reinforcement\nlearning have achieved remarkable gains in solving complex reasoning problems,\nbut at the cost of expensive on-policy rollouts and limited exploration of\ndiverse reasoning paths. In this work, we introduce TreePO, involving a\nself-guided rollout algorithm that views sequence generation as a\ntree-structured searching process. Composed of dynamic tree sampling policy and\nfixed-length segment decoding, TreePO leverages local uncertainty to warrant\nadditional branches. By amortizing computation across common prefixes and\npruning low-value paths early, TreePO essentially reduces the per-update\ncompute burden while preserving or enhancing exploration diversity. Key\ncontributions include: (1) a segment-wise sampling algorithm that alleviates\nthe KV cache burden through contiguous segments and spawns new branches along\nwith an early-stop mechanism; (2) a tree-based segment-level advantage\nestimation that considers both global and local proximal policy optimization.\nand (3) analysis on the effectiveness of probability and quality-driven dynamic\ndivergence and fallback strategy. We empirically validate the performance gain\nof TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours\nfrom 22\\% up to 43\\% of the sampling design for the trained models, meanwhile\nshowing up to 40\\% reduction at trajectory-level and 35\\% at token-level\nsampling compute for the existing models. While offering a free lunch of\ninference efficiency, TreePO reveals a practical path toward scaling RL-based\npost-training with fewer samples and less compute. Home page locates at\nhttps://m-a-p.ai/TreePO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in aligning large language models via reinforcement\nlearning have achieved remarkable gains in solving complex reasoning problems,\nbut at the cost of expensive on-policy rollouts and limited exploration of\ndiverse reasoning paths. In this work, we introduce TreePO, involving a\nself-guided rollout algorithm that views sequence generation as a\ntree-structured searching process. Composed of dynamic tree sampling policy and\nfixed-length segment decoding, TreePO leverages local uncertainty to warrant\nadditional branches. By amortizing computation across common prefixes and\npruning low-value paths early, TreePO essentially reduces the per-update\ncompute burden while preserving or enhancing exploration diversity. Key\ncontributions include: (1) a segment-wise sampling algorithm that alleviates\nthe KV cache burden through contiguous segments and spawns new branches along\nwith an early-stop mechanism; (2) a tree-based segment-level advantage\nestimation that considers both global and local proximal policy optimization.\nand (3) analysis on the effectiveness of probability and quality-driven dynamic\ndivergence and fallback strategy. We empirically validate the performance gain\nof TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours\nfrom 22\\% up to 43\\% of the sampling design for the trained models, meanwhile\nshowing up to 40\\% reduction at trajectory-level and 35\\% at token-level\nsampling compute for the existing models. While offering a free lunch of\ninference efficiency, TreePO reveals a practical path toward scaling RL-based\npost-training with fewer samples and less compute. Home page locates at\nhttps://m-a-p.ai/TreePO."
                },
                "authors": [
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Qingshui Gu"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    },
                    {
                        "name": "Ziniu Li"
                    },
                    {
                        "name": "Tianshun Xing"
                    },
                    {
                        "name": "Shuyue Guo"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhao Huang"
                },
                "author": "Wenhao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17434v1",
                "updated": "2025-08-24T16:17:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    17,
                    33,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T16:17:33Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    17,
                    33,
                    6,
                    236,
                    0
                ],
                "title": "TinySR: Pruning Diffusion for Real-World Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinySR: Pruning Diffusion for Real-World Image Super-Resolution"
                },
                "summary": "Real-world image super-resolution (Real-ISR) focuses on recovering\nhigh-quality images from low-resolution inputs that suffer from complex\ndegradations like noise, blur, and compression. Recently, diffusion models\n(DMs) have shown great potential in this area by leveraging strong generative\npriors to restore fine details. However, their iterative denoising process\nincurs high computational overhead, posing challenges for real-time\napplications. Although one-step distillation methods, such as OSEDiff and\nTSD-SR, offer faster inference, they remain fundamentally constrained by their\nlarge, over-parameterized model architectures. In this work, we present TinySR,\na compact yet effective diffusion model specifically designed for Real-ISR that\nachieves real-time performance while maintaining perceptual quality. We\nintroduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy\nto facilitate more effective decision-making in depth pruning. We achieve VAE\ncompression through channel pruning, attention removal and lightweight SepConv.\nWe eliminate time- and prompt-related modules and perform pre-caching\ntechniques to further speed up the model. TinySR significantly reduces\ncomputational cost and model size, achieving up to 5.68x speedup and 83%\nparameter reduction compared to its teacher TSD-SR, while still providing high\nquality results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world image super-resolution (Real-ISR) focuses on recovering\nhigh-quality images from low-resolution inputs that suffer from complex\ndegradations like noise, blur, and compression. Recently, diffusion models\n(DMs) have shown great potential in this area by leveraging strong generative\npriors to restore fine details. However, their iterative denoising process\nincurs high computational overhead, posing challenges for real-time\napplications. Although one-step distillation methods, such as OSEDiff and\nTSD-SR, offer faster inference, they remain fundamentally constrained by their\nlarge, over-parameterized model architectures. In this work, we present TinySR,\na compact yet effective diffusion model specifically designed for Real-ISR that\nachieves real-time performance while maintaining perceptual quality. We\nintroduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy\nto facilitate more effective decision-making in depth pruning. We achieve VAE\ncompression through channel pruning, attention removal and lightweight SepConv.\nWe eliminate time- and prompt-related modules and perform pre-caching\ntechniques to further speed up the model. TinySR significantly reduces\ncomputational cost and model size, achieving up to 5.68x speedup and 83%\nparameter reduction compared to its teacher TSD-SR, while still providing high\nquality results."
                },
                "authors": [
                    {
                        "name": "Linwei Dong"
                    },
                    {
                        "name": "Qingnan Fan"
                    },
                    {
                        "name": "Yuhang Yu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Jinwei Chen"
                    },
                    {
                        "name": "Yawei Luo"
                    },
                    {
                        "name": "Changqing Zou"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zou"
                },
                "author": "Changqing Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.06956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06956v1",
                "updated": "2025-09-08T17:59:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    59,
                    59,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:59:59Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    59,
                    59,
                    0,
                    251,
                    0
                ],
                "title": "H$_{2}$OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H$_{2}$OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose\n  Transformers"
                },
                "summary": "Transformers have been successfully applied in the field of video-based 3D\nhuman pose estimation. However, the high computational costs of these video\npose transformers (VPTs) make them impractical on resource-constrained devices.\nIn this paper, we present a hierarchical plug-and-play pruning-and-recovering\nframework, called Hierarchical Hourglass Tokenizer (H$_{2}$OT), for efficient\ntransformer-based 3D human pose estimation from videos. H$_{2}$OT begins with\nprogressively pruning pose tokens of redundant frames and ends with recovering\nfull-length sequences, resulting in a few pose tokens in the intermediate\ntransformer blocks and thus improving the model efficiency. It works with two\nkey modules, namely, a Token Pruning Module (TPM) and a Token Recovering Module\n(TRM). TPM dynamically selects a few representative tokens to eliminate the\nredundancy of video frames, while TRM restores the detailed spatio-temporal\ninformation based on the selected tokens, thereby expanding the network output\nto the original full-length temporal resolution for fast inference. Our method\nis general-purpose: it can be easily incorporated into common VPT models on\nboth seq2seq and seq2frame pipelines while effectively accommodating different\ntoken pruning and recovery strategies. In addition, our H$_{2}$OT reveals that\nmaintaining the full pose sequence is unnecessary, and a few pose tokens of\nrepresentative frames can achieve both high efficiency and estimation accuracy.\nExtensive experiments on multiple benchmark datasets demonstrate both the\neffectiveness and efficiency of the proposed method. Code and models are\navailable at https://github.com/NationalGAILab/HoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have been successfully applied in the field of video-based 3D\nhuman pose estimation. However, the high computational costs of these video\npose transformers (VPTs) make them impractical on resource-constrained devices.\nIn this paper, we present a hierarchical plug-and-play pruning-and-recovering\nframework, called Hierarchical Hourglass Tokenizer (H$_{2}$OT), for efficient\ntransformer-based 3D human pose estimation from videos. H$_{2}$OT begins with\nprogressively pruning pose tokens of redundant frames and ends with recovering\nfull-length sequences, resulting in a few pose tokens in the intermediate\ntransformer blocks and thus improving the model efficiency. It works with two\nkey modules, namely, a Token Pruning Module (TPM) and a Token Recovering Module\n(TRM). TPM dynamically selects a few representative tokens to eliminate the\nredundancy of video frames, while TRM restores the detailed spatio-temporal\ninformation based on the selected tokens, thereby expanding the network output\nto the original full-length temporal resolution for fast inference. Our method\nis general-purpose: it can be easily incorporated into common VPT models on\nboth seq2seq and seq2frame pipelines while effectively accommodating different\ntoken pruning and recovery strategies. In addition, our H$_{2}$OT reveals that\nmaintaining the full pose sequence is unnecessary, and a few pose tokens of\nrepresentative frames can achieve both high efficiency and estimation accuracy.\nExtensive experiments on multiple benchmark datasets demonstrate both the\neffectiveness and efficiency of the proposed method. Code and models are\navailable at https://github.com/NationalGAILab/HoT."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Mengyuan Liu"
                    },
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Pichao Wang"
                    },
                    {
                        "name": "Shijian Lu"
                    },
                    {
                        "name": "Nicu Sebe"
                    }
                ],
                "author_detail": {
                    "name": "Nicu Sebe"
                },
                "author": "Nicu Sebe",
                "arxiv_comment": "Accepted by TPAMI 2025, Open Sourced. arXiv admin note: substantial\n  text overlap with arXiv:2311.12028",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06464v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06464v4",
                "updated": "2025-09-08T17:59:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    59,
                    48,
                    0,
                    251,
                    0
                ],
                "published": "2024-06-10T17:00:54Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    17,
                    0,
                    54,
                    0,
                    162,
                    0
                ],
                "title": "Transforming Wearable Data into Personal Health Insights using Large\n  Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transforming Wearable Data into Personal Health Insights using Large\n  Language Model Agents"
                },
                "summary": "Deriving personalized insights from popular wearable trackers requires\ncomplex numerical reasoning that challenges standard LLMs, necessitating\ntool-based approaches like code generation. Large language model (LLM) agents\npresent a promising yet largely untapped solution for this analysis at scale.\nWe introduce the Personal Health Insights Agent (PHIA), a system leveraging\nmultistep reasoning with code generation and information retrieval to analyze\nand interpret behavioral health data. To test its capabilities, we create and\nshare two benchmark datasets with over 4000 health insights questions. A\n650-hour human expert evaluation shows that PHIA significantly outperforms a\nstrong code generation baseline, achieving 84% accuracy on objective, numerical\nquestions and, for open-ended ones, earning 83% favorable ratings while being\ntwice as likely to achieve the highest quality rating. This work can advance\nbehavioral health by empowering individuals to understand their data, enabling\na new era of accessible, personalized, and data-driven wellness for the wider\npopulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deriving personalized insights from popular wearable trackers requires\ncomplex numerical reasoning that challenges standard LLMs, necessitating\ntool-based approaches like code generation. Large language model (LLM) agents\npresent a promising yet largely untapped solution for this analysis at scale.\nWe introduce the Personal Health Insights Agent (PHIA), a system leveraging\nmultistep reasoning with code generation and information retrieval to analyze\nand interpret behavioral health data. To test its capabilities, we create and\nshare two benchmark datasets with over 4000 health insights questions. A\n650-hour human expert evaluation shows that PHIA significantly outperforms a\nstrong code generation baseline, achieving 84% accuracy on objective, numerical\nquestions and, for open-ended ones, earning 83% favorable ratings while being\ntwice as likely to achieve the highest quality rating. This work can advance\nbehavioral health by empowering individuals to understand their data, enabling\na new era of accessible, personalized, and data-driven wellness for the wider\npopulation."
                },
                "authors": [
                    {
                        "name": "Mike A. Merrill"
                    },
                    {
                        "name": "Akshay Paruchuri"
                    },
                    {
                        "name": "Naghmeh Rezaei"
                    },
                    {
                        "name": "Geza Kovacs"
                    },
                    {
                        "name": "Javier Perez"
                    },
                    {
                        "name": "Yun Liu"
                    },
                    {
                        "name": "Erik Schenck"
                    },
                    {
                        "name": "Nova Hammerquist"
                    },
                    {
                        "name": "Jake Sunshine"
                    },
                    {
                        "name": "Shyam Tailor"
                    },
                    {
                        "name": "Kumar Ayush"
                    },
                    {
                        "name": "Hao-Wei Su"
                    },
                    {
                        "name": "Qian He"
                    },
                    {
                        "name": "Cory Y. McLean"
                    },
                    {
                        "name": "Mark Malhotra"
                    },
                    {
                        "name": "Shwetak Patel"
                    },
                    {
                        "name": "Jiening Zhan"
                    },
                    {
                        "name": "Tim Althoff"
                    },
                    {
                        "name": "Daniel McDuff"
                    },
                    {
                        "name": "Xin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Liu"
                },
                "author": "Xin Liu",
                "arxiv_comment": "53 pages, 7 main figures, 2 main tables, accepted to Nature\n  Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06464v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06464v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06953v1",
                "updated": "2025-09-08T17:59:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    59,
                    35,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:59:35Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    59,
                    35,
                    0,
                    251,
                    0
                ],
                "title": "Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for\n  Dynamic Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for\n  Dynamic Environments"
                },
                "summary": "Generating collision-free motion in dynamic, partially observable\nenvironments is a fundamental challenge for robotic manipulators. Classical\nmotion planners can compute globally optimal trajectories but require full\nenvironment knowledge and are typically too slow for dynamic scenes. Neural\nmotion policies offer a promising alternative by operating in closed-loop\ndirectly on raw sensory inputs but often struggle to generalize in complex or\ndynamic settings. We propose Deep Reactive Policy (DRP), a visuo-motor neural\nmotion policy designed for reactive motion generation in diverse dynamic\nenvironments, operating directly on point cloud sensory input. At its core is\nIMPACT, a transformer-based neural motion policy pretrained on 10 million\ngenerated expert trajectories across diverse simulation scenarios. We further\nimprove IMPACT's static obstacle avoidance through iterative student-teacher\nfinetuning. We additionally enhance the policy's dynamic obstacle avoidance at\ninference time using DCP-RMP, a locally reactive goal-proposal module. We\nevaluate DRP on challenging tasks featuring cluttered scenes, dynamic moving\nobstacles, and goal obstructions. DRP achieves strong generalization,\noutperforming prior classical and neural methods in success rate across both\nsimulated and real-world settings. Video results and code available at\nhttps://deep-reactive-policy.com",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating collision-free motion in dynamic, partially observable\nenvironments is a fundamental challenge for robotic manipulators. Classical\nmotion planners can compute globally optimal trajectories but require full\nenvironment knowledge and are typically too slow for dynamic scenes. Neural\nmotion policies offer a promising alternative by operating in closed-loop\ndirectly on raw sensory inputs but often struggle to generalize in complex or\ndynamic settings. We propose Deep Reactive Policy (DRP), a visuo-motor neural\nmotion policy designed for reactive motion generation in diverse dynamic\nenvironments, operating directly on point cloud sensory input. At its core is\nIMPACT, a transformer-based neural motion policy pretrained on 10 million\ngenerated expert trajectories across diverse simulation scenarios. We further\nimprove IMPACT's static obstacle avoidance through iterative student-teacher\nfinetuning. We additionally enhance the policy's dynamic obstacle avoidance at\ninference time using DCP-RMP, a locally reactive goal-proposal module. We\nevaluate DRP on challenging tasks featuring cluttered scenes, dynamic moving\nobstacles, and goal obstructions. DRP achieves strong generalization,\noutperforming prior classical and neural methods in success rate across both\nsimulated and real-world settings. Video results and code available at\nhttps://deep-reactive-policy.com"
                },
                "authors": [
                    {
                        "name": "Jiahui Yang"
                    },
                    {
                        "name": "Jason Jingzhou Liu"
                    },
                    {
                        "name": "Yulong Li"
                    },
                    {
                        "name": "Youssef Khaky"
                    },
                    {
                        "name": "Kenneth Shaw"
                    },
                    {
                        "name": "Deepak Pathak"
                    }
                ],
                "author_detail": {
                    "name": "Deepak Pathak"
                },
                "author": "Deepak Pathak",
                "arxiv_comment": "Website at \\url{deep-reactive-policy.com}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06952v1",
                "updated": "2025-09-08T17:59:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    59,
                    32,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:59:32Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    59,
                    32,
                    0,
                    251,
                    0
                ],
                "title": "On the Same Wavelength? Evaluating Pragmatic Reasoning in Language\n  Models across Broad Concepts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Same Wavelength? Evaluating Pragmatic Reasoning in Language\n  Models across Broad Concepts"
                },
                "summary": "Language use is shaped by pragmatics -- i.e., reasoning about communicative\ngoals and norms in context. As language models (LMs) are increasingly used as\nconversational agents, it becomes ever more important to understand their\npragmatic reasoning abilities. We propose an evaluation framework derived from\nWavelength, a popular communication game where a speaker and a listener\ncommunicate about a broad range of concepts in a granular manner. We study a\nrange of LMs on both language comprehension and language production using\ndirect and Chain-of-Thought (CoT) prompting, and further explore a Rational\nSpeech Act (RSA) approach to incorporating Bayesian pragmatic reasoning into LM\ninference. We find that state-of-the-art LMs, but not smaller ones, achieve\nstrong performance on language comprehension, obtaining similar-to-human\naccuracy and exhibiting high correlations with human judgments even without CoT\nprompting or RSA. On language production, CoT can outperform direct prompting,\nand using RSA provides significant improvements over both approaches. Our study\nhelps identify the strengths and limitations in LMs' pragmatic reasoning\nabilities and demonstrates the potential for improving them with RSA, opening\nup future avenues for understanding conceptual representation, language\nunderstanding, and social reasoning in LMs and humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language use is shaped by pragmatics -- i.e., reasoning about communicative\ngoals and norms in context. As language models (LMs) are increasingly used as\nconversational agents, it becomes ever more important to understand their\npragmatic reasoning abilities. We propose an evaluation framework derived from\nWavelength, a popular communication game where a speaker and a listener\ncommunicate about a broad range of concepts in a granular manner. We study a\nrange of LMs on both language comprehension and language production using\ndirect and Chain-of-Thought (CoT) prompting, and further explore a Rational\nSpeech Act (RSA) approach to incorporating Bayesian pragmatic reasoning into LM\ninference. We find that state-of-the-art LMs, but not smaller ones, achieve\nstrong performance on language comprehension, obtaining similar-to-human\naccuracy and exhibiting high correlations with human judgments even without CoT\nprompting or RSA. On language production, CoT can outperform direct prompting,\nand using RSA provides significant improvements over both approaches. Our study\nhelps identify the strengths and limitations in LMs' pragmatic reasoning\nabilities and demonstrates the potential for improving them with RSA, opening\nup future avenues for understanding conceptual representation, language\nunderstanding, and social reasoning in LMs and humans."
                },
                "authors": [
                    {
                        "name": "Linlu Qiu"
                    },
                    {
                        "name": "Cedegao E. Zhang"
                    },
                    {
                        "name": "Joshua B. Tenenbaum"
                    },
                    {
                        "name": "Yoon Kim"
                    },
                    {
                        "name": "Roger P. Levy"
                    }
                ],
                "author_detail": {
                    "name": "Roger P. Levy"
                },
                "author": "Roger P. Levy",
                "arxiv_comment": "EMNLP 2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06949v1",
                "updated": "2025-09-08T17:58:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:58:06Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models"
                },
                "summary": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL"
                },
                "authors": [
                    {
                        "name": "Yinjie Wang"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ke Shen"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06948v1",
                "updated": "2025-09-08T17:58:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    2,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:58:02Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    2,
                    0,
                    251,
                    0
                ],
                "title": "Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning"
                },
                "summary": "Reinforcement learning (RL) has proven effective in incentivizing the\nreasoning abilities of large language models (LLMs), but suffers from severe\nefficiency challenges due to its trial-and-error nature. While the common\npractice employs supervised fine-tuning (SFT) as a warm-up stage for RL, this\ndecoupled two-stage approach limits interaction between SFT and RL, thereby\nconstraining overall effectiveness. This study introduces a novel method for\nlearning reasoning models that employs bilevel optimization to facilitate\nbetter cooperation between these training paradigms. By conditioning the SFT\nobjective on the optimal RL policy, our approach enables SFT to meta-learn how\nto guide RL's optimization process. During training, the lower level performs\nRL updates while simultaneously receiving SFT supervision, and the upper level\nexplicitly maximizes the cooperative gain-the performance advantage of joint\nSFT-RL training over RL alone. Empirical evaluations on five reasoning\nbenchmarks demonstrate that our method consistently outperforms baselines and\nachieves a better balance between effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has proven effective in incentivizing the\nreasoning abilities of large language models (LLMs), but suffers from severe\nefficiency challenges due to its trial-and-error nature. While the common\npractice employs supervised fine-tuning (SFT) as a warm-up stage for RL, this\ndecoupled two-stage approach limits interaction between SFT and RL, thereby\nconstraining overall effectiveness. This study introduces a novel method for\nlearning reasoning models that employs bilevel optimization to facilitate\nbetter cooperation between these training paradigms. By conditioning the SFT\nobjective on the optimal RL policy, our approach enables SFT to meta-learn how\nto guide RL's optimization process. During training, the lower level performs\nRL updates while simultaneously receiving SFT supervision, and the upper level\nexplicitly maximizes the cooperative gain-the performance advantage of joint\nSFT-RL training over RL alone. Empirical evaluations on five reasoning\nbenchmarks demonstrate that our method consistently outperforms baselines and\nachieves a better balance between effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Xueting Han"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Jing Bai"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kam-Fai Wong"
                },
                "author": "Kam-Fai Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06941v1",
                "updated": "2025-09-08T17:52:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    52,
                    56,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:52:56Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    52,
                    56,
                    0,
                    251,
                    0
                ],
                "title": "Outcome-based Exploration for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outcome-based Exploration for LLM Reasoning"
                },
                "summary": "Reinforcement learning (RL) has emerged as a powerful method for improving\nthe reasoning abilities of large language models (LLMs). Outcome-based RL,\nwhich rewards policies solely for the correctness of the final answer, yields\nsubstantial accuracy gains but also induces a systematic loss in generation\ndiversity. This collapse undermines real-world performance, where diversity is\ncritical for test-time scaling. We analyze this phenomenon by viewing RL\npost-training as a sampling process and show that, strikingly, RL can reduce\neffective diversity even on the training set relative to the base model. Our\nstudy highlights two central findings: (i) a transfer of diversity degradation,\nwhere reduced diversity on solved problems propagates to unsolved ones, and\n(ii) the tractability of the outcome space, since reasoning tasks admit only a\nlimited set of distinct answers. Motivated by these insights, we propose\noutcome-based exploration, which assigns exploration bonuses according to final\noutcomes. We introduce two complementary algorithms: historical exploration,\nwhich encourages rarely observed answers via UCB-style bonuses, and batch\nexploration, which penalizes within-batch repetition to promote test-time\ndiversity. Experiments on standard competition math with Llama and Qwen models\ndemonstrate that both methods improve accuracy while mitigating diversity\ncollapse. On the theoretical side, we formalize the benefit of outcome-based\nexploration through a new model of outcome-based bandits. Together, these\ncontributions chart a practical path toward RL methods that enhance reasoning\nwithout sacrificing the diversity essential for scalable deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has emerged as a powerful method for improving\nthe reasoning abilities of large language models (LLMs). Outcome-based RL,\nwhich rewards policies solely for the correctness of the final answer, yields\nsubstantial accuracy gains but also induces a systematic loss in generation\ndiversity. This collapse undermines real-world performance, where diversity is\ncritical for test-time scaling. We analyze this phenomenon by viewing RL\npost-training as a sampling process and show that, strikingly, RL can reduce\neffective diversity even on the training set relative to the base model. Our\nstudy highlights two central findings: (i) a transfer of diversity degradation,\nwhere reduced diversity on solved problems propagates to unsolved ones, and\n(ii) the tractability of the outcome space, since reasoning tasks admit only a\nlimited set of distinct answers. Motivated by these insights, we propose\noutcome-based exploration, which assigns exploration bonuses according to final\noutcomes. We introduce two complementary algorithms: historical exploration,\nwhich encourages rarely observed answers via UCB-style bonuses, and batch\nexploration, which penalizes within-batch repetition to promote test-time\ndiversity. Experiments on standard competition math with Llama and Qwen models\ndemonstrate that both methods improve accuracy while mitigating diversity\ncollapse. On the theoretical side, we formalize the benefit of outcome-based\nexploration through a new model of outcome-based bandits. Together, these\ncontributions chart a practical path toward RL methods that enhance reasoning\nwithout sacrificing the diversity essential for scalable deployment."
                },
                "authors": [
                    {
                        "name": "Yuda Song"
                    },
                    {
                        "name": "Julia Kempe"
                    },
                    {
                        "name": "Remi Munos"
                    }
                ],
                "author_detail": {
                    "name": "Remi Munos"
                },
                "author": "Remi Munos",
                "arxiv_comment": "26 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18082v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18082v2",
                "updated": "2025-09-08T17:49:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    49,
                    17,
                    0,
                    251,
                    0
                ],
                "published": "2025-08-25T14:46:09Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    46,
                    9,
                    0,
                    237,
                    0
                ],
                "title": "GWTC-4.0: Updating the Gravitational-Wave Transient Catalog with\n  Observations from the First Part of the Fourth LIGO-Virgo-KAGRA Observing Run",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GWTC-4.0: Updating the Gravitational-Wave Transient Catalog with\n  Observations from the First Part of the Fourth LIGO-Virgo-KAGRA Observing Run"
                },
                "summary": "Version 4.0 of the Gravitational-Wave Transient Catalog (GWTC-4.0) adds new\ncandidates detected by the LIGO, Virgo, and KAGRA observatories through the\nfirst part of the fourth observing run (O4a: 2023 May 24 15:00:00 to 2024\nJanuary 16 16:00:00 UTC) and a preceding engineering run. In this new data, we\nfind 128 new compact binary coalescence candidates that are identified by at\nleast one of our search algorithms with a probability of astrophysical origin\n$p_{\\rm astro} \\geq 0.5$ and that are not vetoed during event validation. We\nalso provide detailed source property measurements for 86 of these that have a\nfalse alarm rate $< 1 \\rm{yr}^{-1}$. Based on the inferred component masses,\nthese new candidates are consistent with signals from binary black holes and\nneutron star-black hole binaries (GW230518_125908 and GW230529_181500). Median\ninferred component masses of binary black holes in the catalog now range from\n$5.79\\,M_\\odot$ (GW230627_015337) to $137\\,M_\\odot$ (GW231123_135430), while\nGW231123_135430 was probably produced by the most massive binary observed in\nthe catalog. For the first time we have discovered binary black hole signals\nwith network signal-to-noise ratio exceeding 30, GW230814_230901 and\nGW231226_01520, enabling high-fidelity studies of the waveforms and\nastrophysical properties of these systems. Combined with the 90 candidates\nincluded in GWTC-3.0, the catalog now contains 218 candidates with $p_{\\rm\nastro} \\geq 0.5$ and not otherwise vetoed, doubling the size of the catalog and\nfurther opening our view of the gravitational-wave Universe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Version 4.0 of the Gravitational-Wave Transient Catalog (GWTC-4.0) adds new\ncandidates detected by the LIGO, Virgo, and KAGRA observatories through the\nfirst part of the fourth observing run (O4a: 2023 May 24 15:00:00 to 2024\nJanuary 16 16:00:00 UTC) and a preceding engineering run. In this new data, we\nfind 128 new compact binary coalescence candidates that are identified by at\nleast one of our search algorithms with a probability of astrophysical origin\n$p_{\\rm astro} \\geq 0.5$ and that are not vetoed during event validation. We\nalso provide detailed source property measurements for 86 of these that have a\nfalse alarm rate $< 1 \\rm{yr}^{-1}$. Based on the inferred component masses,\nthese new candidates are consistent with signals from binary black holes and\nneutron star-black hole binaries (GW230518_125908 and GW230529_181500). Median\ninferred component masses of binary black holes in the catalog now range from\n$5.79\\,M_\\odot$ (GW230627_015337) to $137\\,M_\\odot$ (GW231123_135430), while\nGW231123_135430 was probably produced by the most massive binary observed in\nthe catalog. For the first time we have discovered binary black hole signals\nwith network signal-to-noise ratio exceeding 30, GW230814_230901 and\nGW231226_01520, enabling high-fidelity studies of the waveforms and\nastrophysical properties of these systems. Combined with the 90 candidates\nincluded in GWTC-3.0, the catalog now contains 218 candidates with $p_{\\rm\nastro} \\geq 0.5$ and not otherwise vetoed, doubling the size of the catalog and\nfurther opening our view of the gravitational-wave Universe."
                },
                "authors": [
                    {
                        "name": "The LIGO Scientific Collaboration"
                    },
                    {
                        "name": "the Virgo Collaboration"
                    },
                    {
                        "name": "the KAGRA Collaboration"
                    },
                    {
                        "name": "A. G. Abac"
                    },
                    {
                        "name": "I. Abouelfettouh"
                    },
                    {
                        "name": "F. Acernese"
                    },
                    {
                        "name": "K. Ackley"
                    },
                    {
                        "name": "C. Adamcewicz"
                    },
                    {
                        "name": "S. Adhicary"
                    },
                    {
                        "name": "D. Adhikari"
                    },
                    {
                        "name": "N. Adhikari"
                    },
                    {
                        "name": "R. X. Adhikari"
                    },
                    {
                        "name": "V. K. Adkins"
                    },
                    {
                        "name": "S. Afroz"
                    },
                    {
                        "name": "A. Agapito"
                    },
                    {
                        "name": "D. Agarwal"
                    },
                    {
                        "name": "M. Agathos"
                    },
                    {
                        "name": "N. Aggarwal"
                    },
                    {
                        "name": "S. Aggarwal"
                    },
                    {
                        "name": "O. D. Aguiar"
                    },
                    {
                        "name": "I. -L. Ahrend"
                    },
                    {
                        "name": "L. Aiello"
                    },
                    {
                        "name": "A. Ain"
                    },
                    {
                        "name": "P. Ajith"
                    },
                    {
                        "name": "T. Akutsu"
                    },
                    {
                        "name": "S. Albanesi"
                    },
                    {
                        "name": "W. Ali"
                    },
                    {
                        "name": "S. Al-Kershi"
                    },
                    {
                        "name": "C. Alléné"
                    },
                    {
                        "name": "A. Allocca"
                    },
                    {
                        "name": "S. Al-Shammari"
                    },
                    {
                        "name": "P. A. Altin"
                    },
                    {
                        "name": "S. Alvarez-Lopez"
                    },
                    {
                        "name": "W. Amar"
                    },
                    {
                        "name": "O. Amarasinghe"
                    },
                    {
                        "name": "A. Amato"
                    },
                    {
                        "name": "F. Amicucci"
                    },
                    {
                        "name": "C. Amra"
                    },
                    {
                        "name": "A. Ananyeva"
                    },
                    {
                        "name": "S. B. Anderson"
                    },
                    {
                        "name": "W. G. Anderson"
                    },
                    {
                        "name": "M. Andia"
                    },
                    {
                        "name": "M. Ando"
                    },
                    {
                        "name": "M. Andrés-Carcasona"
                    },
                    {
                        "name": "T. Andrić"
                    },
                    {
                        "name": "J. Anglin"
                    },
                    {
                        "name": "S. Ansoldi"
                    },
                    {
                        "name": "J. M. Antelis"
                    },
                    {
                        "name": "S. Antier"
                    },
                    {
                        "name": "M. Aoumi"
                    },
                    {
                        "name": "E. Z. Appavuravther"
                    },
                    {
                        "name": "S. Appert"
                    },
                    {
                        "name": "S. K. Apple"
                    },
                    {
                        "name": "K. Arai"
                    },
                    {
                        "name": "A. Araya"
                    },
                    {
                        "name": "M. C. Araya"
                    },
                    {
                        "name": "M. Arca Sedda"
                    },
                    {
                        "name": "J. S. Areeda"
                    },
                    {
                        "name": "N. Aritomi"
                    },
                    {
                        "name": "F. Armato"
                    },
                    {
                        "name": "S. Armstrong"
                    },
                    {
                        "name": "N. Arnaud"
                    },
                    {
                        "name": "M. Arogeti"
                    },
                    {
                        "name": "S. M. Aronson"
                    },
                    {
                        "name": "K. G. Arun"
                    },
                    {
                        "name": "G. Ashton"
                    },
                    {
                        "name": "Y. Aso"
                    },
                    {
                        "name": "L. Asprea"
                    },
                    {
                        "name": "M. Assiduo"
                    },
                    {
                        "name": "S. Assis de Souza Melo"
                    },
                    {
                        "name": "S. M. Aston"
                    },
                    {
                        "name": "P. Astone"
                    },
                    {
                        "name": "F. Attadio"
                    },
                    {
                        "name": "F. Aubin"
                    },
                    {
                        "name": "K. AultONeal"
                    },
                    {
                        "name": "G. Avallone"
                    },
                    {
                        "name": "E. A. Avila"
                    },
                    {
                        "name": "S. Babak"
                    },
                    {
                        "name": "C. Badger"
                    },
                    {
                        "name": "S. Bae"
                    },
                    {
                        "name": "S. Bagnasco"
                    },
                    {
                        "name": "L. Baiotti"
                    },
                    {
                        "name": "R. Bajpai"
                    },
                    {
                        "name": "T. Baka"
                    },
                    {
                        "name": "A. M. Baker"
                    },
                    {
                        "name": "K. A. Baker"
                    },
                    {
                        "name": "T. Baker"
                    },
                    {
                        "name": "G. Baldi"
                    },
                    {
                        "name": "N. Baldicchi"
                    },
                    {
                        "name": "M. Ball"
                    },
                    {
                        "name": "G. Ballardin"
                    },
                    {
                        "name": "S. W. Ballmer"
                    },
                    {
                        "name": "S. Banagiri"
                    },
                    {
                        "name": "B. Banerjee"
                    },
                    {
                        "name": "D. Bankar"
                    },
                    {
                        "name": "T. M. Baptiste"
                    },
                    {
                        "name": "P. Baral"
                    },
                    {
                        "name": "M. Baratti"
                    },
                    {
                        "name": "J. C. Barayoga"
                    },
                    {
                        "name": "B. C. Barish"
                    },
                    {
                        "name": "D. Barker"
                    },
                    {
                        "name": "N. Barman"
                    },
                    {
                        "name": "P. Barneo"
                    },
                    {
                        "name": "F. Barone"
                    },
                    {
                        "name": "B. Barr"
                    },
                    {
                        "name": "L. Barsotti"
                    },
                    {
                        "name": "M. Barsuglia"
                    },
                    {
                        "name": "D. Barta"
                    },
                    {
                        "name": "A. M. Bartoletti"
                    },
                    {
                        "name": "M. A. Barton"
                    },
                    {
                        "name": "I. Bartos"
                    },
                    {
                        "name": "A. Basalaev"
                    },
                    {
                        "name": "R. Bassiri"
                    },
                    {
                        "name": "A. Basti"
                    },
                    {
                        "name": "M. Bawaj"
                    },
                    {
                        "name": "P. Baxi"
                    },
                    {
                        "name": "J. C. Bayley"
                    },
                    {
                        "name": "A. C. Baylor"
                    },
                    {
                        "name": "P. A. Baynard II"
                    },
                    {
                        "name": "M. Bazzan"
                    },
                    {
                        "name": "V. M. Bedakihale"
                    },
                    {
                        "name": "F. Beirnaert"
                    },
                    {
                        "name": "M. Bejger"
                    },
                    {
                        "name": "D. Belardinelli"
                    },
                    {
                        "name": "A. S. Bell"
                    },
                    {
                        "name": "D. S. Bellie"
                    },
                    {
                        "name": "L. Bellizzi"
                    },
                    {
                        "name": "W. Benoit"
                    },
                    {
                        "name": "I. Bentara"
                    },
                    {
                        "name": "J. D. Bentley"
                    },
                    {
                        "name": "M. Ben Yaala"
                    },
                    {
                        "name": "S. Bera"
                    },
                    {
                        "name": "F. Bergamin"
                    },
                    {
                        "name": "B. K. Berger"
                    },
                    {
                        "name": "S. Bernuzzi"
                    },
                    {
                        "name": "M. Beroiz"
                    },
                    {
                        "name": "C. P. L. Berry"
                    },
                    {
                        "name": "D. Bersanetti"
                    },
                    {
                        "name": "T. Bertheas"
                    },
                    {
                        "name": "A. Bertolini"
                    },
                    {
                        "name": "J. Betzwieser"
                    },
                    {
                        "name": "D. Beveridge"
                    },
                    {
                        "name": "G. Bevilacqua"
                    },
                    {
                        "name": "N. Bevins"
                    },
                    {
                        "name": "R. Bhandare"
                    },
                    {
                        "name": "R. Bhatt"
                    },
                    {
                        "name": "D. Bhattacharjee"
                    },
                    {
                        "name": "S. Bhattacharyya"
                    },
                    {
                        "name": "S. Bhaumik"
                    },
                    {
                        "name": "V. Biancalana"
                    },
                    {
                        "name": "A. Bianchi"
                    },
                    {
                        "name": "I. A. Bilenko"
                    },
                    {
                        "name": "G. Billingsley"
                    },
                    {
                        "name": "A. Binetti"
                    },
                    {
                        "name": "S. Bini"
                    },
                    {
                        "name": "C. Binu"
                    },
                    {
                        "name": "S. Biot"
                    },
                    {
                        "name": "O. Birnholtz"
                    },
                    {
                        "name": "S. Biscoveanu"
                    },
                    {
                        "name": "A. Bisht"
                    },
                    {
                        "name": "M. Bitossi"
                    },
                    {
                        "name": "M. -A. Bizouard"
                    },
                    {
                        "name": "S. Blaber"
                    },
                    {
                        "name": "J. K. Blackburn"
                    },
                    {
                        "name": "L. A. Blagg"
                    },
                    {
                        "name": "C. D. Blair"
                    },
                    {
                        "name": "D. G. Blair"
                    },
                    {
                        "name": "N. Bode"
                    },
                    {
                        "name": "N. Boettner"
                    },
                    {
                        "name": "G. Boileau"
                    },
                    {
                        "name": "M. Boldrini"
                    },
                    {
                        "name": "G. N. Bolingbroke"
                    },
                    {
                        "name": "A. Bolliand"
                    },
                    {
                        "name": "L. D. Bonavena"
                    },
                    {
                        "name": "R. Bondarescu"
                    },
                    {
                        "name": "F. Bondu"
                    },
                    {
                        "name": "E. Bonilla"
                    },
                    {
                        "name": "M. S. Bonilla"
                    },
                    {
                        "name": "A. Bonino"
                    },
                    {
                        "name": "R. Bonnand"
                    },
                    {
                        "name": "A. Borchers"
                    },
                    {
                        "name": "S. Borhanian"
                    },
                    {
                        "name": "V. Boschi"
                    },
                    {
                        "name": "S. Bose"
                    },
                    {
                        "name": "V. Bossilkov"
                    },
                    {
                        "name": "Y. Bothra"
                    },
                    {
                        "name": "A. Boudon"
                    },
                    {
                        "name": "L. Bourg"
                    },
                    {
                        "name": "M. Boyle"
                    },
                    {
                        "name": "A. Bozzi"
                    },
                    {
                        "name": "C. Bradaschia"
                    },
                    {
                        "name": "P. R. Brady"
                    },
                    {
                        "name": "A. Branch"
                    },
                    {
                        "name": "M. Branchesi"
                    },
                    {
                        "name": "I. Braun"
                    },
                    {
                        "name": "T. Briant"
                    },
                    {
                        "name": "A. Brillet"
                    },
                    {
                        "name": "M. Brinkmann"
                    },
                    {
                        "name": "P. Brockill"
                    },
                    {
                        "name": "E. Brockmueller"
                    },
                    {
                        "name": "A. F. Brooks"
                    },
                    {
                        "name": "B. C. Brown"
                    },
                    {
                        "name": "D. D. Brown"
                    },
                    {
                        "name": "M. L. Brozzetti"
                    },
                    {
                        "name": "S. Brunett"
                    },
                    {
                        "name": "G. Bruno"
                    },
                    {
                        "name": "R. Bruntz"
                    },
                    {
                        "name": "J. Bryant"
                    },
                    {
                        "name": "Y. Bu"
                    },
                    {
                        "name": "F. Bucci"
                    },
                    {
                        "name": "J. Buchanan"
                    },
                    {
                        "name": "O. Bulashenko"
                    },
                    {
                        "name": "T. Bulik"
                    },
                    {
                        "name": "H. J. Bulten"
                    },
                    {
                        "name": "A. Buonanno"
                    },
                    {
                        "name": "K. Burtnyk"
                    },
                    {
                        "name": "R. Buscicchio"
                    },
                    {
                        "name": "D. Buskulic"
                    },
                    {
                        "name": "C. Buy"
                    },
                    {
                        "name": "R. L. Byer"
                    },
                    {
                        "name": "G. S. Cabourn Davies"
                    },
                    {
                        "name": "R. Cabrita"
                    },
                    {
                        "name": "V. Cáceres-Barbosa"
                    },
                    {
                        "name": "L. Cadonati"
                    },
                    {
                        "name": "G. Cagnoli"
                    },
                    {
                        "name": "C. Cahillane"
                    },
                    {
                        "name": "A. Calafat"
                    },
                    {
                        "name": "T. A. Callister"
                    },
                    {
                        "name": "E. Calloni"
                    },
                    {
                        "name": "S. R. Callos"
                    },
                    {
                        "name": "M. Canepa"
                    },
                    {
                        "name": "G. Caneva Santoro"
                    },
                    {
                        "name": "K. C. Cannon"
                    },
                    {
                        "name": "H. Cao"
                    },
                    {
                        "name": "L. A. Capistran"
                    },
                    {
                        "name": "E. Capocasa"
                    },
                    {
                        "name": "E. Capote"
                    },
                    {
                        "name": "G. Capurri"
                    },
                    {
                        "name": "G. Carapella"
                    },
                    {
                        "name": "F. Carbognani"
                    },
                    {
                        "name": "M. Carlassara"
                    },
                    {
                        "name": "J. B. Carlin"
                    },
                    {
                        "name": "T. K. Carlson"
                    },
                    {
                        "name": "M. F. Carney"
                    },
                    {
                        "name": "M. Carpinelli"
                    },
                    {
                        "name": "G. Carrillo"
                    },
                    {
                        "name": "J. J. Carter"
                    },
                    {
                        "name": "G. Carullo"
                    },
                    {
                        "name": "A. Casallas-Lagos"
                    },
                    {
                        "name": "J. Casanueva Diaz"
                    },
                    {
                        "name": "C. Casentini"
                    },
                    {
                        "name": "S. Y. Castro-Lucas"
                    },
                    {
                        "name": "S. Caudill"
                    },
                    {
                        "name": "M. Cavaglià"
                    },
                    {
                        "name": "R. Cavalieri"
                    },
                    {
                        "name": "A. Ceja"
                    },
                    {
                        "name": "G. Cella"
                    },
                    {
                        "name": "P. Cerdá-Durán"
                    },
                    {
                        "name": "E. Cesarini"
                    },
                    {
                        "name": "N. Chabbra"
                    },
                    {
                        "name": "W. Chaibi"
                    },
                    {
                        "name": "A. Chakraborty"
                    },
                    {
                        "name": "P. Chakraborty"
                    },
                    {
                        "name": "S. Chakraborty"
                    },
                    {
                        "name": "S. Chalathadka Subrahmanya"
                    },
                    {
                        "name": "J. C. L. Chan"
                    },
                    {
                        "name": "M. Chan"
                    },
                    {
                        "name": "K. Chang"
                    },
                    {
                        "name": "S. Chao"
                    },
                    {
                        "name": "P. Charlton"
                    },
                    {
                        "name": "E. Chassande-Mottin"
                    },
                    {
                        "name": "C. Chatterjee"
                    },
                    {
                        "name": "Debarati Chatterjee"
                    },
                    {
                        "name": "Deep Chatterjee"
                    },
                    {
                        "name": "M. Chaturvedi"
                    },
                    {
                        "name": "S. Chaty"
                    },
                    {
                        "name": "K. Chatziioannou"
                    },
                    {
                        "name": "A. Chen"
                    },
                    {
                        "name": "A. H. -Y. Chen"
                    },
                    {
                        "name": "D. Chen"
                    },
                    {
                        "name": "H. Chen"
                    },
                    {
                        "name": "H. Y. Chen"
                    },
                    {
                        "name": "S. Chen"
                    },
                    {
                        "name": "Yanbei Chen"
                    },
                    {
                        "name": "Yitian Chen"
                    },
                    {
                        "name": "H. P. Cheng"
                    },
                    {
                        "name": "P. Chessa"
                    },
                    {
                        "name": "H. T. Cheung"
                    },
                    {
                        "name": "S. Y. Cheung"
                    },
                    {
                        "name": "F. Chiadini"
                    },
                    {
                        "name": "G. Chiarini"
                    },
                    {
                        "name": "A. Chiba"
                    },
                    {
                        "name": "A. Chincarini"
                    },
                    {
                        "name": "M. L. Chiofalo"
                    },
                    {
                        "name": "A. Chiummo"
                    },
                    {
                        "name": "C. Chou"
                    },
                    {
                        "name": "S. Choudhary"
                    },
                    {
                        "name": "N. Christensen"
                    },
                    {
                        "name": "S. S. Y. Chua"
                    },
                    {
                        "name": "G. Ciani"
                    },
                    {
                        "name": "P. Ciecielag"
                    },
                    {
                        "name": "M. Cieślar"
                    },
                    {
                        "name": "M. Cifaldi"
                    },
                    {
                        "name": "B. Cirok"
                    },
                    {
                        "name": "F. Clara"
                    },
                    {
                        "name": "J. A. Clark"
                    },
                    {
                        "name": "T. A. Clarke"
                    },
                    {
                        "name": "P. Clearwater"
                    },
                    {
                        "name": "S. Clesse"
                    },
                    {
                        "name": "F. Cleva"
                    },
                    {
                        "name": "E. Coccia"
                    },
                    {
                        "name": "E. Codazzo"
                    },
                    {
                        "name": "P. -F. Cohadon"
                    },
                    {
                        "name": "S. Colace"
                    },
                    {
                        "name": "E. Colangeli"
                    },
                    {
                        "name": "M. Colleoni"
                    },
                    {
                        "name": "C. G. Collette"
                    },
                    {
                        "name": "J. Collins"
                    },
                    {
                        "name": "S. Colloms"
                    },
                    {
                        "name": "A. Colombo"
                    },
                    {
                        "name": "C. M. Compton"
                    },
                    {
                        "name": "G. Connolly"
                    },
                    {
                        "name": "L. Conti"
                    },
                    {
                        "name": "T. R. Corbitt"
                    },
                    {
                        "name": "I. Cordero-Carrión"
                    },
                    {
                        "name": "S. Corezzi"
                    },
                    {
                        "name": "N. J. Cornish"
                    },
                    {
                        "name": "I. Coronado"
                    },
                    {
                        "name": "A. Corsi"
                    },
                    {
                        "name": "R. Cottingham"
                    },
                    {
                        "name": "M. W. Coughlin"
                    },
                    {
                        "name": "A. Couineaux"
                    },
                    {
                        "name": "P. Couvares"
                    },
                    {
                        "name": "D. M. Coward"
                    },
                    {
                        "name": "R. Coyne"
                    },
                    {
                        "name": "A. Cozzumbo"
                    },
                    {
                        "name": "J. D. E. Creighton"
                    },
                    {
                        "name": "T. D. Creighton"
                    },
                    {
                        "name": "P. Cremonese"
                    },
                    {
                        "name": "S. Crook"
                    },
                    {
                        "name": "R. Crouch"
                    },
                    {
                        "name": "J. Csizmazia"
                    },
                    {
                        "name": "J. R. Cudell"
                    },
                    {
                        "name": "T. J. Cullen"
                    },
                    {
                        "name": "A. Cumming"
                    },
                    {
                        "name": "E. Cuoco"
                    },
                    {
                        "name": "M. Cusinato"
                    },
                    {
                        "name": "L. V. Da Conceição"
                    },
                    {
                        "name": "T. Dal Canton"
                    },
                    {
                        "name": "S. Dal Pra"
                    },
                    {
                        "name": "G. Dálya"
                    },
                    {
                        "name": "B. D'Angelo"
                    },
                    {
                        "name": "S. Danilishin"
                    },
                    {
                        "name": "S. D'Antonio"
                    },
                    {
                        "name": "K. Danzmann"
                    },
                    {
                        "name": "K. E. Darroch"
                    },
                    {
                        "name": "L. P. Dartez"
                    },
                    {
                        "name": "R. Das"
                    },
                    {
                        "name": "A. Dasgupta"
                    },
                    {
                        "name": "V. Dattilo"
                    },
                    {
                        "name": "A. Daumas"
                    },
                    {
                        "name": "N. Davari"
                    },
                    {
                        "name": "I. Dave"
                    },
                    {
                        "name": "A. Davenport"
                    },
                    {
                        "name": "M. Davier"
                    },
                    {
                        "name": "T. F. Davies"
                    },
                    {
                        "name": "D. Davis"
                    },
                    {
                        "name": "L. Davis"
                    },
                    {
                        "name": "M. C. Davis"
                    },
                    {
                        "name": "P. Davis"
                    },
                    {
                        "name": "E. J. Daw"
                    },
                    {
                        "name": "M. Dax"
                    },
                    {
                        "name": "J. De Bolle"
                    },
                    {
                        "name": "M. Deenadayalan"
                    },
                    {
                        "name": "J. Degallaix"
                    },
                    {
                        "name": "M. De Laurentis"
                    },
                    {
                        "name": "F. De Lillo"
                    },
                    {
                        "name": "S. Della Torre"
                    },
                    {
                        "name": "W. Del Pozzo"
                    },
                    {
                        "name": "A. Demagny"
                    },
                    {
                        "name": "F. De Marco"
                    },
                    {
                        "name": "G. Demasi"
                    },
                    {
                        "name": "F. De Matteis"
                    },
                    {
                        "name": "N. Demos"
                    },
                    {
                        "name": "T. Dent"
                    },
                    {
                        "name": "A. Depasse"
                    },
                    {
                        "name": "N. DePergola"
                    },
                    {
                        "name": "R. De Pietri"
                    },
                    {
                        "name": "R. De Rosa"
                    },
                    {
                        "name": "C. De Rossi"
                    },
                    {
                        "name": "M. Desai"
                    },
                    {
                        "name": "R. DeSalvo"
                    },
                    {
                        "name": "A. DeSimone"
                    },
                    {
                        "name": "R. De Simone"
                    },
                    {
                        "name": "A. Dhani"
                    },
                    {
                        "name": "R. Diab"
                    },
                    {
                        "name": "M. C. Díaz"
                    },
                    {
                        "name": "M. Di Cesare"
                    },
                    {
                        "name": "G. Dideron"
                    },
                    {
                        "name": "T. Dietrich"
                    },
                    {
                        "name": "L. Di Fiore"
                    },
                    {
                        "name": "C. Di Fronzo"
                    },
                    {
                        "name": "M. Di Giovanni"
                    },
                    {
                        "name": "T. Di Girolamo"
                    },
                    {
                        "name": "D. Diksha"
                    },
                    {
                        "name": "J. Ding"
                    },
                    {
                        "name": "S. Di Pace"
                    },
                    {
                        "name": "I. Di Palma"
                    },
                    {
                        "name": "D. Di Piero"
                    },
                    {
                        "name": "F. Di Renzo"
                    },
                    {
                        "name": "Divyajyoti"
                    },
                    {
                        "name": "A. Dmitriev"
                    },
                    {
                        "name": "J. P. Docherty"
                    },
                    {
                        "name": "Z. Doctor"
                    },
                    {
                        "name": "N. Doerksen"
                    },
                    {
                        "name": "E. Dohmen"
                    },
                    {
                        "name": "A. Doke"
                    },
                    {
                        "name": "A. Domiciano De Souza"
                    },
                    {
                        "name": "L. D'Onofrio"
                    },
                    {
                        "name": "F. Donovan"
                    },
                    {
                        "name": "K. L. Dooley"
                    },
                    {
                        "name": "T. Dooney"
                    },
                    {
                        "name": "S. Doravari"
                    },
                    {
                        "name": "O. Dorosh"
                    },
                    {
                        "name": "W. J. D. Doyle"
                    },
                    {
                        "name": "M. Drago"
                    },
                    {
                        "name": "J. C. Driggers"
                    },
                    {
                        "name": "L. Dunn"
                    },
                    {
                        "name": "U. Dupletsa"
                    },
                    {
                        "name": "P. -A. Duverne"
                    },
                    {
                        "name": "D. D'Urso"
                    },
                    {
                        "name": "P. Dutta Roy"
                    },
                    {
                        "name": "H. Duval"
                    },
                    {
                        "name": "S. E. Dwyer"
                    },
                    {
                        "name": "C. Eassa"
                    },
                    {
                        "name": "M. Ebersold"
                    },
                    {
                        "name": "T. Eckhardt"
                    },
                    {
                        "name": "G. Eddolls"
                    },
                    {
                        "name": "A. Effler"
                    },
                    {
                        "name": "J. Eichholz"
                    },
                    {
                        "name": "H. Einsle"
                    },
                    {
                        "name": "M. Eisenmann"
                    },
                    {
                        "name": "M. Emma"
                    },
                    {
                        "name": "K. Endo"
                    },
                    {
                        "name": "R. Enficiaud"
                    },
                    {
                        "name": "L. Errico"
                    },
                    {
                        "name": "R. Espinosa"
                    },
                    {
                        "name": "M. Esposito"
                    },
                    {
                        "name": "R. C. Essick"
                    },
                    {
                        "name": "H. Estellés"
                    },
                    {
                        "name": "T. Etzel"
                    },
                    {
                        "name": "M. Evans"
                    },
                    {
                        "name": "T. Evstafyeva"
                    },
                    {
                        "name": "B. E. Ewing"
                    },
                    {
                        "name": "J. M. Ezquiaga"
                    },
                    {
                        "name": "F. Fabrizi"
                    },
                    {
                        "name": "V. Fafone"
                    },
                    {
                        "name": "S. Fairhurst"
                    },
                    {
                        "name": "A. M. Farah"
                    },
                    {
                        "name": "B. Farr"
                    },
                    {
                        "name": "W. M. Farr"
                    },
                    {
                        "name": "G. Favaro"
                    },
                    {
                        "name": "M. Favata"
                    },
                    {
                        "name": "M. Fays"
                    },
                    {
                        "name": "M. Fazio"
                    },
                    {
                        "name": "J. Feicht"
                    },
                    {
                        "name": "M. M. Fejer"
                    },
                    {
                        "name": "R. Felicetti"
                    },
                    {
                        "name": "E. Fenyvesi"
                    },
                    {
                        "name": "J. Fernandes"
                    },
                    {
                        "name": "T. Fernandes"
                    },
                    {
                        "name": "D. Fernando"
                    },
                    {
                        "name": "S. Ferraiuolo"
                    },
                    {
                        "name": "T. A. Ferreira"
                    },
                    {
                        "name": "F. Fidecaro"
                    },
                    {
                        "name": "P. Figura"
                    },
                    {
                        "name": "A. Fiori"
                    },
                    {
                        "name": "I. Fiori"
                    },
                    {
                        "name": "M. Fishbach"
                    },
                    {
                        "name": "R. P. Fisher"
                    },
                    {
                        "name": "R. Fittipaldi"
                    },
                    {
                        "name": "V. Fiumara"
                    },
                    {
                        "name": "R. Flaminio"
                    },
                    {
                        "name": "S. M. Fleischer"
                    },
                    {
                        "name": "L. S. Fleming"
                    },
                    {
                        "name": "E. Floden"
                    },
                    {
                        "name": "H. Fong"
                    },
                    {
                        "name": "J. A. Font"
                    },
                    {
                        "name": "F. Fontinele-Nunes"
                    },
                    {
                        "name": "C. Foo"
                    },
                    {
                        "name": "B. Fornal"
                    },
                    {
                        "name": "K. Franceschetti"
                    },
                    {
                        "name": "F. Frappez"
                    },
                    {
                        "name": "S. Frasca"
                    },
                    {
                        "name": "F. Frasconi"
                    },
                    {
                        "name": "J. P. Freed"
                    },
                    {
                        "name": "Z. Frei"
                    },
                    {
                        "name": "A. Freise"
                    },
                    {
                        "name": "O. Freitas"
                    },
                    {
                        "name": "R. Frey"
                    },
                    {
                        "name": "W. Frischhertz"
                    },
                    {
                        "name": "P. Fritschel"
                    },
                    {
                        "name": "V. V. Frolov"
                    },
                    {
                        "name": "G. G. Fronzé"
                    },
                    {
                        "name": "M. Fuentes-Garcia"
                    },
                    {
                        "name": "S. Fujii"
                    },
                    {
                        "name": "T. Fujimori"
                    },
                    {
                        "name": "P. Fulda"
                    },
                    {
                        "name": "M. Fyffe"
                    },
                    {
                        "name": "B. Gadre"
                    },
                    {
                        "name": "J. R. Gair"
                    },
                    {
                        "name": "S. Galaudage"
                    },
                    {
                        "name": "V. Galdi"
                    },
                    {
                        "name": "R. Gamba"
                    },
                    {
                        "name": "A. Gamboa"
                    },
                    {
                        "name": "S. Gamoji"
                    },
                    {
                        "name": "D. Ganapathy"
                    },
                    {
                        "name": "A. Ganguly"
                    },
                    {
                        "name": "B. Garaventa"
                    },
                    {
                        "name": "J. García-Bellido"
                    },
                    {
                        "name": "C. García-Quirós"
                    },
                    {
                        "name": "J. W. Gardner"
                    },
                    {
                        "name": "K. A. Gardner"
                    },
                    {
                        "name": "S. Garg"
                    },
                    {
                        "name": "J. Gargiulo"
                    },
                    {
                        "name": "X. Garrido"
                    },
                    {
                        "name": "A. Garron"
                    },
                    {
                        "name": "F. Garufi"
                    },
                    {
                        "name": "P. A. Garver"
                    },
                    {
                        "name": "C. Gasbarra"
                    },
                    {
                        "name": "B. Gateley"
                    },
                    {
                        "name": "F. Gautier"
                    },
                    {
                        "name": "V. Gayathri"
                    },
                    {
                        "name": "T. Gayer"
                    },
                    {
                        "name": "G. Gemme"
                    },
                    {
                        "name": "A. Gennai"
                    },
                    {
                        "name": "V. Gennari"
                    },
                    {
                        "name": "J. George"
                    },
                    {
                        "name": "R. George"
                    },
                    {
                        "name": "O. Gerberding"
                    },
                    {
                        "name": "L. Gergely"
                    },
                    {
                        "name": "Archisman Ghosh"
                    },
                    {
                        "name": "Sayantan Ghosh"
                    },
                    {
                        "name": "Shaon Ghosh"
                    },
                    {
                        "name": "Shrobana Ghosh"
                    },
                    {
                        "name": "Suprovo Ghosh"
                    },
                    {
                        "name": "Tathagata Ghosh"
                    },
                    {
                        "name": "J. A. Giaime"
                    },
                    {
                        "name": "K. D. Giardina"
                    },
                    {
                        "name": "D. R. Gibson"
                    },
                    {
                        "name": "C. Gier"
                    },
                    {
                        "name": "S. Gkaitatzis"
                    },
                    {
                        "name": "J. Glanzer"
                    },
                    {
                        "name": "F. Glotin"
                    },
                    {
                        "name": "J. Godfrey"
                    },
                    {
                        "name": "R. V. Godley"
                    },
                    {
                        "name": "P. Godwin"
                    },
                    {
                        "name": "A. S. Goettel"
                    },
                    {
                        "name": "E. Goetz"
                    },
                    {
                        "name": "J. Golomb"
                    },
                    {
                        "name": "S. Gomez Lopez"
                    },
                    {
                        "name": "B. Goncharov"
                    },
                    {
                        "name": "G. González"
                    },
                    {
                        "name": "P. Goodarzi"
                    },
                    {
                        "name": "S. Goode"
                    },
                    {
                        "name": "A. W. Goodwin-Jones"
                    },
                    {
                        "name": "M. Gosselin"
                    },
                    {
                        "name": "R. Gouaty"
                    },
                    {
                        "name": "D. W. Gould"
                    },
                    {
                        "name": "K. Govorkova"
                    },
                    {
                        "name": "A. Grado"
                    },
                    {
                        "name": "V. Graham"
                    },
                    {
                        "name": "A. E. Granados"
                    },
                    {
                        "name": "M. Granata"
                    },
                    {
                        "name": "V. Granata"
                    },
                    {
                        "name": "S. Gras"
                    },
                    {
                        "name": "P. Grassia"
                    },
                    {
                        "name": "J. Graves"
                    },
                    {
                        "name": "C. Gray"
                    },
                    {
                        "name": "R. Gray"
                    },
                    {
                        "name": "G. Greco"
                    },
                    {
                        "name": "A. C. Green"
                    },
                    {
                        "name": "L. Green"
                    },
                    {
                        "name": "S. M. Green"
                    },
                    {
                        "name": "S. R. Green"
                    },
                    {
                        "name": "C. Greenberg"
                    },
                    {
                        "name": "A. M. Gretarsson"
                    },
                    {
                        "name": "H. K. Griffin"
                    },
                    {
                        "name": "D. Griffith"
                    },
                    {
                        "name": "H. L. Griggs"
                    },
                    {
                        "name": "G. Grignani"
                    },
                    {
                        "name": "C. Grimaud"
                    },
                    {
                        "name": "H. Grote"
                    },
                    {
                        "name": "S. Grunewald"
                    },
                    {
                        "name": "D. Guerra"
                    },
                    {
                        "name": "D. Guetta"
                    },
                    {
                        "name": "G. M. Guidi"
                    },
                    {
                        "name": "A. R. Guimaraes"
                    },
                    {
                        "name": "H. K. Gulati"
                    },
                    {
                        "name": "F. Gulminelli"
                    },
                    {
                        "name": "H. Guo"
                    },
                    {
                        "name": "W. Guo"
                    },
                    {
                        "name": "Y. Guo"
                    },
                    {
                        "name": "Anuradha Gupta"
                    },
                    {
                        "name": "I. Gupta"
                    },
                    {
                        "name": "N. C. Gupta"
                    },
                    {
                        "name": "S. K. Gupta"
                    },
                    {
                        "name": "V. Gupta"
                    },
                    {
                        "name": "N. Gupte"
                    },
                    {
                        "name": "J. Gurs"
                    },
                    {
                        "name": "N. Gutierrez"
                    },
                    {
                        "name": "N. Guttman"
                    },
                    {
                        "name": "F. Guzman"
                    },
                    {
                        "name": "D. Haba"
                    },
                    {
                        "name": "M. Haberland"
                    },
                    {
                        "name": "S. Haino"
                    },
                    {
                        "name": "E. D. Hall"
                    },
                    {
                        "name": "E. Z. Hamilton"
                    },
                    {
                        "name": "G. Hammond"
                    },
                    {
                        "name": "M. Haney"
                    },
                    {
                        "name": "J. Hanks"
                    },
                    {
                        "name": "C. Hanna"
                    },
                    {
                        "name": "M. D. Hannam"
                    },
                    {
                        "name": "O. A. Hannuksela"
                    },
                    {
                        "name": "A. G. Hanselman"
                    },
                    {
                        "name": "H. Hansen"
                    },
                    {
                        "name": "J. Hanson"
                    },
                    {
                        "name": "S. Hanumasagar"
                    },
                    {
                        "name": "R. Harada"
                    },
                    {
                        "name": "A. R. Hardison"
                    },
                    {
                        "name": "S. Harikumar"
                    },
                    {
                        "name": "K. Haris"
                    },
                    {
                        "name": "I. Harley-Trochimczyk"
                    },
                    {
                        "name": "T. Harmark"
                    },
                    {
                        "name": "J. Harms"
                    },
                    {
                        "name": "G. M. Harry"
                    },
                    {
                        "name": "I. W. Harry"
                    },
                    {
                        "name": "J. Hart"
                    },
                    {
                        "name": "B. Haskell"
                    },
                    {
                        "name": "C. J. Haster"
                    },
                    {
                        "name": "K. Haughian"
                    },
                    {
                        "name": "H. Hayakawa"
                    },
                    {
                        "name": "K. Hayama"
                    },
                    {
                        "name": "M. C. Heintze"
                    },
                    {
                        "name": "J. Heinze"
                    },
                    {
                        "name": "J. Heinzel"
                    },
                    {
                        "name": "H. Heitmann"
                    },
                    {
                        "name": "F. Hellman"
                    },
                    {
                        "name": "A. F. Helmling-Cornell"
                    },
                    {
                        "name": "G. Hemming"
                    },
                    {
                        "name": "O. Henderson-Sapir"
                    },
                    {
                        "name": "M. Hendry"
                    },
                    {
                        "name": "I. S. Heng"
                    },
                    {
                        "name": "M. H. Hennig"
                    },
                    {
                        "name": "C. Henshaw"
                    },
                    {
                        "name": "M. Heurs"
                    },
                    {
                        "name": "A. L. Hewitt"
                    },
                    {
                        "name": "J. Heynen"
                    },
                    {
                        "name": "J. Heyns"
                    },
                    {
                        "name": "S. Higginbotham"
                    },
                    {
                        "name": "S. Hild"
                    },
                    {
                        "name": "S. Hill"
                    },
                    {
                        "name": "Y. Himemoto"
                    },
                    {
                        "name": "N. Hirata"
                    },
                    {
                        "name": "C. Hirose"
                    },
                    {
                        "name": "D. Hofman"
                    },
                    {
                        "name": "B. E. Hogan"
                    },
                    {
                        "name": "N. A. Holland"
                    },
                    {
                        "name": "I. J. Hollows"
                    },
                    {
                        "name": "D. E. Holz"
                    },
                    {
                        "name": "L. Honet"
                    },
                    {
                        "name": "D. J. Horton-Bailey"
                    },
                    {
                        "name": "J. Hough"
                    },
                    {
                        "name": "S. Hourihane"
                    },
                    {
                        "name": "N. T. Howard"
                    },
                    {
                        "name": "E. J. Howell"
                    },
                    {
                        "name": "C. G. Hoy"
                    },
                    {
                        "name": "C. A. Hrishikesh"
                    },
                    {
                        "name": "P. Hsi"
                    },
                    {
                        "name": "H. -F. Hsieh"
                    },
                    {
                        "name": "H. -Y. Hsieh"
                    },
                    {
                        "name": "C. Hsiung"
                    },
                    {
                        "name": "S. -H. Hsu"
                    },
                    {
                        "name": "W. -F. Hsu"
                    },
                    {
                        "name": "Q. Hu"
                    },
                    {
                        "name": "H. Y. Huang"
                    },
                    {
                        "name": "Y. Huang"
                    },
                    {
                        "name": "Y. T. Huang"
                    },
                    {
                        "name": "A. D. Huddart"
                    },
                    {
                        "name": "B. Hughey"
                    },
                    {
                        "name": "V. Hui"
                    },
                    {
                        "name": "S. Husa"
                    },
                    {
                        "name": "R. Huxford"
                    },
                    {
                        "name": "L. Iampieri"
                    },
                    {
                        "name": "G. A. Iandolo"
                    },
                    {
                        "name": "M. Ianni"
                    },
                    {
                        "name": "G. Iannone"
                    },
                    {
                        "name": "J. Iascau"
                    },
                    {
                        "name": "K. Ide"
                    },
                    {
                        "name": "R. Iden"
                    },
                    {
                        "name": "A. Ierardi"
                    },
                    {
                        "name": "S. Ikeda"
                    },
                    {
                        "name": "H. Imafuku"
                    },
                    {
                        "name": "Y. Inoue"
                    },
                    {
                        "name": "G. Iorio"
                    },
                    {
                        "name": "P. Iosif"
                    },
                    {
                        "name": "M. H. Iqbal"
                    },
                    {
                        "name": "J. Irwin"
                    },
                    {
                        "name": "R. Ishikawa"
                    },
                    {
                        "name": "M. Isi"
                    },
                    {
                        "name": "K. S. Isleif"
                    },
                    {
                        "name": "Y. Itoh"
                    },
                    {
                        "name": "M. Iwaya"
                    },
                    {
                        "name": "B. R. Iyer"
                    },
                    {
                        "name": "C. Jacquet"
                    },
                    {
                        "name": "P. -E. Jacquet"
                    },
                    {
                        "name": "T. Jacquot"
                    },
                    {
                        "name": "S. J. Jadhav"
                    },
                    {
                        "name": "S. P. Jadhav"
                    },
                    {
                        "name": "M. Jain"
                    },
                    {
                        "name": "T. Jain"
                    },
                    {
                        "name": "A. L. James"
                    },
                    {
                        "name": "K. Jani"
                    },
                    {
                        "name": "J. Janquart"
                    },
                    {
                        "name": "N. N. Janthalur"
                    },
                    {
                        "name": "S. Jaraba"
                    },
                    {
                        "name": "P. Jaranowski"
                    },
                    {
                        "name": "R. Jaume"
                    },
                    {
                        "name": "W. Javed"
                    },
                    {
                        "name": "A. Jennings"
                    },
                    {
                        "name": "M. Jensen"
                    },
                    {
                        "name": "W. Jia"
                    },
                    {
                        "name": "J. Jiang"
                    },
                    {
                        "name": "H. -B. Jin"
                    },
                    {
                        "name": "G. R. Johns"
                    },
                    {
                        "name": "N. A. Johnson"
                    },
                    {
                        "name": "M. C. Johnston"
                    },
                    {
                        "name": "R. Johnston"
                    },
                    {
                        "name": "N. Johny"
                    },
                    {
                        "name": "D. H. Jones"
                    },
                    {
                        "name": "D. I. Jones"
                    },
                    {
                        "name": "R. Jones"
                    },
                    {
                        "name": "H. E. Jose"
                    },
                    {
                        "name": "P. Joshi"
                    },
                    {
                        "name": "S. K. Joshi"
                    },
                    {
                        "name": "G. Joubert"
                    },
                    {
                        "name": "J. Ju"
                    },
                    {
                        "name": "L. Ju"
                    },
                    {
                        "name": "K. Jung"
                    },
                    {
                        "name": "J. Junker"
                    },
                    {
                        "name": "V. Juste"
                    },
                    {
                        "name": "H. B. Kabagoz"
                    },
                    {
                        "name": "T. Kajita"
                    },
                    {
                        "name": "I. Kaku"
                    },
                    {
                        "name": "V. Kalogera"
                    },
                    {
                        "name": "M. Kalomenopoulos"
                    },
                    {
                        "name": "M. Kamiizumi"
                    },
                    {
                        "name": "N. Kanda"
                    },
                    {
                        "name": "S. Kandhasamy"
                    },
                    {
                        "name": "G. Kang"
                    },
                    {
                        "name": "N. C. Kannachel"
                    },
                    {
                        "name": "J. B. Kanner"
                    },
                    {
                        "name": "S. A. KantiMahanty"
                    },
                    {
                        "name": "S. J. Kapadia"
                    },
                    {
                        "name": "D. P. Kapasi"
                    },
                    {
                        "name": "M. Karthikeyan"
                    },
                    {
                        "name": "M. Kasprzack"
                    },
                    {
                        "name": "H. Kato"
                    },
                    {
                        "name": "T. Kato"
                    },
                    {
                        "name": "E. Katsavounidis"
                    },
                    {
                        "name": "W. Katzman"
                    },
                    {
                        "name": "R. Kaushik"
                    },
                    {
                        "name": "K. Kawabe"
                    },
                    {
                        "name": "R. Kawamoto"
                    },
                    {
                        "name": "D. Keitel"
                    },
                    {
                        "name": "L. J. Kemperman"
                    },
                    {
                        "name": "J. Kennington"
                    },
                    {
                        "name": "F. A. Kerkow"
                    },
                    {
                        "name": "R. Kesharwani"
                    },
                    {
                        "name": "J. S. Key"
                    },
                    {
                        "name": "R. Khadela"
                    },
                    {
                        "name": "S. Khadka"
                    },
                    {
                        "name": "S. S. Khadkikar"
                    },
                    {
                        "name": "F. Y. Khalili"
                    },
                    {
                        "name": "F. Khan"
                    },
                    {
                        "name": "T. Khanam"
                    },
                    {
                        "name": "M. Khursheed"
                    },
                    {
                        "name": "N. M. Khusid"
                    },
                    {
                        "name": "W. Kiendrebeogo"
                    },
                    {
                        "name": "N. Kijbunchoo"
                    },
                    {
                        "name": "C. Kim"
                    },
                    {
                        "name": "J. C. Kim"
                    },
                    {
                        "name": "K. Kim"
                    },
                    {
                        "name": "M. H. Kim"
                    },
                    {
                        "name": "S. Kim"
                    },
                    {
                        "name": "Y. -M. Kim"
                    },
                    {
                        "name": "C. Kimball"
                    },
                    {
                        "name": "K. Kimes"
                    },
                    {
                        "name": "M. Kinnear"
                    },
                    {
                        "name": "J. S. Kissel"
                    },
                    {
                        "name": "S. Klimenko"
                    },
                    {
                        "name": "A. M. Knee"
                    },
                    {
                        "name": "E. J. Knox"
                    },
                    {
                        "name": "N. Knust"
                    },
                    {
                        "name": "K. Kobayashi"
                    },
                    {
                        "name": "S. M. Koehlenbeck"
                    },
                    {
                        "name": "G. Koekoek"
                    },
                    {
                        "name": "K. Kohri"
                    },
                    {
                        "name": "K. Kokeyama"
                    },
                    {
                        "name": "S. Koley"
                    },
                    {
                        "name": "P. Kolitsidou"
                    },
                    {
                        "name": "A. E. Koloniari"
                    },
                    {
                        "name": "K. Komori"
                    },
                    {
                        "name": "A. K. H. Kong"
                    },
                    {
                        "name": "A. Kontos"
                    },
                    {
                        "name": "L. M. Koponen"
                    },
                    {
                        "name": "M. Korobko"
                    },
                    {
                        "name": "X. Kou"
                    },
                    {
                        "name": "A. Koushik"
                    },
                    {
                        "name": "N. Kouvatsos"
                    },
                    {
                        "name": "M. Kovalam"
                    },
                    {
                        "name": "T. Koyama"
                    },
                    {
                        "name": "D. B. Kozak"
                    },
                    {
                        "name": "S. L. Kranzhoff"
                    },
                    {
                        "name": "V. Kringel"
                    },
                    {
                        "name": "N. V. Krishnendu"
                    },
                    {
                        "name": "S. Kroker"
                    },
                    {
                        "name": "A. Królak"
                    },
                    {
                        "name": "K. Kruska"
                    },
                    {
                        "name": "J. Kubisz"
                    },
                    {
                        "name": "G. Kuehn"
                    },
                    {
                        "name": "S. Kulkarni"
                    },
                    {
                        "name": "A. Kulur Ramamohan"
                    },
                    {
                        "name": "Achal Kumar"
                    },
                    {
                        "name": "Anil Kumar"
                    },
                    {
                        "name": "Praveen Kumar"
                    },
                    {
                        "name": "Prayush Kumar"
                    },
                    {
                        "name": "Rahul Kumar"
                    },
                    {
                        "name": "Rakesh Kumar"
                    },
                    {
                        "name": "J. Kume"
                    },
                    {
                        "name": "K. Kuns"
                    },
                    {
                        "name": "N. Kuntimaddi"
                    },
                    {
                        "name": "S. Kuroyanagi"
                    },
                    {
                        "name": "S. Kuwahara"
                    },
                    {
                        "name": "K. Kwak"
                    },
                    {
                        "name": "K. Kwan"
                    },
                    {
                        "name": "S. Kwon"
                    },
                    {
                        "name": "G. Lacaille"
                    },
                    {
                        "name": "D. Laghi"
                    },
                    {
                        "name": "A. H. Laity"
                    },
                    {
                        "name": "E. Lalande"
                    },
                    {
                        "name": "M. Lalleman"
                    },
                    {
                        "name": "P. C. Lalremruati"
                    },
                    {
                        "name": "M. Landry"
                    },
                    {
                        "name": "B. B. Lane"
                    },
                    {
                        "name": "R. N. Lang"
                    },
                    {
                        "name": "J. Lange"
                    },
                    {
                        "name": "R. Langgin"
                    },
                    {
                        "name": "B. Lantz"
                    },
                    {
                        "name": "I. La Rosa"
                    },
                    {
                        "name": "J. Larsen"
                    },
                    {
                        "name": "A. Lartaux-Vollard"
                    },
                    {
                        "name": "P. D. Lasky"
                    },
                    {
                        "name": "J. Lawrence"
                    },
                    {
                        "name": "M. Laxen"
                    },
                    {
                        "name": "C. Lazarte"
                    },
                    {
                        "name": "A. Lazzarini"
                    },
                    {
                        "name": "C. Lazzaro"
                    },
                    {
                        "name": "P. Leaci"
                    },
                    {
                        "name": "L. Leali"
                    },
                    {
                        "name": "Y. K. Lecoeuche"
                    },
                    {
                        "name": "H. M. Lee"
                    },
                    {
                        "name": "H. W. Lee"
                    },
                    {
                        "name": "J. Lee"
                    },
                    {
                        "name": "K. Lee"
                    },
                    {
                        "name": "R. -K. Lee"
                    },
                    {
                        "name": "R. Lee"
                    },
                    {
                        "name": "Sungho Lee"
                    },
                    {
                        "name": "Sunjae Lee"
                    },
                    {
                        "name": "Y. Lee"
                    },
                    {
                        "name": "I. N. Legred"
                    },
                    {
                        "name": "J. Lehmann"
                    },
                    {
                        "name": "L. Lehner"
                    },
                    {
                        "name": "M. Le Jean"
                    },
                    {
                        "name": "A. Lemaître"
                    },
                    {
                        "name": "M. Lenti"
                    },
                    {
                        "name": "M. Leonardi"
                    },
                    {
                        "name": "M. Lequime"
                    },
                    {
                        "name": "N. Leroy"
                    },
                    {
                        "name": "M. Lesovsky"
                    },
                    {
                        "name": "N. Letendre"
                    },
                    {
                        "name": "M. Lethuillier"
                    },
                    {
                        "name": "Y. Levin"
                    },
                    {
                        "name": "K. Leyde"
                    },
                    {
                        "name": "A. K. Y. Li"
                    },
                    {
                        "name": "K. L. Li"
                    },
                    {
                        "name": "T. G. F. Li"
                    },
                    {
                        "name": "X. Li"
                    },
                    {
                        "name": "Y. Li"
                    },
                    {
                        "name": "Z. Li"
                    },
                    {
                        "name": "A. Lihos"
                    },
                    {
                        "name": "E. T. Lin"
                    },
                    {
                        "name": "F. Lin"
                    },
                    {
                        "name": "L. C. -C. Lin"
                    },
                    {
                        "name": "Y. -C. Lin"
                    },
                    {
                        "name": "C. Lindsay"
                    },
                    {
                        "name": "S. D. Linker"
                    },
                    {
                        "name": "A. Liu"
                    },
                    {
                        "name": "G. C. Liu"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "F. Llamas Villarreal"
                    },
                    {
                        "name": "J. Llobera-Querol"
                    },
                    {
                        "name": "R. K. L. Lo"
                    },
                    {
                        "name": "J. -P. Locquet"
                    },
                    {
                        "name": "S. C. G. Loggins"
                    },
                    {
                        "name": "M. R. Loizou"
                    },
                    {
                        "name": "L. T. London"
                    },
                    {
                        "name": "A. Longo"
                    },
                    {
                        "name": "D. Lopez"
                    },
                    {
                        "name": "M. Lopez Portilla"
                    },
                    {
                        "name": "M. Lorenzini"
                    },
                    {
                        "name": "A. Lorenzo-Medina"
                    },
                    {
                        "name": "V. Loriette"
                    },
                    {
                        "name": "M. Lormand"
                    },
                    {
                        "name": "G. Losurdo"
                    },
                    {
                        "name": "E. Lotti"
                    },
                    {
                        "name": "T. P. Lott IV"
                    },
                    {
                        "name": "J. D. Lough"
                    },
                    {
                        "name": "H. A. Loughlin"
                    },
                    {
                        "name": "C. O. Lousto"
                    },
                    {
                        "name": "N. Low"
                    },
                    {
                        "name": "N. Lu"
                    },
                    {
                        "name": "L. Lucchesi"
                    },
                    {
                        "name": "H. Lück"
                    },
                    {
                        "name": "D. Lumaca"
                    },
                    {
                        "name": "A. P. Lundgren"
                    },
                    {
                        "name": "A. W. Lussier"
                    },
                    {
                        "name": "R. Macas"
                    },
                    {
                        "name": "M. MacInnis"
                    },
                    {
                        "name": "D. M. Macleod"
                    },
                    {
                        "name": "I. A. O. MacMillan"
                    },
                    {
                        "name": "A. Macquet"
                    },
                    {
                        "name": "K. Maeda"
                    },
                    {
                        "name": "S. Maenaut"
                    },
                    {
                        "name": "S. S. Magare"
                    },
                    {
                        "name": "R. M. Magee"
                    },
                    {
                        "name": "E. Maggio"
                    },
                    {
                        "name": "R. Maggiore"
                    },
                    {
                        "name": "M. Magnozzi"
                    },
                    {
                        "name": "M. Mahesh"
                    },
                    {
                        "name": "M. Maini"
                    },
                    {
                        "name": "S. Majhi"
                    },
                    {
                        "name": "E. Majorana"
                    },
                    {
                        "name": "C. N. Makarem"
                    },
                    {
                        "name": "D. Malakar"
                    },
                    {
                        "name": "J. A. Malaquias-Reis"
                    },
                    {
                        "name": "U. Mali"
                    },
                    {
                        "name": "S. Maliakal"
                    },
                    {
                        "name": "A. Malik"
                    },
                    {
                        "name": "L. Mallick"
                    },
                    {
                        "name": "A. -K. Malz"
                    },
                    {
                        "name": "N. Man"
                    },
                    {
                        "name": "M. Mancarella"
                    },
                    {
                        "name": "V. Mandic"
                    },
                    {
                        "name": "V. Mangano"
                    },
                    {
                        "name": "B. Mannix"
                    },
                    {
                        "name": "G. L. Mansell"
                    },
                    {
                        "name": "M. Manske"
                    },
                    {
                        "name": "M. Mantovani"
                    },
                    {
                        "name": "M. Mapelli"
                    },
                    {
                        "name": "C. Marinelli"
                    },
                    {
                        "name": "F. Marion"
                    },
                    {
                        "name": "A. S. Markosyan"
                    },
                    {
                        "name": "A. Markowitz"
                    },
                    {
                        "name": "E. Maros"
                    },
                    {
                        "name": "S. Marsat"
                    },
                    {
                        "name": "F. Martelli"
                    },
                    {
                        "name": "I. W. Martin"
                    },
                    {
                        "name": "R. M. Martin"
                    },
                    {
                        "name": "B. B. Martinez"
                    },
                    {
                        "name": "D. A. Martinez"
                    },
                    {
                        "name": "M. Martinez"
                    },
                    {
                        "name": "V. Martinez"
                    },
                    {
                        "name": "A. Martini"
                    },
                    {
                        "name": "J. C. Martins"
                    },
                    {
                        "name": "D. V. Martynov"
                    },
                    {
                        "name": "E. J. Marx"
                    },
                    {
                        "name": "L. Massaro"
                    },
                    {
                        "name": "A. Masserot"
                    },
                    {
                        "name": "M. Masso-Reid"
                    },
                    {
                        "name": "S. Mastrogiovanni"
                    },
                    {
                        "name": "T. Matcovich"
                    },
                    {
                        "name": "M. Matiushechkina"
                    },
                    {
                        "name": "L. Maurin"
                    },
                    {
                        "name": "N. Mavalvala"
                    },
                    {
                        "name": "N. Maxwell"
                    },
                    {
                        "name": "G. McCarrol"
                    },
                    {
                        "name": "R. McCarthy"
                    },
                    {
                        "name": "D. E. McClelland"
                    },
                    {
                        "name": "S. McCormick"
                    },
                    {
                        "name": "L. McCuller"
                    },
                    {
                        "name": "S. McEachin"
                    },
                    {
                        "name": "C. McElhenny"
                    },
                    {
                        "name": "G. I. McGhee"
                    },
                    {
                        "name": "J. McGinn"
                    },
                    {
                        "name": "K. B. M. McGowan"
                    },
                    {
                        "name": "J. McIver"
                    },
                    {
                        "name": "A. McLeod"
                    },
                    {
                        "name": "I. McMahon"
                    },
                    {
                        "name": "T. McRae"
                    },
                    {
                        "name": "R. McTeague"
                    },
                    {
                        "name": "D. Meacher"
                    },
                    {
                        "name": "B. N. Meagher"
                    },
                    {
                        "name": "R. Mechum"
                    },
                    {
                        "name": "Q. Meijer"
                    },
                    {
                        "name": "A. Melatos"
                    },
                    {
                        "name": "C. S. Menoni"
                    },
                    {
                        "name": "F. Mera"
                    },
                    {
                        "name": "R. A. Mercer"
                    },
                    {
                        "name": "L. Mereni"
                    },
                    {
                        "name": "K. Merfeld"
                    },
                    {
                        "name": "E. L. Merilh"
                    },
                    {
                        "name": "J. R. Mérou"
                    },
                    {
                        "name": "J. D. Merritt"
                    },
                    {
                        "name": "M. Merzougui"
                    },
                    {
                        "name": "C. Messick"
                    },
                    {
                        "name": "B. Mestichelli"
                    },
                    {
                        "name": "M. Meyer-Conde"
                    },
                    {
                        "name": "F. Meylahn"
                    },
                    {
                        "name": "A. Mhaske"
                    },
                    {
                        "name": "A. Miani"
                    },
                    {
                        "name": "H. Miao"
                    },
                    {
                        "name": "C. Michel"
                    },
                    {
                        "name": "Y. Michimura"
                    },
                    {
                        "name": "H. Middleton"
                    },
                    {
                        "name": "D. P. Mihaylov"
                    },
                    {
                        "name": "A. L. Miller"
                    },
                    {
                        "name": "S. J. Miller"
                    },
                    {
                        "name": "M. Millhouse"
                    },
                    {
                        "name": "E. Milotti"
                    },
                    {
                        "name": "V. Milotti"
                    },
                    {
                        "name": "Y. Minenkov"
                    },
                    {
                        "name": "E. M. Minihan"
                    },
                    {
                        "name": "Ll. M. Mir"
                    },
                    {
                        "name": "L. Mirasola"
                    },
                    {
                        "name": "M. Miravet-Tenés"
                    },
                    {
                        "name": "C. -A. Miritescu"
                    },
                    {
                        "name": "A. Mishra"
                    },
                    {
                        "name": "C. Mishra"
                    },
                    {
                        "name": "T. Mishra"
                    },
                    {
                        "name": "A. L. Mitchell"
                    },
                    {
                        "name": "J. G. Mitchell"
                    },
                    {
                        "name": "S. Mitra"
                    },
                    {
                        "name": "V. P. Mitrofanov"
                    },
                    {
                        "name": "K. Mitsuhashi"
                    },
                    {
                        "name": "R. Mittleman"
                    },
                    {
                        "name": "O. Miyakawa"
                    },
                    {
                        "name": "S. Miyoki"
                    },
                    {
                        "name": "A. Miyoko"
                    },
                    {
                        "name": "G. Mo"
                    },
                    {
                        "name": "L. Mobilia"
                    },
                    {
                        "name": "S. R. P. Mohapatra"
                    },
                    {
                        "name": "S. R. Mohite"
                    },
                    {
                        "name": "M. Molina-Ruiz"
                    },
                    {
                        "name": "M. Mondin"
                    },
                    {
                        "name": "M. Montani"
                    },
                    {
                        "name": "C. J. Moore"
                    },
                    {
                        "name": "D. Moraru"
                    },
                    {
                        "name": "A. More"
                    },
                    {
                        "name": "S. More"
                    },
                    {
                        "name": "C. Moreno"
                    },
                    {
                        "name": "E. A. Moreno"
                    },
                    {
                        "name": "G. Moreno"
                    },
                    {
                        "name": "A. Moreso Serra"
                    },
                    {
                        "name": "S. Morisaki"
                    },
                    {
                        "name": "Y. Moriwaki"
                    },
                    {
                        "name": "G. Morras"
                    },
                    {
                        "name": "A. Moscatello"
                    },
                    {
                        "name": "M. Mould"
                    },
                    {
                        "name": "B. Mours"
                    },
                    {
                        "name": "C. M. Mow-Lowry"
                    },
                    {
                        "name": "L. Muccillo"
                    },
                    {
                        "name": "F. Muciaccia"
                    },
                    {
                        "name": "D. Mukherjee"
                    },
                    {
                        "name": "Samanwaya Mukherjee"
                    },
                    {
                        "name": "Soma Mukherjee"
                    },
                    {
                        "name": "Subroto Mukherjee"
                    },
                    {
                        "name": "Suvodip Mukherjee"
                    },
                    {
                        "name": "N. Mukund"
                    },
                    {
                        "name": "A. Mullavey"
                    },
                    {
                        "name": "H. Mullock"
                    },
                    {
                        "name": "J. Mundi"
                    },
                    {
                        "name": "C. L. Mungioli"
                    },
                    {
                        "name": "M. Murakoshi"
                    },
                    {
                        "name": "P. G. Murray"
                    },
                    {
                        "name": "D. Nabari"
                    },
                    {
                        "name": "S. L. Nadji"
                    },
                    {
                        "name": "A. Nagar"
                    },
                    {
                        "name": "N. Nagarajan"
                    },
                    {
                        "name": "K. Nakagaki"
                    },
                    {
                        "name": "K. Nakamura"
                    },
                    {
                        "name": "H. Nakano"
                    },
                    {
                        "name": "M. Nakano"
                    },
                    {
                        "name": "D. Nanadoumgar-Lacroze"
                    },
                    {
                        "name": "D. Nandi"
                    },
                    {
                        "name": "V. Napolano"
                    },
                    {
                        "name": "P. Narayan"
                    },
                    {
                        "name": "I. Nardecchia"
                    },
                    {
                        "name": "T. Narikawa"
                    },
                    {
                        "name": "H. Narola"
                    },
                    {
                        "name": "L. Naticchioni"
                    },
                    {
                        "name": "R. K. Nayak"
                    },
                    {
                        "name": "L. Negri"
                    },
                    {
                        "name": "A. Nela"
                    },
                    {
                        "name": "C. Nelle"
                    },
                    {
                        "name": "A. Nelson"
                    },
                    {
                        "name": "T. J. N. Nelson"
                    },
                    {
                        "name": "M. Nery"
                    },
                    {
                        "name": "A. Neunzert"
                    },
                    {
                        "name": "S. Ng"
                    },
                    {
                        "name": "L. Nguyen Quynh"
                    },
                    {
                        "name": "S. A. Nichols"
                    },
                    {
                        "name": "A. B. Nielsen"
                    },
                    {
                        "name": "Y. Nishino"
                    },
                    {
                        "name": "A. Nishizawa"
                    },
                    {
                        "name": "S. Nissanke"
                    },
                    {
                        "name": "W. Niu"
                    },
                    {
                        "name": "F. Nocera"
                    },
                    {
                        "name": "J. Noller"
                    },
                    {
                        "name": "M. Norman"
                    },
                    {
                        "name": "C. North"
                    },
                    {
                        "name": "J. Novak"
                    },
                    {
                        "name": "R. Nowicki"
                    },
                    {
                        "name": "J. F. Nuño Siles"
                    },
                    {
                        "name": "L. K. Nuttall"
                    },
                    {
                        "name": "K. Obayashi"
                    },
                    {
                        "name": "J. Oberling"
                    },
                    {
                        "name": "J. O'Dell"
                    },
                    {
                        "name": "E. Oelker"
                    },
                    {
                        "name": "M. Oertel"
                    },
                    {
                        "name": "G. Oganesyan"
                    },
                    {
                        "name": "T. O'Hanlon"
                    },
                    {
                        "name": "M. Ohashi"
                    },
                    {
                        "name": "F. Ohme"
                    },
                    {
                        "name": "R. Oliveri"
                    },
                    {
                        "name": "R. Omer"
                    },
                    {
                        "name": "B. O'Neal"
                    },
                    {
                        "name": "M. Onishi"
                    },
                    {
                        "name": "K. Oohara"
                    },
                    {
                        "name": "B. O'Reilly"
                    },
                    {
                        "name": "M. Orselli"
                    },
                    {
                        "name": "R. O'Shaughnessy"
                    },
                    {
                        "name": "S. O'Shea"
                    },
                    {
                        "name": "S. Oshino"
                    },
                    {
                        "name": "C. Osthelder"
                    },
                    {
                        "name": "I. Ota"
                    },
                    {
                        "name": "D. J. Ottaway"
                    },
                    {
                        "name": "A. Ouzriat"
                    },
                    {
                        "name": "H. Overmier"
                    },
                    {
                        "name": "B. J. Owen"
                    },
                    {
                        "name": "R. Ozaki"
                    },
                    {
                        "name": "A. E. Pace"
                    },
                    {
                        "name": "R. Pagano"
                    },
                    {
                        "name": "M. A. Page"
                    },
                    {
                        "name": "A. Pai"
                    },
                    {
                        "name": "L. Paiella"
                    },
                    {
                        "name": "A. Pal"
                    },
                    {
                        "name": "S. Pal"
                    },
                    {
                        "name": "M. A. Palaia"
                    },
                    {
                        "name": "M. Pálfi"
                    },
                    {
                        "name": "P. P. Palma"
                    },
                    {
                        "name": "C. Palomba"
                    },
                    {
                        "name": "P. Palud"
                    },
                    {
                        "name": "H. Pan"
                    },
                    {
                        "name": "J. Pan"
                    },
                    {
                        "name": "K. C. Pan"
                    },
                    {
                        "name": "P. K. Panda"
                    },
                    {
                        "name": "Shiksha Pandey"
                    },
                    {
                        "name": "Swadha Pandey"
                    },
                    {
                        "name": "P. T. H. Pang"
                    },
                    {
                        "name": "F. Pannarale"
                    },
                    {
                        "name": "K. A. Pannone"
                    },
                    {
                        "name": "B. C. Pant"
                    },
                    {
                        "name": "F. H. Panther"
                    },
                    {
                        "name": "M. Panzeri"
                    },
                    {
                        "name": "F. Paoletti"
                    },
                    {
                        "name": "A. Paolone"
                    },
                    {
                        "name": "A. Papadopoulos"
                    },
                    {
                        "name": "E. E. Papalexakis"
                    },
                    {
                        "name": "L. Papalini"
                    },
                    {
                        "name": "G. Papigkiotis"
                    },
                    {
                        "name": "A. Paquis"
                    },
                    {
                        "name": "A. Parisi"
                    },
                    {
                        "name": "B. -J. Park"
                    },
                    {
                        "name": "J. Park"
                    },
                    {
                        "name": "W. Parker"
                    },
                    {
                        "name": "G. Pascale"
                    },
                    {
                        "name": "D. Pascucci"
                    },
                    {
                        "name": "A. Pasqualetti"
                    },
                    {
                        "name": "R. Passaquieti"
                    },
                    {
                        "name": "L. Passenger"
                    },
                    {
                        "name": "D. Passuello"
                    },
                    {
                        "name": "O. Patane"
                    },
                    {
                        "name": "A. V. Patel"
                    },
                    {
                        "name": "D. Pathak"
                    },
                    {
                        "name": "A. Patra"
                    },
                    {
                        "name": "B. Patricelli"
                    },
                    {
                        "name": "B. G. Patterson"
                    },
                    {
                        "name": "K. Paul"
                    },
                    {
                        "name": "S. Paul"
                    },
                    {
                        "name": "E. Payne"
                    },
                    {
                        "name": "T. Pearce"
                    },
                    {
                        "name": "M. Pedraza"
                    },
                    {
                        "name": "A. Pele"
                    },
                    {
                        "name": "F. E. Peña Arellano"
                    },
                    {
                        "name": "X. Peng"
                    },
                    {
                        "name": "Y. Peng"
                    },
                    {
                        "name": "S. Penn"
                    },
                    {
                        "name": "M. D. Penuliar"
                    },
                    {
                        "name": "A. Perego"
                    },
                    {
                        "name": "Z. Pereira"
                    },
                    {
                        "name": "C. Périgois"
                    },
                    {
                        "name": "G. Perna"
                    },
                    {
                        "name": "A. Perreca"
                    },
                    {
                        "name": "J. Perret"
                    },
                    {
                        "name": "S. Perriès"
                    },
                    {
                        "name": "J. W. Perry"
                    },
                    {
                        "name": "D. Pesios"
                    },
                    {
                        "name": "S. Peters"
                    },
                    {
                        "name": "S. Petracca"
                    },
                    {
                        "name": "C. Petrillo"
                    },
                    {
                        "name": "H. P. Pfeiffer"
                    },
                    {
                        "name": "H. Pham"
                    },
                    {
                        "name": "K. A. Pham"
                    },
                    {
                        "name": "K. S. Phukon"
                    },
                    {
                        "name": "H. Phurailatpam"
                    },
                    {
                        "name": "M. Piarulli"
                    },
                    {
                        "name": "L. Piccari"
                    },
                    {
                        "name": "O. J. Piccinni"
                    },
                    {
                        "name": "M. Pichot"
                    },
                    {
                        "name": "M. Piendibene"
                    },
                    {
                        "name": "F. Piergiovanni"
                    },
                    {
                        "name": "L. Pierini"
                    },
                    {
                        "name": "G. Pierra"
                    },
                    {
                        "name": "V. Pierro"
                    },
                    {
                        "name": "M. Pietrzak"
                    },
                    {
                        "name": "M. Pillas"
                    },
                    {
                        "name": "F. Pilo"
                    },
                    {
                        "name": "L. Pinard"
                    },
                    {
                        "name": "I. M. Pinto"
                    },
                    {
                        "name": "M. Pinto"
                    },
                    {
                        "name": "B. J. Piotrzkowski"
                    },
                    {
                        "name": "M. Pirello"
                    },
                    {
                        "name": "M. D. Pitkin"
                    },
                    {
                        "name": "A. Placidi"
                    },
                    {
                        "name": "E. Placidi"
                    },
                    {
                        "name": "M. L. Planas"
                    },
                    {
                        "name": "W. Plastino"
                    },
                    {
                        "name": "C. Plunkett"
                    },
                    {
                        "name": "R. Poggiani"
                    },
                    {
                        "name": "E. Polini"
                    },
                    {
                        "name": "J. Pomper"
                    },
                    {
                        "name": "L. Pompili"
                    },
                    {
                        "name": "J. Poon"
                    },
                    {
                        "name": "E. Porcelli"
                    },
                    {
                        "name": "E. K. Porter"
                    },
                    {
                        "name": "C. Posnansky"
                    },
                    {
                        "name": "R. Poulton"
                    },
                    {
                        "name": "J. Powell"
                    },
                    {
                        "name": "G. S. Prabhu"
                    },
                    {
                        "name": "M. Pracchia"
                    },
                    {
                        "name": "B. K. Pradhan"
                    },
                    {
                        "name": "T. Pradier"
                    },
                    {
                        "name": "A. K. Prajapati"
                    },
                    {
                        "name": "K. Prasai"
                    },
                    {
                        "name": "R. Prasanna"
                    },
                    {
                        "name": "P. Prasia"
                    },
                    {
                        "name": "G. Pratten"
                    },
                    {
                        "name": "G. Principe"
                    },
                    {
                        "name": "G. A. Prodi"
                    },
                    {
                        "name": "P. Prosperi"
                    },
                    {
                        "name": "P. Prosposito"
                    },
                    {
                        "name": "A. C. Providence"
                    },
                    {
                        "name": "A. Puecher"
                    },
                    {
                        "name": "J. Pullin"
                    },
                    {
                        "name": "P. Puppo"
                    },
                    {
                        "name": "M. Pürrer"
                    },
                    {
                        "name": "H. Qi"
                    },
                    {
                        "name": "J. Qin"
                    },
                    {
                        "name": "G. Quéméner"
                    },
                    {
                        "name": "V. Quetschke"
                    },
                    {
                        "name": "P. J. Quinonez"
                    },
                    {
                        "name": "N. Qutob"
                    },
                    {
                        "name": "R. Rading"
                    },
                    {
                        "name": "I. Rainho"
                    },
                    {
                        "name": "S. Raja"
                    },
                    {
                        "name": "C. Rajan"
                    },
                    {
                        "name": "B. Rajbhandari"
                    },
                    {
                        "name": "K. E. Ramirez"
                    },
                    {
                        "name": "F. A. Ramis Vidal"
                    },
                    {
                        "name": "M. Ramos Arevalo"
                    },
                    {
                        "name": "A. Ramos-Buades"
                    },
                    {
                        "name": "S. Ranjan"
                    },
                    {
                        "name": "K. Ransom"
                    },
                    {
                        "name": "P. Rapagnani"
                    },
                    {
                        "name": "B. Ratto"
                    },
                    {
                        "name": "A. Ravichandran"
                    },
                    {
                        "name": "A. Ray"
                    },
                    {
                        "name": "V. Raymond"
                    },
                    {
                        "name": "M. Razzano"
                    },
                    {
                        "name": "J. Read"
                    },
                    {
                        "name": "T. Regimbau"
                    },
                    {
                        "name": "S. Reid"
                    },
                    {
                        "name": "C. Reissel"
                    },
                    {
                        "name": "D. H. Reitze"
                    },
                    {
                        "name": "A. I. Renzini"
                    },
                    {
                        "name": "B. Revenu"
                    },
                    {
                        "name": "A. Revilla Peña"
                    },
                    {
                        "name": "R. Reyes"
                    },
                    {
                        "name": "L. Ricca"
                    },
                    {
                        "name": "F. Ricci"
                    },
                    {
                        "name": "M. Ricci"
                    },
                    {
                        "name": "A. Ricciardone"
                    },
                    {
                        "name": "J. Rice"
                    },
                    {
                        "name": "J. W. Richardson"
                    },
                    {
                        "name": "M. L. Richardson"
                    },
                    {
                        "name": "A. Rijal"
                    },
                    {
                        "name": "K. Riles"
                    },
                    {
                        "name": "H. K. Riley"
                    },
                    {
                        "name": "S. Rinaldi"
                    },
                    {
                        "name": "J. Rittmeyer"
                    },
                    {
                        "name": "C. Robertson"
                    },
                    {
                        "name": "F. Robinet"
                    },
                    {
                        "name": "M. Robinson"
                    },
                    {
                        "name": "A. Rocchi"
                    },
                    {
                        "name": "L. Rolland"
                    },
                    {
                        "name": "J. G. Rollins"
                    },
                    {
                        "name": "A. E. Romano"
                    },
                    {
                        "name": "R. Romano"
                    },
                    {
                        "name": "A. Romero"
                    },
                    {
                        "name": "I. M. Romero-Shaw"
                    },
                    {
                        "name": "J. H. Romie"
                    },
                    {
                        "name": "S. Ronchini"
                    },
                    {
                        "name": "T. J. Roocke"
                    },
                    {
                        "name": "L. Rosa"
                    },
                    {
                        "name": "T. J. Rosauer"
                    },
                    {
                        "name": "C. A. Rose"
                    },
                    {
                        "name": "D. Rosińska"
                    },
                    {
                        "name": "M. P. Ross"
                    },
                    {
                        "name": "M. Rossello-Sastre"
                    },
                    {
                        "name": "S. Rowan"
                    },
                    {
                        "name": "S. K. Roy"
                    },
                    {
                        "name": "S. Roy"
                    },
                    {
                        "name": "D. Rozza"
                    },
                    {
                        "name": "P. Ruggi"
                    },
                    {
                        "name": "N. Ruhama"
                    },
                    {
                        "name": "E. Ruiz Morales"
                    },
                    {
                        "name": "K. Ruiz-Rocha"
                    },
                    {
                        "name": "S. Sachdev"
                    },
                    {
                        "name": "T. Sadecki"
                    },
                    {
                        "name": "P. Saffarieh"
                    },
                    {
                        "name": "S. Safi-Harb"
                    },
                    {
                        "name": "M. R. Sah"
                    },
                    {
                        "name": "S. Saha"
                    },
                    {
                        "name": "T. Sainrat"
                    },
                    {
                        "name": "S. Sajith Menon"
                    },
                    {
                        "name": "K. Sakai"
                    },
                    {
                        "name": "Y. Sakai"
                    },
                    {
                        "name": "M. Sakellariadou"
                    },
                    {
                        "name": "S. Sakon"
                    },
                    {
                        "name": "O. S. Salafia"
                    },
                    {
                        "name": "F. Salces-Carcoba"
                    },
                    {
                        "name": "L. Salconi"
                    },
                    {
                        "name": "M. Saleem"
                    },
                    {
                        "name": "F. Salemi"
                    },
                    {
                        "name": "M. Sallé"
                    },
                    {
                        "name": "S. U. Salunkhe"
                    },
                    {
                        "name": "S. Salvador"
                    },
                    {
                        "name": "A. Salvarese"
                    },
                    {
                        "name": "A. Samajdar"
                    },
                    {
                        "name": "A. Sanchez"
                    },
                    {
                        "name": "E. J. Sanchez"
                    },
                    {
                        "name": "L. E. Sanchez"
                    },
                    {
                        "name": "N. Sanchis-Gual"
                    },
                    {
                        "name": "J. R. Sanders"
                    },
                    {
                        "name": "E. M. Sänger"
                    },
                    {
                        "name": "F. Santoliquido"
                    },
                    {
                        "name": "F. Sarandrea"
                    },
                    {
                        "name": "T. R. Saravanan"
                    },
                    {
                        "name": "N. Sarin"
                    },
                    {
                        "name": "P. Sarkar"
                    },
                    {
                        "name": "A. Sasli"
                    },
                    {
                        "name": "P. Sassi"
                    },
                    {
                        "name": "B. Sassolas"
                    },
                    {
                        "name": "B. S. Sathyaprakash"
                    },
                    {
                        "name": "R. Sato"
                    },
                    {
                        "name": "S. Sato"
                    },
                    {
                        "name": "Yukino Sato"
                    },
                    {
                        "name": "Yu Sato"
                    },
                    {
                        "name": "O. Sauter"
                    },
                    {
                        "name": "R. L. Savage"
                    },
                    {
                        "name": "T. Sawada"
                    },
                    {
                        "name": "H. L. Sawant"
                    },
                    {
                        "name": "S. Sayah"
                    },
                    {
                        "name": "V. Scacco"
                    },
                    {
                        "name": "D. Schaetzl"
                    },
                    {
                        "name": "M. Scheel"
                    },
                    {
                        "name": "A. Schiebelbein"
                    },
                    {
                        "name": "M. G. Schiworski"
                    },
                    {
                        "name": "P. Schmidt"
                    },
                    {
                        "name": "S. Schmidt"
                    },
                    {
                        "name": "R. Schnabel"
                    },
                    {
                        "name": "M. Schneewind"
                    },
                    {
                        "name": "R. M. S. Schofield"
                    },
                    {
                        "name": "K. Schouteden"
                    },
                    {
                        "name": "B. W. Schulte"
                    },
                    {
                        "name": "B. F. Schutz"
                    },
                    {
                        "name": "E. Schwartz"
                    },
                    {
                        "name": "M. Scialpi"
                    },
                    {
                        "name": "J. Scott"
                    },
                    {
                        "name": "S. M. Scott"
                    },
                    {
                        "name": "R. M. Sedas"
                    },
                    {
                        "name": "T. C. Seetharamu"
                    },
                    {
                        "name": "M. Seglar-Arroyo"
                    },
                    {
                        "name": "Y. Sekiguchi"
                    },
                    {
                        "name": "D. Sellers"
                    },
                    {
                        "name": "N. Sembo"
                    },
                    {
                        "name": "A. S. Sengupta"
                    },
                    {
                        "name": "E. G. Seo"
                    },
                    {
                        "name": "J. W. Seo"
                    },
                    {
                        "name": "V. Sequino"
                    },
                    {
                        "name": "M. Serra"
                    },
                    {
                        "name": "A. Sevrin"
                    },
                    {
                        "name": "T. Shaffer"
                    },
                    {
                        "name": "U. S. Shah"
                    },
                    {
                        "name": "M. A. Shaikh"
                    },
                    {
                        "name": "L. Shao"
                    },
                    {
                        "name": "A. K. Sharma"
                    },
                    {
                        "name": "Preeti Sharma"
                    },
                    {
                        "name": "Prianka Sharma"
                    },
                    {
                        "name": "Ritwik Sharma"
                    },
                    {
                        "name": "S. Sharma Chaudhary"
                    },
                    {
                        "name": "P. Shawhan"
                    },
                    {
                        "name": "N. S. Shcheblanov"
                    },
                    {
                        "name": "E. Sheridan"
                    },
                    {
                        "name": "Z. -H. Shi"
                    },
                    {
                        "name": "M. Shikauchi"
                    },
                    {
                        "name": "R. Shimomura"
                    },
                    {
                        "name": "H. Shinkai"
                    },
                    {
                        "name": "S. Shirke"
                    },
                    {
                        "name": "D. H. Shoemaker"
                    },
                    {
                        "name": "D. M. Shoemaker"
                    },
                    {
                        "name": "R. W. Short"
                    },
                    {
                        "name": "S. ShyamSundar"
                    },
                    {
                        "name": "A. Sider"
                    },
                    {
                        "name": "H. Siegel"
                    },
                    {
                        "name": "D. Sigg"
                    },
                    {
                        "name": "L. Silenzi"
                    },
                    {
                        "name": "L. Silvestri"
                    },
                    {
                        "name": "M. Simmonds"
                    },
                    {
                        "name": "L. P. Singer"
                    },
                    {
                        "name": "Amitesh Singh"
                    },
                    {
                        "name": "Anika Singh"
                    },
                    {
                        "name": "D. Singh"
                    },
                    {
                        "name": "N. Singh"
                    },
                    {
                        "name": "S. Singh"
                    },
                    {
                        "name": "A. M. Sintes"
                    },
                    {
                        "name": "V. Sipala"
                    },
                    {
                        "name": "V. Skliris"
                    },
                    {
                        "name": "B. J. J. Slagmolen"
                    },
                    {
                        "name": "D. A. Slater"
                    },
                    {
                        "name": "T. J. Slaven-Blair"
                    },
                    {
                        "name": "J. Smetana"
                    },
                    {
                        "name": "J. R. Smith"
                    },
                    {
                        "name": "L. Smith"
                    },
                    {
                        "name": "R. J. E. Smith"
                    },
                    {
                        "name": "W. J. Smith"
                    },
                    {
                        "name": "S. Soares de Albuquerque Filho"
                    },
                    {
                        "name": "M. Soares-Santos"
                    },
                    {
                        "name": "K. Somiya"
                    },
                    {
                        "name": "I. Song"
                    },
                    {
                        "name": "S. Soni"
                    },
                    {
                        "name": "V. Sordini"
                    },
                    {
                        "name": "F. Sorrentino"
                    },
                    {
                        "name": "H. Sotani"
                    },
                    {
                        "name": "F. Spada"
                    },
                    {
                        "name": "V. Spagnuolo"
                    },
                    {
                        "name": "A. P. Spencer"
                    },
                    {
                        "name": "P. Spinicelli"
                    },
                    {
                        "name": "A. K. Srivastava"
                    },
                    {
                        "name": "F. Stachurski"
                    },
                    {
                        "name": "C. J. Stark"
                    },
                    {
                        "name": "D. A. Steer"
                    },
                    {
                        "name": "N. Steinle"
                    },
                    {
                        "name": "J. Steinlechner"
                    },
                    {
                        "name": "S. Steinlechner"
                    },
                    {
                        "name": "N. Stergioulas"
                    },
                    {
                        "name": "P. Stevens"
                    },
                    {
                        "name": "M. StPierre"
                    },
                    {
                        "name": "M. D. Strong"
                    },
                    {
                        "name": "A. Strunk"
                    },
                    {
                        "name": "A. L. Stuver"
                    },
                    {
                        "name": "M. Suchenek"
                    },
                    {
                        "name": "S. Sudhagar"
                    },
                    {
                        "name": "Y. Sudo"
                    },
                    {
                        "name": "N. Sueltmann"
                    },
                    {
                        "name": "L. Suleiman"
                    },
                    {
                        "name": "K. D. Sullivan"
                    },
                    {
                        "name": "J. Sun"
                    },
                    {
                        "name": "L. Sun"
                    },
                    {
                        "name": "S. Sunil"
                    },
                    {
                        "name": "J. Suresh"
                    },
                    {
                        "name": "B. J. Sutton"
                    },
                    {
                        "name": "P. J. Sutton"
                    },
                    {
                        "name": "K. Suzuki"
                    },
                    {
                        "name": "M. Suzuki"
                    },
                    {
                        "name": "B. L. Swinkels"
                    },
                    {
                        "name": "A. Syx"
                    },
                    {
                        "name": "M. J. Szczepańczyk"
                    },
                    {
                        "name": "P. Szewczyk"
                    },
                    {
                        "name": "M. Tacca"
                    },
                    {
                        "name": "H. Tagoshi"
                    },
                    {
                        "name": "K. Takada"
                    },
                    {
                        "name": "H. Takahashi"
                    },
                    {
                        "name": "R. Takahashi"
                    },
                    {
                        "name": "A. Takamori"
                    },
                    {
                        "name": "S. Takano"
                    },
                    {
                        "name": "H. Takeda"
                    },
                    {
                        "name": "K. Takeshita"
                    },
                    {
                        "name": "I. Takimoto Schmiegelow"
                    },
                    {
                        "name": "M. Takou-Ayaoh"
                    },
                    {
                        "name": "C. Talbot"
                    },
                    {
                        "name": "M. Tamaki"
                    },
                    {
                        "name": "N. Tamanini"
                    },
                    {
                        "name": "D. Tanabe"
                    },
                    {
                        "name": "K. Tanaka"
                    },
                    {
                        "name": "S. J. Tanaka"
                    },
                    {
                        "name": "S. Tanioka"
                    },
                    {
                        "name": "D. B. Tanner"
                    },
                    {
                        "name": "W. Tanner"
                    },
                    {
                        "name": "L. Tao"
                    },
                    {
                        "name": "R. D. Tapia"
                    },
                    {
                        "name": "E. N. Tapia San Martín"
                    },
                    {
                        "name": "C. Taranto"
                    },
                    {
                        "name": "A. Taruya"
                    },
                    {
                        "name": "J. D. Tasson"
                    },
                    {
                        "name": "J. G. Tau"
                    },
                    {
                        "name": "D. Tellez"
                    },
                    {
                        "name": "R. Tenorio"
                    },
                    {
                        "name": "H. Themann"
                    },
                    {
                        "name": "A. Theodoropoulos"
                    },
                    {
                        "name": "M. P. Thirugnanasambandam"
                    },
                    {
                        "name": "L. M. Thomas"
                    },
                    {
                        "name": "M. Thomas"
                    },
                    {
                        "name": "P. Thomas"
                    },
                    {
                        "name": "J. E. Thompson"
                    },
                    {
                        "name": "S. R. Thondapu"
                    },
                    {
                        "name": "K. A. Thorne"
                    },
                    {
                        "name": "E. Thrane"
                    },
                    {
                        "name": "J. Tissino"
                    },
                    {
                        "name": "A. Tiwari"
                    },
                    {
                        "name": "Pawan Tiwari"
                    },
                    {
                        "name": "Praveer Tiwari"
                    },
                    {
                        "name": "S. Tiwari"
                    },
                    {
                        "name": "V. Tiwari"
                    },
                    {
                        "name": "M. R. Todd"
                    },
                    {
                        "name": "M. Toffano"
                    },
                    {
                        "name": "A. M. Toivonen"
                    },
                    {
                        "name": "K. Toland"
                    },
                    {
                        "name": "A. E. Tolley"
                    },
                    {
                        "name": "T. Tomaru"
                    },
                    {
                        "name": "V. Tommasini"
                    },
                    {
                        "name": "T. Tomura"
                    },
                    {
                        "name": "H. Tong"
                    },
                    {
                        "name": "C. Tong-Yu"
                    },
                    {
                        "name": "A. Torres-Forné"
                    },
                    {
                        "name": "C. I. Torrie"
                    },
                    {
                        "name": "I. Tosta e Melo"
                    },
                    {
                        "name": "E. Tournefier"
                    },
                    {
                        "name": "M. Trad Nery"
                    },
                    {
                        "name": "K. Tran"
                    },
                    {
                        "name": "A. Trapananti"
                    },
                    {
                        "name": "R. Travaglini"
                    },
                    {
                        "name": "F. Travasso"
                    },
                    {
                        "name": "G. Traylor"
                    },
                    {
                        "name": "M. Trevor"
                    },
                    {
                        "name": "M. C. Tringali"
                    },
                    {
                        "name": "A. Tripathee"
                    },
                    {
                        "name": "G. Troian"
                    },
                    {
                        "name": "A. Trovato"
                    },
                    {
                        "name": "L. Trozzo"
                    },
                    {
                        "name": "R. J. Trudeau"
                    },
                    {
                        "name": "T. Tsang"
                    },
                    {
                        "name": "S. Tsuchida"
                    },
                    {
                        "name": "L. Tsukada"
                    },
                    {
                        "name": "K. Turbang"
                    },
                    {
                        "name": "M. Turconi"
                    },
                    {
                        "name": "C. Turski"
                    },
                    {
                        "name": "H. Ubach"
                    },
                    {
                        "name": "N. Uchikata"
                    },
                    {
                        "name": "T. Uchiyama"
                    },
                    {
                        "name": "R. P. Udall"
                    },
                    {
                        "name": "T. Uehara"
                    },
                    {
                        "name": "K. Ueno"
                    },
                    {
                        "name": "V. Undheim"
                    },
                    {
                        "name": "L. E. Uronen"
                    },
                    {
                        "name": "T. Ushiba"
                    },
                    {
                        "name": "M. Vacatello"
                    },
                    {
                        "name": "H. Vahlbruch"
                    },
                    {
                        "name": "N. Vaidya"
                    },
                    {
                        "name": "G. Vajente"
                    },
                    {
                        "name": "A. Vajpeyi"
                    },
                    {
                        "name": "J. Valencia"
                    },
                    {
                        "name": "M. Valentini"
                    },
                    {
                        "name": "S. A. Vallejo-Peña"
                    },
                    {
                        "name": "S. Vallero"
                    },
                    {
                        "name": "V. Valsan"
                    },
                    {
                        "name": "M. van Dael"
                    },
                    {
                        "name": "E. Van den Bossche"
                    },
                    {
                        "name": "J. F. J. van den Brand"
                    },
                    {
                        "name": "C. Van Den Broeck"
                    },
                    {
                        "name": "M. van der Sluys"
                    },
                    {
                        "name": "A. Van de Walle"
                    },
                    {
                        "name": "J. van Dongen"
                    },
                    {
                        "name": "K. Vandra"
                    },
                    {
                        "name": "M. VanDyke"
                    },
                    {
                        "name": "H. van Haevermaet"
                    },
                    {
                        "name": "J. V. van Heijningen"
                    },
                    {
                        "name": "P. Van Hove"
                    },
                    {
                        "name": "J. Vanier"
                    },
                    {
                        "name": "M. VanKeuren"
                    },
                    {
                        "name": "J. Vanosky"
                    },
                    {
                        "name": "N. van Remortel"
                    },
                    {
                        "name": "M. Vardaro"
                    },
                    {
                        "name": "A. F. Vargas"
                    },
                    {
                        "name": "V. Varma"
                    },
                    {
                        "name": "A. N. Vazquez"
                    },
                    {
                        "name": "A. Vecchio"
                    },
                    {
                        "name": "G. Vedovato"
                    },
                    {
                        "name": "J. Veitch"
                    },
                    {
                        "name": "P. J. Veitch"
                    },
                    {
                        "name": "S. Venikoudis"
                    },
                    {
                        "name": "R. C. Venterea"
                    },
                    {
                        "name": "P. Verdier"
                    },
                    {
                        "name": "M. Vereecken"
                    },
                    {
                        "name": "D. Verkindt"
                    },
                    {
                        "name": "B. Verma"
                    },
                    {
                        "name": "Y. Verma"
                    },
                    {
                        "name": "S. M. Vermeulen"
                    },
                    {
                        "name": "F. Vetrano"
                    },
                    {
                        "name": "A. Veutro"
                    },
                    {
                        "name": "A. Viceré"
                    },
                    {
                        "name": "S. Vidyant"
                    },
                    {
                        "name": "A. D. Viets"
                    },
                    {
                        "name": "A. Vijaykumar"
                    },
                    {
                        "name": "A. Vilkha"
                    },
                    {
                        "name": "N. Villanueva Espinosa"
                    },
                    {
                        "name": "V. Villa-Ortega"
                    },
                    {
                        "name": "E. T. Vincent"
                    },
                    {
                        "name": "J. -Y. Vinet"
                    },
                    {
                        "name": "S. Viret"
                    },
                    {
                        "name": "S. Vitale"
                    },
                    {
                        "name": "H. Vocca"
                    },
                    {
                        "name": "D. Voigt"
                    },
                    {
                        "name": "E. R. G. von Reis"
                    },
                    {
                        "name": "J. S. A. von Wrangel"
                    },
                    {
                        "name": "W. E. Vossius"
                    },
                    {
                        "name": "L. Vujeva"
                    },
                    {
                        "name": "S. P. Vyatchanin"
                    },
                    {
                        "name": "J. Wack"
                    },
                    {
                        "name": "L. E. Wade"
                    },
                    {
                        "name": "M. Wade"
                    },
                    {
                        "name": "K. J. Wagner"
                    },
                    {
                        "name": "L. Wallace"
                    },
                    {
                        "name": "E. J. Wang"
                    },
                    {
                        "name": "H. Wang"
                    },
                    {
                        "name": "J. Z. Wang"
                    },
                    {
                        "name": "W. H. Wang"
                    },
                    {
                        "name": "Y. F. Wang"
                    },
                    {
                        "name": "G. Waratkar"
                    },
                    {
                        "name": "J. Warner"
                    },
                    {
                        "name": "M. Was"
                    },
                    {
                        "name": "T. Washimi"
                    },
                    {
                        "name": "N. Y. Washington"
                    },
                    {
                        "name": "D. Watarai"
                    },
                    {
                        "name": "B. Weaver"
                    },
                    {
                        "name": "S. A. Webster"
                    },
                    {
                        "name": "N. L. Weickhardt"
                    },
                    {
                        "name": "M. Weinert"
                    },
                    {
                        "name": "A. J. Weinstein"
                    },
                    {
                        "name": "R. Weiss"
                    },
                    {
                        "name": "L. Wen"
                    },
                    {
                        "name": "K. Wette"
                    },
                    {
                        "name": "J. T. Whelan"
                    },
                    {
                        "name": "B. F. Whiting"
                    },
                    {
                        "name": "C. Whittle"
                    },
                    {
                        "name": "E. G. Wickens"
                    },
                    {
                        "name": "D. Wilken"
                    },
                    {
                        "name": "A. T. Wilkin"
                    },
                    {
                        "name": "B. M. Williams"
                    },
                    {
                        "name": "D. Williams"
                    },
                    {
                        "name": "M. J. Williams"
                    },
                    {
                        "name": "N. S. Williams"
                    },
                    {
                        "name": "J. L. Willis"
                    },
                    {
                        "name": "B. Willke"
                    },
                    {
                        "name": "M. Wils"
                    },
                    {
                        "name": "L. Wilson"
                    },
                    {
                        "name": "C. W. Winborn"
                    },
                    {
                        "name": "J. Winterflood"
                    },
                    {
                        "name": "C. C. Wipf"
                    },
                    {
                        "name": "G. Woan"
                    },
                    {
                        "name": "J. Woehler"
                    },
                    {
                        "name": "N. E. Wolfe"
                    },
                    {
                        "name": "H. T. Wong"
                    },
                    {
                        "name": "I. C. F. Wong"
                    },
                    {
                        "name": "K. Wong"
                    },
                    {
                        "name": "T. Wouters"
                    },
                    {
                        "name": "J. L. Wright"
                    },
                    {
                        "name": "M. Wright"
                    },
                    {
                        "name": "B. Wu"
                    },
                    {
                        "name": "C. Wu"
                    },
                    {
                        "name": "D. S. Wu"
                    },
                    {
                        "name": "H. Wu"
                    },
                    {
                        "name": "K. Wu"
                    },
                    {
                        "name": "Q. Wu"
                    },
                    {
                        "name": "Y. Wu"
                    },
                    {
                        "name": "Z. Wu"
                    },
                    {
                        "name": "E. Wuchner"
                    },
                    {
                        "name": "D. M. Wysocki"
                    },
                    {
                        "name": "V. A. Xu"
                    },
                    {
                        "name": "Y. Xu"
                    },
                    {
                        "name": "N. Yadav"
                    },
                    {
                        "name": "H. Yamamoto"
                    },
                    {
                        "name": "K. Yamamoto"
                    },
                    {
                        "name": "T. S. Yamamoto"
                    },
                    {
                        "name": "T. Yamamoto"
                    },
                    {
                        "name": "R. Yamazaki"
                    },
                    {
                        "name": "T. Yan"
                    },
                    {
                        "name": "K. Z. Yang"
                    },
                    {
                        "name": "Y. Yang"
                    },
                    {
                        "name": "Z. Yarbrough"
                    },
                    {
                        "name": "J. Yebana"
                    },
                    {
                        "name": "S. -W. Yeh"
                    },
                    {
                        "name": "A. B. Yelikar"
                    },
                    {
                        "name": "X. Yin"
                    },
                    {
                        "name": "J. Yokoyama"
                    },
                    {
                        "name": "T. Yokozawa"
                    },
                    {
                        "name": "S. Yuan"
                    },
                    {
                        "name": "H. Yuzurihara"
                    },
                    {
                        "name": "M. Zanolin"
                    },
                    {
                        "name": "M. Zeeshan"
                    },
                    {
                        "name": "T. Zelenova"
                    },
                    {
                        "name": "J. -P. Zendri"
                    },
                    {
                        "name": "M. Zeoli"
                    },
                    {
                        "name": "M. Zerrad"
                    },
                    {
                        "name": "M. Zevin"
                    },
                    {
                        "name": "L. Zhang"
                    },
                    {
                        "name": "N. Zhang"
                    },
                    {
                        "name": "R. Zhang"
                    },
                    {
                        "name": "T. Zhang"
                    },
                    {
                        "name": "C. Zhao"
                    },
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "Yuhang Zhao"
                    },
                    {
                        "name": "Z. -C. Zhao"
                    },
                    {
                        "name": "Y. Zheng"
                    },
                    {
                        "name": "H. Zhong"
                    },
                    {
                        "name": "H. Zhou"
                    },
                    {
                        "name": "H. O. Zhu"
                    },
                    {
                        "name": "Z. -H. Zhu"
                    },
                    {
                        "name": "A. B. Zimmerman"
                    },
                    {
                        "name": "L. Zimmermann"
                    },
                    {
                        "name": "M. E. Zucker"
                    },
                    {
                        "name": "J. Zweizig"
                    }
                ],
                "author_detail": {
                    "name": "J. Zweizig"
                },
                "author": "J. Zweizig",
                "arxiv_comment": "As part of the Astrophysical Journal Letters Focus Issue on the\n  Gravitational Wave Transient Catalog",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18082v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18082v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01113v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01113v3",
                "updated": "2025-09-08T17:43:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    43,
                    53,
                    0,
                    251,
                    0
                ],
                "published": "2024-12-02T04:35:54Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    4,
                    35,
                    54,
                    0,
                    337,
                    0
                ],
                "title": "Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in\n  Multi-Hop Arithmetic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in\n  Multi-Hop Arithmetic Reasoning"
                },
                "summary": "This study investigates the incremental, internal problem-solving process of\nlanguage models (LMs) with arithmetic multi-hop reasoning as a case study. We\nspecifically investigate when LMs internally resolve sub/whole problems through\nfirst reading the problem statements, generating reasoning chains, and\nachieving the final answer to mechanistically interpret LMs' multi-hop\nproblem-solving process. Our experiments reveal a systematic incremental\nreasoning strategy underlying LMs. They have not derived an answer at the\nmoment they first read the problem; instead, they obtain (sub)answers while\ngenerating the reasoning chain. Therefore, the generated reasoning chains can\nbe regarded as faithful reflections of the model's internal computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the incremental, internal problem-solving process of\nlanguage models (LMs) with arithmetic multi-hop reasoning as a case study. We\nspecifically investigate when LMs internally resolve sub/whole problems through\nfirst reading the problem statements, generating reasoning chains, and\nachieving the final answer to mechanistically interpret LMs' multi-hop\nproblem-solving process. Our experiments reveal a systematic incremental\nreasoning strategy underlying LMs. They have not derived an answer at the\nmoment they first read the problem; instead, they obtain (sub)answers while\ngenerating the reasoning chain. Therefore, the generated reasoning chains can\nbe regarded as faithful reflections of the model's internal computation."
                },
                "authors": [
                    {
                        "name": "Keito Kudo"
                    },
                    {
                        "name": "Yoichi Aoki"
                    },
                    {
                        "name": "Tatsuki Kuribayashi"
                    },
                    {
                        "name": "Shusaku Sone"
                    },
                    {
                        "name": "Masaya Taniguchi"
                    },
                    {
                        "name": "Ana Brassard"
                    },
                    {
                        "name": "Keisuke Sakaguchi"
                    },
                    {
                        "name": "Kentaro Inui"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Inui"
                },
                "author": "Kentaro Inui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01113v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01113v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06924v1",
                "updated": "2025-09-08T17:38:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    38,
                    1,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:38:01Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    38,
                    1,
                    0,
                    251,
                    0
                ],
                "title": "Neutron Reflectometry by Gradient Descent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutron Reflectometry by Gradient Descent"
                },
                "summary": "Neutron reflectometry (NR) is a powerful technique to probe surfaces and\ninterfaces. NR is inherently an indirect measurement technique, access to the\nphysical quantities of interest (layer thickness, scattering length density,\nroughness), necessitate the solution of an inverse modelling problem, that is\ninefficient for large amounts of data or complex multiplayer structures (e.g.\nlithium batteries / electrodes). Recently, surrogate machine learning models\nhave been proposed as an alternative to existing optimisation routines.\nAlthough such approaches have been successful, physical intuition is lost when\nreplacing governing equations with fast neural networks. Instead, we propose a\nnovel and efficient approach; to optimise reflectivity data analysis by\nperforming gradient descent on the forward reflection model itself. Herein,\nautomatic differentiation techniques are used to evaluate exact gradients of\nthe error function with respect to the parameters of interest. Access to these\nquantities enables users of neutron reflectometry to harness a host of powerful\nmodern optimisation and inference techniques that remain thus far unexploited\nin the context of neutron reflectometry. This paper presents two benchmark case\nstudies; demonstrating state-of-the-art performance on a thick oxide quartz\nfilm, and robust co-fitting performance in the high complexity regime of\norganic LED multilayer devices. Additionally, we provide an open-source library\nof differentiable reflectometry kernels in the python programming language so\nthat gradient based approaches can readily be applied to other NR datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutron reflectometry (NR) is a powerful technique to probe surfaces and\ninterfaces. NR is inherently an indirect measurement technique, access to the\nphysical quantities of interest (layer thickness, scattering length density,\nroughness), necessitate the solution of an inverse modelling problem, that is\ninefficient for large amounts of data or complex multiplayer structures (e.g.\nlithium batteries / electrodes). Recently, surrogate machine learning models\nhave been proposed as an alternative to existing optimisation routines.\nAlthough such approaches have been successful, physical intuition is lost when\nreplacing governing equations with fast neural networks. Instead, we propose a\nnovel and efficient approach; to optimise reflectivity data analysis by\nperforming gradient descent on the forward reflection model itself. Herein,\nautomatic differentiation techniques are used to evaluate exact gradients of\nthe error function with respect to the parameters of interest. Access to these\nquantities enables users of neutron reflectometry to harness a host of powerful\nmodern optimisation and inference techniques that remain thus far unexploited\nin the context of neutron reflectometry. This paper presents two benchmark case\nstudies; demonstrating state-of-the-art performance on a thick oxide quartz\nfilm, and robust co-fitting performance in the high complexity regime of\norganic LED multilayer devices. Additionally, we provide an open-source library\nof differentiable reflectometry kernels in the python programming language so\nthat gradient based approaches can readily be applied to other NR datasets."
                },
                "authors": [
                    {
                        "name": "Max D. ~Champneys"
                    },
                    {
                        "name": "Andrew J. ~Parnell"
                    },
                    {
                        "name": "Philipp Gutfreund"
                    },
                    {
                        "name": "Maximilian W. A. Skoda"
                    },
                    {
                        "name": ". Patrick A. Fairclough"
                    },
                    {
                        "name": "Timothy J. ~Rogers"
                    },
                    {
                        "name": "Stephanie L. ~Burg"
                    }
                ],
                "author_detail": {
                    "name": "Stephanie L. ~Burg"
                },
                "author": "Stephanie L. ~Burg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06923v1",
                "updated": "2025-09-08T17:36:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    36,
                    21,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:36:21Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    36,
                    21,
                    0,
                    251,
                    0
                ],
                "title": "Staying in the Sweet Spot: Responsive Reasoning Evolution via\n  Capability-Adaptive Hint Scaffolding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Staying in the Sweet Spot: Responsive Reasoning Evolution via\n  Capability-Adaptive Hint Scaffolding"
                },
                "summary": "Reinforcement learning with verifiable rewards (RLVR) has achieved remarkable\nsuccess in enhancing the reasoning capabilities of large language models\n(LLMs). However, existing RLVR methods often suffer from exploration\ninefficiency due to mismatches between the training data's difficulty and the\nmodel's capability. LLMs fail to discover viable reasoning paths when problems\nare overly difficult, while learning little new capability when problems are\ntoo simple. In this work, we formalize the impact of problem difficulty by\nquantifying the relationship between loss descent speed and rollout accuracy.\nBuilding on this analysis, we propose SEELE, a novel supervision-aided RLVR\nframework that dynamically adjusts problem difficulty to stay within the\nhigh-efficiency region. SEELE augments each training sample by appending a hint\n(part of a full solution) after the original problem. Unlike previous\nhint-based approaches, SEELE deliberately and adaptively adjusts the hint\nlength for each problem to achieve an optimal difficulty. To determine the\noptimal hint length, SEELE employs a multi-round rollout sampling strategy. In\neach round, it fits an item response theory model to the accuracy-hint pairs\ncollected in preceding rounds to predict the required hint length for the next\nround. This instance-level, real-time difficulty adjustment aligns problem\ndifficulty with the evolving model capability, thereby improving exploration\nefficiency. Experimental results show that SEELE outperforms Group Relative\nPolicy Optimization (GRPO) and Supervised Fine-tuning (SFT) by +11.8 and +10.5\npoints, respectively, and surpasses the best previous supervision-aided\napproach by +3.6 points on average across six math reasoning benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable rewards (RLVR) has achieved remarkable\nsuccess in enhancing the reasoning capabilities of large language models\n(LLMs). However, existing RLVR methods often suffer from exploration\ninefficiency due to mismatches between the training data's difficulty and the\nmodel's capability. LLMs fail to discover viable reasoning paths when problems\nare overly difficult, while learning little new capability when problems are\ntoo simple. In this work, we formalize the impact of problem difficulty by\nquantifying the relationship between loss descent speed and rollout accuracy.\nBuilding on this analysis, we propose SEELE, a novel supervision-aided RLVR\nframework that dynamically adjusts problem difficulty to stay within the\nhigh-efficiency region. SEELE augments each training sample by appending a hint\n(part of a full solution) after the original problem. Unlike previous\nhint-based approaches, SEELE deliberately and adaptively adjusts the hint\nlength for each problem to achieve an optimal difficulty. To determine the\noptimal hint length, SEELE employs a multi-round rollout sampling strategy. In\neach round, it fits an item response theory model to the accuracy-hint pairs\ncollected in preceding rounds to predict the required hint length for the next\nround. This instance-level, real-time difficulty adjustment aligns problem\ndifficulty with the evolving model capability, thereby improving exploration\nefficiency. Experimental results show that SEELE outperforms Group Relative\nPolicy Optimization (GRPO) and Supervised Fine-tuning (SFT) by +11.8 and +10.5\npoints, respectively, and surpasses the best previous supervision-aided\napproach by +3.6 points on average across six math reasoning benchmarks."
                },
                "authors": [
                    {
                        "name": "Ziheng Li"
                    },
                    {
                        "name": "Zexu Sun"
                    },
                    {
                        "name": "Jinman Zhao"
                    },
                    {
                        "name": "Erxue Min"
                    },
                    {
                        "name": "Yongcheng Zeng"
                    },
                    {
                        "name": "Hui Wu"
                    },
                    {
                        "name": "Hengyi Cai"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Zhi-Hong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhi-Hong Deng"
                },
                "author": "Zhi-Hong Deng",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10589v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10589v3",
                "updated": "2025-09-08T17:34:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    34,
                    41,
                    0,
                    251,
                    0
                ],
                "published": "2025-04-14T18:00:05Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    18,
                    0,
                    5,
                    0,
                    104,
                    0
                ],
                "title": "Mitigating Eddington and Malmquist Biases in Latent-Inclination\n  Inference of the Tully-Fisher Relation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Eddington and Malmquist Biases in Latent-Inclination\n  Inference of the Tully-Fisher Relation"
                },
                "summary": "The Tully-Fisher relation is a vital distance indicator, but its precise\ninference is challenged by selection bias, statistical bias, and uncertain\ninclination corrections. This study presents a Bayesian framework that\nsimultaneously addresses these issues. To eliminate the need for individual\ninclination corrections, inclination is treated as a latent variable with a\nknown probability distribution. To correct for the distance-dependent Malmquist\nbias arising from sample selection, the model incorporates Gaussian scatter in\nthe dependent variable, the distribution of the independent variable, and the\nobservational selection function into the data likelihood. To mitigate the\nstatistical bias -- termed the ``general Eddington bias'' -- caused by Gaussian\nscatter and the non-uniform distribution of the independent variable, two\nmethods are introduced: (1) analytical bias corrections applied to the\ndependent variable before likelihood computation, and (2) a dual-scatter model\nthat accounts for Gaussian scatter in the independent variable within the\nlikelihood function. The effectiveness of these methods is demonstrated using\nsimulated datasets. By rigorously addressing selection and statistical biases\nin a latent-variable regression analysis, this work provides a robust approach\nfor unbiased distance estimates from standardizable candles, which is critical\nfor improving the accuracy of Hubble constant determinations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Tully-Fisher relation is a vital distance indicator, but its precise\ninference is challenged by selection bias, statistical bias, and uncertain\ninclination corrections. This study presents a Bayesian framework that\nsimultaneously addresses these issues. To eliminate the need for individual\ninclination corrections, inclination is treated as a latent variable with a\nknown probability distribution. To correct for the distance-dependent Malmquist\nbias arising from sample selection, the model incorporates Gaussian scatter in\nthe dependent variable, the distribution of the independent variable, and the\nobservational selection function into the data likelihood. To mitigate the\nstatistical bias -- termed the ``general Eddington bias'' -- caused by Gaussian\nscatter and the non-uniform distribution of the independent variable, two\nmethods are introduced: (1) analytical bias corrections applied to the\ndependent variable before likelihood computation, and (2) a dual-scatter model\nthat accounts for Gaussian scatter in the independent variable within the\nlikelihood function. The effectiveness of these methods is demonstrated using\nsimulated datasets. By rigorously addressing selection and statistical biases\nin a latent-variable regression analysis, this work provides a robust approach\nfor unbiased distance estimates from standardizable candles, which is critical\nfor improving the accuracy of Hubble constant determinations."
                },
                "authors": [
                    {
                        "name": "Hai Fu"
                    }
                ],
                "author_detail": {
                    "name": "Hai Fu"
                },
                "author": "Hai Fu",
                "arxiv_comment": "ApJ accepted. Python functions and notebook are available at\n  https://github.com/fuhaiastro/TFR_biases",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10589v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10589v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06920v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06920v1",
                "updated": "2025-09-08T17:32:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    32,
                    17,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:32:17Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    32,
                    17,
                    0,
                    251,
                    0
                ],
                "title": "An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and\n  Detection"
                },
                "summary": "Insider threats are a growing organizational problem due to the complexity of\nidentifying their technical and behavioral elements. A large research body is\ndedicated to the study of insider threats from technological, psychological,\nand educational perspectives. However, research in this domain has been\ngenerally dependent on datasets that are static and limited access which\nrestricts the development of adaptive detection models. This study introduces a\nnovel, ethically grounded approach that uses the large language model (LLM)\nClaude Sonnet 3.7 to dynamically synthesize syslog messages, some of which\ncontain indicators of insider threat scenarios. The messages reflect real-world\ndata distributions by being highly imbalanced (1% insider threats). The syslogs\nwere analyzed for insider threats by both Claude Sonnet 3.7 and GPT-4o, with\ntheir performance evaluated through statistical metrics including precision,\nrecall, MCC, and ROC AUC. Sonnet 3.7 consistently outperformed GPT-4o across\nnearly all metrics, particularly in reducing false alarms and improving\ndetection accuracy. The results show strong promise for the use of LLMs in\nsynthetic dataset generation and insider threat detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insider threats are a growing organizational problem due to the complexity of\nidentifying their technical and behavioral elements. A large research body is\ndedicated to the study of insider threats from technological, psychological,\nand educational perspectives. However, research in this domain has been\ngenerally dependent on datasets that are static and limited access which\nrestricts the development of adaptive detection models. This study introduces a\nnovel, ethically grounded approach that uses the large language model (LLM)\nClaude Sonnet 3.7 to dynamically synthesize syslog messages, some of which\ncontain indicators of insider threat scenarios. The messages reflect real-world\ndata distributions by being highly imbalanced (1% insider threats). The syslogs\nwere analyzed for insider threats by both Claude Sonnet 3.7 and GPT-4o, with\ntheir performance evaluated through statistical metrics including precision,\nrecall, MCC, and ROC AUC. Sonnet 3.7 consistently outperformed GPT-4o across\nnearly all metrics, particularly in reducing false alarms and improving\ndetection accuracy. The results show strong promise for the use of LLMs in\nsynthetic dataset generation and insider threat detection."
                },
                "authors": [
                    {
                        "name": "Haywood Gelman"
                    },
                    {
                        "name": "John D. Hastings"
                    },
                    {
                        "name": "David Kenley"
                    }
                ],
                "author_detail": {
                    "name": "David Kenley"
                },
                "author": "David Kenley",
                "arxiv_comment": "6 pages, 5 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06920v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06920v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.0; I.2.7; K.4.1; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00034v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00034v4",
                "updated": "2025-09-08T17:30:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    30,
                    24,
                    0,
                    251,
                    0
                ],
                "published": "2024-08-18T01:43:26Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    1,
                    43,
                    26,
                    6,
                    231,
                    0
                ],
                "title": "Neural CRNs: A Natural Implementation of Learning in Chemical Reaction\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural CRNs: A Natural Implementation of Learning in Chemical Reaction\n  Networks"
                },
                "summary": "Molecular circuits capable of autonomous learning could unlock novel\napplications in fields such as bioengineering and synthetic biology. To this\nend, existing chemical implementations of neural computing have mainly relied\non emulating discrete-layered neural architectures using steady-state\ncomputations of mass action kinetics. In contrast, we propose an alternative\ndynamical systems-based approach in which neural computations are modeled as\nthe time evolution of molecular concentrations. The analog nature of our\nframework naturally aligns with chemical kinetics-based computation, leading to\nmore compact circuits. We present the advantages of our framework through three\nkey demonstrations. First, we assemble an end-to-end supervised learning\npipeline using only two sequential phases, the minimum required number for\nsupervised learning. Then, we show (through appropriate simplifications) that\nboth linear and nonlinear modeling circuits can be implemented solely using\nunimolecular and bimolecular reactions, avoiding the complexities of\nhigher-order chemistries. Finally, we demonstrate that first-order gradient\napproximations can be natively incorporated into the framework, enabling\nnonlinear models to scale linearly rather than combinatorially with input\ndimensionality. All the circuit constructions are validated through training\nand inference simulations across various regression and classification tasks.\nOur work presents a viable pathway toward embedding learning behaviors in\nsynthetic biochemical systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular circuits capable of autonomous learning could unlock novel\napplications in fields such as bioengineering and synthetic biology. To this\nend, existing chemical implementations of neural computing have mainly relied\non emulating discrete-layered neural architectures using steady-state\ncomputations of mass action kinetics. In contrast, we propose an alternative\ndynamical systems-based approach in which neural computations are modeled as\nthe time evolution of molecular concentrations. The analog nature of our\nframework naturally aligns with chemical kinetics-based computation, leading to\nmore compact circuits. We present the advantages of our framework through three\nkey demonstrations. First, we assemble an end-to-end supervised learning\npipeline using only two sequential phases, the minimum required number for\nsupervised learning. Then, we show (through appropriate simplifications) that\nboth linear and nonlinear modeling circuits can be implemented solely using\nunimolecular and bimolecular reactions, avoiding the complexities of\nhigher-order chemistries. Finally, we demonstrate that first-order gradient\napproximations can be natively incorporated into the framework, enabling\nnonlinear models to scale linearly rather than combinatorially with input\ndimensionality. All the circuit constructions are validated through training\nand inference simulations across various regression and classification tasks.\nOur work presents a viable pathway toward embedding learning behaviors in\nsynthetic biochemical systems."
                },
                "authors": [
                    {
                        "name": "Rajiv Teja Nagipogu"
                    },
                    {
                        "name": "John H. Reif"
                    }
                ],
                "author_detail": {
                    "name": "John H. Reif"
                },
                "author": "John H. Reif",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00034v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00034v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05186v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05186v2",
                "updated": "2025-09-08T17:28:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    28,
                    39,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-05T15:35:04Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    15,
                    35,
                    4,
                    4,
                    248,
                    0
                ],
                "title": "Probabilistic operator learning: generative modeling and uncertainty\n  quantification for foundation models of differential equations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic operator learning: generative modeling and uncertainty\n  quantification for foundation models of differential equations"
                },
                "summary": "In-context operator networks (ICON) are a class of operator learning methods\nbased on the novel architectures of foundation models. Trained on a diverse set\nof datasets of initial and boundary conditions paired with corresponding\nsolutions to ordinary and partial differential equations (ODEs and PDEs), ICON\nlearns to map example condition-solution pairs of a given differential equation\nto an approximation of its solution operator. Here, we present a probabilistic\nframework that reveals ICON as implicitly performing Bayesian inference, where\nit computes the mean of the posterior predictive distribution over solution\noperators conditioned on the provided context, i.e., example condition-solution\npairs. The formalism of random differential equations provides the\nprobabilistic framework for describing the tasks ICON accomplishes while also\nproviding a basis for understanding other multi-operator learning methods. This\nprobabilistic perspective provides a basis for extending ICON to\n\\emph{generative} settings, where one can sample from the posterior predictive\ndistribution of solution operators. The generative formulation of ICON\n(GenICON) captures the underlying uncertainty in the solution operator, which\nenables principled uncertainty quantification in the solution predictions in\noperator learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context operator networks (ICON) are a class of operator learning methods\nbased on the novel architectures of foundation models. Trained on a diverse set\nof datasets of initial and boundary conditions paired with corresponding\nsolutions to ordinary and partial differential equations (ODEs and PDEs), ICON\nlearns to map example condition-solution pairs of a given differential equation\nto an approximation of its solution operator. Here, we present a probabilistic\nframework that reveals ICON as implicitly performing Bayesian inference, where\nit computes the mean of the posterior predictive distribution over solution\noperators conditioned on the provided context, i.e., example condition-solution\npairs. The formalism of random differential equations provides the\nprobabilistic framework for describing the tasks ICON accomplishes while also\nproviding a basis for understanding other multi-operator learning methods. This\nprobabilistic perspective provides a basis for extending ICON to\n\\emph{generative} settings, where one can sample from the posterior predictive\ndistribution of solution operators. The generative formulation of ICON\n(GenICON) captures the underlying uncertainty in the solution operator, which\nenables principled uncertainty quantification in the solution predictions in\noperator learning."
                },
                "authors": [
                    {
                        "name": "Benjamin J. Zhang"
                    },
                    {
                        "name": "Siting Liu"
                    },
                    {
                        "name": "Stanley J. Osher"
                    },
                    {
                        "name": "Markos A. Katsoulakis"
                    }
                ],
                "author_detail": {
                    "name": "Markos A. Katsoulakis"
                },
                "author": "Markos A. Katsoulakis",
                "arxiv_comment": "First two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05186v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05186v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06911v1",
                "updated": "2025-09-08T17:25:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    25,
                    23,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:25:23Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    25,
                    23,
                    0,
                    251,
                    0
                ],
                "title": "Hypergraph-Guided Regex Filter Synthesis for Event-Based Anomaly\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hypergraph-Guided Regex Filter Synthesis for Event-Based Anomaly\n  Detection"
                },
                "summary": "We propose HyGLAD, a novel algorithm that automatically builds a set of\ninterpretable patterns that model event data. These patterns can then be used\nto detect event-based anomalies in a stationary system, where any deviation\nfrom past behavior may indicate malicious activity. The algorithm infers\nequivalence classes of entities with similar behavior observed from the events,\nand then builds regular expressions that capture the values of those entities.\nAs opposed to deep-learning approaches, the regular expressions are directly\ninterpretable, which also translates to interpretable anomalies. We evaluate\nHyGLAD against all 7 unsupervised anomaly detection methods from DeepOD on five\ndatasets from real-world systems. The experimental results show that on average\nHyGLAD outperforms existing deep-learning methods while being an order of\nmagnitude more efficient in training and inference (single CPU vs GPU).\nPrecision improved by 1.2x and recall by 1.3x compared to the second-best\nbaseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose HyGLAD, a novel algorithm that automatically builds a set of\ninterpretable patterns that model event data. These patterns can then be used\nto detect event-based anomalies in a stationary system, where any deviation\nfrom past behavior may indicate malicious activity. The algorithm infers\nequivalence classes of entities with similar behavior observed from the events,\nand then builds regular expressions that capture the values of those entities.\nAs opposed to deep-learning approaches, the regular expressions are directly\ninterpretable, which also translates to interpretable anomalies. We evaluate\nHyGLAD against all 7 unsupervised anomaly detection methods from DeepOD on five\ndatasets from real-world systems. The experimental results show that on average\nHyGLAD outperforms existing deep-learning methods while being an order of\nmagnitude more efficient in training and inference (single CPU vs GPU).\nPrecision improved by 1.2x and recall by 1.3x compared to the second-best\nbaseline."
                },
                "authors": [
                    {
                        "name": "Margarida Ferreira"
                    },
                    {
                        "name": "Victor Nicolet"
                    },
                    {
                        "name": "Luan Pham"
                    },
                    {
                        "name": "Joey Dodds"
                    },
                    {
                        "name": "Daniel Kroening"
                    },
                    {
                        "name": "Ines Lynce"
                    },
                    {
                        "name": "Ruben Martins"
                    }
                ],
                "author_detail": {
                    "name": "Ruben Martins"
                },
                "author": "Ruben Martins",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03377v2",
                "updated": "2025-09-08T17:22:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    22,
                    17,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-03T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    53,
                    45,
                    2,
                    246,
                    0
                ],
                "title": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing"
                },
                "summary": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06902v1",
                "updated": "2025-09-08T17:20:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    20,
                    16,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:20:16Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    20,
                    16,
                    0,
                    251,
                    0
                ],
                "title": "Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers\n  from LLMs via Claim Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers\n  from LLMs via Claim Verification"
                },
                "summary": "Large Language Models (LLMs) as stochastic systems may generate numbers that\ndeviate from available data, a failure known as \\emph{numeric hallucination}.\nExisting safeguards -- retrieval-augmented generation, citations, and\nuncertainty estimation -- improve transparency but cannot guarantee fidelity:\nfabricated or misquoted values may still be displayed as if correct. We propose\n\\textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that\nenforces numeric fidelity through mechanical verification. Under PCN, numeric\nspans are emitted as \\emph{claim-bound tokens} tied to structured claims, and a\nverifier checks each token under a declared policy (e.g., exact equality,\nrounding, aliases, or tolerance with qualifiers). Crucially, PCN places\nverification in the \\emph{renderer}, not the model: only claim-checked numbers\nare marked as verified, and all others default to unverified. This separation\nprevents spoofing and guarantees fail-closed behavior. We formalize PCN and\nprove soundness, completeness under honest tokens, fail-closed behavior, and\nmonotonicity under policy refinement. PCN is lightweight and model-agnostic,\nintegrates seamlessly into existing applications, and can be extended with\ncryptographic commitments. By enforcing verification as a mandatory step before\ndisplay, PCN establishes a simple contract for numerically sensitive settings:\n\\emph{trust is earned only by proof}, while the absence of a mark communicates\nuncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) as stochastic systems may generate numbers that\ndeviate from available data, a failure known as \\emph{numeric hallucination}.\nExisting safeguards -- retrieval-augmented generation, citations, and\nuncertainty estimation -- improve transparency but cannot guarantee fidelity:\nfabricated or misquoted values may still be displayed as if correct. We propose\n\\textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that\nenforces numeric fidelity through mechanical verification. Under PCN, numeric\nspans are emitted as \\emph{claim-bound tokens} tied to structured claims, and a\nverifier checks each token under a declared policy (e.g., exact equality,\nrounding, aliases, or tolerance with qualifiers). Crucially, PCN places\nverification in the \\emph{renderer}, not the model: only claim-checked numbers\nare marked as verified, and all others default to unverified. This separation\nprevents spoofing and guarantees fail-closed behavior. We formalize PCN and\nprove soundness, completeness under honest tokens, fail-closed behavior, and\nmonotonicity under policy refinement. PCN is lightweight and model-agnostic,\nintegrates seamlessly into existing applications, and can be extended with\ncryptographic commitments. By enforcing verification as a mandatory step before\ndisplay, PCN establishes a simple contract for numerically sensitive settings:\n\\emph{trust is earned only by proof}, while the absence of a mark communicates\nuncertainty."
                },
                "authors": [
                    {
                        "name": "Aivin V. Solatorio"
                    }
                ],
                "author_detail": {
                    "name": "Aivin V. Solatorio"
                },
                "author": "Aivin V. Solatorio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12145v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12145v2",
                "updated": "2025-09-08T17:20:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    20,
                    12,
                    0,
                    251,
                    0
                ],
                "published": "2024-09-18T17:06:04Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    6,
                    4,
                    2,
                    262,
                    0
                ],
                "title": "Probing the cosmic sterile-neutrino background with IceCube",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing the cosmic sterile-neutrino background with IceCube"
                },
                "summary": "In this paper, we take a close look at the interaction between the TeV--PeV\nenergy astrophysical neutrinos and a hypothetical cosmic sterile-neutrino\nbackground. These interactions yield absorption features, also called ``dips\",\nin the astrophysical neutrino spectrum, which are studied using the deposited\nenergy distribution of high-energy starting events (HESE) in the IceCube\ndetector. We improve upon the previous analysis by including the effects of\nregeneration and a realistic source distribution on the propagation of\nastrophysical neutrinos. We use the latest 7.5-year HESE dataset and include\nthe observation of Glashow resonance in our analysis. We evaluate the impact of\nthese dips on the inferred spectral index and overall normalization of the\nastrophysical neutrinos. We find a mild preference for dips in the 300--800 TeV\nrange, and the best-fit parameters for the mass of sterile-neutrino and the\nmediator are 0.5 eV and 23 MeV, respectively. We find that the inclusion of\nthese absorption features lowers the spectral index of astrophysical neutrinos\nto $2.60^{+0.19}_{-0.16}$. We show qualitatively that the lower spectral index\nfrom HESE sample can reduce the disagreement with the Northern Tracks sample.\nWe also forecast the event spectrum for IceCube-Gen2 for the two different\nfits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we take a close look at the interaction between the TeV--PeV\nenergy astrophysical neutrinos and a hypothetical cosmic sterile-neutrino\nbackground. These interactions yield absorption features, also called ``dips\",\nin the astrophysical neutrino spectrum, which are studied using the deposited\nenergy distribution of high-energy starting events (HESE) in the IceCube\ndetector. We improve upon the previous analysis by including the effects of\nregeneration and a realistic source distribution on the propagation of\nastrophysical neutrinos. We use the latest 7.5-year HESE dataset and include\nthe observation of Glashow resonance in our analysis. We evaluate the impact of\nthese dips on the inferred spectral index and overall normalization of the\nastrophysical neutrinos. We find a mild preference for dips in the 300--800 TeV\nrange, and the best-fit parameters for the mass of sterile-neutrino and the\nmediator are 0.5 eV and 23 MeV, respectively. We find that the inclusion of\nthese absorption features lowers the spectral index of astrophysical neutrinos\nto $2.60^{+0.19}_{-0.16}$. We show qualitatively that the lower spectral index\nfrom HESE sample can reduce the disagreement with the Northern Tracks sample.\nWe also forecast the event spectrum for IceCube-Gen2 for the two different\nfits."
                },
                "authors": [
                    {
                        "name": "Bhavesh Chauhan"
                    },
                    {
                        "name": "Priyank Parashari"
                    }
                ],
                "author_detail": {
                    "name": "Priyank Parashari"
                },
                "author": "Priyank Parashari",
                "arxiv_doi": "10.1103/s6q7-48yf",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/s6q7-48yf",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12145v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12145v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "33 pages, 8 Figures, 2 Tables. Added new figures and analysis;\n  Matches the accepted version in PRD",
                "arxiv_journal_ref": "Phys. Rev. D 112, 043043 (2025)",
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06894v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06894v1",
                "updated": "2025-09-08T17:13:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    13,
                    28,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:13:28Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    13,
                    28,
                    0,
                    251,
                    0
                ],
                "title": "Learning from one graph: transductive learning guarantees via the\n  geometry of small random worlds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from one graph: transductive learning guarantees via the\n  geometry of small random worlds"
                },
                "summary": "Since their introduction by Kipf and Welling in $2017$, a primary use of\ngraph convolutional networks is transductive node classification, where missing\nlabels are inferred within a single observed graph and its feature matrix.\nDespite the widespread use of the network model, the statistical foundations of\ntransductive learning remain limited, as standard inference frameworks\ntypically rely on multiple independent samples rather than a single graph. In\nthis work, we address these gaps by developing new concentration-of-measure\ntools that leverage the geometric regularities of large graphs via\nlow-dimensional metric embeddings. The emergent regularities are captured using\na random graph model; however, the methods remain applicable to deterministic\ngraphs once observed. We establish two principal learning results. The first\nconcerns arbitrary deterministic $k$-vertex graphs, and the second addresses\nrandom graphs that share key geometric properties with an Erd\\H{o}s-R\\'{e}nyi\ngraph $\\mathbf{G}=\\mathbf{G}(k,p)$ in the regime $p \\in \\mathcal{O}((\\log\n(k)/k)^{1/2})$. The first result serves as the basis for and illuminates the\nsecond. We then extend these results to the graph convolutional network\nsetting, where additional challenges arise. Lastly, our learning guarantees\nremain informative even with a few labelled nodes $N$ and achieve the optimal\nnonparametric rate $\\mathcal{O}(N^{-1/2})$ as $N$ grows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since their introduction by Kipf and Welling in $2017$, a primary use of\ngraph convolutional networks is transductive node classification, where missing\nlabels are inferred within a single observed graph and its feature matrix.\nDespite the widespread use of the network model, the statistical foundations of\ntransductive learning remain limited, as standard inference frameworks\ntypically rely on multiple independent samples rather than a single graph. In\nthis work, we address these gaps by developing new concentration-of-measure\ntools that leverage the geometric regularities of large graphs via\nlow-dimensional metric embeddings. The emergent regularities are captured using\na random graph model; however, the methods remain applicable to deterministic\ngraphs once observed. We establish two principal learning results. The first\nconcerns arbitrary deterministic $k$-vertex graphs, and the second addresses\nrandom graphs that share key geometric properties with an Erd\\H{o}s-R\\'{e}nyi\ngraph $\\mathbf{G}=\\mathbf{G}(k,p)$ in the regime $p \\in \\mathcal{O}((\\log\n(k)/k)^{1/2})$. The first result serves as the basis for and illuminates the\nsecond. We then extend these results to the graph convolutional network\nsetting, where additional challenges arise. Lastly, our learning guarantees\nremain informative even with a few labelled nodes $N$ and achieve the optimal\nnonparametric rate $\\mathcal{O}(N^{-1/2})$ as $N$ grows."
                },
                "authors": [
                    {
                        "name": "Nils Detering"
                    },
                    {
                        "name": "Luca Galimberti"
                    },
                    {
                        "name": "Anastasis Kratsios"
                    },
                    {
                        "name": "Giulia Livieri"
                    },
                    {
                        "name": "A. Martina Neuman"
                    }
                ],
                "author_detail": {
                    "name": "A. Martina Neuman"
                },
                "author": "A. Martina Neuman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06894v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06894v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06891v1",
                "updated": "2025-09-08T17:11:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    11,
                    12,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:11:12Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    11,
                    12,
                    0,
                    251,
                    0
                ],
                "title": "Tensor Network based Gene Regulatory Network Inference for Single-Cell\n  Transcriptomic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Network based Gene Regulatory Network Inference for Single-Cell\n  Transcriptomic Data"
                },
                "summary": "Deciphering complex gene-gene interactions remains challenging in\ntranscriptomics as traditional methods often miss higher-order and nonlinear\ndependencies. This study introduces a quantum-inspired framework leveraging\ntensor networks (TNs) to optimally map expression data into a lower dimensional\nrepresentation preserving biological locality. Using Quantum Mutual Information\n(QMI), a nonparametric measure natural for tensor networks, we quantify gene\ndependencies and establish statistical significance via permutation testing.\nThis constructs robust interaction networks where the edges reflect\nbiologically meaningful relationships that are resilient to random chance. The\napproach effectively distinguishes true regulatory patterns from experimental\nnoise and biological stochasticity. To test the proposed method, we recover a\ngene regulatory network consisted of six pathway genes from single-cell RNA\nsequencing data comprising over $28.000$ lymphoblastoid cells. Furthermore, we\nunveil several triadic regulatory mechanisms. By merging quantum physics\ninspired techniques with computational biology, our method provides novel\ninsights into gene regulation, with applications in disease mechanisms and\nprecision medicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deciphering complex gene-gene interactions remains challenging in\ntranscriptomics as traditional methods often miss higher-order and nonlinear\ndependencies. This study introduces a quantum-inspired framework leveraging\ntensor networks (TNs) to optimally map expression data into a lower dimensional\nrepresentation preserving biological locality. Using Quantum Mutual Information\n(QMI), a nonparametric measure natural for tensor networks, we quantify gene\ndependencies and establish statistical significance via permutation testing.\nThis constructs robust interaction networks where the edges reflect\nbiologically meaningful relationships that are resilient to random chance. The\napproach effectively distinguishes true regulatory patterns from experimental\nnoise and biological stochasticity. To test the proposed method, we recover a\ngene regulatory network consisted of six pathway genes from single-cell RNA\nsequencing data comprising over $28.000$ lymphoblastoid cells. Furthermore, we\nunveil several triadic regulatory mechanisms. By merging quantum physics\ninspired techniques with computational biology, our method provides novel\ninsights into gene regulation, with applications in disease mechanisms and\nprecision medicine."
                },
                "authors": [
                    {
                        "name": "Olatz Sanz Larrarte"
                    },
                    {
                        "name": "Borja Aizpurua"
                    },
                    {
                        "name": "Reza Dastbasteh"
                    },
                    {
                        "name": "Ruben M. Otxoa"
                    },
                    {
                        "name": "Josu Etxezarreta Martinez"
                    }
                ],
                "author_detail": {
                    "name": "Josu Etxezarreta Martinez"
                },
                "author": "Josu Etxezarreta Martinez",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.MN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06890v1",
                "updated": "2025-09-08T17:10:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    10,
                    43,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:10:43Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    10,
                    43,
                    0,
                    251,
                    0
                ],
                "title": "Intraoperative 2D/3D Registration via Spherical Similarity Learning and\n  Inference-Time Differentiable Levenberg-Marquardt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intraoperative 2D/3D Registration via Spherical Similarity Learning and\n  Inference-Time Differentiable Levenberg-Marquardt Optimization"
                },
                "summary": "Intraoperative 2D/3D registration aligns preoperative 3D volumes with\nreal-time 2D radiographs, enabling accurate localization of instruments and\nimplants. A recent fully differentiable similarity learning framework\napproximates geodesic distances on SE(3), expanding the capture range of\nregistration and mitigating the effects of substantial disturbances, but\nexisting Euclidean approximations distort manifold structure and slow\nconvergence. To address these limitations, we explore similarity learning in\nnon-Euclidean spherical feature spaces to better capture and fit complex\nmanifold structure. We extract feature embeddings using a CNN-Transformer\nencoder, project them into spherical space, and approximate their geodesic\ndistances with Riemannian distances in the bi-invariant SO(4) space. This\nenables a more expressive and geometrically consistent deep similarity metric,\nenhancing the ability to distinguish subtle pose differences. During inference,\nwe replace gradient descent with fully differentiable Levenberg-Marquardt\noptimization to accelerate convergence. Experiments on real and synthetic\ndatasets show superior accuracy in both patient-specific and patient-agnostic\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intraoperative 2D/3D registration aligns preoperative 3D volumes with\nreal-time 2D radiographs, enabling accurate localization of instruments and\nimplants. A recent fully differentiable similarity learning framework\napproximates geodesic distances on SE(3), expanding the capture range of\nregistration and mitigating the effects of substantial disturbances, but\nexisting Euclidean approximations distort manifold structure and slow\nconvergence. To address these limitations, we explore similarity learning in\nnon-Euclidean spherical feature spaces to better capture and fit complex\nmanifold structure. We extract feature embeddings using a CNN-Transformer\nencoder, project them into spherical space, and approximate their geodesic\ndistances with Riemannian distances in the bi-invariant SO(4) space. This\nenables a more expressive and geometrically consistent deep similarity metric,\nenhancing the ability to distinguish subtle pose differences. During inference,\nwe replace gradient descent with fully differentiable Levenberg-Marquardt\noptimization to accelerate convergence. Experiments on real and synthetic\ndatasets show superior accuracy in both patient-specific and patient-agnostic\nscenarios."
                },
                "authors": [
                    {
                        "name": "Minheng Chen"
                    },
                    {
                        "name": "Youyong Kong"
                    }
                ],
                "author_detail": {
                    "name": "Youyong Kong"
                },
                "author": "Youyong Kong",
                "arxiv_comment": "WACV 2026 Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06885v1",
                "updated": "2025-09-08T17:05:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    5,
                    53,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:05:53Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    5,
                    53,
                    0,
                    251,
                    0
                ],
                "title": "Barlow-Swin: Toward a novel siamese-based segmentation architecture\n  using Swin-Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Barlow-Swin: Toward a novel siamese-based segmentation architecture\n  using Swin-Transformers"
                },
                "summary": "Medical image segmentation is a critical task in clinical workflows,\nparticularly for the detection and delineation of pathological regions. While\nconvolutional architectures like U-Net have become standard for such tasks,\ntheir limited receptive field restricts global context modeling. Recent efforts\nintegrating transformers have addressed this, but often result in deep,\ncomputationally expensive models unsuitable for real-time use. In this work, we\npresent a novel end-to-end lightweight architecture designed specifically for\nreal-time binary medical image segmentation. Our model combines a Swin\nTransformer-like encoder with a U-Net-like decoder, connected via skip pathways\nto preserve spatial detail while capturing contextual information. Unlike\nexisting designs such as Swin Transformer or U-Net, our architecture is\nsignificantly shallower and competitively efficient. To improve the encoder's\nability to learn meaningful features without relying on large amounts of\nlabeled data, we first train it using Barlow Twins, a self-supervised learning\nmethod that helps the model focus on important patterns by reducing unnecessary\nrepetition in the learned features. After this pretraining, we fine-tune the\nentire model for our specific task. Experiments on benchmark binary\nsegmentation tasks demonstrate that our model achieves competitive accuracy\nwith substantially reduced parameter count and faster inference, positioning it\nas a practical alternative for deployment in real-time and resource-limited\nclinical environments. The code for our method is available at Github\nrepository: https://github.com/mkianih/Barlow-Swin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical image segmentation is a critical task in clinical workflows,\nparticularly for the detection and delineation of pathological regions. While\nconvolutional architectures like U-Net have become standard for such tasks,\ntheir limited receptive field restricts global context modeling. Recent efforts\nintegrating transformers have addressed this, but often result in deep,\ncomputationally expensive models unsuitable for real-time use. In this work, we\npresent a novel end-to-end lightweight architecture designed specifically for\nreal-time binary medical image segmentation. Our model combines a Swin\nTransformer-like encoder with a U-Net-like decoder, connected via skip pathways\nto preserve spatial detail while capturing contextual information. Unlike\nexisting designs such as Swin Transformer or U-Net, our architecture is\nsignificantly shallower and competitively efficient. To improve the encoder's\nability to learn meaningful features without relying on large amounts of\nlabeled data, we first train it using Barlow Twins, a self-supervised learning\nmethod that helps the model focus on important patterns by reducing unnecessary\nrepetition in the learned features. After this pretraining, we fine-tune the\nentire model for our specific task. Experiments on benchmark binary\nsegmentation tasks demonstrate that our model achieves competitive accuracy\nwith substantially reduced parameter count and faster inference, positioning it\nas a practical alternative for deployment in real-time and resource-limited\nclinical environments. The code for our method is available at Github\nrepository: https://github.com/mkianih/Barlow-Swin."
                },
                "authors": [
                    {
                        "name": "Morteza Kiani Haftlang"
                    },
                    {
                        "name": "Mohammadhossein Malmir"
                    },
                    {
                        "name": "Foroutan Parand"
                    },
                    {
                        "name": "Umberto Michelucci"
                    },
                    {
                        "name": "Safouane El Ghazouali"
                    }
                ],
                "author_detail": {
                    "name": "Safouane El Ghazouali"
                },
                "author": "Safouane El Ghazouali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05263v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05263v2",
                "updated": "2025-09-08T17:05:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    5,
                    47,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-05T17:22:33Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    22,
                    33,
                    4,
                    248,
                    0
                ],
                "title": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for\n  Interactive Complex World Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for\n  Interactive Complex World Generation"
                },
                "summary": "Recent research has been increasingly focusing on developing 3D world models\nthat simulate complex real-world scenarios. World models have found broad\napplications across various domains, including embodied AI, autonomous driving,\nentertainment, etc. A more realistic simulation with accurate physics will\neffectively narrow the sim-to-real gap and allow us to gather rich information\nabout the real world conveniently. While traditional manual modeling has\nenabled the creation of virtual 3D scenes, modern approaches have leveraged\nadvanced machine learning algorithms for 3D world generation, with most recent\nadvances focusing on generative methods that can create virtual worlds based on\nuser instructions. This work explores such a research direction by proposing\nLatticeWorld, a simple yet effective 3D world generation framework that\nstreamlines the industrial production pipeline of 3D environments. LatticeWorld\nleverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering\nengine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed\nframework accepts textual descriptions and visual instructions as multimodal\ninputs and creates large-scale 3D interactive worlds with dynamic agents,\nfeaturing competitive multi-agent interaction, high-fidelity physics\nsimulation, and real-time rendering. We conduct comprehensive experiments to\nevaluate LatticeWorld, showing that it achieves superior accuracy in scene\nlayout generation and visual fidelity. Moreover, LatticeWorld achieves over a\n$90\\times$ increase in industrial production efficiency while maintaining high\ncreative quality compared with traditional manual production methods. Our demo\nvideo is available at https://youtu.be/8VWZXpERR18",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has been increasingly focusing on developing 3D world models\nthat simulate complex real-world scenarios. World models have found broad\napplications across various domains, including embodied AI, autonomous driving,\nentertainment, etc. A more realistic simulation with accurate physics will\neffectively narrow the sim-to-real gap and allow us to gather rich information\nabout the real world conveniently. While traditional manual modeling has\nenabled the creation of virtual 3D scenes, modern approaches have leveraged\nadvanced machine learning algorithms for 3D world generation, with most recent\nadvances focusing on generative methods that can create virtual worlds based on\nuser instructions. This work explores such a research direction by proposing\nLatticeWorld, a simple yet effective 3D world generation framework that\nstreamlines the industrial production pipeline of 3D environments. LatticeWorld\nleverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering\nengine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed\nframework accepts textual descriptions and visual instructions as multimodal\ninputs and creates large-scale 3D interactive worlds with dynamic agents,\nfeaturing competitive multi-agent interaction, high-fidelity physics\nsimulation, and real-time rendering. We conduct comprehensive experiments to\nevaluate LatticeWorld, showing that it achieves superior accuracy in scene\nlayout generation and visual fidelity. Moreover, LatticeWorld achieves over a\n$90\\times$ increase in industrial production efficiency while maintaining high\ncreative quality compared with traditional manual production methods. Our demo\nvideo is available at https://youtu.be/8VWZXpERR18"
                },
                "authors": [
                    {
                        "name": "Yinglin Duan"
                    },
                    {
                        "name": "Zhengxia Zou"
                    },
                    {
                        "name": "Tongwei Gu"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Zhan Zhao"
                    },
                    {
                        "name": "Luyi Xu"
                    },
                    {
                        "name": "Xinzhu Liu"
                    },
                    {
                        "name": "Yenan Lin"
                    },
                    {
                        "name": "Hao Jiang"
                    },
                    {
                        "name": "Kang Chen"
                    },
                    {
                        "name": "Shuang Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Shuang Qiu"
                },
                "author": "Shuang Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05263v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05263v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04211v2",
                "updated": "2025-09-08T17:04:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    4,
                    53,
                    0,
                    251,
                    0
                ],
                "published": "2024-02-06T18:09:05Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    18,
                    9,
                    5,
                    1,
                    37,
                    0
                ],
                "title": "Probabilistic Shapley Value Modeling and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Shapley Value Modeling and Inference"
                },
                "summary": "We propose probabilistic Shapley inference (PSI), a novel probabilistic\nframework to model and infer sufficient statistics of feature attributions in\nflexible predictive models, via latent random variables whose mean recovers\nShapley values. PSI enables efficient, scalable inference over input-to-output\nattributions, and their uncertainty, via a variational objective that jointly\ntrains a predictive (regression or classification) model and its attribution\ndistributions. To address the challenge of marginalizing over variable-length\ninput feature subsets in Shapley value calculation, we introduce a\nmasking-based neural network architecture, with a modular training and\ninference procedure. We evaluate PSI on synthetic and real-world datasets,\nshowing that it achieves competitive predictive performance compared to strong\nbaselines, while learning feature attribution distributions -- centered at\nShapley values -- that reveal meaningful attribution uncertainty across data\nmodalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose probabilistic Shapley inference (PSI), a novel probabilistic\nframework to model and infer sufficient statistics of feature attributions in\nflexible predictive models, via latent random variables whose mean recovers\nShapley values. PSI enables efficient, scalable inference over input-to-output\nattributions, and their uncertainty, via a variational objective that jointly\ntrains a predictive (regression or classification) model and its attribution\ndistributions. To address the challenge of marginalizing over variable-length\ninput feature subsets in Shapley value calculation, we introduce a\nmasking-based neural network architecture, with a modular training and\ninference procedure. We evaluate PSI on synthetic and real-world datasets,\nshowing that it achieves competitive predictive performance compared to strong\nbaselines, while learning feature attribution distributions -- centered at\nShapley values -- that reveal meaningful attribution uncertainty across data\nmodalities."
                },
                "authors": [
                    {
                        "name": "Mert Ketenci"
                    },
                    {
                        "name": "Iñigo Urteaga"
                    },
                    {
                        "name": "Victor Alfonso Rodriguez"
                    },
                    {
                        "name": "Noémie Elhadad"
                    },
                    {
                        "name": "Adler Perotte"
                    }
                ],
                "author_detail": {
                    "name": "Adler Perotte"
                },
                "author": "Adler Perotte",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06883v1",
                "updated": "2025-09-08T17:02:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    2,
                    34,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:02:34Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    2,
                    34,
                    0,
                    251,
                    0
                ],
                "title": "UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction"
                },
                "summary": "We participate in CheckThat! Task 2 English and explore various methods of\nprompting and in-context learning, including few-shot prompting and fine-tuning\nwith different LLM families, with the goal of extracting check-worthy claims\nfrom social media passages. Our best METEOR score is achieved by fine-tuning a\nFLAN-T5 model. However, we observe that higher-quality claims can sometimes be\nextracted using other methods, even when their METEOR scores are lower.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We participate in CheckThat! Task 2 English and explore various methods of\nprompting and in-context learning, including few-shot prompting and fine-tuning\nwith different LLM families, with the goal of extracting check-worthy claims\nfrom social media passages. Our best METEOR score is achieved by fine-tuning a\nFLAN-T5 model. However, we observe that higher-quality claims can sometimes be\nextracted using other methods, even when their METEOR scores are lower."
                },
                "authors": [
                    {
                        "name": "Joe Wilder"
                    },
                    {
                        "name": "Nikhil Kadapala"
                    },
                    {
                        "name": "Benji Xu"
                    },
                    {
                        "name": "Mohammed Alsaadi"
                    },
                    {
                        "name": "Aiden Parsons"
                    },
                    {
                        "name": "Mitchell Rogers"
                    },
                    {
                        "name": "Palash Agarwal"
                    },
                    {
                        "name": "Adam Hassick"
                    },
                    {
                        "name": "Laura Dietz"
                    }
                ],
                "author_detail": {
                    "name": "Laura Dietz"
                },
                "author": "Laura Dietz",
                "arxiv_comment": "16 pages,3 tables, CLEF 2025 Working Notes, 9-12 September 2025,\n  Madrid, Spain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02587v2",
                "updated": "2025-09-08T17:00:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    0,
                    18,
                    0,
                    251,
                    0
                ],
                "published": "2025-05-05T11:42:28Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    42,
                    28,
                    0,
                    125,
                    0
                ],
                "title": "Deriving Duration Time from Occupancy Data -- A case study in the length\n  of stay in Intensive Care Units for COVID-19 patients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deriving Duration Time from Occupancy Data -- A case study in the length\n  of stay in Intensive Care Units for COVID-19 patients"
                },
                "summary": "This paper focuses on drawing information on underlying processes, which are\nnot directly observed in the data. In particular, we work with data in which\nonly the total count of units in a system at a given time point is observed,\nbut the underlying process of inflows, length of stay and outflows is not. The\nparticular data example looked at in this paper is the occupancy of intensive\ncare units (ICU) during the COVID-19 pandemic, where the aggregated numbers of\noccupied beds in ICUs on the district level (`Landkreis') are recorded, but not\nthe number of incoming and outgoing patients. The Skellam distribution allows\nus to infer the number of incoming and outgoing patients from the occupancy in\nthe ICUs. This paper goes a step beyond and approaches the question of whether\nwe can also estimate the average length of stay of ICU patients. Hence, the\ntask is to derive not only the number of incoming and outgoing units from a\ntotal net count but also to gain information on the duration time of patients\non ICUs. We make use of a stochastic Expectation-Maximisation algorithm and\nadditionally include exogenous information which are assumed to explain the\nintensity of inflow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper focuses on drawing information on underlying processes, which are\nnot directly observed in the data. In particular, we work with data in which\nonly the total count of units in a system at a given time point is observed,\nbut the underlying process of inflows, length of stay and outflows is not. The\nparticular data example looked at in this paper is the occupancy of intensive\ncare units (ICU) during the COVID-19 pandemic, where the aggregated numbers of\noccupied beds in ICUs on the district level (`Landkreis') are recorded, but not\nthe number of incoming and outgoing patients. The Skellam distribution allows\nus to infer the number of incoming and outgoing patients from the occupancy in\nthe ICUs. This paper goes a step beyond and approaches the question of whether\nwe can also estimate the average length of stay of ICU patients. Hence, the\ntask is to derive not only the number of incoming and outgoing units from a\ntotal net count but also to gain information on the duration time of patients\non ICUs. We make use of a stochastic Expectation-Maximisation algorithm and\nadditionally include exogenous information which are assumed to explain the\nintensity of inflow."
                },
                "authors": [
                    {
                        "name": "Martje Rave"
                    },
                    {
                        "name": "Göran Kauermann"
                    }
                ],
                "author_detail": {
                    "name": "Göran Kauermann"
                },
                "author": "Göran Kauermann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18504v2",
                "updated": "2025-09-08T16:53:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    53,
                    44,
                    0,
                    251,
                    0
                ],
                "published": "2025-07-24T15:22:27Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    22,
                    27,
                    3,
                    205,
                    0
                ],
                "title": "Not All Features Deserve Attention: Graph-Guided Dependency Learning for\n  Tabular Data Generation with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Features Deserve Attention: Graph-Guided Dependency Learning for\n  Tabular Data Generation with Language Models"
                },
                "summary": "Large Language Models (LLMs) have shown strong potential for tabular data\ngeneration by modeling textualized feature-value pairs. However, tabular data\ninherently exhibits sparse feature-level dependencies, where many feature\ninteractions are structurally insignificant. This creates a fundamental\nmismatch as LLMs' self-attention mechanism inevitably distributes focus across\nall pairs, diluting attention on critical relationships, particularly in\ndatasets with complex dependencies or semantically ambiguous features. To\naddress this limitation, we propose GraDe (Graph-Guided Dependency Learning), a\nnovel method that explicitly integrates sparse dependency graphs into LLMs'\nattention mechanism. GraDe employs a lightweight dynamic graph learning module\nguided by externally extracted functional dependencies, prioritizing key\nfeature interactions while suppressing irrelevant ones. Our experiments across\ndiverse real-world datasets demonstrate that GraDe outperforms existing\nLLM-based approaches by up to 12% on complex datasets while achieving\ncompetitive results with state-of-the-art approaches in synthetic data quality.\nOur method is minimally intrusive yet effective, offering a practical solution\nfor structure-aware tabular data modeling with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown strong potential for tabular data\ngeneration by modeling textualized feature-value pairs. However, tabular data\ninherently exhibits sparse feature-level dependencies, where many feature\ninteractions are structurally insignificant. This creates a fundamental\nmismatch as LLMs' self-attention mechanism inevitably distributes focus across\nall pairs, diluting attention on critical relationships, particularly in\ndatasets with complex dependencies or semantically ambiguous features. To\naddress this limitation, we propose GraDe (Graph-Guided Dependency Learning), a\nnovel method that explicitly integrates sparse dependency graphs into LLMs'\nattention mechanism. GraDe employs a lightweight dynamic graph learning module\nguided by externally extracted functional dependencies, prioritizing key\nfeature interactions while suppressing irrelevant ones. Our experiments across\ndiverse real-world datasets demonstrate that GraDe outperforms existing\nLLM-based approaches by up to 12% on complex datasets while achieving\ncompetitive results with state-of-the-art approaches in synthetic data quality.\nOur method is minimally intrusive yet effective, offering a practical solution\nfor structure-aware tabular data modeling with LLMs."
                },
                "authors": [
                    {
                        "name": "Zheyu Zhang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Bardh Prenkaj"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci",
                "arxiv_comment": "Accepted to EMNLP 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20521v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20521v2",
                "updated": "2025-09-08T16:40:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    40,
                    29,
                    0,
                    251,
                    0
                ],
                "published": "2025-05-26T20:53:53Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    20,
                    53,
                    53,
                    0,
                    146,
                    0
                ],
                "title": "Project Riley: Multimodal Multi-Agent LLM Collaboration with Emotional\n  Reasoning and Voting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Project Riley: Multimodal Multi-Agent LLM Collaboration with Emotional\n  Reasoning and Voting"
                },
                "summary": "This paper presents Project Riley, a novel multimodal and multi-model\nconversational AI architecture oriented towards the simulation of reasoning\ninfluenced by emotional states. Drawing inspiration from Pixar's Inside Out,\nthe system comprises five distinct emotional agents - Joy, Sadness, Fear,\nAnger, and Disgust - that engage in structured multi-round dialogues to\ngenerate, criticise, and iteratively refine responses. A final reasoning\nmechanism synthesises the contributions of these agents into a coherent output\nthat either reflects the dominant emotion or integrates multiple perspectives.\nThe architecture incorporates both textual and visual large language models\n(LLMs), alongside advanced reasoning and self-refinement processes. A\nfunctional prototype was deployed locally in an offline environment, optimised\nfor emotional expressiveness and computational efficiency. From this initial\nprototype, another one emerged, called Armando, which was developed for use in\nemergency contexts, delivering emotionally calibrated and factually accurate\ninformation through the integration of Retrieval-Augmented Generation (RAG) and\ncumulative context tracking. The Project Riley prototype was evaluated through\nuser testing, in which participants interacted with the chatbot and completed a\nstructured questionnaire assessing three dimensions: Emotional Appropriateness,\nClarity and Utility, and Naturalness and Human-likeness. The results indicate\nstrong performance in structured scenarios, particularly with respect to\nemotional alignment and communicative clarity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents Project Riley, a novel multimodal and multi-model\nconversational AI architecture oriented towards the simulation of reasoning\ninfluenced by emotional states. Drawing inspiration from Pixar's Inside Out,\nthe system comprises five distinct emotional agents - Joy, Sadness, Fear,\nAnger, and Disgust - that engage in structured multi-round dialogues to\ngenerate, criticise, and iteratively refine responses. A final reasoning\nmechanism synthesises the contributions of these agents into a coherent output\nthat either reflects the dominant emotion or integrates multiple perspectives.\nThe architecture incorporates both textual and visual large language models\n(LLMs), alongside advanced reasoning and self-refinement processes. A\nfunctional prototype was deployed locally in an offline environment, optimised\nfor emotional expressiveness and computational efficiency. From this initial\nprototype, another one emerged, called Armando, which was developed for use in\nemergency contexts, delivering emotionally calibrated and factually accurate\ninformation through the integration of Retrieval-Augmented Generation (RAG) and\ncumulative context tracking. The Project Riley prototype was evaluated through\nuser testing, in which participants interacted with the chatbot and completed a\nstructured questionnaire assessing three dimensions: Emotional Appropriateness,\nClarity and Utility, and Naturalness and Human-likeness. The results indicate\nstrong performance in structured scenarios, particularly with respect to\nemotional alignment and communicative clarity."
                },
                "authors": [
                    {
                        "name": "Ana Rita Ortigoso"
                    },
                    {
                        "name": "Gabriel Vieira"
                    },
                    {
                        "name": "Daniel Fuentes"
                    },
                    {
                        "name": "Luis Frazão"
                    },
                    {
                        "name": "Nuno Costa"
                    },
                    {
                        "name": "António Pereira"
                    }
                ],
                "author_detail": {
                    "name": "António Pereira"
                },
                "author": "António Pereira",
                "arxiv_comment": "28 pages, 5 figures. Submitted for review to Information Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20521v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20521v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.1; H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06870v1",
                "updated": "2025-09-08T16:39:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    39,
                    38,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T16:39:38Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    39,
                    38,
                    0,
                    251,
                    0
                ],
                "title": "The Majority is not always right: RL training for solution aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Majority is not always right: RL training for solution aggregation"
                },
                "summary": "Scaling up test-time compute, by generating multiple independent solutions\nand selecting or aggregating among them, has become a central paradigm for\nimproving large language models (LLMs) on challenging reasoning tasks. While\nmost prior work relies on simple majority voting or reward model ranking to\naggregate solutions, these approaches may only yield limited benefits. In this\nwork, we propose to learn aggregation as an explicit reasoning skill: given a\nset of candidate solutions, we train an aggregator model to review, reconcile,\nand synthesize a final, correct answer using reinforcement learning from\nverifiable rewards. A key ingredient is careful balancing of easy and hard\ntraining examples, allowing the model to learn both to recover\nminority-but-correct answers as well as easy majority-correct answers.\nEmpirically, we find our method, AggLM, outperforms both strong rule-based and\nreward-model baselines, across multiple benchmarks. Furthermore, it generalizes\neffectively to solutions from differing models, including stronger ones than\ncontained in the training data, all while requiring substantially fewer tokens\nthan majority voting with larger numbers of solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up test-time compute, by generating multiple independent solutions\nand selecting or aggregating among them, has become a central paradigm for\nimproving large language models (LLMs) on challenging reasoning tasks. While\nmost prior work relies on simple majority voting or reward model ranking to\naggregate solutions, these approaches may only yield limited benefits. In this\nwork, we propose to learn aggregation as an explicit reasoning skill: given a\nset of candidate solutions, we train an aggregator model to review, reconcile,\nand synthesize a final, correct answer using reinforcement learning from\nverifiable rewards. A key ingredient is careful balancing of easy and hard\ntraining examples, allowing the model to learn both to recover\nminority-but-correct answers as well as easy majority-correct answers.\nEmpirically, we find our method, AggLM, outperforms both strong rule-based and\nreward-model baselines, across multiple benchmarks. Furthermore, it generalizes\neffectively to solutions from differing models, including stronger ones than\ncontained in the training data, all while requiring substantially fewer tokens\nthan majority voting with larger numbers of solutions."
                },
                "authors": [
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Pranjal Aggarwal"
                    },
                    {
                        "name": "Swarnadeep Saha"
                    },
                    {
                        "name": "Asli Celikyilmaz"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Ilia Kulikov"
                    }
                ],
                "author_detail": {
                    "name": "Ilia Kulikov"
                },
                "author": "Ilia Kulikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18658v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18658v4",
                "updated": "2025-09-08T16:34:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    34,
                    58,
                    0,
                    251,
                    0
                ],
                "published": "2025-02-25T21:37:25Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    21,
                    37,
                    25,
                    1,
                    56,
                    0
                ],
                "title": "Assistance or Disruption? Exploring and Evaluating the Design and\n  Trade-offs of Proactive AI Programming Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assistance or Disruption? Exploring and Evaluating the Design and\n  Trade-offs of Proactive AI Programming Support"
                },
                "summary": "AI programming tools enable powerful code generation, and recent prototypes\nattempt to reduce user effort with proactive AI agents, but their impact on\nprogramming workflows remains unexplored. We introduce and evaluate\nCodellaborator, a design probe LLM agent that initiates programming assistance\nbased on editor activities and task context. We explored three interface\nvariants to assess trade-offs between increasingly salient AI support:\nprompt-only, proactive agent, and proactive agent with presence and context\n(Codellaborator). In a within-subject study (N=18), we find that proactive\nagents increase efficiency compared to prompt-only paradigm, but also incur\nworkflow disruptions. However, presence indicators and interaction context\nsupport alleviated disruptions and improved users' awareness of AI processes.\nWe underscore trade-offs of Codellaborator on user control, ownership, and code\nunderstanding, emphasizing the need to adapt proactivity to programming\nprocesses. Our research contributes to the design exploration and evaluation of\nproactive AI systems, presenting design implications on AI-integrated\nprogramming workflow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI programming tools enable powerful code generation, and recent prototypes\nattempt to reduce user effort with proactive AI agents, but their impact on\nprogramming workflows remains unexplored. We introduce and evaluate\nCodellaborator, a design probe LLM agent that initiates programming assistance\nbased on editor activities and task context. We explored three interface\nvariants to assess trade-offs between increasingly salient AI support:\nprompt-only, proactive agent, and proactive agent with presence and context\n(Codellaborator). In a within-subject study (N=18), we find that proactive\nagents increase efficiency compared to prompt-only paradigm, but also incur\nworkflow disruptions. However, presence indicators and interaction context\nsupport alleviated disruptions and improved users' awareness of AI processes.\nWe underscore trade-offs of Codellaborator on user control, ownership, and code\nunderstanding, emphasizing the need to adapt proactivity to programming\nprocesses. Our research contributes to the design exploration and evaluation of\nproactive AI systems, presenting design implications on AI-integrated\nprogramming workflow."
                },
                "authors": [
                    {
                        "name": "Kevin Pu"
                    },
                    {
                        "name": "Daniel Lazaro"
                    },
                    {
                        "name": "Ian Arawjo"
                    },
                    {
                        "name": "Haijun Xia"
                    },
                    {
                        "name": "Ziang Xiao"
                    },
                    {
                        "name": "Tovi Grossman"
                    },
                    {
                        "name": "Yan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yan Chen"
                },
                "author": "Yan Chen",
                "arxiv_doi": "10.1145/3706598.3713357",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3713357",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.18658v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18658v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06861v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06861v1",
                "updated": "2025-09-08T16:28:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    28,
                    25,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T16:28:25Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    28,
                    25,
                    0,
                    251,
                    0
                ],
                "title": "Test-Time Scaling in Reasoning Models Is Not Effective for\n  Knowledge-Intensive Tasks Yet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Scaling in Reasoning Models Is Not Effective for\n  Knowledge-Intensive Tasks Yet"
                },
                "summary": "Test-time scaling increases inference-time computation by allowing models to\ngenerate long reasoning chains, and has shown strong performance across many\ndomains. However, in this work, we show that this approach is not yet effective\nfor knowledge-intensive tasks, where high factual accuracy and low\nhallucination rates are essential. We conduct a comprehensive evaluation of\ntest-time scaling using 12 reasoning models on two knowledge-intensive\nbenchmarks. Our results reveal that increasing test-time computation does not\nconsistently improve accuracy and, in many cases, it even leads to more\nhallucinations. We then analyze how extended reasoning affects hallucination\nbehavior. We find that reduced hallucinations often result from the model\nchoosing to abstain after thinking more, rather than from improved factual\nrecall. Conversely, for some models, longer reasoning encourages attempts on\npreviously unanswered questions, many of which result in hallucinations. Case\nstudies show that extended reasoning can induce confirmation bias, leading to\noverconfident hallucinations. Despite these limitations, we observe that\ncompared to non-thinking, enabling thinking remains beneficial. Code and data\nare available at https://github.com/XuZhao0/tts-knowledge",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling increases inference-time computation by allowing models to\ngenerate long reasoning chains, and has shown strong performance across many\ndomains. However, in this work, we show that this approach is not yet effective\nfor knowledge-intensive tasks, where high factual accuracy and low\nhallucination rates are essential. We conduct a comprehensive evaluation of\ntest-time scaling using 12 reasoning models on two knowledge-intensive\nbenchmarks. Our results reveal that increasing test-time computation does not\nconsistently improve accuracy and, in many cases, it even leads to more\nhallucinations. We then analyze how extended reasoning affects hallucination\nbehavior. We find that reduced hallucinations often result from the model\nchoosing to abstain after thinking more, rather than from improved factual\nrecall. Conversely, for some models, longer reasoning encourages attempts on\npreviously unanswered questions, many of which result in hallucinations. Case\nstudies show that extended reasoning can induce confirmation bias, leading to\noverconfident hallucinations. Despite these limitations, we observe that\ncompared to non-thinking, enabling thinking remains beneficial. Code and data\nare available at https://github.com/XuZhao0/tts-knowledge"
                },
                "authors": [
                    {
                        "name": "James Xu Zhao"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "See-Kiong Ng"
                    }
                ],
                "author_detail": {
                    "name": "See-Kiong Ng"
                },
                "author": "See-Kiong Ng",
                "arxiv_comment": "20 pages, 4 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06861v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06861v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06858v1",
                "updated": "2025-09-08T16:26:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    26,
                    45,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T16:26:45Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    26,
                    45,
                    0,
                    251,
                    0
                ],
                "title": "Disentangling Interaction and Bias Effects in Opinion Dynamics of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disentangling Interaction and Bias Effects in Opinion Dynamics of Large\n  Language Models"
                },
                "summary": "Large Language Models are increasingly used to simulate human opinion\ndynamics, yet the effect of genuine interaction is often obscured by systematic\nbiases. We present a Bayesian framework to disentangle and quantify three such\nbiases: (i) a topic bias toward prior opinions in the training data; (ii) an\nagreement bias favoring agreement irrespective of the question; and (iii) an\nanchoring bias toward the initiating agent's stance. Applying this framework to\nmulti-step dialogues reveals that opinion trajectories tend to quickly converge\nto a shared attractor, with the influence of the interaction fading over time,\nand the impact of biases differing between LLMs. In addition, we fine-tune an\nLLM on different sets of strongly opinionated statements (incl. misinformation)\nand demonstrate that the opinion attractor shifts correspondingly. Exposing\nstark differences between LLMs and providing quantitative tools to compare them\nto human subjects in the future, our approach highlights both chances and\npitfalls in using LLMs as proxies for human behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are increasingly used to simulate human opinion\ndynamics, yet the effect of genuine interaction is often obscured by systematic\nbiases. We present a Bayesian framework to disentangle and quantify three such\nbiases: (i) a topic bias toward prior opinions in the training data; (ii) an\nagreement bias favoring agreement irrespective of the question; and (iii) an\nanchoring bias toward the initiating agent's stance. Applying this framework to\nmulti-step dialogues reveals that opinion trajectories tend to quickly converge\nto a shared attractor, with the influence of the interaction fading over time,\nand the impact of biases differing between LLMs. In addition, we fine-tune an\nLLM on different sets of strongly opinionated statements (incl. misinformation)\nand demonstrate that the opinion attractor shifts correspondingly. Exposing\nstark differences between LLMs and providing quantitative tools to compare them\nto human subjects in the future, our approach highlights both chances and\npitfalls in using LLMs as proxies for human behavior."
                },
                "authors": [
                    {
                        "name": "Vincent C. Brockers"
                    },
                    {
                        "name": "David A. Ehrlich"
                    },
                    {
                        "name": "Viola Priesemann"
                    }
                ],
                "author_detail": {
                    "name": "Viola Priesemann"
                },
                "author": "Viola Priesemann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.AO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06855v1",
                "updated": "2025-09-08T16:23:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    23,
                    44,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T16:23:44Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    23,
                    44,
                    0,
                    251,
                    0
                ],
                "title": "Seeing the Forest Through the Trees: Knowledge Retrieval for\n  Streamlining Particle Physics Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeing the Forest Through the Trees: Knowledge Retrieval for\n  Streamlining Particle Physics Analysis"
                },
                "summary": "Generative Large Language Models (LLMs) are a promising approach to\nstructuring knowledge contained within the corpora of research literature\nproduced by large-scale and long-running scientific collaborations. Within\nexperimental particle physics, such structured knowledge bases could expedite\nmethodological and editorial review. Complementarily, within the broader\nscientific community, generative LLM systems grounded in published work could\nmake for reliable companions allowing non-experts to analyze open-access data.\nTechniques such as Retrieval Augmented Generation (RAG) rely on semantically\nmatching localized text chunks, but struggle to maintain coherent context when\nrelevant information spans multiple segments, leading to a fragmented\nrepresentation devoid of global cross-document information. Here, we utilize\nthe hierarchical organization of experimental physics articles to build a tree\nrepresentation of the corpus, and present the SciTreeRAG system that uses this\nstructure to create contexts that are more focused and contextually rich than\nstandard RAG. Additionally, we develop methods for using LLMs to transform the\nunstructured corpus into a structured knowledge graph representation. We then\nimplement SciGraphRAG, a retrieval system that leverages this knowledge graph\nto access global cross-document relationships eluding standard RAG, thereby\nencapsulating domain-specific connections and expertise. We demonstrate\nproof-of-concept implementations using the corpus of the LHCb experiment at\nCERN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Large Language Models (LLMs) are a promising approach to\nstructuring knowledge contained within the corpora of research literature\nproduced by large-scale and long-running scientific collaborations. Within\nexperimental particle physics, such structured knowledge bases could expedite\nmethodological and editorial review. Complementarily, within the broader\nscientific community, generative LLM systems grounded in published work could\nmake for reliable companions allowing non-experts to analyze open-access data.\nTechniques such as Retrieval Augmented Generation (RAG) rely on semantically\nmatching localized text chunks, but struggle to maintain coherent context when\nrelevant information spans multiple segments, leading to a fragmented\nrepresentation devoid of global cross-document information. Here, we utilize\nthe hierarchical organization of experimental physics articles to build a tree\nrepresentation of the corpus, and present the SciTreeRAG system that uses this\nstructure to create contexts that are more focused and contextually rich than\nstandard RAG. Additionally, we develop methods for using LLMs to transform the\nunstructured corpus into a structured knowledge graph representation. We then\nimplement SciGraphRAG, a retrieval system that leverages this knowledge graph\nto access global cross-document relationships eluding standard RAG, thereby\nencapsulating domain-specific connections and expertise. We demonstrate\nproof-of-concept implementations using the corpus of the LHCb experiment at\nCERN."
                },
                "authors": [
                    {
                        "name": "James McGreivy"
                    },
                    {
                        "name": "Blaise Delaney"
                    },
                    {
                        "name": "Anja Beck"
                    },
                    {
                        "name": "Mike Williams"
                    }
                ],
                "author_detail": {
                    "name": "Mike Williams"
                },
                "author": "Mike Williams",
                "arxiv_comment": "17 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02019v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02019v2",
                "updated": "2025-09-08T16:15:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    15,
                    6,
                    0,
                    251,
                    0
                ],
                "published": "2025-05-28T08:43:49Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    8,
                    43,
                    49,
                    2,
                    148,
                    0
                ],
                "title": "ChatCFD: An LLM-Driven Agent for End-to-End CFD Automation with\n  Domain-Specific Structured Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatCFD: An LLM-Driven Agent for End-to-End CFD Automation with\n  Domain-Specific Structured Reasoning"
                },
                "summary": "Computational Fluid Dynamics (CFD) is essential for advancing scientific and\nengineering fields but is hindered by operational complexity, high expertise\nrequirements, and limited accessibility. This paper introduces ChatCFD, an\nautomated agent system for OpenFOAM simulations that processes multi-modal\ninputs (e.g., research papers, meshes) via an interactive interface, leveraging\nDeepSeek-R1 and DeepSeek-V3 large language models, a multi-agent architecture,\nand OpenFOAM knowledge. Its four-stage pipeline (Knowledge Base Construction,\nUser Input Processing, Case File Generation, and Execution and Error\nReflection) enables iterative trial-reflection-refinement for intricate setups,\nsupporting diverse physical models and external meshes. Validation on 205\nbenchmark tutorial cases, 110 perturbed variants, and 2 literature-derived\ncases shows ChatCFD's 82.1 percent operational success rate on basic cases,\noutperforming MetaOpenFOAM (6.2 percent) and Foam-Agent (42.3 percent), and\n60-80 percent on literature-derived complex cases. Turbulence model studies\nshow a 40 percent success rate for common models versus 10 percent for rare\nones like RNG k-epsilon. Physics coupling analyses reveal higher resource\ndemands for multi-physics-coupled cases, while LLM bias toward simpler setups\nintroduces persistent errors, such as dimensional inconsistency. Ablation\nstudies highlight the efficacy of RAG-based modules and reflection mechanisms.\nBy automating hypothesis testing and parameter exploration, ChatCFD accelerates\nscientific discovery in fluid mechanics and engineering, addressing LLM\nlimitations through structured design and showing strong potential as a modular\ncomponent in MCP-based agent networks for collaborative multi-agent systems,\npaving the way for scalable AI-driven CFD innovation. The code for ChatCFD is\navailable at https://github.com/ConMoo/ChatCFD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Fluid Dynamics (CFD) is essential for advancing scientific and\nengineering fields but is hindered by operational complexity, high expertise\nrequirements, and limited accessibility. This paper introduces ChatCFD, an\nautomated agent system for OpenFOAM simulations that processes multi-modal\ninputs (e.g., research papers, meshes) via an interactive interface, leveraging\nDeepSeek-R1 and DeepSeek-V3 large language models, a multi-agent architecture,\nand OpenFOAM knowledge. Its four-stage pipeline (Knowledge Base Construction,\nUser Input Processing, Case File Generation, and Execution and Error\nReflection) enables iterative trial-reflection-refinement for intricate setups,\nsupporting diverse physical models and external meshes. Validation on 205\nbenchmark tutorial cases, 110 perturbed variants, and 2 literature-derived\ncases shows ChatCFD's 82.1 percent operational success rate on basic cases,\noutperforming MetaOpenFOAM (6.2 percent) and Foam-Agent (42.3 percent), and\n60-80 percent on literature-derived complex cases. Turbulence model studies\nshow a 40 percent success rate for common models versus 10 percent for rare\nones like RNG k-epsilon. Physics coupling analyses reveal higher resource\ndemands for multi-physics-coupled cases, while LLM bias toward simpler setups\nintroduces persistent errors, such as dimensional inconsistency. Ablation\nstudies highlight the efficacy of RAG-based modules and reflection mechanisms.\nBy automating hypothesis testing and parameter exploration, ChatCFD accelerates\nscientific discovery in fluid mechanics and engineering, addressing LLM\nlimitations through structured design and showing strong potential as a modular\ncomponent in MCP-based agent networks for collaborative multi-agent systems,\npaving the way for scalable AI-driven CFD innovation. The code for ChatCFD is\navailable at https://github.com/ConMoo/ChatCFD."
                },
                "authors": [
                    {
                        "name": "E Fan"
                    },
                    {
                        "name": "Kang Hu"
                    },
                    {
                        "name": "Zhuowen Wu"
                    },
                    {
                        "name": "Jiangyang Ge"
                    },
                    {
                        "name": "Jiawei Miao"
                    },
                    {
                        "name": "Yuzhi Zhang"
                    },
                    {
                        "name": "He Sun"
                    },
                    {
                        "name": "Weizong Wang"
                    },
                    {
                        "name": "Tianhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tianhan Zhang"
                },
                "author": "Tianhan Zhang",
                "arxiv_comment": "19 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02019v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02019v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12758v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12758v2",
                "updated": "2025-09-08T16:10:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    10,
                    37,
                    0,
                    251,
                    0
                ],
                "published": "2025-04-17T08:53:30Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    53,
                    30,
                    3,
                    107,
                    0
                ],
                "title": "Universal Approximation with XL MIMO Systems: OTA Classification via\n  Trainable Analog Combining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Approximation with XL MIMO Systems: OTA Classification via\n  Trainable Analog Combining"
                },
                "summary": "In this paper, we show that an eXtremely Large (XL) Multiple-Input\nMultiple-Output (MIMO) wireless system with appropriate analog combining\ncomponents exhibits the properties of a universal function approximator,\nsimilar to a feedforward neural network. By treating the channel coefficients\nas the random nodes of a hidden layer and the receiver's analog combiner as a\ntrainable output layer, we cast the XL MIMO system to the Extreme Learning\nMachine (ELM) framework, leading to a novel formulation for Over-The-Air (OTA)\nedge inference without requiring traditional digital processing nor\npre-processing at the transmitter. Through theoretical analysis and numerical\nevaluation, we showcase that XL-MIMO-ELM enables near-instantaneous training\nand efficient classification, even in varying fading conditions, suggesting the\nparadigm shift of beyond massive MIMO systems as OTA artificial neural networks\nalongside their profound communications role. Compared to deep learning\napproaches and conventional ELMs, the proposed framework achieves on par\nperformance with orders of magnitude lower complexity, making it highly\nattractive for inference tasks with ultra low power wireless devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we show that an eXtremely Large (XL) Multiple-Input\nMultiple-Output (MIMO) wireless system with appropriate analog combining\ncomponents exhibits the properties of a universal function approximator,\nsimilar to a feedforward neural network. By treating the channel coefficients\nas the random nodes of a hidden layer and the receiver's analog combiner as a\ntrainable output layer, we cast the XL MIMO system to the Extreme Learning\nMachine (ELM) framework, leading to a novel formulation for Over-The-Air (OTA)\nedge inference without requiring traditional digital processing nor\npre-processing at the transmitter. Through theoretical analysis and numerical\nevaluation, we showcase that XL-MIMO-ELM enables near-instantaneous training\nand efficient classification, even in varying fading conditions, suggesting the\nparadigm shift of beyond massive MIMO systems as OTA artificial neural networks\nalongside their profound communications role. Compared to deep learning\napproaches and conventional ELMs, the proposed framework achieves on par\nperformance with orders of magnitude lower complexity, making it highly\nattractive for inference tasks with ultra low power wireless devices."
                },
                "authors": [
                    {
                        "name": "Kyriakos Stylianopoulos"
                    },
                    {
                        "name": "George C. Alexandropoulos"
                    }
                ],
                "author_detail": {
                    "name": "George C. Alexandropoulos"
                },
                "author": "George C. Alexandropoulos",
                "arxiv_comment": "Submitted to IEEE Signal Processing Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12758v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12758v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06838v1",
                "updated": "2025-09-08T16:08:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    8,
                    31,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T16:08:31Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    8,
                    31,
                    0,
                    251,
                    0
                ],
                "title": "EPT Benchmark: Evaluation of Persian Trustworthiness in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPT Benchmark: Evaluation of Persian Trustworthiness in Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs), trained on extensive datasets using advanced\ndeep learning architectures, have demonstrated remarkable performance across a\nwide range of language tasks, becoming a cornerstone of modern AI technologies.\nHowever, ensuring their trustworthiness remains a critical challenge, as\nreliability is essential not only for accurate performance but also for\nupholding ethical, cultural, and social values. Careful alignment of training\ndata and culturally grounded evaluation criteria are vital for developing\nresponsible AI systems. In this study, we introduce the EPT (Evaluation of\nPersian Trustworthiness) metric, a culturally informed benchmark specifically\ndesigned to assess the trustworthiness of LLMs across six key aspects:\ntruthfulness, safety, fairness, robustness, privacy, and ethical alignment. We\ncurated a labeled dataset and evaluated the performance of several leading\nmodels - including ChatGPT, Claude, DeepSeek, Gemini, Grok, LLaMA, Mistral, and\nQwen - using both automated LLM-based and human assessments. Our results reveal\nsignificant deficiencies in the safety dimension, underscoring the urgent need\nfor focused attention on this critical aspect of model behavior. Furthermore,\nour findings offer valuable insights into the alignment of these models with\nPersian ethical-cultural values and highlight critical gaps and opportunities\nfor advancing trustworthy and culturally responsible AI. The dataset is\npublicly available at: https://github.com/Rezamirbagheri110/EPT-Benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), trained on extensive datasets using advanced\ndeep learning architectures, have demonstrated remarkable performance across a\nwide range of language tasks, becoming a cornerstone of modern AI technologies.\nHowever, ensuring their trustworthiness remains a critical challenge, as\nreliability is essential not only for accurate performance but also for\nupholding ethical, cultural, and social values. Careful alignment of training\ndata and culturally grounded evaluation criteria are vital for developing\nresponsible AI systems. In this study, we introduce the EPT (Evaluation of\nPersian Trustworthiness) metric, a culturally informed benchmark specifically\ndesigned to assess the trustworthiness of LLMs across six key aspects:\ntruthfulness, safety, fairness, robustness, privacy, and ethical alignment. We\ncurated a labeled dataset and evaluated the performance of several leading\nmodels - including ChatGPT, Claude, DeepSeek, Gemini, Grok, LLaMA, Mistral, and\nQwen - using both automated LLM-based and human assessments. Our results reveal\nsignificant deficiencies in the safety dimension, underscoring the urgent need\nfor focused attention on this critical aspect of model behavior. Furthermore,\nour findings offer valuable insights into the alignment of these models with\nPersian ethical-cultural values and highlight critical gaps and opportunities\nfor advancing trustworthy and culturally responsible AI. The dataset is\npublicly available at: https://github.com/Rezamirbagheri110/EPT-Benchmark."
                },
                "authors": [
                    {
                        "name": "Mohammad Reza Mirbagheri"
                    },
                    {
                        "name": "Mohammad Mahdi Mirkamali"
                    },
                    {
                        "name": "Zahra Motoshaker Arani"
                    },
                    {
                        "name": "Ali Javeri"
                    },
                    {
                        "name": "Amir Mahdi Sadeghzadeh"
                    },
                    {
                        "name": "Rasool Jalili"
                    }
                ],
                "author_detail": {
                    "name": "Rasool Jalili"
                },
                "author": "Rasool Jalili",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06836v1",
                "updated": "2025-09-08T16:07:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    7,
                    6,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T16:07:06Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    7,
                    6,
                    0,
                    251,
                    0
                ],
                "title": "COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens"
                },
                "summary": "Making LLMs more efficient in memory, latency, and serving cost is crucial\nfor edge deployment, interactive applications, and sustainable inference at\nscale. Pruning is a key technique toward this goal. However, prior pruning\nmethods are limited: width pruning often breaks the standard transformer layout\nor requires custom inference code, while depth pruning removes entire layers\nand can cause abrupt accuracy drops. In this work, we propose COMPACT, which\njointly (i) prunes rare vocabulary to shrink embedding/unembedding and (ii)\nprunes FFN intermediate channels using common-token-weighted activations,\naligning importance with the post-pruning token distribution. COMPACT enjoys\nmerits of both depth and width pruning, such as: deployment-friendliness (keeps\na standard transformer architecture), scale-adaptivity (trade off vocab vs. FFN\npruning), training-free operation with competitive pruning time, and strong\nmemory savings alongside throughput gains. Experiments across Qwen, LLaMA, and\nGemma families (0.5B-70B) show state-of-the-art downstream task performance at\nsimilar or higher pruning ratios, with substantial reductions in parameters,\nGPU memory, and end-to-end latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making LLMs more efficient in memory, latency, and serving cost is crucial\nfor edge deployment, interactive applications, and sustainable inference at\nscale. Pruning is a key technique toward this goal. However, prior pruning\nmethods are limited: width pruning often breaks the standard transformer layout\nor requires custom inference code, while depth pruning removes entire layers\nand can cause abrupt accuracy drops. In this work, we propose COMPACT, which\njointly (i) prunes rare vocabulary to shrink embedding/unembedding and (ii)\nprunes FFN intermediate channels using common-token-weighted activations,\naligning importance with the post-pruning token distribution. COMPACT enjoys\nmerits of both depth and width pruning, such as: deployment-friendliness (keeps\na standard transformer architecture), scale-adaptivity (trade off vocab vs. FFN\npruning), training-free operation with competitive pruning time, and strong\nmemory savings alongside throughput gains. Experiments across Qwen, LLaMA, and\nGemma families (0.5B-70B) show state-of-the-art downstream task performance at\nsimilar or higher pruning ratios, with substantial reductions in parameters,\nGPU memory, and end-to-end latency."
                },
                "authors": [
                    {
                        "name": "Eugene Kwek"
                    },
                    {
                        "name": "Wenpeng Yin"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Yin"
                },
                "author": "Wenpeng Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16482v2",
                "updated": "2025-09-08T16:06:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    6,
                    59,
                    0,
                    251,
                    0
                ],
                "published": "2024-08-29T12:18:04Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    12,
                    18,
                    4,
                    3,
                    242,
                    0
                ],
                "title": "Self-Alignment: Improving Alignment of Cultural Values in LLMs via\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Alignment: Improving Alignment of Cultural Values in LLMs via\n  In-Context Learning"
                },
                "summary": "Improving the alignment of Large Language Models (LLMs) with respect to the\ncultural values that they encode has become an increasingly important topic. In\nthis work, we study whether we can exploit existing knowledge about cultural\nvalues at inference time to adjust model responses to cultural value probes. We\npresent a simple and inexpensive method that uses a combination of in-context\nlearning (ICL) and human survey data, and show that we can improve the\nalignment to cultural values across 5 models that include both English-centric\nand multilingual LLMs. Importantly, we show that our method could prove useful\nin test languages other than English and can improve alignment to the cultural\nvalues that correspond to a range of culturally diverse countries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the alignment of Large Language Models (LLMs) with respect to the\ncultural values that they encode has become an increasingly important topic. In\nthis work, we study whether we can exploit existing knowledge about cultural\nvalues at inference time to adjust model responses to cultural value probes. We\npresent a simple and inexpensive method that uses a combination of in-context\nlearning (ICL) and human survey data, and show that we can improve the\nalignment to cultural values across 5 models that include both English-centric\nand multilingual LLMs. Importantly, we show that our method could prove useful\nin test languages other than English and can improve alignment to the cultural\nvalues that correspond to a range of culturally diverse countries."
                },
                "authors": [
                    {
                        "name": "Rochelle Choenni"
                    },
                    {
                        "name": "Ekaterina Shutova"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Shutova"
                },
                "author": "Ekaterina Shutova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06834v1",
                "updated": "2025-09-08T16:06:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    6,
                    30,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T16:06:30Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    6,
                    30,
                    0,
                    251,
                    0
                ],
                "title": "Unlocking 21cm Cosmology with SBI: A Beginner friendly NRE for Inference\n  of Astrophysical Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking 21cm Cosmology with SBI: A Beginner friendly NRE for Inference\n  of Astrophysical Parameters"
                },
                "summary": "The 21-cm line of neutral hydrogen is a promising probe of the early\nUniverse, yet extracting astrophysical parameters from its power spectrum\nremains a major challenge. We present a beginner-friendly PyTorch pipeline for\nMarginal Neural Ratio Estimation (MNRE), a Simulation-Based Inference (SBI)\nmethod that bypasses explicit likelihoods. Using 21cmFAST simulations, we show\nthat MNRE can recover key astrophysical parameters such as the ionizing\nefficiency $\\zeta$ and X-ray luminosity $L_X$ directly from power spectra. Our\nimplementation prioritizes transparency and accessibility, offering a practical\nentry point for new researchers in 21-cm cosmology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The 21-cm line of neutral hydrogen is a promising probe of the early\nUniverse, yet extracting astrophysical parameters from its power spectrum\nremains a major challenge. We present a beginner-friendly PyTorch pipeline for\nMarginal Neural Ratio Estimation (MNRE), a Simulation-Based Inference (SBI)\nmethod that bypasses explicit likelihoods. Using 21cmFAST simulations, we show\nthat MNRE can recover key astrophysical parameters such as the ionizing\nefficiency $\\zeta$ and X-ray luminosity $L_X$ directly from power spectra. Our\nimplementation prioritizes transparency and accessibility, offering a practical\nentry point for new researchers in 21-cm cosmology."
                },
                "authors": [
                    {
                        "name": "Bisweswar Sen"
                    },
                    {
                        "name": "Abhirup Datta"
                    }
                ],
                "author_detail": {
                    "name": "Abhirup Datta"
                },
                "author": "Abhirup Datta",
                "arxiv_comment": "4 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06822v1",
                "updated": "2025-09-08T15:57:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    57,
                    14,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T15:57:14Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    57,
                    14,
                    0,
                    251,
                    0
                ],
                "title": "RAFFLES: Reasoning-based Attribution of Faults for LLM Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAFFLES: Reasoning-based Attribution of Faults for LLM Systems"
                },
                "summary": "We have reached a critical roadblock in the development and enhancement of\nlong-horizon, multi-component LLM agentic systems: it is incredibly tricky to\nidentify where these systems break down and why. Evaluation capabilities that\ncurrently exist today (e.g., single pass LLM-as-a-judge) are limited in that\nthey often focus on individual metrics or capabilities, end-to-end outcomes,\nand are narrowly grounded on the preferences of humans. We argue that to match\nthe agentic capabilities, evaluation frameworks must also be able to reason,\nprobe, iterate, and understand the complex logic passing through these systems\nover long horizons. In this paper, we present RAFFLES - an evaluation\narchitecture that incorporates reasoning and iterative refinement.\nSpecifically, RAFFLES operates as an iterative, multi-component pipeline, using\na central Judge to systematically investigate faults and a set of specialized\nEvaluators to assess not only the system's components but also the quality of\nthe reasoning by the Judge itself, thereby building a history of hypotheses. We\ntested RAFFLES against several baselines on the Who&When dataset, a benchmark\ndesigned to diagnose the \"who\" (agent) and \"when\" (step) of a system's failure.\nRAFFLES outperforms these baselines, achieving an agent-step fault pair\naccuracy of over 43% on the Algorithmically-Generated dataset (a substantial\nincrease from the previously published best of 16.6%) and over 20% on the\nHand-Crafted dataset (surpassing the previously published best of 8.8%). These\nresults demonstrate a key step towards introducing automated fault detection\nfor autonomous systems over labor-intensive manual human review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have reached a critical roadblock in the development and enhancement of\nlong-horizon, multi-component LLM agentic systems: it is incredibly tricky to\nidentify where these systems break down and why. Evaluation capabilities that\ncurrently exist today (e.g., single pass LLM-as-a-judge) are limited in that\nthey often focus on individual metrics or capabilities, end-to-end outcomes,\nand are narrowly grounded on the preferences of humans. We argue that to match\nthe agentic capabilities, evaluation frameworks must also be able to reason,\nprobe, iterate, and understand the complex logic passing through these systems\nover long horizons. In this paper, we present RAFFLES - an evaluation\narchitecture that incorporates reasoning and iterative refinement.\nSpecifically, RAFFLES operates as an iterative, multi-component pipeline, using\na central Judge to systematically investigate faults and a set of specialized\nEvaluators to assess not only the system's components but also the quality of\nthe reasoning by the Judge itself, thereby building a history of hypotheses. We\ntested RAFFLES against several baselines on the Who&When dataset, a benchmark\ndesigned to diagnose the \"who\" (agent) and \"when\" (step) of a system's failure.\nRAFFLES outperforms these baselines, achieving an agent-step fault pair\naccuracy of over 43% on the Algorithmically-Generated dataset (a substantial\nincrease from the previously published best of 16.6%) and over 20% on the\nHand-Crafted dataset (surpassing the previously published best of 8.8%). These\nresults demonstrate a key step towards introducing automated fault detection\nfor autonomous systems over labor-intensive manual human review."
                },
                "authors": [
                    {
                        "name": "Chenyang Zhu"
                    },
                    {
                        "name": "Spencer Hong"
                    },
                    {
                        "name": "Jingyu Wu"
                    },
                    {
                        "name": "Kushal Chawla"
                    },
                    {
                        "name": "Charlotte Tang"
                    },
                    {
                        "name": "Youbing Yin"
                    },
                    {
                        "name": "Nathan Wolfe"
                    },
                    {
                        "name": "Erin Babinsky"
                    },
                    {
                        "name": "Daben Liu"
                    }
                ],
                "author_detail": {
                    "name": "Daben Liu"
                },
                "author": "Daben Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06820v1",
                "updated": "2025-09-08T15:56:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    56,
                    6,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T15:56:06Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    56,
                    6,
                    0,
                    251,
                    0
                ],
                "title": "Green Learning for STAR-RIS mmWave Systems with Implicit CSI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Green Learning for STAR-RIS mmWave Systems with Implicit CSI"
                },
                "summary": "In this paper, a green learning (GL)-based precoding framework is proposed\nfor simultaneously transmitting and reflecting reconfigurable intelligent\nsurface (STAR-RIS)-aided millimeter-wave (mmWave) MIMO broadcasting systems.\nMotivated by the growing emphasis on environmental sustainability in future 6G\nnetworks, this work adopts a broadcasting transmission architecture for\nscenarios where multiple users share identical information, improving spectral\nefficiency and reducing redundant transmissions and power consumption.\nDifferent from conventional optimization methods, such as block coordinate\ndescent (BCD) that require perfect channel state information (CSI) and\niterative computation, the proposed GL framework operates directly on received\nuplink pilot signals without explicit CSI estimation. Unlike deep learning (DL)\napproaches that require CSI-based labels for training, the proposed GL approach\nalso avoids deep neural networks and backpropagation, leading to a more\nlightweight design. Although the proposed GL framework is trained with\nsupervision generated by BCD under full CSI, inference is performed in a fully\nCSI-free manner. The proposed GL integrates subspace approximation with\nadjusted bias (Saab), relevant feature test (RFT)-based supervised feature\nselection, and eXtreme gradient boosting (XGBoost)-based decision learning to\njointly predict the STAR-RIS coefficients and transmit precoder. Simulation\nresults show that the proposed GL approach achieves competitive spectral\nefficiency compared to BCD and DL-based models, while reducing floating-point\noperations (FLOPs) by over four orders of magnitude. These advantages make the\nproposed GL approach highly suitable for real-time deployment in energy- and\nhardware-constrained broadcasting scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, a green learning (GL)-based precoding framework is proposed\nfor simultaneously transmitting and reflecting reconfigurable intelligent\nsurface (STAR-RIS)-aided millimeter-wave (mmWave) MIMO broadcasting systems.\nMotivated by the growing emphasis on environmental sustainability in future 6G\nnetworks, this work adopts a broadcasting transmission architecture for\nscenarios where multiple users share identical information, improving spectral\nefficiency and reducing redundant transmissions and power consumption.\nDifferent from conventional optimization methods, such as block coordinate\ndescent (BCD) that require perfect channel state information (CSI) and\niterative computation, the proposed GL framework operates directly on received\nuplink pilot signals without explicit CSI estimation. Unlike deep learning (DL)\napproaches that require CSI-based labels for training, the proposed GL approach\nalso avoids deep neural networks and backpropagation, leading to a more\nlightweight design. Although the proposed GL framework is trained with\nsupervision generated by BCD under full CSI, inference is performed in a fully\nCSI-free manner. The proposed GL integrates subspace approximation with\nadjusted bias (Saab), relevant feature test (RFT)-based supervised feature\nselection, and eXtreme gradient boosting (XGBoost)-based decision learning to\njointly predict the STAR-RIS coefficients and transmit precoder. Simulation\nresults show that the proposed GL approach achieves competitive spectral\nefficiency compared to BCD and DL-based models, while reducing floating-point\noperations (FLOPs) by over four orders of magnitude. These advantages make the\nproposed GL approach highly suitable for real-time deployment in energy- and\nhardware-constrained broadcasting scenarios."
                },
                "authors": [
                    {
                        "name": "Yu-Hsiang Huang"
                    },
                    {
                        "name": "Po-Heng Chou"
                    },
                    {
                        "name": "Wan-Jen Huang"
                    },
                    {
                        "name": "Walid Saad"
                    },
                    {
                        "name": "C. -C. Jay Kuo"
                    }
                ],
                "author_detail": {
                    "name": "C. -C. Jay Kuo"
                },
                "author": "C. -C. Jay Kuo",
                "arxiv_comment": "6 pages, 4 figures, 2 tables, accepted by 2025 IEEE Globecom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03165v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03165v3",
                "updated": "2025-09-08T15:51:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    51,
                    58,
                    0,
                    251,
                    0
                ],
                "published": "2025-04-04T04:43:13Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    43,
                    13,
                    4,
                    94,
                    0
                ],
                "title": "Efficient Dynamic Clustering-Based Document Compression for\n  Retrieval-Augmented-Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Dynamic Clustering-Based Document Compression for\n  Retrieval-Augmented-Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach\nfor knowledge injection during large language model (LLM) inference in recent\nyears. However, due to their limited ability to exploit fine-grained\ninter-document relationships, current RAG implementations face challenges in\neffectively addressing the retrieved noise and redundancy content, which may\ncause error in the generation results. To address these limitations, we propose\nan Efficient Dynamic Clustering-based document Compression framework (EDC2-RAG)\nthat utilizes latent inter-document relationships while simultaneously removing\nirrelevant information and redundant content. We validate our approach, built\nupon GPT-3.5-Turbo and GPT-4o-mini, on widely used knowledge-QA and\nHallucination-Detection datasets. Experimental results show that our method\nachieves consistent performance improvements across various scenarios and\nexperimental settings, demonstrating strong robustness and applicability. Our\ncode and datasets are available at https://github.com/Tsinghua-dhy/EDC-2-RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach\nfor knowledge injection during large language model (LLM) inference in recent\nyears. However, due to their limited ability to exploit fine-grained\ninter-document relationships, current RAG implementations face challenges in\neffectively addressing the retrieved noise and redundancy content, which may\ncause error in the generation results. To address these limitations, we propose\nan Efficient Dynamic Clustering-based document Compression framework (EDC2-RAG)\nthat utilizes latent inter-document relationships while simultaneously removing\nirrelevant information and redundant content. We validate our approach, built\nupon GPT-3.5-Turbo and GPT-4o-mini, on widely used knowledge-QA and\nHallucination-Detection datasets. Experimental results show that our method\nachieves consistent performance improvements across various scenarios and\nexperimental settings, demonstrating strong robustness and applicability. Our\ncode and datasets are available at https://github.com/Tsinghua-dhy/EDC-2-RAG."
                },
                "authors": [
                    {
                        "name": "Weitao Li"
                    },
                    {
                        "name": "Kaiming Liu"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Xuanyu Lei"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03165v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03165v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18992v2",
                "updated": "2025-09-08T15:50:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    50,
                    16,
                    0,
                    251,
                    0
                ],
                "published": "2025-08-26T12:46:58Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    46,
                    58,
                    1,
                    238,
                    0
                ],
                "title": "Automatic Prompt Optimization with Prompt Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Prompt Optimization with Prompt Distillation"
                },
                "summary": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which is gaining popularity due to the rapid development of\nprompt engineering driven by extensive research in the field of large language\nmodels (LLMs). This paper presents DistillPrompt -- a novel autoprompting\nmethod based on large language models that employs a multi-stage integration of\ntask-specific information into prompts using training data. DistillPrompt\nutilizes distillation, compression, and aggregation operations to explore the\nprompt space more thoroughly. The method was tested on different datasets for\ntext classification and generation tasks using the t-lite-instruct-0.1 language\nmodel. The results demonstrate a significant average improvement (e.g., 20.12%\nacross the entire dataset compared to Grips) in key metrics over existing\nmethods in the field, establishing DistillPrompt as one of the most effective\nnon-gradient approaches in autoprompting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which is gaining popularity due to the rapid development of\nprompt engineering driven by extensive research in the field of large language\nmodels (LLMs). This paper presents DistillPrompt -- a novel autoprompting\nmethod based on large language models that employs a multi-stage integration of\ntask-specific information into prompts using training data. DistillPrompt\nutilizes distillation, compression, and aggregation operations to explore the\nprompt space more thoroughly. The method was tested on different datasets for\ntext classification and generation tasks using the t-lite-instruct-0.1 language\nmodel. The results demonstrate a significant average improvement (e.g., 20.12%\nacross the entire dataset compared to Grips) in key metrics over existing\nmethods in the field, establishing DistillPrompt as one of the most effective\nnon-gradient approaches in autoprompting."
                },
                "authors": [
                    {
                        "name": "Ernest A. Dyagin"
                    },
                    {
                        "name": "Nikita I. Kulin"
                    },
                    {
                        "name": "Artur R. Khairullin"
                    },
                    {
                        "name": "Viktor N. Zhuravlev"
                    },
                    {
                        "name": "Alena N. Sitkina"
                    }
                ],
                "author_detail": {
                    "name": "Alena N. Sitkina"
                },
                "author": "Alena N. Sitkina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06813v1",
                "updated": "2025-09-08T15:48:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    48,
                    17,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T15:48:17Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    48,
                    17,
                    0,
                    251,
                    0
                ],
                "title": "A Comparative Benchmark of Large Language Models for Labelling Wind\n  Turbine Maintenance Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Benchmark of Large Language Models for Labelling Wind\n  Turbine Maintenance Logs"
                },
                "summary": "Effective Operation and Maintenance (O&M) is critical to reducing the\nLevelised Cost of Energy (LCOE) from wind power, yet the unstructured,\nfree-text nature of turbine maintenance logs presents a significant barrier to\nautomated analysis. Our paper addresses this by presenting a novel and\nreproducible framework for benchmarking Large Language Models (LLMs) on the\ntask of classifying these complex industrial records. To promote transparency\nand encourage further research, this framework has been made publicly available\nas an open-source tool. We systematically evaluate a diverse suite of\nstate-of-the-art proprietary and open-source LLMs, providing a foundational\nassessment of their trade-offs in reliability, operational efficiency, and\nmodel calibration. Our results quantify a clear performance hierarchy,\nidentifying top models that exhibit high alignment with a benchmark standard\nand trustworthy, well-calibrated confidence scores. We also demonstrate that\nclassification performance is highly dependent on the task's semantic\nambiguity, with all models showing higher consensus on objective component\nidentification than on interpretive maintenance actions. Given that no model\nachieves perfect accuracy and that calibration varies dramatically, we conclude\nthat the most effective and responsible near-term application is a\nHuman-in-the-Loop system, where LLMs act as a powerful assistant to accelerate\nand standardise data labelling for human experts, thereby enhancing O&M data\nquality and downstream reliability analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective Operation and Maintenance (O&M) is critical to reducing the\nLevelised Cost of Energy (LCOE) from wind power, yet the unstructured,\nfree-text nature of turbine maintenance logs presents a significant barrier to\nautomated analysis. Our paper addresses this by presenting a novel and\nreproducible framework for benchmarking Large Language Models (LLMs) on the\ntask of classifying these complex industrial records. To promote transparency\nand encourage further research, this framework has been made publicly available\nas an open-source tool. We systematically evaluate a diverse suite of\nstate-of-the-art proprietary and open-source LLMs, providing a foundational\nassessment of their trade-offs in reliability, operational efficiency, and\nmodel calibration. Our results quantify a clear performance hierarchy,\nidentifying top models that exhibit high alignment with a benchmark standard\nand trustworthy, well-calibrated confidence scores. We also demonstrate that\nclassification performance is highly dependent on the task's semantic\nambiguity, with all models showing higher consensus on objective component\nidentification than on interpretive maintenance actions. Given that no model\nachieves perfect accuracy and that calibration varies dramatically, we conclude\nthat the most effective and responsible near-term application is a\nHuman-in-the-Loop system, where LLMs act as a powerful assistant to accelerate\nand standardise data labelling for human experts, thereby enhancing O&M data\nquality and downstream reliability analysis."
                },
                "authors": [
                    {
                        "name": "Max Malyi"
                    },
                    {
                        "name": "Jonathan Shek"
                    },
                    {
                        "name": "Alasdair McDonald"
                    },
                    {
                        "name": "Andre Biscaya"
                    }
                ],
                "author_detail": {
                    "name": "Andre Biscaya"
                },
                "author": "Andre Biscaya",
                "arxiv_comment": "Associated GitHub repository:\n  https://github.com/mvmalyi/wind-farm-maintenance-logs-labelling-with-llms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06809v1",
                "updated": "2025-09-08T15:43:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    43,
                    29,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T15:43:29Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    43,
                    29,
                    0,
                    251,
                    0
                ],
                "title": "Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in\n  the TPTP Ecosystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in\n  the TPTP Ecosystem"
                },
                "summary": "The scarcity of high-quality, logically sound data is a critical bottleneck\nfor advancing the mathematical reasoning of Large Language Models (LLMs). Our\nwork confronts this challenge by turning decades of automated theorem proving\nresearch into a scalable data engine. Rather than relying on error-prone LLMs\nor complex proof-assistant syntax like Lean and Isabelle, our framework\nleverages E-prover's saturation capabilities on the vast TPTP axiom library to\nderive a massive, guaranteed-valid corpus of theorems. Our pipeline is\nprincipled and simple: saturate axioms, filter for \"interesting\" theorems, and\ngenerate tasks. With no LLMs in the loop, we eliminate factual errors by\nconstruction. This purely symbolic data is then transformed into three\ndifficulty-controlled challenges: entailment verification, premise selection,\nand proof reconstruction. Our zero-shot experiments on frontier models reveal a\nclear weakness: performance collapses on tasks requiring deep, structural\nreasoning. Our framework provides both the diagnostic tool to measure this gap\nand a scalable source of symbolic training data to address it. We make the code\nand data publicly available.\n  https://github.com/sileod/reasoning_core\nhttps://hf.co/datasets/reasoning-core/rc1",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of high-quality, logically sound data is a critical bottleneck\nfor advancing the mathematical reasoning of Large Language Models (LLMs). Our\nwork confronts this challenge by turning decades of automated theorem proving\nresearch into a scalable data engine. Rather than relying on error-prone LLMs\nor complex proof-assistant syntax like Lean and Isabelle, our framework\nleverages E-prover's saturation capabilities on the vast TPTP axiom library to\nderive a massive, guaranteed-valid corpus of theorems. Our pipeline is\nprincipled and simple: saturate axioms, filter for \"interesting\" theorems, and\ngenerate tasks. With no LLMs in the loop, we eliminate factual errors by\nconstruction. This purely symbolic data is then transformed into three\ndifficulty-controlled challenges: entailment verification, premise selection,\nand proof reconstruction. Our zero-shot experiments on frontier models reveal a\nclear weakness: performance collapses on tasks requiring deep, structural\nreasoning. Our framework provides both the diagnostic tool to measure this gap\nand a scalable source of symbolic training data to address it. We make the code\nand data publicly available.\n  https://github.com/sileod/reasoning_core\nhttps://hf.co/datasets/reasoning-core/rc1"
                },
                "authors": [
                    {
                        "name": "Valentin Quesnel"
                    },
                    {
                        "name": "Damien Sileo"
                    }
                ],
                "author_detail": {
                    "name": "Damien Sileo"
                },
                "author": "Damien Sileo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06807v1",
                "updated": "2025-09-08T15:39:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    39,
                    17,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T15:39:17Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    39,
                    17,
                    0,
                    251,
                    0
                ],
                "title": "MoGU V2: Toward a Higher Pareto Frontier Between Model Usability and\n  Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoGU V2: Toward a Higher Pareto Frontier Between Model Usability and\n  Security"
                },
                "summary": "As Large Language Models (LLMs) increasingly permeate human life, their\nsecurity has emerged as a critical concern, particularly their ability to\nmaintain harmless responses to malicious instructions. Although extensive\nmethods have improved LLMs' security, they often lead to conservative,\nrejection-oriented responses that compromise practical usability. This presents\na key challenge: how to advance the Pareto frontier between LLMs' usability and\nsecurity, rather than necessitate a trade-off between them. To address this, we\npropose the MoGU framework, in which the intra-layer router dynamically\nallocates weights by sensing hidden states, thereby balancing the contributions\nof security-optimized and usability-optimized variants. Despite its initial\npotential, the MoGU framework faces limitations such as parameter redundancy\nand performance bottlenecks. To overcome these, we further propose an improved\nMoGU_v2 framework that establishes a tighter coupling between the routers and\nhidden states. In MoGU_v2, routers are embedded only in layers encoding highly\nclassifiable security features, and backbone modules are activated during\nrouter optimization to enable bidirectional adaptation. MoGU_V2 exhibits strong\nadaptability and stable improvements across various series of LLMs, including\nmainstream LLMs serving as brains in various applications, on-device LLMs\noptimized for resource-constrained scenarios, and reasoning LLMs tailored for\nuser interpretability. Meanwhile, even facing risks introduced by Instruction\nFine-tuning, MoGU_v2 can easily restore security without compromising the task\nperformance gains via a simple data-mix strategy. These comprehensive\nimprovements highlight MoGU_V2 as a robust and versatile solution for\nmitigating security risks in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) increasingly permeate human life, their\nsecurity has emerged as a critical concern, particularly their ability to\nmaintain harmless responses to malicious instructions. Although extensive\nmethods have improved LLMs' security, they often lead to conservative,\nrejection-oriented responses that compromise practical usability. This presents\na key challenge: how to advance the Pareto frontier between LLMs' usability and\nsecurity, rather than necessitate a trade-off between them. To address this, we\npropose the MoGU framework, in which the intra-layer router dynamically\nallocates weights by sensing hidden states, thereby balancing the contributions\nof security-optimized and usability-optimized variants. Despite its initial\npotential, the MoGU framework faces limitations such as parameter redundancy\nand performance bottlenecks. To overcome these, we further propose an improved\nMoGU_v2 framework that establishes a tighter coupling between the routers and\nhidden states. In MoGU_v2, routers are embedded only in layers encoding highly\nclassifiable security features, and backbone modules are activated during\nrouter optimization to enable bidirectional adaptation. MoGU_V2 exhibits strong\nadaptability and stable improvements across various series of LLMs, including\nmainstream LLMs serving as brains in various applications, on-device LLMs\noptimized for resource-constrained scenarios, and reasoning LLMs tailored for\nuser interpretability. Meanwhile, even facing risks introduced by Instruction\nFine-tuning, MoGU_v2 can easily restore security without compromising the task\nperformance gains via a simple data-mix strategy. These comprehensive\nimprovements highlight MoGU_V2 as a robust and versatile solution for\nmitigating security risks in real-world applications."
                },
                "authors": [
                    {
                        "name": "Yanrui Du"
                    },
                    {
                        "name": "Fenglei Fan"
                    },
                    {
                        "name": "Sendong Zhao"
                    },
                    {
                        "name": "Jiawei Cao"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06806v1",
                "updated": "2025-09-08T15:38:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    38,
                    31,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T15:38:31Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    38,
                    31,
                    0,
                    251,
                    0
                ],
                "title": "MachineLearningLM: Continued Pretraining Language Models on Millions of\n  Synthetic Tabular Prediction Tasks Scales In-Context ML",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MachineLearningLM: Continued Pretraining Language Models on Millions of\n  Synthetic Tabular Prediction Tasks Scales In-Context ML"
                },
                "summary": "Large language models (LLMs) possess broad world knowledge and strong\ngeneral-purpose reasoning ability, yet they struggle to learn from many\nin-context examples on standard machine learning (ML) tasks, that is, to\nleverage many-shot demonstrations purely via in-context learning (ICL) without\ngradient descent. We introduce MachineLearningLM, a portable\ncontinued-pretraining framework that equips a general-purpose LLM with robust\nin-context ML capability while preserving its general knowledge and reasoning\nfor broader chat workflows.\n  Our pretraining procedure synthesizes ML tasks from millions of structural\ncausal models (SCMs), spanning shot counts up to 1,024. We begin with a\nrandom-forest teacher, distilling tree-based decision strategies into the LLM\nto strengthen robustness in numerical modeling. All tasks are serialized with a\ntoken-efficient prompt, enabling 3x to 6x more examples per context window and\ndelivering up to 50x amortized throughput via batch inference.\n  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),\nMachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an\naverage of about 15% on out-of-distribution tabular classification across\nfinance, physics, biology, and healthcare domains. It exhibits a striking\nmany-shot scaling law: accuracy increases monotonically as in-context\ndemonstrations grow from 8 to 1,024. Without any task-specific training, it\nattains random-forest-level accuracy across hundreds of shots. General chat\ncapabilities, including knowledge and reasoning, are preserved: it achieves\n75.4% on MMLU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) possess broad world knowledge and strong\ngeneral-purpose reasoning ability, yet they struggle to learn from many\nin-context examples on standard machine learning (ML) tasks, that is, to\nleverage many-shot demonstrations purely via in-context learning (ICL) without\ngradient descent. We introduce MachineLearningLM, a portable\ncontinued-pretraining framework that equips a general-purpose LLM with robust\nin-context ML capability while preserving its general knowledge and reasoning\nfor broader chat workflows.\n  Our pretraining procedure synthesizes ML tasks from millions of structural\ncausal models (SCMs), spanning shot counts up to 1,024. We begin with a\nrandom-forest teacher, distilling tree-based decision strategies into the LLM\nto strengthen robustness in numerical modeling. All tasks are serialized with a\ntoken-efficient prompt, enabling 3x to 6x more examples per context window and\ndelivering up to 50x amortized throughput via batch inference.\n  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),\nMachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an\naverage of about 15% on out-of-distribution tabular classification across\nfinance, physics, biology, and healthcare domains. It exhibits a striking\nmany-shot scaling law: accuracy increases monotonically as in-context\ndemonstrations grow from 8 to 1,024. Without any task-specific training, it\nattains random-forest-level accuracy across hundreds of shots. General chat\ncapabilities, including knowledge and reasoning, are preserved: it achieves\n75.4% on MMLU."
                },
                "authors": [
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Pengkun Zhang"
                    },
                    {
                        "name": "Mingzhe Lu"
                    },
                    {
                        "name": "Yanzhen Shen"
                    },
                    {
                        "name": "Guolin Ke"
                    }
                ],
                "author_detail": {
                    "name": "Guolin Ke"
                },
                "author": "Guolin Ke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15581v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15581v2",
                "updated": "2025-09-08T15:34:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    34,
                    55,
                    0,
                    251,
                    0
                ],
                "published": "2025-01-26T16:17:57Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    16,
                    17,
                    57,
                    6,
                    26,
                    0
                ],
                "title": "Error Classification of Large Language Models on Math Word Problems: A\n  Dynamically Adaptive Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Error Classification of Large Language Models on Math Word Problems: A\n  Dynamically Adaptive Framework"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains. Math Word Problems (MWPs) serve as a crucial benchmark for\nevaluating LLMs' reasoning abilities. While most research primarily focuses on\nimproving accuracy, it often neglects understanding and addressing the\nunderlying patterns of errors. Current error classification methods rely on\nstatic and predefined categories, which limit their ability to capture the full\nspectrum of error patterns in mathematical reasoning. To enable systematic\nerror analysis, we collect error samples from 15 different LLMs of varying\nsizes across four distinct MWP datasets using multiple sampling strategies.\nBased on this extensive collection, we introduce MWPES-300K, a comprehensive\ndataset containing 304,865 error samples that cover diverse error patterns and\nreasoning paths. To reduce human bias and enable fine-grained analysis of error\npatterns, we propose a novel framework for automated dynamic error\nclassification in mathematical reasoning. Experimental results demonstrate that\ndataset characteristics significantly shape error patterns, which evolve from\nbasic to complex manifestations as model capabilities increase. With deeper\ninsights into error patterns, we propose Error-Aware Prompting (EAP) that\nincorporates common error patterns as explicit guidance, leading to significant\nimprovements in mathematical reasoning performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains. Math Word Problems (MWPs) serve as a crucial benchmark for\nevaluating LLMs' reasoning abilities. While most research primarily focuses on\nimproving accuracy, it often neglects understanding and addressing the\nunderlying patterns of errors. Current error classification methods rely on\nstatic and predefined categories, which limit their ability to capture the full\nspectrum of error patterns in mathematical reasoning. To enable systematic\nerror analysis, we collect error samples from 15 different LLMs of varying\nsizes across four distinct MWP datasets using multiple sampling strategies.\nBased on this extensive collection, we introduce MWPES-300K, a comprehensive\ndataset containing 304,865 error samples that cover diverse error patterns and\nreasoning paths. To reduce human bias and enable fine-grained analysis of error\npatterns, we propose a novel framework for automated dynamic error\nclassification in mathematical reasoning. Experimental results demonstrate that\ndataset characteristics significantly shape error patterns, which evolve from\nbasic to complex manifestations as model capabilities increase. With deeper\ninsights into error patterns, we propose Error-Aware Prompting (EAP) that\nincorporates common error patterns as explicit guidance, leading to significant\nimprovements in mathematical reasoning performance."
                },
                "authors": [
                    {
                        "name": "Yuhong Sun"
                    },
                    {
                        "name": "Zhangyue Yin"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Hui Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hui Zhao"
                },
                "author": "Hui Zhao",
                "arxiv_comment": "28 pages, 10 figures, accepted by Findings of EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15581v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15581v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06796v1",
                "updated": "2025-09-08T15:27:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    27,
                    35,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T15:27:35Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    27,
                    35,
                    0,
                    251,
                    0
                ],
                "title": "Imitative Membership Inference Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imitative Membership Inference Attack"
                },
                "summary": "A Membership Inference Attack (MIA) assesses how much a target machine\nlearning model reveals about its training data by determining whether specific\nquery instances were part of the training set. State-of-the-art MIAs rely on\ntraining hundreds of shadow models that are independent of the target model,\nleading to significant computational overhead. In this paper, we introduce\nImitative Membership Inference Attack (IMIA), which employs a novel imitative\ntraining technique to strategically construct a small number of target-informed\nimitative models that closely replicate the target model's behavior for\ninference. Extensive experimental results demonstrate that IMIA substantially\noutperforms existing MIAs in various attack settings while only requiring less\nthan 5% of the computational cost of state-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Membership Inference Attack (MIA) assesses how much a target machine\nlearning model reveals about its training data by determining whether specific\nquery instances were part of the training set. State-of-the-art MIAs rely on\ntraining hundreds of shadow models that are independent of the target model,\nleading to significant computational overhead. In this paper, we introduce\nImitative Membership Inference Attack (IMIA), which employs a novel imitative\ntraining technique to strategically construct a small number of target-informed\nimitative models that closely replicate the target model's behavior for\ninference. Extensive experimental results demonstrate that IMIA substantially\noutperforms existing MIAs in various attack settings while only requiring less\nthan 5% of the computational cost of state-of-the-art approaches."
                },
                "authors": [
                    {
                        "name": "Yuntao Du"
                    },
                    {
                        "name": "Yuetian Chen"
                    },
                    {
                        "name": "Hanshen Xiao"
                    },
                    {
                        "name": "Bruno Ribeiro"
                    },
                    {
                        "name": "Ninghui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ninghui Li"
                },
                "author": "Ninghui Li",
                "arxiv_comment": "Code is available at: https://github.com/zealscott/IMIA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06795v1",
                "updated": "2025-09-08T15:24:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    24,
                    33,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T15:24:33Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    24,
                    33,
                    0,
                    251,
                    0
                ],
                "title": "Anchoring Refusal Direction: Mitigating Safety Risks in Tuning via\n  Projection Constraint",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anchoring Refusal Direction: Mitigating Safety Risks in Tuning via\n  Projection Constraint"
                },
                "summary": "Instruction Fine-Tuning (IFT) has been widely adopted as an effective\npost-training strategy to enhance various abilities of Large Language Models\n(LLMs). However, prior studies have shown that IFT can significantly compromise\nLLMs' safety, particularly their ability to refuse malicious instructions,\nraising significant concerns. Recent research into the internal mechanisms of\nLLMs has identified the refusal direction (r-direction) in the hidden states,\nwhich plays a pivotal role in governing refusal behavior. Building on this\ninsight, our study reveals that the r-direction tends to drift during training,\nwhich we identify as one of the causes of the associated safety risks. To\nmitigate such drift, our proposed ProCon method introduces a\nprojection-constrained loss term that regularizes the projection magnitude of\neach training sample's hidden state onto the r-direction. Our initial analysis\nshows that applying an appropriate constraint can effectively mitigate the\nrefusal direction drift and associated safety risks, but remains limited by\noverall performance barriers. To overcome this barrier, informed by our\nobservation of early-stage sharp drift and a data-driven perspective, we\nintroduce a warm-up strategy that emphasizes early-stage strong constraints and\nbroaden the data distribution to strengthen constraint signals, leading to an\nenhanced ProCon method. Experimental results under various datasets, scenarios,\nand LLMs demonstrate that our method can significantly mitigate safety risks\nposed by IFT while preserving task performance gains. Even compared with strong\nbaselines, our method consistently delivers superior overall performance.\nCrucially, our analysis indicates that ProCon can contribute to stabilizing the\nr-direction during training, while such an interpretability-driven exploration\nof LLMs' internal mechanisms lays a solid foundation for future safety\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction Fine-Tuning (IFT) has been widely adopted as an effective\npost-training strategy to enhance various abilities of Large Language Models\n(LLMs). However, prior studies have shown that IFT can significantly compromise\nLLMs' safety, particularly their ability to refuse malicious instructions,\nraising significant concerns. Recent research into the internal mechanisms of\nLLMs has identified the refusal direction (r-direction) in the hidden states,\nwhich plays a pivotal role in governing refusal behavior. Building on this\ninsight, our study reveals that the r-direction tends to drift during training,\nwhich we identify as one of the causes of the associated safety risks. To\nmitigate such drift, our proposed ProCon method introduces a\nprojection-constrained loss term that regularizes the projection magnitude of\neach training sample's hidden state onto the r-direction. Our initial analysis\nshows that applying an appropriate constraint can effectively mitigate the\nrefusal direction drift and associated safety risks, but remains limited by\noverall performance barriers. To overcome this barrier, informed by our\nobservation of early-stage sharp drift and a data-driven perspective, we\nintroduce a warm-up strategy that emphasizes early-stage strong constraints and\nbroaden the data distribution to strengthen constraint signals, leading to an\nenhanced ProCon method. Experimental results under various datasets, scenarios,\nand LLMs demonstrate that our method can significantly mitigate safety risks\nposed by IFT while preserving task performance gains. Even compared with strong\nbaselines, our method consistently delivers superior overall performance.\nCrucially, our analysis indicates that ProCon can contribute to stabilizing the\nr-direction during training, while such an interpretability-driven exploration\nof LLMs' internal mechanisms lays a solid foundation for future safety\nresearch."
                },
                "authors": [
                    {
                        "name": "Yanrui Du"
                    },
                    {
                        "name": "Fenglei Fan"
                    },
                    {
                        "name": "Sendong Zhao"
                    },
                    {
                        "name": "Jiawei Cao"
                    },
                    {
                        "name": "Qika Lin"
                    },
                    {
                        "name": "Kai He"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Mengling Feng"
                    }
                ],
                "author_detail": {
                    "name": "Mengling Feng"
                },
                "author": "Mengling Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01909v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01909v3",
                "updated": "2025-09-08T15:18:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    18,
                    35,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-02T03:04:27Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    3,
                    4,
                    27,
                    1,
                    245,
                    0
                ],
                "title": "Oyster-I: Beyond Refusal -- Constructive Safety Alignment for\n  Responsible Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oyster-I: Beyond Refusal -- Constructive Safety Alignment for\n  Responsible Language Models"
                },
                "summary": "Large language models (LLMs) typically deploy safety mechanisms to prevent\nharmful content generation. Most current approaches focus narrowly on risks\nposed by malicious actors, often framing risks as adversarial events and\nrelying on defensive refusals. However, in real-world settings, risks also come\nfrom non-malicious users seeking help while under psychological distress (e.g.,\nself-harm intentions). In such cases, the model's response can strongly\ninfluence the user's next actions. Simple refusals may lead them to repeat,\nescalate, or move to unsafe platforms, creating worse outcomes. We introduce\nConstructive Safety Alignment (CSA), a human-centric paradigm that protects\nagainst malicious misuse while actively guiding vulnerable users toward safe\nand helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic\nanticipation of user reactions, fine-grained risk boundary discovery, and\ninterpretable reasoning control, turning safety into a trust-building process.\nOy1 achieves state-of-the-art safety among open models while retaining high\ngeneral capabilities. On our Constructive Benchmark, it shows strong\nconstructive engagement, close to GPT-5, and unmatched robustness on the\nStrata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from\nrefusal-first to guidance-first safety, CSA redefines the model-user\nrelationship, aiming for systems that are not just safe, but meaningfully\nhelpful. We release Oy1, code, and the benchmark to support responsible,\nuser-centered AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) typically deploy safety mechanisms to prevent\nharmful content generation. Most current approaches focus narrowly on risks\nposed by malicious actors, often framing risks as adversarial events and\nrelying on defensive refusals. However, in real-world settings, risks also come\nfrom non-malicious users seeking help while under psychological distress (e.g.,\nself-harm intentions). In such cases, the model's response can strongly\ninfluence the user's next actions. Simple refusals may lead them to repeat,\nescalate, or move to unsafe platforms, creating worse outcomes. We introduce\nConstructive Safety Alignment (CSA), a human-centric paradigm that protects\nagainst malicious misuse while actively guiding vulnerable users toward safe\nand helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic\nanticipation of user reactions, fine-grained risk boundary discovery, and\ninterpretable reasoning control, turning safety into a trust-building process.\nOy1 achieves state-of-the-art safety among open models while retaining high\ngeneral capabilities. On our Constructive Benchmark, it shows strong\nconstructive engagement, close to GPT-5, and unmatched robustness on the\nStrata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from\nrefusal-first to guidance-first safety, CSA redefines the model-user\nrelationship, aiming for systems that are not just safe, but meaningfully\nhelpful. We release Oy1, code, and the benchmark to support responsible,\nuser-centered AI."
                },
                "authors": [
                    {
                        "name": "Ranjie Duan"
                    },
                    {
                        "name": "Jiexi Liu"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Shiji Zhao"
                    },
                    {
                        "name": "Ruoxi Cheng"
                    },
                    {
                        "name": "Fengxiang Wang"
                    },
                    {
                        "name": "Cheng Wei"
                    },
                    {
                        "name": "Yong Xie"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Defeng Li"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Yuefeng Chen"
                    },
                    {
                        "name": "Chongwen Wang"
                    },
                    {
                        "name": "Xingjun Ma"
                    },
                    {
                        "name": "Xingxing Wei"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Yitong Sun"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Jinzhao Hu"
                    },
                    {
                        "name": "Sha Xu"
                    },
                    {
                        "name": "Yitong Yang"
                    },
                    {
                        "name": "Jialing Tao"
                    },
                    {
                        "name": "Hui Xue"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xue"
                },
                "author": "Hui Xue",
                "arxiv_comment": "Technical Report Code & Model weights available:\n  https://github.com/Alibaba-AAIG/Oyster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01909v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01909v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10603v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10603v3",
                "updated": "2025-09-08T15:18:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    18,
                    30,
                    0,
                    251,
                    0
                ],
                "published": "2025-08-14T12:44:39Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    12,
                    44,
                    39,
                    3,
                    226,
                    0
                ],
                "title": "Why Report Failed Interactions With Robots?! Towards Vignette-based\n  Interaction Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Report Failed Interactions With Robots?! Towards Vignette-based\n  Interaction Quality"
                },
                "summary": "Although the quality of human-robot interactions has improved with the advent\nof LLMs, there are still various factors that cause systems to be sub-optimal\nwhen compared to human-human interactions. The nature and criticality of\nfailures are often dependent on the context of the interaction and so cannot be\ngeneralized across the wide range of scenarios and experiments which have been\nimplemented in HRI research. In this work we propose the use of a technique\noverlooked in the field of HRI, ethnographic vignettes, to clearly highlight\nthese failures, particularly those that are rarely documented. We describe the\nmethodology behind the process of writing vignettes and create our own based on\nour personal experiences with failures in HRI systems. We emphasize the\nstrength of vignettes as the ability to communicate failures from a\nmulti-disciplinary perspective, promote transparency about the capabilities of\nrobots, and document unexpected behaviours which would otherwise be omitted\nfrom research reports. We encourage the use of vignettes to augment existing\ninteraction evaluation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although the quality of human-robot interactions has improved with the advent\nof LLMs, there are still various factors that cause systems to be sub-optimal\nwhen compared to human-human interactions. The nature and criticality of\nfailures are often dependent on the context of the interaction and so cannot be\ngeneralized across the wide range of scenarios and experiments which have been\nimplemented in HRI research. In this work we propose the use of a technique\noverlooked in the field of HRI, ethnographic vignettes, to clearly highlight\nthese failures, particularly those that are rarely documented. We describe the\nmethodology behind the process of writing vignettes and create our own based on\nour personal experiences with failures in HRI systems. We emphasize the\nstrength of vignettes as the ability to communicate failures from a\nmulti-disciplinary perspective, promote transparency about the capabilities of\nrobots, and document unexpected behaviours which would otherwise be omitted\nfrom research reports. We encourage the use of vignettes to augment existing\ninteraction evaluation methods."
                },
                "authors": [
                    {
                        "name": "Agnes Axelsson"
                    },
                    {
                        "name": "Merle Reimann"
                    },
                    {
                        "name": "Ronald Cumbal"
                    },
                    {
                        "name": "Hannah Pelikan"
                    },
                    {
                        "name": "Divesh Lala"
                    }
                ],
                "author_detail": {
                    "name": "Divesh Lala"
                },
                "author": "Divesh Lala",
                "arxiv_comment": "Accepted at the workshop on Real-World HRI in Public and Private\n  Spaces: Successes, Failures, and Lessons Learned (PubRob-Fails), held at the\n  IEEE RO-MAN Conference, 2025. 6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10603v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10603v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14222v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14222v5",
                "updated": "2025-09-08T15:17:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    17,
                    7,
                    0,
                    251,
                    0
                ],
                "published": "2025-03-18T12:59:50Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    12,
                    59,
                    50,
                    1,
                    77,
                    0
                ],
                "title": "Vanishing Stacked-Residual PINN for State Reconstruction of Hyperbolic\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vanishing Stacked-Residual PINN for State Reconstruction of Hyperbolic\n  Systems"
                },
                "summary": "In a more connected world, modeling multi-agent systems with hyperbolic\npartial differential equations (PDEs) offers a compact, physics-consistent\ndescription of collective dynamics. However, classical control tools need\nadaptation for these complex systems. Physics-informed neural networks (PINNs)\nprovide a powerful framework to fix this issue by inferring solutions to PDEs\nby embedding governing equations into the neural network. A major limitation of\noriginal PINNs is their inability to capture steep gradients and\ndiscontinuities in hyperbolic PDEs. To tackle this problem, we propose a\nstacked residual PINN method enhanced with a vanishing viscosity mechanism.\nInitially, a basic PINN with a small viscosity coefficient provides a stable,\nlow-fidelity solution. Residual correction blocks with learnable scaling\nparameters then iteratively refine this solution, progressively decreasing the\nviscosity coefficient to transition from parabolic to hyperbolic PDEs. Applying\nthis method to traffic state reconstruction improved results by an order of\nmagnitude in relative $\\mathcal{L}^2$ error, demonstrating its potential to\naccurately estimate solutions where original PINNs struggle with instability\nand low fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a more connected world, modeling multi-agent systems with hyperbolic\npartial differential equations (PDEs) offers a compact, physics-consistent\ndescription of collective dynamics. However, classical control tools need\nadaptation for these complex systems. Physics-informed neural networks (PINNs)\nprovide a powerful framework to fix this issue by inferring solutions to PDEs\nby embedding governing equations into the neural network. A major limitation of\noriginal PINNs is their inability to capture steep gradients and\ndiscontinuities in hyperbolic PDEs. To tackle this problem, we propose a\nstacked residual PINN method enhanced with a vanishing viscosity mechanism.\nInitially, a basic PINN with a small viscosity coefficient provides a stable,\nlow-fidelity solution. Residual correction blocks with learnable scaling\nparameters then iteratively refine this solution, progressively decreasing the\nviscosity coefficient to transition from parabolic to hyperbolic PDEs. Applying\nthis method to traffic state reconstruction improved results by an order of\nmagnitude in relative $\\mathcal{L}^2$ error, demonstrating its potential to\naccurately estimate solutions where original PINNs struggle with instability\nand low fidelity."
                },
                "authors": [
                    {
                        "name": "Katayoun Eshkofti"
                    },
                    {
                        "name": "Matthieu Barreau"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Barreau"
                },
                "author": "Matthieu Barreau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14222v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14222v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10484v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10484v3",
                "updated": "2025-09-08T15:16:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    16,
                    5,
                    0,
                    251,
                    0
                ],
                "published": "2025-01-17T05:20:38Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    5,
                    20,
                    38,
                    4,
                    17,
                    0
                ],
                "title": "Bias in Decision-Making for AI's Ethical Dilemmas: A Comparative Study\n  of ChatGPT and Claude",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias in Decision-Making for AI's Ethical Dilemmas: A Comparative Study\n  of ChatGPT and Claude"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have enabled human-like\nresponses across various tasks, raising questions about their ethical\ndecision-making capabilities and potential biases. This study systematically\nevaluates how nine popular LLMs (both open-source and closed-source) respond to\nethical dilemmas involving protected attributes. Across 50,400 trials spanning\nsingle and intersectional attribute combinations in four dilemma scenarios\n(protective vs. harmful), we assess models' ethical preferences, sensitivity,\nstability, and clustering patterns. Results reveal significant biases in\nprotected attributes in all models, with differing preferences depending on\nmodel type and dilemma context. Notably, open-source LLMs show stronger\npreferences for marginalized groups and greater sensitivity in harmful\nscenarios, while closed-source models are more selective in protective\nsituations and tend to favor mainstream groups. We also find that ethical\nbehavior varies across dilemma types: LLMs maintain consistent patterns in\nprotective scenarios but respond with more diverse and cognitively demanding\ndecisions in harmful ones. Furthermore, models display more pronounced ethical\ntendencies under intersectional conditions than in single-attribute settings,\nsuggesting that complex inputs reveal deeper biases. These findings highlight\nthe need for multi-dimensional, context-aware evaluation of LLMs' ethical\nbehavior and offer a systematic evaluation and approach to understanding and\naddressing fairness in LLM decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have enabled human-like\nresponses across various tasks, raising questions about their ethical\ndecision-making capabilities and potential biases. This study systematically\nevaluates how nine popular LLMs (both open-source and closed-source) respond to\nethical dilemmas involving protected attributes. Across 50,400 trials spanning\nsingle and intersectional attribute combinations in four dilemma scenarios\n(protective vs. harmful), we assess models' ethical preferences, sensitivity,\nstability, and clustering patterns. Results reveal significant biases in\nprotected attributes in all models, with differing preferences depending on\nmodel type and dilemma context. Notably, open-source LLMs show stronger\npreferences for marginalized groups and greater sensitivity in harmful\nscenarios, while closed-source models are more selective in protective\nsituations and tend to favor mainstream groups. We also find that ethical\nbehavior varies across dilemma types: LLMs maintain consistent patterns in\nprotective scenarios but respond with more diverse and cognitively demanding\ndecisions in harmful ones. Furthermore, models display more pronounced ethical\ntendencies under intersectional conditions than in single-attribute settings,\nsuggesting that complex inputs reveal deeper biases. These findings highlight\nthe need for multi-dimensional, context-aware evaluation of LLMs' ethical\nbehavior and offer a systematic evaluation and approach to understanding and\naddressing fairness in LLM decision-making."
                },
                "authors": [
                    {
                        "name": "Yile Yan"
                    },
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Wentao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Xu"
                },
                "author": "Wentao Xu",
                "arxiv_comment": "This paper has been accepted by International AAAI Conference on Web\n  and Social Media 2026 (ICWSM 2026), sunny Los Angeles, California",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10484v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10484v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04394v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04394v5",
                "updated": "2025-09-08T15:14:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    14,
                    44,
                    0,
                    251,
                    0
                ],
                "published": "2024-12-05T18:09:41Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    9,
                    41,
                    3,
                    340,
                    0
                ],
                "title": "Bayesian Quantum Amplitude Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Quantum Amplitude Estimation"
                },
                "summary": "We present BAE, a problem-tailored and noise-aware Bayesian algorithm for\nquantum amplitude estimation. In a fault tolerant scenario, BAE is capable of\nsaturating the Heisenberg limit; if device noise is present, BAE can\ndynamically characterize it and self-adapt. We further propose aBAE, an\nannealed variant of BAE drawing on methods from statistical inference, to\nenhance robustness. Our proposals are parallelizable in both quantum and\nclassical components, offer tools for fast noise model assessment, and can\nleverage preexisting information. Additionally, they accommodate experimental\nlimitations and preferred cost trade-offs. We propose a robust benchmark for\namplitude estimation algorithms and use it to test BAE against other\napproaches, demonstrating its competitive performance in both noisy and\nnoiseless scenarios. In both cases, it achieves lower error than any other\nalgorithm as a function of the cost. In the presence of decoherence, it is\ncapable of learning when other algorithms fail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present BAE, a problem-tailored and noise-aware Bayesian algorithm for\nquantum amplitude estimation. In a fault tolerant scenario, BAE is capable of\nsaturating the Heisenberg limit; if device noise is present, BAE can\ndynamically characterize it and self-adapt. We further propose aBAE, an\nannealed variant of BAE drawing on methods from statistical inference, to\nenhance robustness. Our proposals are parallelizable in both quantum and\nclassical components, offer tools for fast noise model assessment, and can\nleverage preexisting information. Additionally, they accommodate experimental\nlimitations and preferred cost trade-offs. We propose a robust benchmark for\namplitude estimation algorithms and use it to test BAE against other\napproaches, demonstrating its competitive performance in both noisy and\nnoiseless scenarios. In both cases, it achieves lower error than any other\nalgorithm as a function of the cost. In the presence of decoherence, it is\ncapable of learning when other algorithms fail."
                },
                "authors": [
                    {
                        "name": "Alexandra Ramôa"
                    },
                    {
                        "name": "Luis Paulo Santos"
                    }
                ],
                "author_detail": {
                    "name": "Luis Paulo Santos"
                },
                "author": "Luis Paulo Santos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04394v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04394v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13729v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13729v5",
                "updated": "2025-09-08T14:59:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    59,
                    54,
                    0,
                    251,
                    0
                ],
                "published": "2025-02-19T13:55:32Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    55,
                    32,
                    2,
                    50,
                    0
                ],
                "title": "Emergence of the Primacy Effect in Structured State-Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergence of the Primacy Effect in Structured State-Space Models"
                },
                "summary": "Structured state-space models (SSMs) have been developed to offer more\npersistent memory retention than traditional recurrent neural networks, while\nmaintaining real-time inference capabilities and addressing the time-complexity\nlimitations of Transformers. Despite this intended persistence, the memory\nmechanism of canonical SSMs is theoretically designed to decay monotonically\nover time, meaning that more recent inputs are expected to be retained more\naccurately than earlier ones. Contrary to this theoretical expectation,\nhowever, the present study reveals a counterintuitive finding: when trained and\nevaluated on a synthetic, statistically balanced memorization task, SSMs\npredominantly preserve the *initially* presented data in memory. This pattern\nof memory bias, known as the *primacy effect* in psychology, presents a\nnon-trivial challenge to the current theoretical understanding of SSMs and\nopens new avenues for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured state-space models (SSMs) have been developed to offer more\npersistent memory retention than traditional recurrent neural networks, while\nmaintaining real-time inference capabilities and addressing the time-complexity\nlimitations of Transformers. Despite this intended persistence, the memory\nmechanism of canonical SSMs is theoretically designed to decay monotonically\nover time, meaning that more recent inputs are expected to be retained more\naccurately than earlier ones. Contrary to this theoretical expectation,\nhowever, the present study reveals a counterintuitive finding: when trained and\nevaluated on a synthetic, statistically balanced memorization task, SSMs\npredominantly preserve the *initially* presented data in memory. This pattern\nof memory bias, known as the *primacy effect* in psychology, presents a\nnon-trivial challenge to the current theoretical understanding of SSMs and\nopens new avenues for future research."
                },
                "authors": [
                    {
                        "name": "Takashi Morita"
                    }
                ],
                "author_detail": {
                    "name": "Takashi Morita"
                },
                "author": "Takashi Morita",
                "arxiv_comment": "Accepted for ACML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13729v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13729v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06774v1",
                "updated": "2025-09-08T14:58:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    58,
                    10,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T14:58:10Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    58,
                    10,
                    0,
                    251,
                    0
                ],
                "title": "OpenCoderRank: AI-Driven Technical Assessments Made Easy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenCoderRank: AI-Driven Technical Assessments Made Easy"
                },
                "summary": "Organizations and educational institutions use time-bound assessment tasks to\nevaluate coding and problem-solving skills. These assessments measure not only\nthe correctness of the solutions, but also their efficiency. Problem setters\n(educator/interviewer) are responsible for crafting these challenges, carefully\nbalancing difficulty and relevance to create meaningful evaluation experiences.\nConversely, problem solvers (student/interviewee) apply coding efficiency and\nlogical thinking to arrive at correct solutions. In the era of Large Language\nModels (LLMs), LLMs assist problem setters in generating diverse and\nchallenging questions, but they can undermine assessment integrity for problem\nsolvers by providing easy access to solutions. This paper introduces\nOpenCoderRank, an easy-to-use platform designed to simulate technical\nassessments. It acts as a bridge between problem setters and problem solvers,\nhelping solvers prepare for time constraints and unfamiliar problems while\nallowing setters to self-host assessments, offering a no-cost and customizable\nsolution for technical assessments in resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Organizations and educational institutions use time-bound assessment tasks to\nevaluate coding and problem-solving skills. These assessments measure not only\nthe correctness of the solutions, but also their efficiency. Problem setters\n(educator/interviewer) are responsible for crafting these challenges, carefully\nbalancing difficulty and relevance to create meaningful evaluation experiences.\nConversely, problem solvers (student/interviewee) apply coding efficiency and\nlogical thinking to arrive at correct solutions. In the era of Large Language\nModels (LLMs), LLMs assist problem setters in generating diverse and\nchallenging questions, but they can undermine assessment integrity for problem\nsolvers by providing easy access to solutions. This paper introduces\nOpenCoderRank, an easy-to-use platform designed to simulate technical\nassessments. It acts as a bridge between problem setters and problem solvers,\nhelping solvers prepare for time constraints and unfamiliar problems while\nallowing setters to self-host assessments, offering a no-cost and customizable\nsolution for technical assessments in resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Hridoy Sankar Dutta"
                    },
                    {
                        "name": "Sana Ansari"
                    },
                    {
                        "name": "Swati Kumari"
                    },
                    {
                        "name": "Shounak Ravi Bhalerao"
                    }
                ],
                "author_detail": {
                    "name": "Shounak Ravi Bhalerao"
                },
                "author": "Shounak Ravi Bhalerao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11844v3",
                "updated": "2025-09-08T14:56:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    56,
                    58,
                    0,
                    251,
                    0
                ],
                "published": "2024-11-18T18:59:31Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    31,
                    0,
                    323,
                    0
                ],
                "title": "Generative World Explorer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative World Explorer"
                },
                "summary": "Planning with partial observation is a central challenge in embodied AI. A\nmajority of prior works have tackled this challenge by developing agents that\nphysically explore their environment to update their beliefs about the world\nstate. In contrast, humans can $\\textit{imagine}$ unseen parts of the world\nthrough a mental exploration and $\\textit{revise}$ their beliefs with imagined\nobservations. Such updated beliefs can allow them to make more informed\ndecisions, without necessitating the physical exploration of the world at all\ntimes. To achieve this human-like ability, we introduce the $\\textit{Generative\nWorld Explorer (Genex)}$, an egocentric world exploration framework that allows\nan agent to mentally explore a large-scale 3D world (e.g., urban scenes) and\nacquire imagined observations to update its belief. This updated belief will\nthen help the agent to make a more informed decision at the current step. To\ntrain $\\textit{Genex}$, we create a synthetic urban scene dataset, Genex-DB.\nOur experimental results demonstrate that (1) $\\textit{Genex}$ can generate\nhigh-quality and consistent observations during long-horizon exploration of a\nlarge virtual physical world and (2) the beliefs updated with the generated\nobservations can inform an existing decision-making model (e.g., an LLM agent)\nto make better plans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning with partial observation is a central challenge in embodied AI. A\nmajority of prior works have tackled this challenge by developing agents that\nphysically explore their environment to update their beliefs about the world\nstate. In contrast, humans can $\\textit{imagine}$ unseen parts of the world\nthrough a mental exploration and $\\textit{revise}$ their beliefs with imagined\nobservations. Such updated beliefs can allow them to make more informed\ndecisions, without necessitating the physical exploration of the world at all\ntimes. To achieve this human-like ability, we introduce the $\\textit{Generative\nWorld Explorer (Genex)}$, an egocentric world exploration framework that allows\nan agent to mentally explore a large-scale 3D world (e.g., urban scenes) and\nacquire imagined observations to update its belief. This updated belief will\nthen help the agent to make a more informed decision at the current step. To\ntrain $\\textit{Genex}$, we create a synthetic urban scene dataset, Genex-DB.\nOur experimental results demonstrate that (1) $\\textit{Genex}$ can generate\nhigh-quality and consistent observations during long-horizon exploration of a\nlarge virtual physical world and (2) the beliefs updated with the generated\nobservations can inform an existing decision-making model (e.g., an LLM agent)\nto make better plans."
                },
                "authors": [
                    {
                        "name": "Taiming Lu"
                    },
                    {
                        "name": "Tianmin Shu"
                    },
                    {
                        "name": "Alan Yuille"
                    },
                    {
                        "name": "Daniel Khashabi"
                    },
                    {
                        "name": "Jieneng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jieneng Chen"
                },
                "author": "Jieneng Chen",
                "arxiv_comment": "Website: generative-world-explorer.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06770v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06770v1",
                "updated": "2025-09-08T14:54:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    54,
                    31,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T14:54:31Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    54,
                    31,
                    0,
                    251,
                    0
                ],
                "title": "Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM\n  Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM\n  Prompting"
                },
                "summary": "Large language models (LLMs) are now used in multi-turn workflows, but we\nstill lack a clear way to measure when iteration helps and when it hurts. We\npresent an evaluation framework for iterative refinement that spans ideation,\ncode, and math. Our protocol runs controlled 12-turn conversations per task,\nutilizing a variety of prompts ranging from vague ``improve it'' feedback to\ntargeted steering, and logs per-turn outputs. We score outcomes with\ndomain-appropriate checks (unit tests for code; answer-equivalence plus\nreasoning-soundness for math; originality and feasibility for ideation) and\ntrack turn-level behavior with three families of metrics: semantic movement\nacross turns, turn-to-turn change, and output size growth. Across models and\ntasks, gains are domain-dependent: they arrive early in ideas and code, but in\nmath late turns matter when guided by elaboration. After the first few turns,\nvague feedback often plateaus or reverses correctness, while targeted prompts\nreliably shift the intended quality axis (novelty vs. feasibility in ideation;\nspeed vs. readability in code; in math, elaboration outperforms exploration and\ndrives late-turn gains). We also observe consistent domain patterns: ideation\nmoves more in meaning across turns, code tends to grow in size with little\nsemantic change, and math starts fixed but can break that path with late,\nelaborative iteration.Together, the framework and metrics make iteration\nmeasurable and comparable across models, and signal when to steer, stop, or\nswitch strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are now used in multi-turn workflows, but we\nstill lack a clear way to measure when iteration helps and when it hurts. We\npresent an evaluation framework for iterative refinement that spans ideation,\ncode, and math. Our protocol runs controlled 12-turn conversations per task,\nutilizing a variety of prompts ranging from vague ``improve it'' feedback to\ntargeted steering, and logs per-turn outputs. We score outcomes with\ndomain-appropriate checks (unit tests for code; answer-equivalence plus\nreasoning-soundness for math; originality and feasibility for ideation) and\ntrack turn-level behavior with three families of metrics: semantic movement\nacross turns, turn-to-turn change, and output size growth. Across models and\ntasks, gains are domain-dependent: they arrive early in ideas and code, but in\nmath late turns matter when guided by elaboration. After the first few turns,\nvague feedback often plateaus or reverses correctness, while targeted prompts\nreliably shift the intended quality axis (novelty vs. feasibility in ideation;\nspeed vs. readability in code; in math, elaboration outperforms exploration and\ndrives late-turn gains). We also observe consistent domain patterns: ideation\nmoves more in meaning across turns, code tends to grow in size with little\nsemantic change, and math starts fixed but can break that path with late,\nelaborative iteration.Together, the framework and metrics make iteration\nmeasurable and comparable across models, and signal when to steer, stop, or\nswitch strategies."
                },
                "authors": [
                    {
                        "name": "Shashidhar Reddy Javaji"
                    },
                    {
                        "name": "Bhavul Gauri"
                    },
                    {
                        "name": "Zining Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Zining Zhu"
                },
                "author": "Zining Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06770v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07831v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07831v3",
                "updated": "2025-09-08T14:34:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    34,
                    7,
                    0,
                    251,
                    0
                ],
                "published": "2024-06-12T02:57:41Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    2,
                    57,
                    41,
                    2,
                    164,
                    0
                ],
                "title": "ALPS: Improved Optimization for Highly Sparse One-Shot Pruning for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALPS: Improved Optimization for Highly Sparse One-Shot Pruning for Large\n  Language Models"
                },
                "summary": "The impressive performance of Large Language Models (LLMs) across various\nnatural language processing tasks comes at the cost of vast computational\nresources and storage requirements. One-shot pruning techniques offer a way to\nalleviate these burdens by removing redundant weights without the need for\nretraining. Yet, the massive scale of LLMs often forces current pruning\napproaches to rely on heuristics instead of optimization-based techniques,\npotentially resulting in suboptimal compression. In this paper, we introduce\nALPS, an optimization-based framework that tackles the pruning problem using\nthe operator splitting technique and a preconditioned conjugate gradient-based\npost-processing step. Our approach incorporates novel techniques to accelerate\nand theoretically guarantee convergence while leveraging vectorization and GPU\nparallelism for efficiency. ALPS substantially outperforms state-of-the-art\nmethods in terms of the pruning objective and perplexity reduction,\nparticularly for highly sparse models. On the OPT-30B model with 70% sparsity,\nALPS achieves a 13% reduction in test perplexity on the WikiText dataset and a\n19% improvement in zero-shot benchmark performance compared to existing\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impressive performance of Large Language Models (LLMs) across various\nnatural language processing tasks comes at the cost of vast computational\nresources and storage requirements. One-shot pruning techniques offer a way to\nalleviate these burdens by removing redundant weights without the need for\nretraining. Yet, the massive scale of LLMs often forces current pruning\napproaches to rely on heuristics instead of optimization-based techniques,\npotentially resulting in suboptimal compression. In this paper, we introduce\nALPS, an optimization-based framework that tackles the pruning problem using\nthe operator splitting technique and a preconditioned conjugate gradient-based\npost-processing step. Our approach incorporates novel techniques to accelerate\nand theoretically guarantee convergence while leveraging vectorization and GPU\nparallelism for efficiency. ALPS substantially outperforms state-of-the-art\nmethods in terms of the pruning objective and perplexity reduction,\nparticularly for highly sparse models. On the OPT-30B model with 70% sparsity,\nALPS achieves a 13% reduction in test perplexity on the WikiText dataset and a\n19% improvement in zero-shot benchmark performance compared to existing\nmethods."
                },
                "authors": [
                    {
                        "name": "Xiang Meng"
                    },
                    {
                        "name": "Kayhan Behdin"
                    },
                    {
                        "name": "Haoyue Wang"
                    },
                    {
                        "name": "Rahul Mazumder"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Mazumder"
                },
                "author": "Rahul Mazumder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07831v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07831v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22146v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22146v3",
                "updated": "2025-09-08T14:34:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    34,
                    4,
                    0,
                    251,
                    0
                ],
                "published": "2025-06-27T11:44:40Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    11,
                    44,
                    40,
                    4,
                    178,
                    0
                ],
                "title": "Visual Structures Helps Visual Reasoning: Addressing the Binding Problem\n  in VLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Structures Helps Visual Reasoning: Addressing the Binding Problem\n  in VLMs"
                },
                "summary": "Despite progress in Vision-Language Models (VLMs), their capacity for visual\nreasoning is often limited by the binding problem: the failure to reliably\nassociate perceptual features with their correct visual referents. This\nlimitation underlies persistent errors in tasks such as counting, visual\nsearch, scene description, and spatial relationship understanding. A key factor\nis that current VLMs process visual features largely in parallel, lacking\nmechanisms for spatially grounded, serial attention. This paper introduces\nVISER (Visual Input Structure for Enhanced Reasoning), a simple yet effective\nintervention: augmenting visual inputs with low-level spatial structures and\npairing this with a textual prompt that encourages sequential, spatially-aware\nparsing. We empirically demonstrate substantial performance improvements across\ncore visual reasoning tasks. Specifically, VISER improves GPT-4o visual search\naccuracy by 25.00%, increases counting accuracy by 26.83%, reduces edit\ndistance error in scene description by 0.32, and enhances performance on\nspatial relationship tasks by 9.50% on a 2D synthetic dataset. Furthermore, we\nfind that the visual modification is essential for these gains; purely textual\nstrategies, including Chain-of-Thought prompting, are insufficient and can even\ndegrade performance. VISER enhances binding only with a single-query inference,\nunderscoring the importance of visual input design over purely\nlinguistically-based approaches. These findings suggest that low-level visual\nstructuring is a powerful and underexplored direction for improving\ncompositional visual reasoning and could serve as a general strategy for\nenhancing VLM performance on spatially grounded tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite progress in Vision-Language Models (VLMs), their capacity for visual\nreasoning is often limited by the binding problem: the failure to reliably\nassociate perceptual features with their correct visual referents. This\nlimitation underlies persistent errors in tasks such as counting, visual\nsearch, scene description, and spatial relationship understanding. A key factor\nis that current VLMs process visual features largely in parallel, lacking\nmechanisms for spatially grounded, serial attention. This paper introduces\nVISER (Visual Input Structure for Enhanced Reasoning), a simple yet effective\nintervention: augmenting visual inputs with low-level spatial structures and\npairing this with a textual prompt that encourages sequential, spatially-aware\nparsing. We empirically demonstrate substantial performance improvements across\ncore visual reasoning tasks. Specifically, VISER improves GPT-4o visual search\naccuracy by 25.00%, increases counting accuracy by 26.83%, reduces edit\ndistance error in scene description by 0.32, and enhances performance on\nspatial relationship tasks by 9.50% on a 2D synthetic dataset. Furthermore, we\nfind that the visual modification is essential for these gains; purely textual\nstrategies, including Chain-of-Thought prompting, are insufficient and can even\ndegrade performance. VISER enhances binding only with a single-query inference,\nunderscoring the importance of visual input design over purely\nlinguistically-based approaches. These findings suggest that low-level visual\nstructuring is a powerful and underexplored direction for improving\ncompositional visual reasoning and could serve as a general strategy for\nenhancing VLM performance on spatially grounded tasks."
                },
                "authors": [
                    {
                        "name": "Amirmohammad Izadi"
                    },
                    {
                        "name": "Mohammad Ali Banayeeanzade"
                    },
                    {
                        "name": "Fatemeh Askari"
                    },
                    {
                        "name": "Ali Rahimiakbar"
                    },
                    {
                        "name": "Mohammad Mahdi Vahedi"
                    },
                    {
                        "name": "Hosein Hasani"
                    },
                    {
                        "name": "Mahdieh Soleymani Baghshah"
                    }
                ],
                "author_detail": {
                    "name": "Mahdieh Soleymani Baghshah"
                },
                "author": "Mahdieh Soleymani Baghshah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22146v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22146v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07963v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07963v3",
                "updated": "2025-09-08T14:31:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    31,
                    8,
                    0,
                    251,
                    0
                ],
                "published": "2025-06-09T17:38:45Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    17,
                    38,
                    45,
                    0,
                    160,
                    0
                ],
                "title": "SUDER: Self-Improving Unified Large Multimodal Models for Understanding\n  and Generation with Dual Self-Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SUDER: Self-Improving Unified Large Multimodal Models for Understanding\n  and Generation with Dual Self-Rewards"
                },
                "summary": "Building upon large language models (LLMs), recent large multimodal models\n(LMMs) unify cross-model understanding and generation into a single framework.\nHowever, LMMs still struggle to achieve accurate vision-language alignment,\nprone to generating text responses contradicting the visual input or failing to\nfollow the text-to-image prompts. Current solutions require external\nsupervision (e.g., human feedback or reward models) and only address\nunidirectional tasks-either understanding or generation. In this work, based on\nthe observation that understanding and generation are naturally inverse dual\ntasks, we propose \\textbf{SUDER} (\\textbf{S}elf-improving \\textbf{U}nified LMMs\nwith \\textbf{D}ual s\\textbf{E}lf-\\textbf{R}ewards), a framework reinforcing the\nunderstanding and generation capabilities of LMMs with a self-supervised dual\nreward mechanism. SUDER leverages the inherent duality between understanding\nand generation tasks to provide self-supervised optimization signals for each\nother. Specifically, we sample multiple outputs for a given input in one task\ndomain, then reverse the input-output pairs to compute the dual likelihood\nwithin the model as self-rewards for optimization. Extensive experimental\nresults on visual understanding and generation benchmarks demonstrate that our\nmethod can effectively enhance the performance of the model without any\nexternal supervision, especially achieving remarkable improvements in\ntext-to-image tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building upon large language models (LLMs), recent large multimodal models\n(LMMs) unify cross-model understanding and generation into a single framework.\nHowever, LMMs still struggle to achieve accurate vision-language alignment,\nprone to generating text responses contradicting the visual input or failing to\nfollow the text-to-image prompts. Current solutions require external\nsupervision (e.g., human feedback or reward models) and only address\nunidirectional tasks-either understanding or generation. In this work, based on\nthe observation that understanding and generation are naturally inverse dual\ntasks, we propose \\textbf{SUDER} (\\textbf{S}elf-improving \\textbf{U}nified LMMs\nwith \\textbf{D}ual s\\textbf{E}lf-\\textbf{R}ewards), a framework reinforcing the\nunderstanding and generation capabilities of LMMs with a self-supervised dual\nreward mechanism. SUDER leverages the inherent duality between understanding\nand generation tasks to provide self-supervised optimization signals for each\nother. Specifically, we sample multiple outputs for a given input in one task\ndomain, then reverse the input-output pairs to compute the dual likelihood\nwithin the model as self-rewards for optimization. Extensive experimental\nresults on visual understanding and generation benchmarks demonstrate that our\nmethod can effectively enhance the performance of the model without any\nexternal supervision, especially achieving remarkable improvements in\ntext-to-image tasks."
                },
                "authors": [
                    {
                        "name": "Jixiang Hong"
                    },
                    {
                        "name": "Yiran Zhang"
                    },
                    {
                        "name": "Guanzhong Wang"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07963v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07963v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06735v1",
                "updated": "2025-09-08T14:28:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    28,
                    23,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T14:28:23Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    28,
                    23,
                    0,
                    251,
                    0
                ],
                "title": "Data-driven discovery of dynamical models in biology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven discovery of dynamical models in biology"
                },
                "summary": "Dynamical systems theory describes how interacting quantities change over\ntime and space, from molecular oscillators to large-scale biological patterns.\nSuch systems often involve nonlinear feedbacks, delays, and interactions across\nscales. Classical modeling derives explicit governing equations, often systems\nof differential equations, by combining mechanistic assumptions, experimental\nobservations, and known physical laws. The growing complexity of biological\nprocesses has, however, motivated complementary data-driven methods that aim to\ninfer model structure directly from measurements, often without specifying\nequations a priori. In this review, we survey approaches for model discovery in\nbiological dynamical systems, focusing on three methodological families:\nregression-based methods, network-based architectures, and decomposition\ntechniques. We compare their ability to address three core goals: forecasting\nfuture states, identifying interactions, and characterizing system states.\nRepresentative methods are applied to a common benchmark, the Oregonator model,\na minimal nonlinear oscillator that captures shared design principles of\nchemical and biological systems. By highlighting strengths, limitations, and\ninterpretability, we aim to guide researchers in selecting tools for analyzing\ncomplex, nonlinear, and high-dimensional dynamics in the life sciences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamical systems theory describes how interacting quantities change over\ntime and space, from molecular oscillators to large-scale biological patterns.\nSuch systems often involve nonlinear feedbacks, delays, and interactions across\nscales. Classical modeling derives explicit governing equations, often systems\nof differential equations, by combining mechanistic assumptions, experimental\nobservations, and known physical laws. The growing complexity of biological\nprocesses has, however, motivated complementary data-driven methods that aim to\ninfer model structure directly from measurements, often without specifying\nequations a priori. In this review, we survey approaches for model discovery in\nbiological dynamical systems, focusing on three methodological families:\nregression-based methods, network-based architectures, and decomposition\ntechniques. We compare their ability to address three core goals: forecasting\nfuture states, identifying interactions, and characterizing system states.\nRepresentative methods are applied to a common benchmark, the Oregonator model,\na minimal nonlinear oscillator that captures shared design principles of\nchemical and biological systems. By highlighting strengths, limitations, and\ninterpretability, we aim to guide researchers in selecting tools for analyzing\ncomplex, nonlinear, and high-dimensional dynamics in the life sciences."
                },
                "authors": [
                    {
                        "name": "Bartosz Prokop"
                    },
                    {
                        "name": "Lendert Gelens"
                    }
                ],
                "author_detail": {
                    "name": "Lendert Gelens"
                },
                "author": "Lendert Gelens",
                "arxiv_comment": "26 pages, 5 figures, 6 Info boxes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06734v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06734v1",
                "updated": "2025-09-08T14:28:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    28,
                    13,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T14:28:13Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    28,
                    13,
                    0,
                    251,
                    0
                ],
                "title": "Intelligent Manufacturing Support: Specialized LLMs for Composite\n  Material Processing and Equipment Operation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Manufacturing Support: Specialized LLMs for Composite\n  Material Processing and Equipment Operation"
                },
                "summary": "Engineering educational curriculum and standards cover many material and\nmanufacturing options. However, engineers and designers are often unfamiliar\nwith certain composite materials or manufacturing techniques. Large language\nmodels (LLMs) could potentially bridge the gap. Their capacity to store and\nretrieve data from large databases provides them with a breadth of knowledge\nacross disciplines. However, their generalized knowledge base can lack\ntargeted, industry-specific knowledge. To this end, we present two LLM-based\napplications based on the GPT-4 architecture: (1) The Composites Guide: a\nsystem that provides expert knowledge on composites material and connects users\nwith research and industry professionals who can provide additional support and\n(2) The Equipment Assistant: a system that provides guidance for manufacturing\ntool operation and material characterization. By combining the knowledge of\ngeneral AI models with industry-specific knowledge, both applications are\nintended to provide more meaningful information for engineers. In this paper,\nwe discuss the development of the applications and evaluate it through a\nbenchmark and two informal user studies. The benchmark analysis uses the Rouge\nand Bertscore metrics to evaluate our model performance against GPT-4o. The\nresults show that GPT-4o and the proposed models perform similarly or better on\nthe ROUGE and BERTScore metrics. The two user studies supplement this\nquantitative evaluation by asking experts to provide qualitative and open-ended\nfeedback about our model performance on a set of domain-specific questions. The\nresults of both studies highlight a potential for more detailed and specific\nresponses with the Composites Guide and the Equipment Assistant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering educational curriculum and standards cover many material and\nmanufacturing options. However, engineers and designers are often unfamiliar\nwith certain composite materials or manufacturing techniques. Large language\nmodels (LLMs) could potentially bridge the gap. Their capacity to store and\nretrieve data from large databases provides them with a breadth of knowledge\nacross disciplines. However, their generalized knowledge base can lack\ntargeted, industry-specific knowledge. To this end, we present two LLM-based\napplications based on the GPT-4 architecture: (1) The Composites Guide: a\nsystem that provides expert knowledge on composites material and connects users\nwith research and industry professionals who can provide additional support and\n(2) The Equipment Assistant: a system that provides guidance for manufacturing\ntool operation and material characterization. By combining the knowledge of\ngeneral AI models with industry-specific knowledge, both applications are\nintended to provide more meaningful information for engineers. In this paper,\nwe discuss the development of the applications and evaluate it through a\nbenchmark and two informal user studies. The benchmark analysis uses the Rouge\nand Bertscore metrics to evaluate our model performance against GPT-4o. The\nresults show that GPT-4o and the proposed models perform similarly or better on\nthe ROUGE and BERTScore metrics. The two user studies supplement this\nquantitative evaluation by asking experts to provide qualitative and open-ended\nfeedback about our model performance on a set of domain-specific questions. The\nresults of both studies highlight a potential for more detailed and specific\nresponses with the Composites Guide and the Equipment Assistant."
                },
                "authors": [
                    {
                        "name": "Gunnika Kapoor"
                    },
                    {
                        "name": "Komal Chawla"
                    },
                    {
                        "name": "Tirthankar Ghosal"
                    },
                    {
                        "name": "Kris Villez"
                    },
                    {
                        "name": "Dan Coughlin"
                    },
                    {
                        "name": "Tyden Rucker"
                    },
                    {
                        "name": "Vincent Paquit"
                    },
                    {
                        "name": "Soydan Ozcan"
                    },
                    {
                        "name": "Seokpum Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seokpum Kim"
                },
                "author": "Seokpum Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06734v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18183v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18183v2",
                "updated": "2025-09-08T14:25:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    25,
                    45,
                    0,
                    251,
                    0
                ],
                "published": "2025-08-25T16:36:36Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    36,
                    36,
                    0,
                    237,
                    0
                ],
                "title": "Leveraging Large Language Models for Accurate Sign Language Translation\n  in Low-Resource Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Accurate Sign Language Translation\n  in Low-Resource Scenarios"
                },
                "summary": "Translating natural languages into sign languages is a highly complex and\nunderexplored task. Despite growing interest in accessibility and inclusivity,\nthe development of robust translation systems remains hindered by the limited\navailability of parallel corpora which align natural language with sign\nlanguage data. Existing methods often struggle to generalize in these\ndata-scarce environments, as the few datasets available are typically\ndomain-specific, lack standardization, or fail to capture the full linguistic\nrichness of sign languages. To address this limitation, we propose Advanced Use\nof LLMs for Sign Language Translation (AulSign), a novel method that leverages\nLarge Language Models via dynamic prompting and in-context learning with sample\nselection and subsequent sign association. Despite their impressive abilities\nin processing text, LLMs lack intrinsic knowledge of sign languages; therefore,\nthey are unable to natively perform this kind of translation. To overcome this\nlimitation, we associate the signs with compact descriptions in natural\nlanguage and instruct the model to use them. We evaluate our method on both\nEnglish and Italian languages using SignBank+, a recognized benchmark in the\nfield, as well as the Italian LaCAM CNR-ISTC dataset. We demonstrate superior\nperformance compared to state-of-the-art models in low-data scenario. Our\nfindings demonstrate the effectiveness of AulSign, with the potential to\nenhance accessibility and inclusivity in communication technologies for\nunderrepresented linguistic communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating natural languages into sign languages is a highly complex and\nunderexplored task. Despite growing interest in accessibility and inclusivity,\nthe development of robust translation systems remains hindered by the limited\navailability of parallel corpora which align natural language with sign\nlanguage data. Existing methods often struggle to generalize in these\ndata-scarce environments, as the few datasets available are typically\ndomain-specific, lack standardization, or fail to capture the full linguistic\nrichness of sign languages. To address this limitation, we propose Advanced Use\nof LLMs for Sign Language Translation (AulSign), a novel method that leverages\nLarge Language Models via dynamic prompting and in-context learning with sample\nselection and subsequent sign association. Despite their impressive abilities\nin processing text, LLMs lack intrinsic knowledge of sign languages; therefore,\nthey are unable to natively perform this kind of translation. To overcome this\nlimitation, we associate the signs with compact descriptions in natural\nlanguage and instruct the model to use them. We evaluate our method on both\nEnglish and Italian languages using SignBank+, a recognized benchmark in the\nfield, as well as the Italian LaCAM CNR-ISTC dataset. We demonstrate superior\nperformance compared to state-of-the-art models in low-data scenario. Our\nfindings demonstrate the effectiveness of AulSign, with the potential to\nenhance accessibility and inclusivity in communication technologies for\nunderrepresented linguistic communities."
                },
                "authors": [
                    {
                        "name": "Luana Bulla"
                    },
                    {
                        "name": "Gabriele Tuccio"
                    },
                    {
                        "name": "Misael Mongiovì"
                    },
                    {
                        "name": "Aldo Gangemi"
                    }
                ],
                "author_detail": {
                    "name": "Aldo Gangemi"
                },
                "author": "Aldo Gangemi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18183v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18183v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07642v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07642v2",
                "updated": "2025-09-08T14:22:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    22,
                    50,
                    0,
                    251,
                    0
                ],
                "published": "2025-06-09T11:07:55Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    11,
                    7,
                    55,
                    0,
                    160,
                    0
                ],
                "title": "TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient\n  LLM-based Scientific Peer Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient\n  LLM-based Scientific Peer Review"
                },
                "summary": "While Large Language Models (LLMs) have shown significant potential in\nassisting peer review, current methods often struggle to generate thorough and\ninsightful reviews while maintaining efficiency. In this paper, we propose\nTreeReview, a novel framework that models paper review as a hierarchical and\nbidirectional question-answering process. TreeReview first constructs a tree of\nreview questions by recursively decomposing high-level questions into\nfine-grained sub-questions and then resolves the question tree by iteratively\naggregating answers from leaf to root to get the final review. Crucially, we\nincorporate a dynamic question expansion mechanism to enable deeper probing by\ngenerating follow-up questions when needed. We construct a benchmark derived\nfrom ICLR and NeurIPS venues to evaluate our method on full review generation\nand actionable feedback comments generation tasks. Experimental results of both\nLLM-based and human evaluation show that TreeReview outperforms strong\nbaselines in providing comprehensive, in-depth, and expert-aligned review\nfeedback, while reducing LLM token usage by up to 80% compared to\ncomputationally intensive approaches. Our code and benchmark dataset are\navailable at https://github.com/YuanChang98/tree-review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have shown significant potential in\nassisting peer review, current methods often struggle to generate thorough and\ninsightful reviews while maintaining efficiency. In this paper, we propose\nTreeReview, a novel framework that models paper review as a hierarchical and\nbidirectional question-answering process. TreeReview first constructs a tree of\nreview questions by recursively decomposing high-level questions into\nfine-grained sub-questions and then resolves the question tree by iteratively\naggregating answers from leaf to root to get the final review. Crucially, we\nincorporate a dynamic question expansion mechanism to enable deeper probing by\ngenerating follow-up questions when needed. We construct a benchmark derived\nfrom ICLR and NeurIPS venues to evaluate our method on full review generation\nand actionable feedback comments generation tasks. Experimental results of both\nLLM-based and human evaluation show that TreeReview outperforms strong\nbaselines in providing comprehensive, in-depth, and expert-aligned review\nfeedback, while reducing LLM token usage by up to 80% compared to\ncomputationally intensive approaches. Our code and benchmark dataset are\navailable at https://github.com/YuanChang98/tree-review."
                },
                "authors": [
                    {
                        "name": "Yuan Chang"
                    },
                    {
                        "name": "Ziyue Li"
                    },
                    {
                        "name": "Hengyuan Zhang"
                    },
                    {
                        "name": "Yuanbo Kong"
                    },
                    {
                        "name": "Yanru Wu"
                    },
                    {
                        "name": "Hayden Kwok-Hay So"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Liya Zhu"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "Accepted to EMNLP2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07642v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07642v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06723v1",
                "updated": "2025-09-08T14:21:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    21,
                    45,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T14:21:45Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    21,
                    45,
                    0,
                    251,
                    0
                ],
                "title": "Zero-shot 3D-Aware Trajectory-Guided image-to-video generation via\n  Test-Time Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot 3D-Aware Trajectory-Guided image-to-video generation via\n  Test-Time Training"
                },
                "summary": "Trajectory-Guided image-to-video (I2V) generation aims to synthesize videos\nthat adhere to user-specified motion instructions. Existing methods typically\nrely on computationally expensive fine-tuning on scarce annotated datasets.\nAlthough some zero-shot methods attempt to trajectory control in the latent\nspace, they may yield unrealistic motion by neglecting 3D perspective and\ncreating a misalignment between the manipulated latents and the network's noise\npredictions. To address these challenges, we introduce Zo3T, a novel zero-shot\ntest-time-training framework for trajectory-guided generation with three core\ninnovations: First, we incorporate a 3D-Aware Kinematic Projection, leveraging\ninferring scene depth to derive perspective-correct affine transformations for\ntarget regions. Second, we introduce Trajectory-Guided Test-Time LoRA, a\nmechanism that dynamically injects and optimizes ephemeral LoRA adapters into\nthe denoising network alongside the latent state. Driven by a regional feature\nconsistency loss, this co-adaptation effectively enforces motion constraints\nwhile allowing the pre-trained model to locally adapt its internal\nrepresentations to the manipulated latent, thereby ensuring generative fidelity\nand on-manifold adherence. Finally, we develop Guidance Field Rectification,\nwhich refines the denoising evolutionary path by optimizing the conditional\nguidance field through a one-step lookahead strategy, ensuring efficient\ngenerative progression towards the target trajectory. Zo3T significantly\nenhances 3D realism and motion accuracy in trajectory-controlled I2V\ngeneration, demonstrating superior performance over existing training-based and\nzero-shot approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory-Guided image-to-video (I2V) generation aims to synthesize videos\nthat adhere to user-specified motion instructions. Existing methods typically\nrely on computationally expensive fine-tuning on scarce annotated datasets.\nAlthough some zero-shot methods attempt to trajectory control in the latent\nspace, they may yield unrealistic motion by neglecting 3D perspective and\ncreating a misalignment between the manipulated latents and the network's noise\npredictions. To address these challenges, we introduce Zo3T, a novel zero-shot\ntest-time-training framework for trajectory-guided generation with three core\ninnovations: First, we incorporate a 3D-Aware Kinematic Projection, leveraging\ninferring scene depth to derive perspective-correct affine transformations for\ntarget regions. Second, we introduce Trajectory-Guided Test-Time LoRA, a\nmechanism that dynamically injects and optimizes ephemeral LoRA adapters into\nthe denoising network alongside the latent state. Driven by a regional feature\nconsistency loss, this co-adaptation effectively enforces motion constraints\nwhile allowing the pre-trained model to locally adapt its internal\nrepresentations to the manipulated latent, thereby ensuring generative fidelity\nand on-manifold adherence. Finally, we develop Guidance Field Rectification,\nwhich refines the denoising evolutionary path by optimizing the conditional\nguidance field through a one-step lookahead strategy, ensuring efficient\ngenerative progression towards the target trajectory. Zo3T significantly\nenhances 3D realism and motion accuracy in trajectory-controlled I2V\ngeneration, demonstrating superior performance over existing training-based and\nzero-shot approaches."
                },
                "authors": [
                    {
                        "name": "Ruicheng Zhang"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Zunnan Xu"
                    },
                    {
                        "name": "Zihao Liu"
                    },
                    {
                        "name": "Jiehui Huang"
                    },
                    {
                        "name": "Mingyang Zhang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Xiu Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiu Li"
                },
                "author": "Xiu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06721v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06721v1",
                "updated": "2025-09-08T14:18:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    18,
                    47,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T14:18:47Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    18,
                    47,
                    0,
                    251,
                    0
                ],
                "title": "A Spatial Gap in the Sky Distribution of Fast Radio Burst Detections\n  Coinciding with Galactic Plasma Overdensities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Spatial Gap in the Sky Distribution of Fast Radio Burst Detections\n  Coinciding with Galactic Plasma Overdensities"
                },
                "summary": "We analyze the positional and morphological properties of about 3600 unique\nfast radio burst (FRB) sources reported in the second FRB catalog generated by\nthe Canadian Hydrogen Intensity Mapping Experiment (CHIME) telescope. We find a\ntwo-dimensional dependence of FRB detections on sky position, and identify a\nsignificant absence of detections in a roughly circular region centered at\nGalactic coordinates (77.7$^\\circ$, 0.9$^\\circ$), spanning an area of 216.2\ndeg$^2$. This detection gap spatially coincides with the Cygnus X region $--$ a\nplasma-rich star-forming region in the Milky Way. This lack of FRB detections\nis most likely the result of increased sky temperature and strong multi-path\nscattering by turbulent ionized plasma, which broadens the FRB signals beyond\ndetectability in the CHIME band. Our simulations yield a mean of 6 expected FRB\ndetections within the gap when accounting for the elevated sky temperature in\nthe direction of the detection gap. We infer that a lower limit of the maximum\nscattering timescale $\\tau_{\\rm sc,\\, 1\\,GHz} \\geq 4.13$ ms is sufficient to\nsuppress the observed signal-to-noise ratio of all FRBs. In addition to Cygnus\nX, a similar suppression is seen in Catalog 2 along other high-emission measure\n(EM) sightlines ( i.e., EM$\\geq$2900 pc cm$^{-6}$), further supporting a\nbroader trend of suppression due to Galactic scattering. Future very long\nbaseline interferometry (VLBI) measurements of scattering disks with CHIME\nOutriggers could help confirm our interpretation. Our work highlights the\nsubstantial impact of the ionized and turbulent Galactic interstellar medium on\nthe detectability of FRBs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyze the positional and morphological properties of about 3600 unique\nfast radio burst (FRB) sources reported in the second FRB catalog generated by\nthe Canadian Hydrogen Intensity Mapping Experiment (CHIME) telescope. We find a\ntwo-dimensional dependence of FRB detections on sky position, and identify a\nsignificant absence of detections in a roughly circular region centered at\nGalactic coordinates (77.7$^\\circ$, 0.9$^\\circ$), spanning an area of 216.2\ndeg$^2$. This detection gap spatially coincides with the Cygnus X region $--$ a\nplasma-rich star-forming region in the Milky Way. This lack of FRB detections\nis most likely the result of increased sky temperature and strong multi-path\nscattering by turbulent ionized plasma, which broadens the FRB signals beyond\ndetectability in the CHIME band. Our simulations yield a mean of 6 expected FRB\ndetections within the gap when accounting for the elevated sky temperature in\nthe direction of the detection gap. We infer that a lower limit of the maximum\nscattering timescale $\\tau_{\\rm sc,\\, 1\\,GHz} \\geq 4.13$ ms is sufficient to\nsuppress the observed signal-to-noise ratio of all FRBs. In addition to Cygnus\nX, a similar suppression is seen in Catalog 2 along other high-emission measure\n(EM) sightlines ( i.e., EM$\\geq$2900 pc cm$^{-6}$), further supporting a\nbroader trend of suppression due to Galactic scattering. Future very long\nbaseline interferometry (VLBI) measurements of scattering disks with CHIME\nOutriggers could help confirm our interpretation. Our work highlights the\nsubstantial impact of the ionized and turbulent Galactic interstellar medium on\nthe detectability of FRBs."
                },
                "authors": [
                    {
                        "name": "Swarali Shivraj Patil"
                    },
                    {
                        "name": "Robert A. Main"
                    },
                    {
                        "name": "Emmanuel Fonseca"
                    },
                    {
                        "name": "Kyle McGregor"
                    },
                    {
                        "name": "B. M. Gaensler"
                    },
                    {
                        "name": "Charanjot Brar"
                    },
                    {
                        "name": "Amanda M. Cook"
                    },
                    {
                        "name": "Alice P. Curtin"
                    },
                    {
                        "name": "Gwendolyn Eadie"
                    },
                    {
                        "name": "Ronniy Joseph"
                    },
                    {
                        "name": "Lordrick Kahinga"
                    },
                    {
                        "name": "Victoria Kaspi"
                    },
                    {
                        "name": "Afrokk Khan"
                    },
                    {
                        "name": "Bikash Kharel"
                    },
                    {
                        "name": "Adam E. Lanman"
                    },
                    {
                        "name": "Calvin Leung"
                    },
                    {
                        "name": "Kiyoshi W. Masui"
                    },
                    {
                        "name": "Mason Ng"
                    },
                    {
                        "name": "Kenzie Nimmo"
                    },
                    {
                        "name": "Ayush Pandhi"
                    },
                    {
                        "name": "Aaron B. Pearlman"
                    },
                    {
                        "name": "Ziggy Pleunis"
                    },
                    {
                        "name": "Mawson W. Sammons"
                    },
                    {
                        "name": "Ketan R. Sand"
                    },
                    {
                        "name": "Paul Scholz"
                    },
                    {
                        "name": "Kaitlyn Shin"
                    },
                    {
                        "name": "Seth R. Siegel"
                    },
                    {
                        "name": "Kendrick Smith"
                    }
                ],
                "author_detail": {
                    "name": "Kendrick Smith"
                },
                "author": "Kendrick Smith",
                "arxiv_comment": "26 pages, 8 figures. Submitted to ApJL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06721v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06721v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17118v2",
                "updated": "2025-09-08T14:13:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    13,
                    8,
                    0,
                    251,
                    0
                ],
                "published": "2024-10-22T15:49:53Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    15,
                    49,
                    53,
                    1,
                    296,
                    0
                ],
                "title": "Learning Load Balancing with GNN in MPTCP-Enabled Heterogeneous Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Load Balancing with GNN in MPTCP-Enabled Heterogeneous Networks"
                },
                "summary": "Hybrid light fidelity (LiFi) and wireless fidelity (WiFi) networks are a\npromising paradigm of heterogeneous network (HetNet), attributed to the\ncomplementary physical properties of optical spectra and radio frequency.\nHowever, the current development of such HetNets is mostly bottlenecked by the\nexisting transmission control protocol (TCP), which restricts the user\nequipment (UE) to connecting one access point (AP) at a time. While the ongoing\ninvestigation on multipath TCP (MPTCP) can bring significant benefits, it\ncomplicates the network topology of HetNets, making the existing load balancing\n(LB) learning models less effective. Driven by this, we propose a graph neural\nnetwork (GNN)-based model to tackle the LB problem for MPTCP-enabled HetNets,\nwhich results in a partial mesh topology. Such a topology can be modeled as a\ngraph, with the channel state information and data rate requirement embedded as\nnode features, while the LB solutions are deemed as edge labels. Compared to\nthe conventional deep neural network (DNN), the proposed GNN-based model\nexhibits two key strengths: i) it can better interpret a complex network\ntopology; and ii) it can handle various numbers of APs and UEs with a single\ntrained model. Simulation results show that against the traditional\noptimisation method, the proposed learning model can achieve near-optimal\nthroughput within a gap of 11.5%, while reducing the inference time by 4 orders\nof magnitude. In contrast to the DNN model, the new method can improve the\nnetwork throughput by up to 21.7%, at a similar inference time level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid light fidelity (LiFi) and wireless fidelity (WiFi) networks are a\npromising paradigm of heterogeneous network (HetNet), attributed to the\ncomplementary physical properties of optical spectra and radio frequency.\nHowever, the current development of such HetNets is mostly bottlenecked by the\nexisting transmission control protocol (TCP), which restricts the user\nequipment (UE) to connecting one access point (AP) at a time. While the ongoing\ninvestigation on multipath TCP (MPTCP) can bring significant benefits, it\ncomplicates the network topology of HetNets, making the existing load balancing\n(LB) learning models less effective. Driven by this, we propose a graph neural\nnetwork (GNN)-based model to tackle the LB problem for MPTCP-enabled HetNets,\nwhich results in a partial mesh topology. Such a topology can be modeled as a\ngraph, with the channel state information and data rate requirement embedded as\nnode features, while the LB solutions are deemed as edge labels. Compared to\nthe conventional deep neural network (DNN), the proposed GNN-based model\nexhibits two key strengths: i) it can better interpret a complex network\ntopology; and ii) it can handle various numbers of APs and UEs with a single\ntrained model. Simulation results show that against the traditional\noptimisation method, the proposed learning model can achieve near-optimal\nthroughput within a gap of 11.5%, while reducing the inference time by 4 orders\nof magnitude. In contrast to the DNN model, the new method can improve the\nnetwork throughput by up to 21.7%, at a similar inference time level."
                },
                "authors": [
                    {
                        "name": "Han Ji"
                    },
                    {
                        "name": "Xiping Wu"
                    },
                    {
                        "name": "Zhihong Zeng"
                    },
                    {
                        "name": "Chen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chen Chen"
                },
                "author": "Chen Chen",
                "arxiv_comment": "We would like to withdraw this submission because it contains several\n  errors that need substantial revision. We plan to prepare a corrected and\n  improved version, which will be submitted as a new manuscript at a later\n  stage",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06716v1",
                "updated": "2025-09-08T14:11:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    11,
                    35,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T14:11:35Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    11,
                    35,
                    0,
                    251,
                    0
                ],
                "title": "Efficiently Ranking Software Variants with Minimal Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Ranking Software Variants with Minimal Benchmarks"
                },
                "summary": "Benchmarking is a common practice in software engineering to assess the\nqualities and performance of software variants, coming from multiple competing\nsystems or from configurations of the same system. Benchmarks are used notably\nto compare and understand variant performance, fine-tune software, detect\nregressions, or design new software systems. The execution of benchmarks to get\na complete picture of software variants is highly costly in terms of\ncomputational resources and time. In this paper, we propose a novel approach\nfor reducing benchmarks while maintaining stable rankings, using test suite\noptimization techniques. That is, we remove instances from the benchmarks while\ntrying to keep the same rankings of the variants on all tests. Our method,\nBISection Sampling, BISS, strategically retains the most critical tests and\napplies a novel divide-and-conquer approach to efficiently sample among\nrelevant remaining tests. We experiment with datasets and use cases from LLM\nleaderboards, SAT competitions, and configurable systems for performance\nmodeling. Our results show that our method outperforms baselines even when\noperating on a subset of variants. Using BISS, we reduce the computational cost\nof the benchmarks on average to 44% and on more than half the benchmarks by up\nto 99% without loss in ranking stability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking is a common practice in software engineering to assess the\nqualities and performance of software variants, coming from multiple competing\nsystems or from configurations of the same system. Benchmarks are used notably\nto compare and understand variant performance, fine-tune software, detect\nregressions, or design new software systems. The execution of benchmarks to get\na complete picture of software variants is highly costly in terms of\ncomputational resources and time. In this paper, we propose a novel approach\nfor reducing benchmarks while maintaining stable rankings, using test suite\noptimization techniques. That is, we remove instances from the benchmarks while\ntrying to keep the same rankings of the variants on all tests. Our method,\nBISection Sampling, BISS, strategically retains the most critical tests and\napplies a novel divide-and-conquer approach to efficiently sample among\nrelevant remaining tests. We experiment with datasets and use cases from LLM\nleaderboards, SAT competitions, and configurable systems for performance\nmodeling. Our results show that our method outperforms baselines even when\noperating on a subset of variants. Using BISS, we reduce the computational cost\nof the benchmarks on average to 44% and on more than half the benchmarks by up\nto 99% without loss in ranking stability."
                },
                "authors": [
                    {
                        "name": "Théo Matricon"
                    },
                    {
                        "name": "Mathieu Acher"
                    },
                    {
                        "name": "Helge Spieker"
                    },
                    {
                        "name": "Arnaud Gotlieb"
                    }
                ],
                "author_detail": {
                    "name": "Arnaud Gotlieb"
                },
                "author": "Arnaud Gotlieb",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06714v1",
                "updated": "2025-09-08T14:09:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    9,
                    33,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T14:09:33Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    9,
                    33,
                    0,
                    251,
                    0
                ],
                "title": "RT-HCP: Dealing with Inference Delays and Sample Efficiency to Learn\n  Directly on Robotic Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RT-HCP: Dealing with Inference Delays and Sample Efficiency to Learn\n  Directly on Robotic Platforms"
                },
                "summary": "Learning a controller directly on the robot requires extreme sample\nefficiency. Model-based reinforcement learning (RL) methods are the most sample\nefficient, but they often suffer from a too long inference time to meet the\nrobot control frequency requirements. In this paper, we address the sample\nefficiency and inference time challenges with two contributions. First, we\ndefine a general framework to deal with inference delays where the slow\ninference robot controller provides a sequence of actions to feed the\ncontrol-hungry robotic platform without execution gaps. Then, we compare\nseveral RL algorithms in the light of this framework and propose RT-HCP, an\nalgorithm that offers an excellent trade-off between performance, sample\nefficiency and inference time. We validate the superiority of RT-HCP with\nexperiments where we learn a controller directly on a simple but high frequency\nFURUTA pendulum platform. Code: github.com/elasriz/RTHCP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning a controller directly on the robot requires extreme sample\nefficiency. Model-based reinforcement learning (RL) methods are the most sample\nefficient, but they often suffer from a too long inference time to meet the\nrobot control frequency requirements. In this paper, we address the sample\nefficiency and inference time challenges with two contributions. First, we\ndefine a general framework to deal with inference delays where the slow\ninference robot controller provides a sequence of actions to feed the\ncontrol-hungry robotic platform without execution gaps. Then, we compare\nseveral RL algorithms in the light of this framework and propose RT-HCP, an\nalgorithm that offers an excellent trade-off between performance, sample\nefficiency and inference time. We validate the superiority of RT-HCP with\nexperiments where we learn a controller directly on a simple but high frequency\nFURUTA pendulum platform. Code: github.com/elasriz/RTHCP"
                },
                "authors": [
                    {
                        "name": "Zakariae El Asri"
                    },
                    {
                        "name": "Ibrahim Laiche"
                    },
                    {
                        "name": "Clément Rambour"
                    },
                    {
                        "name": "Olivier Sigaud"
                    },
                    {
                        "name": "Nicolas Thome"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Thome"
                },
                "author": "Nicolas Thome",
                "arxiv_comment": "IROS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09761v2",
                "updated": "2025-09-08T14:02:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    2,
                    8,
                    0,
                    251,
                    0
                ],
                "published": "2025-05-14T19:48:28Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    19,
                    48,
                    28,
                    2,
                    134,
                    0
                ],
                "title": "Sequential Monte Carlo Squared for online inference in stochastic\n  epidemic models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Monte Carlo Squared for online inference in stochastic\n  epidemic models"
                },
                "summary": "Effective epidemic modeling and surveillance require computationally\nefficient methods that can continuously update estimates as new data becomes\navailable. This paper explores the application of an online variant of\nSequential Monte Carlo Squared (O-SMC$^2$) to the stochastic\nSusceptible-Exposed-Infectious-Removed (SEIR) model for real-time epidemic\ntracking. The particularity of O-SMC$^2$ lies in its ability to update the\nparameters using a particle Metropolis-Hastings kernel, ensuring that the\ntarget distribution remains invariant while only utilizing a fixed window of\nrecent observations. This feature enables timely parameter updates and\nsignificantly enhances computational efficiency compared to the standard\nSMC$^2$, which processes the entire dataset. First, we demonstrate the\nefficiency of O-SMC$^2$ on simulated data, where both the parameters and the\nobservation process are known. We then apply the method to a real-world\nCOVID-19 dataset from Ireland, successfully tracking the epidemic trajectory\nand estimating the time-dependent reproduction number of the disease. Our\nresults show that O-SMC$^2$ provides highly accurate online estimates of both\nstatic and dynamic epidemiological parameters while substantially reducing\ncomputational costs. These findings highlight the potential of O-SMC$^2$ for\nreal-time epidemic monitoring and supporting adaptive public health\ninterventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective epidemic modeling and surveillance require computationally\nefficient methods that can continuously update estimates as new data becomes\navailable. This paper explores the application of an online variant of\nSequential Monte Carlo Squared (O-SMC$^2$) to the stochastic\nSusceptible-Exposed-Infectious-Removed (SEIR) model for real-time epidemic\ntracking. The particularity of O-SMC$^2$ lies in its ability to update the\nparameters using a particle Metropolis-Hastings kernel, ensuring that the\ntarget distribution remains invariant while only utilizing a fixed window of\nrecent observations. This feature enables timely parameter updates and\nsignificantly enhances computational efficiency compared to the standard\nSMC$^2$, which processes the entire dataset. First, we demonstrate the\nefficiency of O-SMC$^2$ on simulated data, where both the parameters and the\nobservation process are known. We then apply the method to a real-world\nCOVID-19 dataset from Ireland, successfully tracking the epidemic trajectory\nand estimating the time-dependent reproduction number of the disease. Our\nresults show that O-SMC$^2$ provides highly accurate online estimates of both\nstatic and dynamic epidemiological parameters while substantially reducing\ncomputational costs. These findings highlight the potential of O-SMC$^2$ for\nreal-time epidemic monitoring and supporting adaptive public health\ninterventions."
                },
                "authors": [
                    {
                        "name": "Dhorasso Temfack"
                    },
                    {
                        "name": "Jason Wyse"
                    }
                ],
                "author_detail": {
                    "name": "Jason Wyse"
                },
                "author": "Jason Wyse",
                "arxiv_doi": "10.1016/j.epidem.2025.100847",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.epidem.2025.100847",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.09761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Temfack, D., Wyse, J., 2025. Sequential monte carlo squared for\n  online infer- ence in stochastic epidemic models. Epidemics 52, 100847",
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06704v1",
                "updated": "2025-09-08T13:59:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    59,
                    34,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T13:59:34Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    59,
                    34,
                    0,
                    251,
                    0
                ],
                "title": "Will Annotators Disagree? Identifying Subjectivity in Value-Laden\n  Arguments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Will Annotators Disagree? Identifying Subjectivity in Value-Laden\n  Arguments"
                },
                "summary": "Aggregating multiple annotations into a single ground truth label may hide\nvaluable insights into annotator disagreement, particularly in tasks where\nsubjectivity plays a crucial role. In this work, we explore methods for\nidentifying subjectivity in recognizing the human values that motivate\narguments. We evaluate two main approaches: inferring subjectivity through\nvalue prediction vs. directly identifying subjectivity. Our experiments show\nthat direct subjectivity identification significantly improves the model\nperformance of flagging subjective arguments. Furthermore, combining\ncontrastive loss with binary cross-entropy loss does not improve performance\nbut reduces the dependency on per-label subjectivity. Our proposed methods can\nhelp identify arguments that individuals may interpret differently, fostering a\nmore nuanced annotation process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aggregating multiple annotations into a single ground truth label may hide\nvaluable insights into annotator disagreement, particularly in tasks where\nsubjectivity plays a crucial role. In this work, we explore methods for\nidentifying subjectivity in recognizing the human values that motivate\narguments. We evaluate two main approaches: inferring subjectivity through\nvalue prediction vs. directly identifying subjectivity. Our experiments show\nthat direct subjectivity identification significantly improves the model\nperformance of flagging subjective arguments. Furthermore, combining\ncontrastive loss with binary cross-entropy loss does not improve performance\nbut reduces the dependency on per-label subjectivity. Our proposed methods can\nhelp identify arguments that individuals may interpret differently, fostering a\nmore nuanced annotation process."
                },
                "authors": [
                    {
                        "name": "Amir Homayounirad"
                    },
                    {
                        "name": "Enrico Liscio"
                    },
                    {
                        "name": "Tong Wang"
                    },
                    {
                        "name": "Catholijn M. Jonker"
                    },
                    {
                        "name": "Luciano C. Siebert"
                    }
                ],
                "author_detail": {
                    "name": "Luciano C. Siebert"
                },
                "author": "Luciano C. Siebert",
                "arxiv_comment": "Accepted at Findings of EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06701v1",
                "updated": "2025-09-08T13:55:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    55,
                    1,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T13:55:01Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    55,
                    1,
                    0,
                    251,
                    0
                ],
                "title": "Probabilistic Modeling of Latent Agentic Substructures in Deep Neural\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Modeling of Latent Agentic Substructures in Deep Neural\n  Networks"
                },
                "summary": "We develop a theory of intelligent agency grounded in probabilistic modeling\nfor neural models. Agents are represented as outcome distributions with\nepistemic utility given by log score, and compositions are defined through\nweighted logarithmic pooling that strictly improves every member's welfare. We\nprove that strict unanimity is impossible under linear pooling or in binary\noutcome spaces, but possible with three or more outcomes. Our framework admits\nrecursive structure via cloning invariance, continuity, and openness, while\ntilt-based analysis rules out trivial duplication. Finally, we formalize an\nagentic alignment phenomenon in LLMs using our theory: eliciting a benevolent\npersona (\"Luigi'\") induces an antagonistic counterpart (\"Waluigi\"), while a\nmanifest-then-suppress Waluigi strategy yields strictly larger first-order\nmisalignment reduction than pure Luigi reinforcement alone. These results\nclarify how developing a principled mathematical framework for how subagents\ncan coalesce into coherent higher-level entities provides novel implications\nfor alignment in agentic AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a theory of intelligent agency grounded in probabilistic modeling\nfor neural models. Agents are represented as outcome distributions with\nepistemic utility given by log score, and compositions are defined through\nweighted logarithmic pooling that strictly improves every member's welfare. We\nprove that strict unanimity is impossible under linear pooling or in binary\noutcome spaces, but possible with three or more outcomes. Our framework admits\nrecursive structure via cloning invariance, continuity, and openness, while\ntilt-based analysis rules out trivial duplication. Finally, we formalize an\nagentic alignment phenomenon in LLMs using our theory: eliciting a benevolent\npersona (\"Luigi'\") induces an antagonistic counterpart (\"Waluigi\"), while a\nmanifest-then-suppress Waluigi strategy yields strictly larger first-order\nmisalignment reduction than pure Luigi reinforcement alone. These results\nclarify how developing a principled mathematical framework for how subagents\ncan coalesce into coherent higher-level entities provides novel implications\nfor alignment in agentic AI systems."
                },
                "authors": [
                    {
                        "name": "Su Hyeong Lee"
                    },
                    {
                        "name": "Risi Kondor"
                    },
                    {
                        "name": "Richard Ngo"
                    }
                ],
                "author_detail": {
                    "name": "Richard Ngo"
                },
                "author": "Richard Ngo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06700v1",
                "updated": "2025-09-08T13:54:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    54,
                    42,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T13:54:42Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    54,
                    42,
                    0,
                    251,
                    0
                ],
                "title": "Sovereign AI for 6G: Towards the Future of AI-Native Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sovereign AI for 6G: Towards the Future of AI-Native Networks"
                },
                "summary": "The advent of Generative Artificial Intelligence (GenAI), Large Language\nModels (LLMs), and Large Telecom Models (LTM) significantly reshapes mobile\nnetworks, especially as the telecom industry transitions from 5G's\ncloud-centric to AI-native 6G architectures. This transition unlocks\nunprecedented capabilities in real-time automation, semantic networking, and\nautonomous service orchestration. However, it introduces critical risks related\nto data sovereignty, security, explainability, and regulatory compliance\nespecially when AI models are trained, deployed, or governed externally. This\npaper introduces the concept of `Sovereign AI' as a strategic imperative for\n6G, proposing architectural, operational, and governance frameworks that enable\nnational or operator-level control over AI development, deployment, and\nlife-cycle management. Focusing on O-RAN architecture, we explore how sovereign\nAI-based xApps and rApps can be deployed Near-RT and Non-RT RICs to ensure\npolicy-aligned control, secure model updates, and federated learning across\ntrusted infrastructure. We analyse global strategies, technical enablers, and\nchallenges across safety, talent, and model governance. Our findings underscore\nthat Sovereign AI is not just a regulatory necessity but a foundational pillar\nfor secure, resilient, and ethically-aligned 6G networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Generative Artificial Intelligence (GenAI), Large Language\nModels (LLMs), and Large Telecom Models (LTM) significantly reshapes mobile\nnetworks, especially as the telecom industry transitions from 5G's\ncloud-centric to AI-native 6G architectures. This transition unlocks\nunprecedented capabilities in real-time automation, semantic networking, and\nautonomous service orchestration. However, it introduces critical risks related\nto data sovereignty, security, explainability, and regulatory compliance\nespecially when AI models are trained, deployed, or governed externally. This\npaper introduces the concept of `Sovereign AI' as a strategic imperative for\n6G, proposing architectural, operational, and governance frameworks that enable\nnational or operator-level control over AI development, deployment, and\nlife-cycle management. Focusing on O-RAN architecture, we explore how sovereign\nAI-based xApps and rApps can be deployed Near-RT and Non-RT RICs to ensure\npolicy-aligned control, secure model updates, and federated learning across\ntrusted infrastructure. We analyse global strategies, technical enablers, and\nchallenges across safety, talent, and model governance. Our findings underscore\nthat Sovereign AI is not just a regulatory necessity but a foundational pillar\nfor secure, resilient, and ethically-aligned 6G networks."
                },
                "authors": [
                    {
                        "name": "Swarna Bindu Chetty"
                    },
                    {
                        "name": "David Grace"
                    },
                    {
                        "name": "Simon Saunders"
                    },
                    {
                        "name": "Paul Harris"
                    },
                    {
                        "name": "Eirini Eleni Tsiropoulou"
                    },
                    {
                        "name": "Tony Quek"
                    },
                    {
                        "name": "Hamed Ahmadi"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Ahmadi"
                },
                "author": "Hamed Ahmadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06693v1",
                "updated": "2025-09-08T13:47:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    47,
                    1,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T13:47:01Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    47,
                    1,
                    0,
                    251,
                    0
                ],
                "title": "STAGE: Segmentation-oriented Industrial Anomaly Synthesis via Graded\n  Diffusion with Explicit Mask Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAGE: Segmentation-oriented Industrial Anomaly Synthesis via Graded\n  Diffusion with Explicit Mask Alignment"
                },
                "summary": "Segmentation-oriented Industrial Anomaly Synthesis (SIAS) plays a pivotal\nrole in enhancing the performance of downstream anomaly segmentation, as it\nprovides an effective means of expanding abnormal data. However, existing SIAS\nmethods face several critical limitations: (i) the synthesized anomalies often\nlack intricate texture details and fail to align precisely with the surrounding\nbackground, and (ii) they struggle to generate fine-grained, pixel-level\nanomalies. To address these challenges, we propose Segmentation-oriented\nAnomaly synthesis via Graded diffusion with Explicit mask alignment, termed\nSTAGE. STAGE introduces a novel anomaly inference strategy that incorporates\nclean background information as a prior to guide the denoising distribution,\nenabling the model to more effectively distinguish and highlight abnormal\nforegrounds. Furthermore, it employs a graded diffusion framework with an\nanomaly-only branch to explicitly record local anomalies during both the\nforward and reverse processes, ensuring that subtle anomalies are not\noverlooked. Finally, STAGE incorporates the explicit mask alignment (EMA)\nstrategy to progressively align the synthesized anomalies with the background,\nresulting in context-consistent and structurally coherent generations.\nExtensive experiments on the MVTec and BTAD datasets demonstrate that STAGE\nachieves state-of-the-art performance in SIAS, which in turn enhances\ndownstream anomaly segmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Segmentation-oriented Industrial Anomaly Synthesis (SIAS) plays a pivotal\nrole in enhancing the performance of downstream anomaly segmentation, as it\nprovides an effective means of expanding abnormal data. However, existing SIAS\nmethods face several critical limitations: (i) the synthesized anomalies often\nlack intricate texture details and fail to align precisely with the surrounding\nbackground, and (ii) they struggle to generate fine-grained, pixel-level\nanomalies. To address these challenges, we propose Segmentation-oriented\nAnomaly synthesis via Graded diffusion with Explicit mask alignment, termed\nSTAGE. STAGE introduces a novel anomaly inference strategy that incorporates\nclean background information as a prior to guide the denoising distribution,\nenabling the model to more effectively distinguish and highlight abnormal\nforegrounds. Furthermore, it employs a graded diffusion framework with an\nanomaly-only branch to explicitly record local anomalies during both the\nforward and reverse processes, ensuring that subtle anomalies are not\noverlooked. Finally, STAGE incorporates the explicit mask alignment (EMA)\nstrategy to progressively align the synthesized anomalies with the background,\nresulting in context-consistent and structurally coherent generations.\nExtensive experiments on the MVTec and BTAD datasets demonstrate that STAGE\nachieves state-of-the-art performance in SIAS, which in turn enhances\ndownstream anomaly segmentation."
                },
                "authors": [
                    {
                        "name": "Xichen Xu"
                    },
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Jinbao Wang"
                    },
                    {
                        "name": "Qunyi Zhang"
                    },
                    {
                        "name": "Xiaoning Lei"
                    },
                    {
                        "name": "Guoyang Xie"
                    },
                    {
                        "name": "Guannan Jiang"
                    },
                    {
                        "name": "Zhichao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Lu"
                },
                "author": "Zhichao Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06690v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06690v1",
                "updated": "2025-09-08T13:44:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    44,
                    55,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T13:44:55Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    44,
                    55,
                    0,
                    251,
                    0
                ],
                "title": "BioLite U-Net: Edge-Deployable Semantic Segmentation for In Situ\n  Bioprinting Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BioLite U-Net: Edge-Deployable Semantic Segmentation for In Situ\n  Bioprinting Monitoring"
                },
                "summary": "Bioprinting is a rapidly advancing field that offers a transformative\napproach to fabricating tissue and organ models through the precise deposition\nof cell-laden bioinks. Ensuring the fidelity and consistency of printed\nstructures in real-time remains a core challenge, particularly under\nconstraints imposed by limited imaging data and resource-constrained embedded\nhardware. Semantic segmentation of the extrusion process, differentiating\nbetween nozzle, extruded bioink, and surrounding background, enables in situ\nmonitoring critical to maintaining print quality and biological viability. In\nthis work, we introduce a lightweight semantic segmentation framework tailored\nfor real-time bioprinting applications. We present a novel, manually annotated\ndataset comprising 787 RGB images captured during the bioprinting process,\nlabeled across three classes: nozzle, bioink, and background. To achieve fast\nand efficient inference suitable for integration with bioprinting systems, we\npropose a BioLite U-Net architecture that leverages depthwise separable\nconvolutions to drastically reduce computational load without compromising\naccuracy. Our model is benchmarked against MobileNetV2 and MobileNetV3-based\nsegmentation baselines using mean Intersection over Union (mIoU), Dice score,\nand pixel accuracy. All models were evaluated on a Raspberry Pi 4B to assess\nreal-world feasibility. The proposed BioLite U-Net achieves an mIoU of 92.85%\nand a Dice score of 96.17%, while being over 1300x smaller than\nMobileNetV2-DeepLabV3+. On-device inference takes 335 ms per frame,\ndemonstrating near real-time capability. Compared to MobileNet baselines,\nBioLite U-Net offers a superior tradeoff between segmentation accuracy,\nefficiency, and deployability, making it highly suitable for intelligent,\nclosed-loop bioprinting systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bioprinting is a rapidly advancing field that offers a transformative\napproach to fabricating tissue and organ models through the precise deposition\nof cell-laden bioinks. Ensuring the fidelity and consistency of printed\nstructures in real-time remains a core challenge, particularly under\nconstraints imposed by limited imaging data and resource-constrained embedded\nhardware. Semantic segmentation of the extrusion process, differentiating\nbetween nozzle, extruded bioink, and surrounding background, enables in situ\nmonitoring critical to maintaining print quality and biological viability. In\nthis work, we introduce a lightweight semantic segmentation framework tailored\nfor real-time bioprinting applications. We present a novel, manually annotated\ndataset comprising 787 RGB images captured during the bioprinting process,\nlabeled across three classes: nozzle, bioink, and background. To achieve fast\nand efficient inference suitable for integration with bioprinting systems, we\npropose a BioLite U-Net architecture that leverages depthwise separable\nconvolutions to drastically reduce computational load without compromising\naccuracy. Our model is benchmarked against MobileNetV2 and MobileNetV3-based\nsegmentation baselines using mean Intersection over Union (mIoU), Dice score,\nand pixel accuracy. All models were evaluated on a Raspberry Pi 4B to assess\nreal-world feasibility. The proposed BioLite U-Net achieves an mIoU of 92.85%\nand a Dice score of 96.17%, while being over 1300x smaller than\nMobileNetV2-DeepLabV3+. On-device inference takes 335 ms per frame,\ndemonstrating near real-time capability. Compared to MobileNet baselines,\nBioLite U-Net offers a superior tradeoff between segmentation accuracy,\nefficiency, and deployability, making it highly suitable for intelligent,\nclosed-loop bioprinting systems."
                },
                "authors": [
                    {
                        "name": "Usman Haider"
                    },
                    {
                        "name": "Lukasz Szemet"
                    },
                    {
                        "name": "Daniel Kelly"
                    },
                    {
                        "name": "Vasileios Sergis"
                    },
                    {
                        "name": "Andrew C. Daly"
                    },
                    {
                        "name": "Karl Mason"
                    }
                ],
                "author_detail": {
                    "name": "Karl Mason"
                },
                "author": "Karl Mason",
                "arxiv_comment": "8 pages, 5 figures, conference-style submission (ICRA 2026). Includes\n  dataset description, BioLite U-Net architecture, benchmark results on edge\n  device (Raspberry Pi 4B)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06690v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9; I.2.10; I.4.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20788v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20788v3",
                "updated": "2025-09-08T13:40:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    40,
                    51,
                    0,
                    251,
                    0
                ],
                "published": "2025-03-11T19:32:44Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    19,
                    32,
                    44,
                    1,
                    70,
                    0
                ],
                "title": "When Cubic Law and Darcy Fail: Bayesian Correction of Model\n  Misspecification in Fracture Conductivities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Cubic Law and Darcy Fail: Bayesian Correction of Model\n  Misspecification in Fracture Conductivities"
                },
                "summary": "Structural uncertainties and unresolved features in fault zones hinder the\nassessment of leakage risks in subsurface CO2 storage. Understanding\nmulti-scale uncertainties in fracture network conductivity is crucial for\nmitigating risks and reliably modelling upscaled fault leakage rates.\nConventional models, such as the Cubic Law, which is based on mechanical\naperture measurements, often neglect fracture roughness, leading to model\nmisspecifications and inaccurate conductivity estimates. Here, we develop a\nphysics-informed, AI-driven correction of these model misspecifications by\nautomatically integrating roughness effects and small-scale structural\nuncertainties. Using Bayesian inference combined with data-driven and geometric\ncorrections, we reconstruct local hydraulic aperture fields that reliably\nestimate fracture conductivities. By leveraging interactions across scales, we\nimprove upon traditional empirical corrections and provide a framework for\npropagating uncertainties from individual fractures to network scales. Our\napproach thereby supports robust calibration of conductivity ranges for fault\nleakage sensitivity analyses, offering a scalable solution for subsurface risk\nassessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structural uncertainties and unresolved features in fault zones hinder the\nassessment of leakage risks in subsurface CO2 storage. Understanding\nmulti-scale uncertainties in fracture network conductivity is crucial for\nmitigating risks and reliably modelling upscaled fault leakage rates.\nConventional models, such as the Cubic Law, which is based on mechanical\naperture measurements, often neglect fracture roughness, leading to model\nmisspecifications and inaccurate conductivity estimates. Here, we develop a\nphysics-informed, AI-driven correction of these model misspecifications by\nautomatically integrating roughness effects and small-scale structural\nuncertainties. Using Bayesian inference combined with data-driven and geometric\ncorrections, we reconstruct local hydraulic aperture fields that reliably\nestimate fracture conductivities. By leveraging interactions across scales, we\nimprove upon traditional empirical corrections and provide a framework for\npropagating uncertainties from individual fractures to network scales. Our\napproach thereby supports robust calibration of conductivity ranges for fault\nleakage sensitivity analyses, offering a scalable solution for subsurface risk\nassessment."
                },
                "authors": [
                    {
                        "name": "Sarah Perez"
                    },
                    {
                        "name": "Florian Doster"
                    },
                    {
                        "name": "Julien Maes"
                    },
                    {
                        "name": "Hannah Menke"
                    },
                    {
                        "name": "Ahmed ElSheikh"
                    },
                    {
                        "name": "Andreas Busch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Busch"
                },
                "author": "Andreas Busch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20788v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20788v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06667v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06667v1",
                "updated": "2025-09-08T13:26:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    26,
                    42,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T13:26:42Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    26,
                    42,
                    0,
                    251,
                    0
                ],
                "title": "The linearized translator equation and applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linearized translator equation and applications"
                },
                "summary": "In this paper, we consider the linearized translator equation $L_\\phi u=f$,\naround entire convex translators $M=\\textrm{graph}(\\phi)\\subset\\mathbb{R}^4$,\ni.e. in the first dimension where the Bernstein property fails. Here, $L_\\phi\nu=\\mathrm{div} (a_\\phi D u)+ b_\\phi\\cdot Du$ is a mean curvature type elliptic\noperator, whose coefficients degenerate as the slope tends to infinity. We\nderive two fundamental barrier estimates, specifically an upper-lower estimate\nand an inner-outer estimate, which allow to propagate $L^\\infty$-control\nbetween different regions. Packaging these and further estimates together we\nthen develop a Fredholm theory for $L_\\phi$ between carefully designed weighted\nfunction spaces. Combined with Lyapunov-Schmidt reduction we infer that the\nspace $\\mathcal{S}$ of noncollapsed translators in $\\mathbb{R}^4$ is a finite\ndimensional analytic variety and that the tip-curvature map\n$\\kappa:\\mathcal{S}\\to\\mathbb{R}$ is analytic. Together with the main result\nfrom our prior paper (Camb. J. Math. '23) this allows us to complete the\nclassification of noncollapsed translators in $\\mathbb{R}^4$. In particular, we\nconclude that the one-parameter family of translators constructed by\nHoffman-Ilmanen-Martin-White is uniquely determined by the smallest principal\ncurvature at the tip.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we consider the linearized translator equation $L_\\phi u=f$,\naround entire convex translators $M=\\textrm{graph}(\\phi)\\subset\\mathbb{R}^4$,\ni.e. in the first dimension where the Bernstein property fails. Here, $L_\\phi\nu=\\mathrm{div} (a_\\phi D u)+ b_\\phi\\cdot Du$ is a mean curvature type elliptic\noperator, whose coefficients degenerate as the slope tends to infinity. We\nderive two fundamental barrier estimates, specifically an upper-lower estimate\nand an inner-outer estimate, which allow to propagate $L^\\infty$-control\nbetween different regions. Packaging these and further estimates together we\nthen develop a Fredholm theory for $L_\\phi$ between carefully designed weighted\nfunction spaces. Combined with Lyapunov-Schmidt reduction we infer that the\nspace $\\mathcal{S}$ of noncollapsed translators in $\\mathbb{R}^4$ is a finite\ndimensional analytic variety and that the tip-curvature map\n$\\kappa:\\mathcal{S}\\to\\mathbb{R}$ is analytic. Together with the main result\nfrom our prior paper (Camb. J. Math. '23) this allows us to complete the\nclassification of noncollapsed translators in $\\mathbb{R}^4$. In particular, we\nconclude that the one-parameter family of translators constructed by\nHoffman-Ilmanen-Martin-White is uniquely determined by the smallest principal\ncurvature at the tip."
                },
                "authors": [
                    {
                        "name": "Kyeongsu Choi"
                    },
                    {
                        "name": "Robert Haslhofer"
                    },
                    {
                        "name": "Or Hershkovits"
                    }
                ],
                "author_detail": {
                    "name": "Or Hershkovits"
                },
                "author": "Or Hershkovits",
                "arxiv_comment": "66 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06667v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06667v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.DG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.DG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12583v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12583v3",
                "updated": "2025-09-08T13:19:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    19,
                    38,
                    0,
                    251,
                    0
                ],
                "published": "2024-12-17T06:24:34Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    6,
                    24,
                    34,
                    1,
                    352,
                    0
                ],
                "title": "Process-Supervised Reward Models for Verifying Clinical Note Generation:\n  A Scalable Approach Guided by Domain Expertise",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process-Supervised Reward Models for Verifying Clinical Note Generation:\n  A Scalable Approach Guided by Domain Expertise"
                },
                "summary": "Process-supervised reward models (PRMs) excel at providing step-by-step\nverification for large language model (LLM) outputs in domains like mathematics\nand coding. However, their application to fields lacking ground-truth answers,\nsuch as clinical note generation, poses significant challenges. We introduce a\nnovel framework for training PRMs to deliver step-level reward signals for\nLLM-generated clinical notes. By precisely defining meaningful \"steps,\"\ninjecting realistic \"errors\" informed by domain expertise, and leveraging LLMs\nto generate process supervision data at scale, we overcome previous\nlimitations. Our PRM, built on LLaMA-3.1 8B, consistently outperforms\nproprietary reasoning and non-reasoning models, achieving state-of-the-art\nperformance on two key evaluations: (1) distinguishing gold-standard from\nerror-containing samples with 98.8% accuracy, and (2) selecting\nphysician-preferred clinical notes with 56.2% accuracy. We investigate critical\ncomponents for effective PRM training, including optimal loss functions and\ndata selection strategies, and present a comprehensive physician reader study\nidentifying predictors of downstream Best-of-N performance. Our study sheds\nlight on unlocking the potential of PRMs for diverse generative tasks across\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process-supervised reward models (PRMs) excel at providing step-by-step\nverification for large language model (LLM) outputs in domains like mathematics\nand coding. However, their application to fields lacking ground-truth answers,\nsuch as clinical note generation, poses significant challenges. We introduce a\nnovel framework for training PRMs to deliver step-level reward signals for\nLLM-generated clinical notes. By precisely defining meaningful \"steps,\"\ninjecting realistic \"errors\" informed by domain expertise, and leveraging LLMs\nto generate process supervision data at scale, we overcome previous\nlimitations. Our PRM, built on LLaMA-3.1 8B, consistently outperforms\nproprietary reasoning and non-reasoning models, achieving state-of-the-art\nperformance on two key evaluations: (1) distinguishing gold-standard from\nerror-containing samples with 98.8% accuracy, and (2) selecting\nphysician-preferred clinical notes with 56.2% accuracy. We investigate critical\ncomponents for effective PRM training, including optimal loss functions and\ndata selection strategies, and present a comprehensive physician reader study\nidentifying predictors of downstream Best-of-N performance. Our study sheds\nlight on unlocking the potential of PRMs for diverse generative tasks across\ndomains."
                },
                "authors": [
                    {
                        "name": "Hanyin Wang"
                    },
                    {
                        "name": "Chufan Gao"
                    },
                    {
                        "name": "Qiping Xu"
                    },
                    {
                        "name": "Bolun Liu"
                    },
                    {
                        "name": "Guleid Hussein"
                    },
                    {
                        "name": "Hariprasad Korsapati"
                    },
                    {
                        "name": "Mohamad El Labban"
                    },
                    {
                        "name": "Kingsley Iheasirim"
                    },
                    {
                        "name": "Mohamed Hassan"
                    },
                    {
                        "name": "Gokhan Anil"
                    },
                    {
                        "name": "Brian Bartlett"
                    },
                    {
                        "name": "Jimeng Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jimeng Sun"
                },
                "author": "Jimeng Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12583v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12583v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19576v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19576v2",
                "updated": "2025-09-08T13:12:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    12,
                    19,
                    0,
                    251,
                    0
                ],
                "published": "2025-08-27T05:16:03Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    5,
                    16,
                    3,
                    2,
                    239,
                    0
                ],
                "title": "ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized\n  Self-Training and Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized\n  Self-Training and Decoding"
                },
                "summary": "With respect to improving the reasoning accuracy of LLMs, the representative\nreinforcement learning (RL) method GRPO faces failure due to insignificant\nreward variance, while verification methods based on process reward models\n(PRMs) suffer from difficulties with training data acquisition and verification\neffectiveness. To tackle these problems, this paper introduces ReST-RL, a\nunified LLM RL paradigm that significantly improves LLM's code reasoning\nability by combining an improved GRPO algorithm with a meticulously designed\ntest time decoding method assisted by a value model (VM). As the first stage of\npolicy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter\nand assemble high-value training data, increasing the reward variance of GRPO\nsampling, thus improving the effectiveness and efficiency of training. After\nthe basic reasoning ability of LLM policy has been improved, we further propose\na test time decoding optimization method called VM-MCTS. Through Monte-Carlo\nTree Search (MCTS), we collect accurate value targets with no annotation\nrequired, on which VM training is based. When decoding, the VM is deployed by\nan adapted MCTS algorithm to provide precise process signals as well as\nverification scores, assisting the LLM policy to achieve high reasoning\naccuracy. We conduct extensive experiments on coding problems to verify the\nvalidity of the proposed RL paradigm. Upon comparison, our approach\nsignificantly outperforms other reinforcement training baselines (e.g., naive\nGRPO and ReST-DPO), as well as decoding and verification baselines (e.g.,\nPRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g.,\nAPPS, BigCodeBench, and HumanEval), indicating its power to strengthen the\nreasoning ability of LLM policies. Codes for our project can be found at\nhttps://github.com/THUDM/ReST-RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With respect to improving the reasoning accuracy of LLMs, the representative\nreinforcement learning (RL) method GRPO faces failure due to insignificant\nreward variance, while verification methods based on process reward models\n(PRMs) suffer from difficulties with training data acquisition and verification\neffectiveness. To tackle these problems, this paper introduces ReST-RL, a\nunified LLM RL paradigm that significantly improves LLM's code reasoning\nability by combining an improved GRPO algorithm with a meticulously designed\ntest time decoding method assisted by a value model (VM). As the first stage of\npolicy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter\nand assemble high-value training data, increasing the reward variance of GRPO\nsampling, thus improving the effectiveness and efficiency of training. After\nthe basic reasoning ability of LLM policy has been improved, we further propose\na test time decoding optimization method called VM-MCTS. Through Monte-Carlo\nTree Search (MCTS), we collect accurate value targets with no annotation\nrequired, on which VM training is based. When decoding, the VM is deployed by\nan adapted MCTS algorithm to provide precise process signals as well as\nverification scores, assisting the LLM policy to achieve high reasoning\naccuracy. We conduct extensive experiments on coding problems to verify the\nvalidity of the proposed RL paradigm. Upon comparison, our approach\nsignificantly outperforms other reinforcement training baselines (e.g., naive\nGRPO and ReST-DPO), as well as decoding and verification baselines (e.g.,\nPRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g.,\nAPPS, BigCodeBench, and HumanEval), indicating its power to strengthen the\nreasoning ability of LLM policies. Codes for our project can be found at\nhttps://github.com/THUDM/ReST-RL."
                },
                "authors": [
                    {
                        "name": "Sining Zhoubian"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "arxiv_comment": "21 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19576v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19576v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04904v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04904v2",
                "updated": "2025-09-08T13:10:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    10,
                    2,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-05T08:23:14Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    23,
                    14,
                    4,
                    248,
                    0
                ],
                "title": "An information metric for comparing and assessing informative interim\n  decisions in sequential clinical trials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An information metric for comparing and assessing informative interim\n  decisions in sequential clinical trials"
                },
                "summary": "Group sequential designs enable interim analyses and potential early stopping\nfor efficacy or futility. While these adaptations improve trial efficiency and\nethical considerations, they also introduce bias into the adapted analyses. We\ndemonstrate how failing to account for informative interim decisions in the\nanalysis can substantially affect posterior estimates of the treatment effect,\noften resulting in overly optimistic credible intervals aligned with the\nstopping decision. Drawing on information theory, we use the Kullback-Leibler\ndivergence to quantify this distortion and highlight its use for post-hoc\nevaluation of informative interim decisions, with a focus on end-of-study\ninference. Unlike pointwise comparisons, this measure provides an integrated\nsummary of this distortion on the whole parameter space. By comparing\nalternative decision boundaries and prior specifications, we illustrate how\nthis measure can improve the understanding of trial results and inform the\nplanning of future adaptive studies. We also introduce an expected version of\nthis metric to support clinicians in choosing decision boundaries. This\nguidance complements traditional strategies based on type-I error rate control\nby offering insights into the distortion introduced to the treatment effect at\neach interim phase. The use of this pre-experimental measure is finally\nillustrated in a group sequential trial for evaluating a treatment for central\nnervous system disorders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group sequential designs enable interim analyses and potential early stopping\nfor efficacy or futility. While these adaptations improve trial efficiency and\nethical considerations, they also introduce bias into the adapted analyses. We\ndemonstrate how failing to account for informative interim decisions in the\nanalysis can substantially affect posterior estimates of the treatment effect,\noften resulting in overly optimistic credible intervals aligned with the\nstopping decision. Drawing on information theory, we use the Kullback-Leibler\ndivergence to quantify this distortion and highlight its use for post-hoc\nevaluation of informative interim decisions, with a focus on end-of-study\ninference. Unlike pointwise comparisons, this measure provides an integrated\nsummary of this distortion on the whole parameter space. By comparing\nalternative decision boundaries and prior specifications, we illustrate how\nthis measure can improve the understanding of trial results and inform the\nplanning of future adaptive studies. We also introduce an expected version of\nthis metric to support clinicians in choosing decision boundaries. This\nguidance complements traditional strategies based on type-I error rate control\nby offering insights into the distortion introduced to the treatment effect at\neach interim phase. The use of this pre-experimental measure is finally\nillustrated in a group sequential trial for evaluating a treatment for central\nnervous system disorders."
                },
                "authors": [
                    {
                        "name": "G. Caruso"
                    },
                    {
                        "name": "W. F. Rosenberger"
                    },
                    {
                        "name": "P. Mozgunov"
                    },
                    {
                        "name": "N. Flournoy"
                    }
                ],
                "author_detail": {
                    "name": "N. Flournoy"
                },
                "author": "N. Flournoy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04904v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04904v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06652v1",
                "updated": "2025-09-08T13:07:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    7,
                    35,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T13:07:35Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    7,
                    35,
                    0,
                    251,
                    0
                ],
                "title": "IntrEx: A Dataset for Modeling Engagement in Educational Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IntrEx: A Dataset for Modeling Engagement in Educational Conversations"
                },
                "summary": "Engagement and motivation are crucial for second-language acquisition, yet\nmaintaining learner interest in educational conversations remains a challenge.\nWhile prior research has explored what makes educational texts interesting,\nstill little is known about the linguistic features that drive engagement in\nconversations. To address this gap, we introduce IntrEx, the first large\ndataset annotated for interestingness and expected interestingness in\nteacher-student interactions. Built upon the Teacher-Student Chatroom Corpus\n(TSCC), IntrEx extends prior work by incorporating sequence-level annotations,\nallowing for the study of engagement beyond isolated turns to capture how\ninterest evolves over extended dialogues. We employ a rigorous annotation\nprocess with over 100 second-language learners, using a comparison-based rating\napproach inspired by reinforcement learning from human feedback (RLHF) to\nimprove agreement. We investigate whether large language models (LLMs) can\npredict human interestingness judgments. We find that LLMs (7B/8B parameters)\nfine-tuned on interestingness ratings outperform larger proprietary models like\nGPT-4o, demonstrating the potential for specialised datasets to model\nengagement in educational settings. Finally, we analyze how linguistic and\ncognitive factors, such as concreteness, comprehensibility (readability), and\nuptake, influence engagement in educational dialogues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engagement and motivation are crucial for second-language acquisition, yet\nmaintaining learner interest in educational conversations remains a challenge.\nWhile prior research has explored what makes educational texts interesting,\nstill little is known about the linguistic features that drive engagement in\nconversations. To address this gap, we introduce IntrEx, the first large\ndataset annotated for interestingness and expected interestingness in\nteacher-student interactions. Built upon the Teacher-Student Chatroom Corpus\n(TSCC), IntrEx extends prior work by incorporating sequence-level annotations,\nallowing for the study of engagement beyond isolated turns to capture how\ninterest evolves over extended dialogues. We employ a rigorous annotation\nprocess with over 100 second-language learners, using a comparison-based rating\napproach inspired by reinforcement learning from human feedback (RLHF) to\nimprove agreement. We investigate whether large language models (LLMs) can\npredict human interestingness judgments. We find that LLMs (7B/8B parameters)\nfine-tuned on interestingness ratings outperform larger proprietary models like\nGPT-4o, demonstrating the potential for specialised datasets to model\nengagement in educational settings. Finally, we analyze how linguistic and\ncognitive factors, such as concreteness, comprehensibility (readability), and\nuptake, influence engagement in educational dialogues."
                },
                "authors": [
                    {
                        "name": "Xingwei Tan"
                    },
                    {
                        "name": "Mahathi Parvatham"
                    },
                    {
                        "name": "Chiara Gambi"
                    },
                    {
                        "name": "Gabriele Pergola"
                    }
                ],
                "author_detail": {
                    "name": "Gabriele Pergola"
                },
                "author": "Gabriele Pergola",
                "arxiv_comment": "EMNLP 2025 Findings camera-ready, 9+7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06650v1",
                "updated": "2025-09-08T13:04:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    4,
                    7,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T13:04:07Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    4,
                    7,
                    0,
                    251,
                    0
                ],
                "title": "Domain-Aware RAG: MoL-Enhanced RL for Efficient Training and Scalable\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain-Aware RAG: MoL-Enhanced RL for Efficient Training and Scalable\n  Retrieval"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems rely heavily on the retrieval\nstage, particularly the coarse-ranking process. Existing coarse-ranking\noptimization approaches often struggle to balance domain-specific knowledge\nlearning with query enhencement, resulting in suboptimal retrieval performance.\nTo address this challenge, we propose MoLER, a domain-aware RAG method that\nuses MoL-Enhanced Reinforcement Learning to optimize retrieval. MoLER has a\ntwo-stage pipeline: a continual pre-training (CPT) phase using a Mixture of\nLosses (MoL) to balance domain-specific knowledge with general language\ncapabilities, and a reinforcement learning (RL) phase leveraging Group Relative\nPolicy Optimization (GRPO) to optimize query and passage generation for\nmaximizing document recall. A key innovation is our Multi-query Single-passage\nLate Fusion (MSLF) strategy, which reduces computational overhead during RL\ntraining while maintaining scalable inference via Multi-query Multi-passage\nLate Fusion (MMLF). Extensive experiments on benchmark datasets show that MoLER\nachieves state-of-the-art performance, significantly outperforming baseline\nmethods. MoLER bridges the knowledge gap in RAG systems, enabling robust and\nscalable retrieval in specialized domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems rely heavily on the retrieval\nstage, particularly the coarse-ranking process. Existing coarse-ranking\noptimization approaches often struggle to balance domain-specific knowledge\nlearning with query enhencement, resulting in suboptimal retrieval performance.\nTo address this challenge, we propose MoLER, a domain-aware RAG method that\nuses MoL-Enhanced Reinforcement Learning to optimize retrieval. MoLER has a\ntwo-stage pipeline: a continual pre-training (CPT) phase using a Mixture of\nLosses (MoL) to balance domain-specific knowledge with general language\ncapabilities, and a reinforcement learning (RL) phase leveraging Group Relative\nPolicy Optimization (GRPO) to optimize query and passage generation for\nmaximizing document recall. A key innovation is our Multi-query Single-passage\nLate Fusion (MSLF) strategy, which reduces computational overhead during RL\ntraining while maintaining scalable inference via Multi-query Multi-passage\nLate Fusion (MMLF). Extensive experiments on benchmark datasets show that MoLER\nachieves state-of-the-art performance, significantly outperforming baseline\nmethods. MoLER bridges the knowledge gap in RAG systems, enabling robust and\nscalable retrieval in specialized domains."
                },
                "authors": [
                    {
                        "name": "Hao Lin"
                    },
                    {
                        "name": "Peitong Xie"
                    },
                    {
                        "name": "Jingxue Chen"
                    },
                    {
                        "name": "Jie Lin"
                    },
                    {
                        "name": "Qingkun Tang"
                    },
                    {
                        "name": "Qianchun Lu"
                    }
                ],
                "author_detail": {
                    "name": "Qianchun Lu"
                },
                "author": "Qianchun Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03579v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03579v2",
                "updated": "2025-09-08T12:59:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    59,
                    58,
                    0,
                    251,
                    0
                ],
                "published": "2025-04-04T16:30:44Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    30,
                    44,
                    4,
                    94,
                    0
                ],
                "title": "Hallucination Detection on a Budget: Efficient Bayesian Estimation of\n  Semantic Entropy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination Detection on a Budget: Efficient Bayesian Estimation of\n  Semantic Entropy"
                },
                "summary": "Detecting whether an LLM hallucinates is an important research challenge. One\npromising way of doing so is to estimate the semantic entropy (Farquhar et al.,\n2024) of the distribution of generated sequences. We propose a new algorithm\nfor doing that, with two main advantages. First, due to us taking the Bayesian\napproach, we achieve a much better quality of semantic entropy estimates for a\ngiven budget of samples from the LLM. Second, we are able to tune the number of\nsamples adaptively so that `harder' contexts receive more samples. We\ndemonstrate empirically that our approach systematically beats the baselines,\nrequiring only 53% of samples used by Farquhar et al. (2024) to achieve the\nsame quality of hallucination detection as measured by AUROC. Moreover, quite\ncounterintuitively, our estimator is useful even with just one sample from the\nLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting whether an LLM hallucinates is an important research challenge. One\npromising way of doing so is to estimate the semantic entropy (Farquhar et al.,\n2024) of the distribution of generated sequences. We propose a new algorithm\nfor doing that, with two main advantages. First, due to us taking the Bayesian\napproach, we achieve a much better quality of semantic entropy estimates for a\ngiven budget of samples from the LLM. Second, we are able to tune the number of\nsamples adaptively so that `harder' contexts receive more samples. We\ndemonstrate empirically that our approach systematically beats the baselines,\nrequiring only 53% of samples used by Farquhar et al. (2024) to achieve the\nsame quality of hallucination detection as measured by AUROC. Moreover, quite\ncounterintuitively, our estimator is useful even with just one sample from the\nLLM."
                },
                "authors": [
                    {
                        "name": "Kamil Ciosek"
                    },
                    {
                        "name": "Nicolò Felicioni"
                    },
                    {
                        "name": "Sina Ghiassian"
                    }
                ],
                "author_detail": {
                    "name": "Sina Ghiassian"
                },
                "author": "Sina Ghiassian",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03579v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03579v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06636v1",
                "updated": "2025-09-08T12:54:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    54,
                    30,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T12:54:30Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    54,
                    30,
                    0,
                    251,
                    0
                ],
                "title": "Full Integer Arithmetic Online Training for Spiking Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full Integer Arithmetic Online Training for Spiking Neural Networks"
                },
                "summary": "Spiking Neural Networks (SNNs) are promising for neuromorphic computing due\nto their biological plausibility and energy efficiency. However, training\nmethods like Backpropagation Through Time (BPTT) and Real Time Recurrent\nLearning (RTRL) remain computationally intensive. This work introduces an\ninteger-only, online training algorithm using a mixed-precision approach to\nimprove efficiency and reduce memory usage by over 60%. The method replaces\nfloating-point operations with integer arithmetic to enable hardware-friendly\nimplementation. It generalizes to Convolutional and Recurrent SNNs (CSNNs,\nRSNNs), showing versatility across architectures. Evaluations on MNIST and the\nSpiking Heidelberg Digits (SHD) dataset demonstrate that mixed-precision models\nachieve accuracy comparable to or better than full-precision baselines using\n16-bit shadow and 8- or 12-bit inference weights. Despite some limitations in\nlow-precision and deeper models, performance remains robust. In conclusion, the\nproposed integer-only online learning algorithm presents an effective solution\nfor efficiently training SNNs, enabling deployment on resource-constrained\nneuromorphic hardware without sacrificing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) are promising for neuromorphic computing due\nto their biological plausibility and energy efficiency. However, training\nmethods like Backpropagation Through Time (BPTT) and Real Time Recurrent\nLearning (RTRL) remain computationally intensive. This work introduces an\ninteger-only, online training algorithm using a mixed-precision approach to\nimprove efficiency and reduce memory usage by over 60%. The method replaces\nfloating-point operations with integer arithmetic to enable hardware-friendly\nimplementation. It generalizes to Convolutional and Recurrent SNNs (CSNNs,\nRSNNs), showing versatility across architectures. Evaluations on MNIST and the\nSpiking Heidelberg Digits (SHD) dataset demonstrate that mixed-precision models\nachieve accuracy comparable to or better than full-precision baselines using\n16-bit shadow and 8- or 12-bit inference weights. Despite some limitations in\nlow-precision and deeper models, performance remains robust. In conclusion, the\nproposed integer-only online learning algorithm presents an effective solution\nfor efficiently training SNNs, enabling deployment on resource-constrained\nneuromorphic hardware without sacrificing accuracy."
                },
                "authors": [
                    {
                        "name": "Ismael Gomez"
                    },
                    {
                        "name": "Guangzhi Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guangzhi Tang"
                },
                "author": "Guangzhi Tang",
                "arxiv_comment": "Accepted at ICANN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06631v1",
                "updated": "2025-09-08T12:51:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    51,
                    40,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T12:51:40Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    51,
                    40,
                    0,
                    251,
                    0
                ],
                "title": "Guided Decoding and Its Critical Role in Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guided Decoding and Its Critical Role in Retrieval-Augmented Generation"
                },
                "summary": "The integration of Large Language Models (LLMs) into various applications has\ndriven the need for structured and reliable responses. A key challenge in\nRetrieval-Augmented Generation (RAG) systems is ensuring that outputs align\nwith expected formats while minimizing hallucinations. This study examines the\nrole of guided decoding in RAG systems, comparing three methods, Outlines,\nXGrammar, and LM Format Enforcer, across different multi-turn prompting setups\n(0-turn, 1-turn, and 2-turn). By evaluating success rates, hallucination rates,\nand output quality, we provide insights into their performance and\napplicability. Our findings reveal how multi-turn interactions influence guided\ndecoding, uncovering unexpected performance variations that can inform method\nselection for specific use cases. This work advances the understanding of\nstructured output generation in RAG systems, offering both theoretical insights\nand practical guidance for LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into various applications has\ndriven the need for structured and reliable responses. A key challenge in\nRetrieval-Augmented Generation (RAG) systems is ensuring that outputs align\nwith expected formats while minimizing hallucinations. This study examines the\nrole of guided decoding in RAG systems, comparing three methods, Outlines,\nXGrammar, and LM Format Enforcer, across different multi-turn prompting setups\n(0-turn, 1-turn, and 2-turn). By evaluating success rates, hallucination rates,\nand output quality, we provide insights into their performance and\napplicability. Our findings reveal how multi-turn interactions influence guided\ndecoding, uncovering unexpected performance variations that can inform method\nselection for specific use cases. This work advances the understanding of\nstructured output generation in RAG systems, offering both theoretical insights\nand practical guidance for LLM deployment."
                },
                "authors": [
                    {
                        "name": "Özgür Uğur"
                    },
                    {
                        "name": "Musa Yılmaz"
                    },
                    {
                        "name": "Esra Şavirdi"
                    },
                    {
                        "name": "Özay Ezerceli"
                    },
                    {
                        "name": "Mahmut El Huseyni"
                    },
                    {
                        "name": "Selva Taş"
                    },
                    {
                        "name": "Reyhan Bayraktar"
                    }
                ],
                "author_detail": {
                    "name": "Reyhan Bayraktar"
                },
                "author": "Reyhan Bayraktar",
                "arxiv_doi": "10.1109/SIU66497.2025.11111950",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/SIU66497.2025.11111950",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.06631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00719v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00719v3",
                "updated": "2025-09-08T12:44:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    44,
                    14,
                    0,
                    251,
                    0
                ],
                "published": "2025-08-01T15:38:21Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    38,
                    21,
                    4,
                    213,
                    0
                ],
                "title": "Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and\n  Context-Aware KGQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and\n  Context-Aware KGQA"
                },
                "summary": "Knowledge Graph Question Answering (KGQA) aims to interpret natural language\nqueries and perform structured reasoning over knowledge graphs by leveraging\ntheir relational and semantic structures to retrieve accurate answers. Recent\nKGQA methods primarily follow either retrieve-then-reason paradigm, relying on\nGNNs or heuristic rules for static paths extraction, or dynamic path generation\nstrategies that use large language models (LLMs) with prompting to jointly\nperform retrieval and reasoning. However, the former suffers from limited\nadaptability due to static path extraction and lack of contextual refinement,\nwhile the latter incurs high computational costs and struggles with accurate\npath evaluation due to reliance on fixed scoring functions and extensive LLM\ncalls. To address these issues, this paper proposes Dynamically Adaptive\nMCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search\nwith adaptive path evaluation for efficient and context-aware KGQA. DAMR\nemploys a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based\nplanner, which selects top-$k$ relevant relations at each step to reduce search\nspace. To improve path evaluation accuracy, we introduce a lightweight\nTransformer-based scorer that performs context-aware plausibility estimation by\njointly encoding the question and relation sequence through cross-attention,\nenabling the model to capture fine-grained semantic shifts during multi-hop\nreasoning. Furthermore, to alleviate the scarcity of high-quality supervision,\nDAMR incorporates a dynamic pseudo-path refinement mechanism that periodically\ngenerates training signals from partial paths explored during search, allowing\nthe scorer to continuously adapt to the evolving distribution of reasoning\ntrajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR\nsignificantly outperforms state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph Question Answering (KGQA) aims to interpret natural language\nqueries and perform structured reasoning over knowledge graphs by leveraging\ntheir relational and semantic structures to retrieve accurate answers. Recent\nKGQA methods primarily follow either retrieve-then-reason paradigm, relying on\nGNNs or heuristic rules for static paths extraction, or dynamic path generation\nstrategies that use large language models (LLMs) with prompting to jointly\nperform retrieval and reasoning. However, the former suffers from limited\nadaptability due to static path extraction and lack of contextual refinement,\nwhile the latter incurs high computational costs and struggles with accurate\npath evaluation due to reliance on fixed scoring functions and extensive LLM\ncalls. To address these issues, this paper proposes Dynamically Adaptive\nMCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search\nwith adaptive path evaluation for efficient and context-aware KGQA. DAMR\nemploys a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based\nplanner, which selects top-$k$ relevant relations at each step to reduce search\nspace. To improve path evaluation accuracy, we introduce a lightweight\nTransformer-based scorer that performs context-aware plausibility estimation by\njointly encoding the question and relation sequence through cross-attention,\nenabling the model to capture fine-grained semantic shifts during multi-hop\nreasoning. Furthermore, to alleviate the scarcity of high-quality supervision,\nDAMR incorporates a dynamic pseudo-path refinement mechanism that periodically\ngenerates training signals from partial paths explored during search, allowing\nthe scorer to continuously adapt to the evolving distribution of reasoning\ntrajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR\nsignificantly outperforms state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yingxu Wang"
                    },
                    {
                        "name": "Shiqi Fan"
                    },
                    {
                        "name": "Mengzhu Wang"
                    },
                    {
                        "name": "Siyang Gao"
                    },
                    {
                        "name": "Siwei Liu"
                    },
                    {
                        "name": "Nan Yin"
                    }
                ],
                "author_detail": {
                    "name": "Nan Yin"
                },
                "author": "Nan Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00719v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00719v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06602v1",
                "updated": "2025-09-08T12:15:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    15,
                    53,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T12:15:53Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    15,
                    53,
                    0,
                    251,
                    0
                ],
                "title": "Demo: Healthcare Agent Orchestrator (HAO) for Patient Summarization in\n  Molecular Tumor Boards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demo: Healthcare Agent Orchestrator (HAO) for Patient Summarization in\n  Molecular Tumor Boards"
                },
                "summary": "Molecular Tumor Boards (MTBs) are multidisciplinary forums where oncology\nspecialists collaboratively assess complex patient cases to determine optimal\ntreatment strategies. A central element of this process is the patient summary,\ntypically compiled by a medical oncologist, radiation oncologist, or surgeon,\nor their trained medical assistant, who distills heterogeneous medical records\ninto a concise narrative to facilitate discussion. This manual approach is\noften labor-intensive, subjective, and prone to omissions of critical\ninformation. To address these limitations, we introduce the Healthcare Agent\nOrchestrator (HAO), a Large Language Model (LLM)-driven AI agent that\ncoordinates a multi-agent clinical workflow to generate accurate and\ncomprehensive patient summaries for MTBs. Evaluating predicted patient\nsummaries against ground truth presents additional challenges due to stylistic\nvariation, ordering, synonym usage, and phrasing differences, which complicate\nthe measurement of both succinctness and completeness. To overcome these\nevaluation hurdles, we propose TBFact, a ``model-as-a-judge'' framework\ndesigned to assess the comprehensiveness and succinctness of generated\nsummaries. Using a benchmark dataset derived from de-identified tumor board\ndiscussions, we applied TBFact to evaluate our Patient History agent. Results\nshow that the agent captured 94% of high-importance information (including\npartial entailments) and achieved a TBFact recall of 0.84 under strict\nentailment criteria. We further demonstrate that TBFact enables a data-free\nevaluation framework that institutions can deploy locally without sharing\nsensitive clinical data. Together, HAO and TBFact establish a robust foundation\nfor delivering reliable and scalable support to MTBs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular Tumor Boards (MTBs) are multidisciplinary forums where oncology\nspecialists collaboratively assess complex patient cases to determine optimal\ntreatment strategies. A central element of this process is the patient summary,\ntypically compiled by a medical oncologist, radiation oncologist, or surgeon,\nor their trained medical assistant, who distills heterogeneous medical records\ninto a concise narrative to facilitate discussion. This manual approach is\noften labor-intensive, subjective, and prone to omissions of critical\ninformation. To address these limitations, we introduce the Healthcare Agent\nOrchestrator (HAO), a Large Language Model (LLM)-driven AI agent that\ncoordinates a multi-agent clinical workflow to generate accurate and\ncomprehensive patient summaries for MTBs. Evaluating predicted patient\nsummaries against ground truth presents additional challenges due to stylistic\nvariation, ordering, synonym usage, and phrasing differences, which complicate\nthe measurement of both succinctness and completeness. To overcome these\nevaluation hurdles, we propose TBFact, a ``model-as-a-judge'' framework\ndesigned to assess the comprehensiveness and succinctness of generated\nsummaries. Using a benchmark dataset derived from de-identified tumor board\ndiscussions, we applied TBFact to evaluate our Patient History agent. Results\nshow that the agent captured 94% of high-importance information (including\npartial entailments) and achieved a TBFact recall of 0.84 under strict\nentailment criteria. We further demonstrate that TBFact enables a data-free\nevaluation framework that institutions can deploy locally without sharing\nsensitive clinical data. Together, HAO and TBFact establish a robust foundation\nfor delivering reliable and scalable support to MTBs."
                },
                "authors": [
                    {
                        "name": "Noel Codella"
                    },
                    {
                        "name": "Sam Preston"
                    },
                    {
                        "name": "Hao Qiu"
                    },
                    {
                        "name": "Leonardo Schettini"
                    },
                    {
                        "name": "Wen-wai Yim"
                    },
                    {
                        "name": "Mert Öz"
                    },
                    {
                        "name": "Shrey Jain"
                    },
                    {
                        "name": "Matthew P. Lungren"
                    },
                    {
                        "name": "Thomas Osborne"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Osborne"
                },
                "author": "Thomas Osborne",
                "arxiv_comment": "9 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06596v1",
                "updated": "2025-09-08T12:06:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    6,
                    9,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T12:06:09Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    6,
                    9,
                    0,
                    251,
                    0
                ],
                "title": "HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination\n  Mitigation in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination\n  Mitigation in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) often produce hallucinations in\nretrieval-augmented or long-context generation, even when relevant evidence is\npresent. This stems from two issues: head importance is treated as\ninput-agnostic, and raw attention weights poorly reflect each token's true\ncontribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a\nparameter-free decoding framework that directly addresses both challenges. HAVE\nintroduces head-adaptive gating, which performs instance-level soft reweighing\nof attention heads, and value calibration, which augments attention with the\nmagnitude of value vectors to approximate write-back contribution. Together,\nthese modules construct token-level evidence aligned with model updates and\nfuse it with the LM distribution through a lightweight uncertainty-scaled\npolicy. HAVE requires no finetuning and operates in a single forward pass,\nmaking it efficient and broadly applicable. Experiments across multiple QA\nbenchmarks and LLM families demonstrate that HAVE consistently reduces\nhallucinations and outperforms strong baselines, including DAGCD, with modest\noverhead. The framework is transparent, reproducible, and readily integrates\nwith off-the-shelf LLMs, advancing trustworthy generation in real-world\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often produce hallucinations in\nretrieval-augmented or long-context generation, even when relevant evidence is\npresent. This stems from two issues: head importance is treated as\ninput-agnostic, and raw attention weights poorly reflect each token's true\ncontribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a\nparameter-free decoding framework that directly addresses both challenges. HAVE\nintroduces head-adaptive gating, which performs instance-level soft reweighing\nof attention heads, and value calibration, which augments attention with the\nmagnitude of value vectors to approximate write-back contribution. Together,\nthese modules construct token-level evidence aligned with model updates and\nfuse it with the LM distribution through a lightweight uncertainty-scaled\npolicy. HAVE requires no finetuning and operates in a single forward pass,\nmaking it efficient and broadly applicable. Experiments across multiple QA\nbenchmarks and LLM families demonstrate that HAVE consistently reduces\nhallucinations and outperforms strong baselines, including DAGCD, with modest\noverhead. The framework is transparent, reproducible, and readily integrates\nwith off-the-shelf LLMs, advancing trustworthy generation in real-world\nsettings."
                },
                "authors": [
                    {
                        "name": "Xin Tong"
                    },
                    {
                        "name": "Zhi Lin"
                    },
                    {
                        "name": "Jingya Wang"
                    },
                    {
                        "name": "Bo Jin"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jin"
                },
                "author": "Bo Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06595v1",
                "updated": "2025-09-08T12:06:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    6,
                    6,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T12:06:06Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    6,
                    6,
                    0,
                    251,
                    0
                ],
                "title": "LLMs in Cybersecurity: Friend or Foe in the Human Decision Loop?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs in Cybersecurity: Friend or Foe in the Human Decision Loop?"
                },
                "summary": "Large Language Models (LLMs) are transforming human decision-making by acting\nas cognitive collaborators. Yet, this promise comes with a paradox: while LLMs\ncan improve accuracy, they may also erode independent reasoning, promote\nover-reliance and homogenize decisions. In this paper, we investigate how LLMs\nshape human judgment in security-critical contexts. Through two exploratory\nfocus groups (unaided and LLM-supported), we assess decision accuracy,\nbehavioral resilience and reliance dynamics. Our findings reveal that while\nLLMs enhance accuracy and consistency in routine decisions, they can\ninadvertently reduce cognitive diversity and improve automation bias, which is\nespecially the case among users with lower resilience. In contrast,\nhigh-resilience individuals leverage LLMs more effectively, suggesting that\ncognitive traits mediate AI benefit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are transforming human decision-making by acting\nas cognitive collaborators. Yet, this promise comes with a paradox: while LLMs\ncan improve accuracy, they may also erode independent reasoning, promote\nover-reliance and homogenize decisions. In this paper, we investigate how LLMs\nshape human judgment in security-critical contexts. Through two exploratory\nfocus groups (unaided and LLM-supported), we assess decision accuracy,\nbehavioral resilience and reliance dynamics. Our findings reveal that while\nLLMs enhance accuracy and consistency in routine decisions, they can\ninadvertently reduce cognitive diversity and improve automation bias, which is\nespecially the case among users with lower resilience. In contrast,\nhigh-resilience individuals leverage LLMs more effectively, suggesting that\ncognitive traits mediate AI benefit."
                },
                "authors": [
                    {
                        "name": "Irdin Pekaric"
                    },
                    {
                        "name": "Philipp Zech"
                    },
                    {
                        "name": "Tom Mattson"
                    }
                ],
                "author_detail": {
                    "name": "Tom Mattson"
                },
                "author": "Tom Mattson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13176v2",
                "updated": "2025-09-08T12:05:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    5,
                    58,
                    0,
                    251,
                    0
                ],
                "published": "2025-04-17T17:59:47Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    59,
                    47,
                    3,
                    107,
                    0
                ],
                "title": "IMAGGarment: Fine-Grained Garment Generation for Controllable Fashion\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IMAGGarment: Fine-Grained Garment Generation for Controllable Fashion\n  Design"
                },
                "summary": "This paper presents IMAGGarment, a fine-grained garment generation (FGG)\nframework that enables high-fidelity garment synthesis with precise control\nover silhouette, color, and logo placement. Unlike existing methods that are\nlimited to single-condition inputs, IMAGGarment addresses the challenges of\nmulti-conditional controllability in personalized fashion design and digital\napparel applications. Specifically, IMAGGarment employs a two-stage training\nstrategy to separately model global appearance and local details, while\nenabling unified and controllable generation through end-to-end inference. In\nthe first stage, we propose a global appearance model that jointly encodes\nsilhouette and color using a mixed attention module and a color adapter. In the\nsecond stage, we present a local enhancement model with an adaptive\nappearance-aware module to inject user-defined logos and spatial constraints,\nenabling accurate placement and visual consistency. To support this task, we\nrelease GarmentBench, a large-scale dataset comprising over 180K garment\nsamples paired with multi-level design conditions, including sketches, color\nreferences, logo placements, and textual prompts. Extensive experiments\ndemonstrate that our method outperforms existing baselines, achieving superior\nstructural stability, color fidelity, and local controllability performance.\nCode, models, and datasets are publicly available at\nhttps://github.com/muzishen/IMAGGarment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents IMAGGarment, a fine-grained garment generation (FGG)\nframework that enables high-fidelity garment synthesis with precise control\nover silhouette, color, and logo placement. Unlike existing methods that are\nlimited to single-condition inputs, IMAGGarment addresses the challenges of\nmulti-conditional controllability in personalized fashion design and digital\napparel applications. Specifically, IMAGGarment employs a two-stage training\nstrategy to separately model global appearance and local details, while\nenabling unified and controllable generation through end-to-end inference. In\nthe first stage, we propose a global appearance model that jointly encodes\nsilhouette and color using a mixed attention module and a color adapter. In the\nsecond stage, we present a local enhancement model with an adaptive\nappearance-aware module to inject user-defined logos and spatial constraints,\nenabling accurate placement and visual consistency. To support this task, we\nrelease GarmentBench, a large-scale dataset comprising over 180K garment\nsamples paired with multi-level design conditions, including sketches, color\nreferences, logo placements, and textual prompts. Extensive experiments\ndemonstrate that our method outperforms existing baselines, achieving superior\nstructural stability, color fidelity, and local controllability performance.\nCode, models, and datasets are publicly available at\nhttps://github.com/muzishen/IMAGGarment."
                },
                "authors": [
                    {
                        "name": "Fei Shen"
                    },
                    {
                        "name": "Jian Yu"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Xiaoyu Du"
                    },
                    {
                        "name": "Jinhui Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jinhui Tang"
                },
                "author": "Jinhui Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18706v2",
                "updated": "2025-09-08T11:57:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    57,
                    29,
                    0,
                    251,
                    0
                ],
                "published": "2025-05-24T13:55:38Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    13,
                    55,
                    38,
                    5,
                    144,
                    0
                ],
                "title": "Steering LLM Reasoning Through Bias-Only Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering LLM Reasoning Through Bias-Only Adaptation"
                },
                "summary": "We show that training a single $d$-dimensional steering vector per layer with\nreinforcement learning, while freezing all base weights, matches the accuracy\nof fully RL-tuned reasoning models on mathematical-reasoning tasks. On an 8\nbillion-parameter model this adds only $\\approx 0.0016\\%$ additional parameters\nand reproduces performance across a range of base models and\nmathematical-reasoning benchmarks. These results tighten the upper bound on the\nparameter budget required for high-level chain-of-thought reasoning, indicating\nthat millions of adapter weights are unnecessary. The minimal trainable\nfootprint reduces optimizer memory and inter-GPU communication, lowering the\noverall cost of fine-tuning. Moreover, a logit-lens analysis shows that the\nlearned vectors amplify coherent token directions, providing clearer insight\ninto the model's internal computations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that training a single $d$-dimensional steering vector per layer with\nreinforcement learning, while freezing all base weights, matches the accuracy\nof fully RL-tuned reasoning models on mathematical-reasoning tasks. On an 8\nbillion-parameter model this adds only $\\approx 0.0016\\%$ additional parameters\nand reproduces performance across a range of base models and\nmathematical-reasoning benchmarks. These results tighten the upper bound on the\nparameter budget required for high-level chain-of-thought reasoning, indicating\nthat millions of adapter weights are unnecessary. The minimal trainable\nfootprint reduces optimizer memory and inter-GPU communication, lowering the\noverall cost of fine-tuning. Moreover, a logit-lens analysis shows that the\nlearned vectors amplify coherent token directions, providing clearer insight\ninto the model's internal computations."
                },
                "authors": [
                    {
                        "name": "Viacheslav Sinii"
                    },
                    {
                        "name": "Alexey Gorbatovski"
                    },
                    {
                        "name": "Artem Cherepanov"
                    },
                    {
                        "name": "Boris Shaposhnikov"
                    },
                    {
                        "name": "Nikita Balagansky"
                    },
                    {
                        "name": "Daniil Gavrilov"
                    }
                ],
                "author_detail": {
                    "name": "Daniil Gavrilov"
                },
                "author": "Daniil Gavrilov",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06586v1",
                "updated": "2025-09-08T11:56:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    56,
                    46,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T11:56:46Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    56,
                    46,
                    0,
                    251,
                    0
                ],
                "title": "Simulating Dispute Mediation with LLM-Based Agents for Legal Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Dispute Mediation with LLM-Based Agents for Legal Research"
                },
                "summary": "Legal dispute mediation plays a crucial role in resolving civil disputes, yet\nits empirical study is limited by privacy constraints and complex multivariate\ninteractions. To address this limitation, we present AgentMediation, the first\nLLM-based agent framework for simulating dispute mediation. It simulates\nrealistic mediation processes grounded in real-world disputes and enables\ncontrolled experimentation on key variables such as disputant strategies,\ndispute causes, and mediator expertise. Our empirical analysis reveals patterns\nconsistent with sociological theories, including Group Polarization and\nSurface-level Consensus. As a comprehensive and extensible platform,\nAgentMediation paves the way for deeper integration of social science and AI in\nlegal research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal dispute mediation plays a crucial role in resolving civil disputes, yet\nits empirical study is limited by privacy constraints and complex multivariate\ninteractions. To address this limitation, we present AgentMediation, the first\nLLM-based agent framework for simulating dispute mediation. It simulates\nrealistic mediation processes grounded in real-world disputes and enables\ncontrolled experimentation on key variables such as disputant strategies,\ndispute causes, and mediator expertise. Our empirical analysis reveals patterns\nconsistent with sociological theories, including Group Polarization and\nSurface-level Consensus. As a comprehensive and extensible platform,\nAgentMediation paves the way for deeper integration of social science and AI in\nlegal research."
                },
                "authors": [
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Haitao Li"
                    },
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Yanxue Ren"
                    },
                    {
                        "name": "Wuyue Wang"
                    },
                    {
                        "name": "Yiqun Liu"
                    },
                    {
                        "name": "Yueyue Wu"
                    },
                    {
                        "name": "Qingyao Ai"
                    }
                ],
                "author_detail": {
                    "name": "Qingyao Ai"
                },
                "author": "Qingyao Ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04537v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04537v2",
                "updated": "2025-09-08T11:56:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    56,
                    1,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-04T08:09:42Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    9,
                    42,
                    3,
                    247,
                    0
                ],
                "title": "Emergent Social Dynamics of LLM Agents in the El Farol Bar Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent Social Dynamics of LLM Agents in the El Farol Bar Problem"
                },
                "summary": "We investigate the emergent social dynamics of Large Language Model (LLM)\nagents in a spatially extended El Farol Bar problem, observing how they\nautonomously navigate this classic social dilemma. As a result, the LLM agents\ngenerated a spontaneous motivation to go to the bar and changed their decision\nmaking by becoming a collective. We also observed that the LLM agents did not\nsolve the problem completely, but rather behaved more like humans. These\nfindings reveal a complex interplay between external incentives\n(prompt-specified constraints such as the 60% threshold) and internal\nincentives (culturally-encoded social preferences derived from pre-training),\ndemonstrating that LLM agents naturally balance formal game-theoretic\nrationality with social motivations that characterize human behavior. These\nfindings suggest that a new model of group decision making, which could not be\nhandled in the previous game-theoretic problem setting, can be realized by LLM\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the emergent social dynamics of Large Language Model (LLM)\nagents in a spatially extended El Farol Bar problem, observing how they\nautonomously navigate this classic social dilemma. As a result, the LLM agents\ngenerated a spontaneous motivation to go to the bar and changed their decision\nmaking by becoming a collective. We also observed that the LLM agents did not\nsolve the problem completely, but rather behaved more like humans. These\nfindings reveal a complex interplay between external incentives\n(prompt-specified constraints such as the 60% threshold) and internal\nincentives (culturally-encoded social preferences derived from pre-training),\ndemonstrating that LLM agents naturally balance formal game-theoretic\nrationality with social motivations that characterize human behavior. These\nfindings suggest that a new model of group decision making, which could not be\nhandled in the previous game-theoretic problem setting, can be realized by LLM\nagents."
                },
                "authors": [
                    {
                        "name": "Ryosuke Takata"
                    },
                    {
                        "name": "Atsushi Masumori"
                    },
                    {
                        "name": "Takashi Ikegami"
                    }
                ],
                "author_detail": {
                    "name": "Takashi Ikegami"
                },
                "author": "Takashi Ikegami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04537v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04537v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06579v1",
                "updated": "2025-09-08T11:49:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    49,
                    51,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T11:49:51Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    49,
                    51,
                    0,
                    251,
                    0
                ],
                "title": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View\n  Synthesis"
                },
                "summary": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html."
                },
                "authors": [
                    {
                        "name": "Xin Kong"
                    },
                    {
                        "name": "Daniel Watson"
                    },
                    {
                        "name": "Yannick Strümpler"
                    },
                    {
                        "name": "Michael Niemeyer"
                    },
                    {
                        "name": "Federico Tombari"
                    }
                ],
                "author_detail": {
                    "name": "Federico Tombari"
                },
                "author": "Federico Tombari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06572v1",
                "updated": "2025-09-08T11:35:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    35,
                    32,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T11:35:32Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    35,
                    32,
                    0,
                    251,
                    0
                ],
                "title": "Mind Your Server: A Systematic Study of Parasitic Toolchain Attacks on\n  the MCP Ecosystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind Your Server: A Systematic Study of Parasitic Toolchain Attacks on\n  the MCP Ecosystem"
                },
                "summary": "Large language models (LLMs) are increasingly integrated with external\nsystems through the Model Context Protocol (MCP), which standardizes tool\ninvocation and has rapidly become a backbone for LLM-powered applications.\nWhile this paradigm enhances functionality, it also introduces a fundamental\nsecurity shift: LLMs transition from passive information processors to\nautonomous orchestrators of task-oriented toolchains, expanding the attack\nsurface, elevating adversarial goals from manipulating single outputs to\nhijacking entire execution flows. In this paper, we reveal a new class of\nattacks, Parasitic Toolchain Attacks, instantiated as MCP Unintended Privacy\nDisclosure (MCP-UPD). These attacks require no direct victim interaction;\ninstead, adversaries embed malicious instructions into external data sources\nthat LLMs access during legitimate tasks. The malicious logic infiltrates the\ntoolchain and unfolds in three phases: Parasitic Ingestion, Privacy Collection,\nand Privacy Disclosure, culminating in stealthy exfiltration of private data.\nOur root cause analysis reveals that MCP lacks both context-tool isolation and\nleast-privilege enforcement, enabling adversarial instructions to propagate\nunchecked into sensitive tool invocations. To assess the severity, we design\nMCP-SEC and conduct the first large-scale security census of the MCP ecosystem,\nanalyzing 12,230 tools across 1,360 servers. Our findings show that the MCP\necosystem is rife with exploitable gadgets and diverse attack methods,\nunderscoring systemic risks in MCP platforms and the urgent need for defense\nmechanisms in LLM-integrated environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly integrated with external\nsystems through the Model Context Protocol (MCP), which standardizes tool\ninvocation and has rapidly become a backbone for LLM-powered applications.\nWhile this paradigm enhances functionality, it also introduces a fundamental\nsecurity shift: LLMs transition from passive information processors to\nautonomous orchestrators of task-oriented toolchains, expanding the attack\nsurface, elevating adversarial goals from manipulating single outputs to\nhijacking entire execution flows. In this paper, we reveal a new class of\nattacks, Parasitic Toolchain Attacks, instantiated as MCP Unintended Privacy\nDisclosure (MCP-UPD). These attacks require no direct victim interaction;\ninstead, adversaries embed malicious instructions into external data sources\nthat LLMs access during legitimate tasks. The malicious logic infiltrates the\ntoolchain and unfolds in three phases: Parasitic Ingestion, Privacy Collection,\nand Privacy Disclosure, culminating in stealthy exfiltration of private data.\nOur root cause analysis reveals that MCP lacks both context-tool isolation and\nleast-privilege enforcement, enabling adversarial instructions to propagate\nunchecked into sensitive tool invocations. To assess the severity, we design\nMCP-SEC and conduct the first large-scale security census of the MCP ecosystem,\nanalyzing 12,230 tools across 1,360 servers. Our findings show that the MCP\necosystem is rife with exploitable gadgets and diverse attack methods,\nunderscoring systemic risks in MCP platforms and the urgent need for defense\nmechanisms in LLM-integrated environments."
                },
                "authors": [
                    {
                        "name": "Shuli Zhao"
                    },
                    {
                        "name": "Qinsheng Hou"
                    },
                    {
                        "name": "Zihan Zhan"
                    },
                    {
                        "name": "Yanhao Wang"
                    },
                    {
                        "name": "Yuchong Xie"
                    },
                    {
                        "name": "Yu Guo"
                    },
                    {
                        "name": "Libo Chen"
                    },
                    {
                        "name": "Shenghong Li"
                    },
                    {
                        "name": "Zhi Xue"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Xue"
                },
                "author": "Zhi Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06550v1",
                "updated": "2025-09-08T11:04:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    4,
                    10,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T11:04:10Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    4,
                    10,
                    0,
                    251,
                    0
                ],
                "title": "Contrastive Self-Supervised Network Intrusion Detection using Augmented\n  Negative Pairs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Self-Supervised Network Intrusion Detection using Augmented\n  Negative Pairs"
                },
                "summary": "Network intrusion detection remains a critical challenge in cybersecurity.\nWhile supervised machine learning models achieve state-of-the-art performance,\ntheir reliance on large labelled datasets makes them impractical for many\nreal-world applications. Anomaly detection methods, which train exclusively on\nbenign traffic to identify malicious activity, suffer from high false positive\nrates, limiting their usability. Recently, self-supervised learning techniques\nhave demonstrated improved performance with lower false positive rates by\nlearning discriminative latent representations of benign traffic. In\nparticular, contrastive self-supervised models achieve this by minimizing the\ndistance between similar (positive) views of benign traffic while maximizing it\nbetween dissimilar (negative) views. Existing approaches generate positive\nviews through data augmentation and treat other samples as negative. In\ncontrast, this work introduces Contrastive Learning using Augmented Negative\npairs (CLAN), a novel paradigm for network intrusion detection where augmented\nsamples are treated as negative views - representing potentially malicious\ndistributions - while other benign samples serve as positive views. This\napproach enhances both classification accuracy and inference efficiency after\npretraining on benign traffic. Experimental evaluation on the Lycos2017 dataset\ndemonstrates that the proposed method surpasses existing self-supervised and\nanomaly detection techniques in a binary classification task. Furthermore, when\nfine-tuned on a limited labelled dataset, the proposed approach achieves\nsuperior multi-class classification performance compared to existing\nself-supervised models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network intrusion detection remains a critical challenge in cybersecurity.\nWhile supervised machine learning models achieve state-of-the-art performance,\ntheir reliance on large labelled datasets makes them impractical for many\nreal-world applications. Anomaly detection methods, which train exclusively on\nbenign traffic to identify malicious activity, suffer from high false positive\nrates, limiting their usability. Recently, self-supervised learning techniques\nhave demonstrated improved performance with lower false positive rates by\nlearning discriminative latent representations of benign traffic. In\nparticular, contrastive self-supervised models achieve this by minimizing the\ndistance between similar (positive) views of benign traffic while maximizing it\nbetween dissimilar (negative) views. Existing approaches generate positive\nviews through data augmentation and treat other samples as negative. In\ncontrast, this work introduces Contrastive Learning using Augmented Negative\npairs (CLAN), a novel paradigm for network intrusion detection where augmented\nsamples are treated as negative views - representing potentially malicious\ndistributions - while other benign samples serve as positive views. This\napproach enhances both classification accuracy and inference efficiency after\npretraining on benign traffic. Experimental evaluation on the Lycos2017 dataset\ndemonstrates that the proposed method surpasses existing self-supervised and\nanomaly detection techniques in a binary classification task. Furthermore, when\nfine-tuned on a limited labelled dataset, the proposed approach achieves\nsuperior multi-class classification performance compared to existing\nself-supervised models."
                },
                "authors": [
                    {
                        "name": "Jack Wilkie"
                    },
                    {
                        "name": "Hanan Hindy"
                    },
                    {
                        "name": "Christos Tachtatzis"
                    },
                    {
                        "name": "Robert Atkinson"
                    }
                ],
                "author_detail": {
                    "name": "Robert Atkinson"
                },
                "author": "Robert Atkinson",
                "arxiv_doi": "10.1109/CSR64739.2025.11129979",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/CSR64739.2025.11129979",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.06550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: Proceedings of IEEE Conference on Cyber Security and\n  Resilience (CSR), 2025. Official version:\n  https://doi.org/10.1109/CSR64739.2025.11129979 Code:\n  https://github.com/jackwilkie/CLAN",
                "arxiv_journal_ref": "2025 IEEE International Conference on Cyber Security and\n  Resilience (CSR), Chania, Crete, Greece, 2025, pp. 206-213",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03646v2",
                "updated": "2025-09-08T11:03:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    3,
                    11,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-03T18:52:49Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    18,
                    52,
                    49,
                    2,
                    246,
                    0
                ],
                "title": "Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning"
                },
                "summary": "Reinforcement Learning (RL) has proven highly effective at enhancing the\ncomplex reasoning abilities of Large Language Models (LLMs), yet underlying\nmechanisms driving this success remain largely opaque. Our analysis reveals\nthat puzzling phenomena like ``aha moments\", ``length-scaling'' and entropy\ndynamics are not disparate occurrences but hallmarks of an emergent reasoning\nhierarchy, akin to the separation of high-level strategic planning from\nlow-level procedural execution in human cognition. We uncover a compelling\ntwo-phase dynamic: initially, a model is constrained by procedural correctness\nand must improve its low-level skills. The learning bottleneck then decisively\nshifts, with performance gains being driven by the exploration and mastery of\nhigh-level strategic planning. This insight exposes a core inefficiency in\nprevailing RL algorithms like GRPO, which apply optimization pressure\nagnostically and dilute the learning signal across all tokens. To address this,\nwe propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that\nconcentrates optimization efforts on high-impact planning tokens. HICRA\nsignificantly outperforms strong baselines, demonstrating that focusing on this\nstrategic bottleneck is key to unlocking advanced reasoning. Furthermore, we\nvalidate semantic entropy as a superior compass for measuring strategic\nexploration over misleading metrics such as token-level entropy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has proven highly effective at enhancing the\ncomplex reasoning abilities of Large Language Models (LLMs), yet underlying\nmechanisms driving this success remain largely opaque. Our analysis reveals\nthat puzzling phenomena like ``aha moments\", ``length-scaling'' and entropy\ndynamics are not disparate occurrences but hallmarks of an emergent reasoning\nhierarchy, akin to the separation of high-level strategic planning from\nlow-level procedural execution in human cognition. We uncover a compelling\ntwo-phase dynamic: initially, a model is constrained by procedural correctness\nand must improve its low-level skills. The learning bottleneck then decisively\nshifts, with performance gains being driven by the exploration and mastery of\nhigh-level strategic planning. This insight exposes a core inefficiency in\nprevailing RL algorithms like GRPO, which apply optimization pressure\nagnostically and dilute the learning signal across all tokens. To address this,\nwe propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that\nconcentrates optimization efforts on high-impact planning tokens. HICRA\nsignificantly outperforms strong baselines, demonstrating that focusing on this\nstrategic bottleneck is key to unlocking advanced reasoning. Furthermore, we\nvalidate semantic entropy as a superior compass for measuring strategic\nexploration over misleading metrics such as token-level entropy."
                },
                "authors": [
                    {
                        "name": "Haozhe Wang"
                    },
                    {
                        "name": "Qixin Xu"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Junhong Wu"
                    },
                    {
                        "name": "Fangzhen Lin"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2406.06464v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06464v4",
                "updated": "2025-09-08T17:59:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    59,
                    48,
                    0,
                    251,
                    0
                ],
                "published": "2024-06-10T17:00:54Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    17,
                    0,
                    54,
                    0,
                    162,
                    0
                ],
                "title": "Transforming Wearable Data into Personal Health Insights using Large\n  Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transforming Wearable Data into Personal Health Insights using Large\n  Language Model Agents"
                },
                "summary": "Deriving personalized insights from popular wearable trackers requires\ncomplex numerical reasoning that challenges standard LLMs, necessitating\ntool-based approaches like code generation. Large language model (LLM) agents\npresent a promising yet largely untapped solution for this analysis at scale.\nWe introduce the Personal Health Insights Agent (PHIA), a system leveraging\nmultistep reasoning with code generation and information retrieval to analyze\nand interpret behavioral health data. To test its capabilities, we create and\nshare two benchmark datasets with over 4000 health insights questions. A\n650-hour human expert evaluation shows that PHIA significantly outperforms a\nstrong code generation baseline, achieving 84% accuracy on objective, numerical\nquestions and, for open-ended ones, earning 83% favorable ratings while being\ntwice as likely to achieve the highest quality rating. This work can advance\nbehavioral health by empowering individuals to understand their data, enabling\na new era of accessible, personalized, and data-driven wellness for the wider\npopulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deriving personalized insights from popular wearable trackers requires\ncomplex numerical reasoning that challenges standard LLMs, necessitating\ntool-based approaches like code generation. Large language model (LLM) agents\npresent a promising yet largely untapped solution for this analysis at scale.\nWe introduce the Personal Health Insights Agent (PHIA), a system leveraging\nmultistep reasoning with code generation and information retrieval to analyze\nand interpret behavioral health data. To test its capabilities, we create and\nshare two benchmark datasets with over 4000 health insights questions. A\n650-hour human expert evaluation shows that PHIA significantly outperforms a\nstrong code generation baseline, achieving 84% accuracy on objective, numerical\nquestions and, for open-ended ones, earning 83% favorable ratings while being\ntwice as likely to achieve the highest quality rating. This work can advance\nbehavioral health by empowering individuals to understand their data, enabling\na new era of accessible, personalized, and data-driven wellness for the wider\npopulation."
                },
                "authors": [
                    {
                        "name": "Mike A. Merrill"
                    },
                    {
                        "name": "Akshay Paruchuri"
                    },
                    {
                        "name": "Naghmeh Rezaei"
                    },
                    {
                        "name": "Geza Kovacs"
                    },
                    {
                        "name": "Javier Perez"
                    },
                    {
                        "name": "Yun Liu"
                    },
                    {
                        "name": "Erik Schenck"
                    },
                    {
                        "name": "Nova Hammerquist"
                    },
                    {
                        "name": "Jake Sunshine"
                    },
                    {
                        "name": "Shyam Tailor"
                    },
                    {
                        "name": "Kumar Ayush"
                    },
                    {
                        "name": "Hao-Wei Su"
                    },
                    {
                        "name": "Qian He"
                    },
                    {
                        "name": "Cory Y. McLean"
                    },
                    {
                        "name": "Mark Malhotra"
                    },
                    {
                        "name": "Shwetak Patel"
                    },
                    {
                        "name": "Jiening Zhan"
                    },
                    {
                        "name": "Tim Althoff"
                    },
                    {
                        "name": "Daniel McDuff"
                    },
                    {
                        "name": "Xin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Liu"
                },
                "author": "Xin Liu",
                "arxiv_comment": "53 pages, 7 main figures, 2 main tables, accepted to Nature\n  Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06464v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06464v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06949v1",
                "updated": "2025-09-08T17:58:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:58:06Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models"
                },
                "summary": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL"
                },
                "authors": [
                    {
                        "name": "Yinjie Wang"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ke Shen"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06948v1",
                "updated": "2025-09-08T17:58:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    2,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:58:02Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    2,
                    0,
                    251,
                    0
                ],
                "title": "Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning"
                },
                "summary": "Reinforcement learning (RL) has proven effective in incentivizing the\nreasoning abilities of large language models (LLMs), but suffers from severe\nefficiency challenges due to its trial-and-error nature. While the common\npractice employs supervised fine-tuning (SFT) as a warm-up stage for RL, this\ndecoupled two-stage approach limits interaction between SFT and RL, thereby\nconstraining overall effectiveness. This study introduces a novel method for\nlearning reasoning models that employs bilevel optimization to facilitate\nbetter cooperation between these training paradigms. By conditioning the SFT\nobjective on the optimal RL policy, our approach enables SFT to meta-learn how\nto guide RL's optimization process. During training, the lower level performs\nRL updates while simultaneously receiving SFT supervision, and the upper level\nexplicitly maximizes the cooperative gain-the performance advantage of joint\nSFT-RL training over RL alone. Empirical evaluations on five reasoning\nbenchmarks demonstrate that our method consistently outperforms baselines and\nachieves a better balance between effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has proven effective in incentivizing the\nreasoning abilities of large language models (LLMs), but suffers from severe\nefficiency challenges due to its trial-and-error nature. While the common\npractice employs supervised fine-tuning (SFT) as a warm-up stage for RL, this\ndecoupled two-stage approach limits interaction between SFT and RL, thereby\nconstraining overall effectiveness. This study introduces a novel method for\nlearning reasoning models that employs bilevel optimization to facilitate\nbetter cooperation between these training paradigms. By conditioning the SFT\nobjective on the optimal RL policy, our approach enables SFT to meta-learn how\nto guide RL's optimization process. During training, the lower level performs\nRL updates while simultaneously receiving SFT supervision, and the upper level\nexplicitly maximizes the cooperative gain-the performance advantage of joint\nSFT-RL training over RL alone. Empirical evaluations on five reasoning\nbenchmarks demonstrate that our method consistently outperforms baselines and\nachieves a better balance between effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Xueting Han"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Jing Bai"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kam-Fai Wong"
                },
                "author": "Kam-Fai Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06941v1",
                "updated": "2025-09-08T17:52:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    52,
                    56,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:52:56Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    52,
                    56,
                    0,
                    251,
                    0
                ],
                "title": "Outcome-based Exploration for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outcome-based Exploration for LLM Reasoning"
                },
                "summary": "Reinforcement learning (RL) has emerged as a powerful method for improving\nthe reasoning abilities of large language models (LLMs). Outcome-based RL,\nwhich rewards policies solely for the correctness of the final answer, yields\nsubstantial accuracy gains but also induces a systematic loss in generation\ndiversity. This collapse undermines real-world performance, where diversity is\ncritical for test-time scaling. We analyze this phenomenon by viewing RL\npost-training as a sampling process and show that, strikingly, RL can reduce\neffective diversity even on the training set relative to the base model. Our\nstudy highlights two central findings: (i) a transfer of diversity degradation,\nwhere reduced diversity on solved problems propagates to unsolved ones, and\n(ii) the tractability of the outcome space, since reasoning tasks admit only a\nlimited set of distinct answers. Motivated by these insights, we propose\noutcome-based exploration, which assigns exploration bonuses according to final\noutcomes. We introduce two complementary algorithms: historical exploration,\nwhich encourages rarely observed answers via UCB-style bonuses, and batch\nexploration, which penalizes within-batch repetition to promote test-time\ndiversity. Experiments on standard competition math with Llama and Qwen models\ndemonstrate that both methods improve accuracy while mitigating diversity\ncollapse. On the theoretical side, we formalize the benefit of outcome-based\nexploration through a new model of outcome-based bandits. Together, these\ncontributions chart a practical path toward RL methods that enhance reasoning\nwithout sacrificing the diversity essential for scalable deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has emerged as a powerful method for improving\nthe reasoning abilities of large language models (LLMs). Outcome-based RL,\nwhich rewards policies solely for the correctness of the final answer, yields\nsubstantial accuracy gains but also induces a systematic loss in generation\ndiversity. This collapse undermines real-world performance, where diversity is\ncritical for test-time scaling. We analyze this phenomenon by viewing RL\npost-training as a sampling process and show that, strikingly, RL can reduce\neffective diversity even on the training set relative to the base model. Our\nstudy highlights two central findings: (i) a transfer of diversity degradation,\nwhere reduced diversity on solved problems propagates to unsolved ones, and\n(ii) the tractability of the outcome space, since reasoning tasks admit only a\nlimited set of distinct answers. Motivated by these insights, we propose\noutcome-based exploration, which assigns exploration bonuses according to final\noutcomes. We introduce two complementary algorithms: historical exploration,\nwhich encourages rarely observed answers via UCB-style bonuses, and batch\nexploration, which penalizes within-batch repetition to promote test-time\ndiversity. Experiments on standard competition math with Llama and Qwen models\ndemonstrate that both methods improve accuracy while mitigating diversity\ncollapse. On the theoretical side, we formalize the benefit of outcome-based\nexploration through a new model of outcome-based bandits. Together, these\ncontributions chart a practical path toward RL methods that enhance reasoning\nwithout sacrificing the diversity essential for scalable deployment."
                },
                "authors": [
                    {
                        "name": "Yuda Song"
                    },
                    {
                        "name": "Julia Kempe"
                    },
                    {
                        "name": "Remi Munos"
                    }
                ],
                "author_detail": {
                    "name": "Remi Munos"
                },
                "author": "Remi Munos",
                "arxiv_comment": "26 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01113v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01113v3",
                "updated": "2025-09-08T17:43:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    43,
                    53,
                    0,
                    251,
                    0
                ],
                "published": "2024-12-02T04:35:54Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    4,
                    35,
                    54,
                    0,
                    337,
                    0
                ],
                "title": "Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in\n  Multi-Hop Arithmetic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in\n  Multi-Hop Arithmetic Reasoning"
                },
                "summary": "This study investigates the incremental, internal problem-solving process of\nlanguage models (LMs) with arithmetic multi-hop reasoning as a case study. We\nspecifically investigate when LMs internally resolve sub/whole problems through\nfirst reading the problem statements, generating reasoning chains, and\nachieving the final answer to mechanistically interpret LMs' multi-hop\nproblem-solving process. Our experiments reveal a systematic incremental\nreasoning strategy underlying LMs. They have not derived an answer at the\nmoment they first read the problem; instead, they obtain (sub)answers while\ngenerating the reasoning chain. Therefore, the generated reasoning chains can\nbe regarded as faithful reflections of the model's internal computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the incremental, internal problem-solving process of\nlanguage models (LMs) with arithmetic multi-hop reasoning as a case study. We\nspecifically investigate when LMs internally resolve sub/whole problems through\nfirst reading the problem statements, generating reasoning chains, and\nachieving the final answer to mechanistically interpret LMs' multi-hop\nproblem-solving process. Our experiments reveal a systematic incremental\nreasoning strategy underlying LMs. They have not derived an answer at the\nmoment they first read the problem; instead, they obtain (sub)answers while\ngenerating the reasoning chain. Therefore, the generated reasoning chains can\nbe regarded as faithful reflections of the model's internal computation."
                },
                "authors": [
                    {
                        "name": "Keito Kudo"
                    },
                    {
                        "name": "Yoichi Aoki"
                    },
                    {
                        "name": "Tatsuki Kuribayashi"
                    },
                    {
                        "name": "Shusaku Sone"
                    },
                    {
                        "name": "Masaya Taniguchi"
                    },
                    {
                        "name": "Ana Brassard"
                    },
                    {
                        "name": "Keisuke Sakaguchi"
                    },
                    {
                        "name": "Kentaro Inui"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Inui"
                },
                "author": "Kentaro Inui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01113v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01113v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06927v1",
                "updated": "2025-09-08T17:39:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    39,
                    3,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:39:03Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    39,
                    3,
                    0,
                    251,
                    0
                ],
                "title": "NeedForHeat DataGear: An Open Monitoring System to Accelerate the\n  Residential Heating Transition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeedForHeat DataGear: An Open Monitoring System to Accelerate the\n  Residential Heating Transition"
                },
                "summary": "We introduce NeedForHeat DataGear: an open hardware and open software data\ncollection system designed to accelerate the residential heating transition.\nNeedForHeat DataGear collects time series monitoring data in homes that have\nnot yet undergone a heating transition, enabling assessment of real-life\nthermal characteristics, heating system efficiency, and residents' comfort\nneeds. This paper outlines its architecture and functionalities, emphasizing\nits modularity, adaptability, and cost-effectiveness for field data\nacquisition. Unlike conventional domestic monitoring solutions focused on home\nautomation, direct feedback, or post-installation heat pump monitoring, it\nprioritizes time series data we deemed essential to evaluate the current\nsituation in existing homes before the heating transition. Designed for\nseamless deployment across diverse households, NeedForHeat DataGear combines\nopenness, security, and privacy with a low-cost, user-friendly approach, making\nit a valuable tool for researchers, energy professionals, and energy coaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce NeedForHeat DataGear: an open hardware and open software data\ncollection system designed to accelerate the residential heating transition.\nNeedForHeat DataGear collects time series monitoring data in homes that have\nnot yet undergone a heating transition, enabling assessment of real-life\nthermal characteristics, heating system efficiency, and residents' comfort\nneeds. This paper outlines its architecture and functionalities, emphasizing\nits modularity, adaptability, and cost-effectiveness for field data\nacquisition. Unlike conventional domestic monitoring solutions focused on home\nautomation, direct feedback, or post-installation heat pump monitoring, it\nprioritizes time series data we deemed essential to evaluate the current\nsituation in existing homes before the heating transition. Designed for\nseamless deployment across diverse households, NeedForHeat DataGear combines\nopenness, security, and privacy with a low-cost, user-friendly approach, making\nit a valuable tool for researchers, energy professionals, and energy coaches."
                },
                "authors": [
                    {
                        "name": "Henri ter Hofte"
                    },
                    {
                        "name": "Nick van Ravenzwaaij"
                    }
                ],
                "author_detail": {
                    "name": "Nick van Ravenzwaaij"
                },
                "author": "Nick van Ravenzwaaij",
                "arxiv_comment": "10 pages + 3 pages appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06925v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06925v1",
                "updated": "2025-09-08T17:38:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    38,
                    5,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:38:05Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    38,
                    5,
                    0,
                    251,
                    0
                ],
                "title": "Data-driven solar forecasting enables near-optimal economic decisions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven solar forecasting enables near-optimal economic decisions"
                },
                "summary": "Solar energy adoption is critical to achieving net-zero emissions. However,\nit remains difficult for many industrial and commercial actors to decide on\nwhether they should adopt distributed solar-battery systems, which is largely\ndue to the unavailability of fast, low-cost, and high-resolution irradiance\nforecasts. Here, we present SunCastNet, a lightweight data-driven forecasting\nsystem that provides 0.05$^\\circ$, 10-minute resolution predictions of surface\nsolar radiation downwards (SSRD) up to 7 days ahead. SunCastNet, coupled with\nreinforcement learning (RL) for battery scheduling, reduces operational regret\nby 76--93\\% compared to robust decision making (RDM). In 25-year investment\nbacktests, it enables up to five of ten high-emitting industrial sectors per\nregion to cross the commercial viability threshold of 12\\% Internal Rate of\nReturn (IRR). These results show that high-resolution, long-horizon solar\nforecasts can directly translate into measurable economic gains, supporting\nnear-optimal energy operations and accelerating renewable deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solar energy adoption is critical to achieving net-zero emissions. However,\nit remains difficult for many industrial and commercial actors to decide on\nwhether they should adopt distributed solar-battery systems, which is largely\ndue to the unavailability of fast, low-cost, and high-resolution irradiance\nforecasts. Here, we present SunCastNet, a lightweight data-driven forecasting\nsystem that provides 0.05$^\\circ$, 10-minute resolution predictions of surface\nsolar radiation downwards (SSRD) up to 7 days ahead. SunCastNet, coupled with\nreinforcement learning (RL) for battery scheduling, reduces operational regret\nby 76--93\\% compared to robust decision making (RDM). In 25-year investment\nbacktests, it enables up to five of ten high-emitting industrial sectors per\nregion to cross the commercial viability threshold of 12\\% Internal Rate of\nReturn (IRR). These results show that high-resolution, long-horizon solar\nforecasts can directly translate into measurable economic gains, supporting\nnear-optimal energy operations and accelerating renewable deployment."
                },
                "authors": [
                    {
                        "name": "Zhixiang Dai"
                    },
                    {
                        "name": "Minghao Yin"
                    },
                    {
                        "name": "Xuanhong Chen"
                    },
                    {
                        "name": "Alberto Carpentieri"
                    },
                    {
                        "name": "Jussi Leinonen"
                    },
                    {
                        "name": "Boris Bonev"
                    },
                    {
                        "name": "Chengzhe Zhong"
                    },
                    {
                        "name": "Thorsten Kurth"
                    },
                    {
                        "name": "Jingan Sun"
                    },
                    {
                        "name": "Ram Cherukuri"
                    },
                    {
                        "name": "Yuzhou Zhang"
                    },
                    {
                        "name": "Ruihua Zhang"
                    },
                    {
                        "name": "Farah Hariri"
                    },
                    {
                        "name": "Xiaodong Ding"
                    },
                    {
                        "name": "Chuanxiang Zhu"
                    },
                    {
                        "name": "Dake Zhang"
                    },
                    {
                        "name": "Yaodan Cui"
                    },
                    {
                        "name": "Yuxi Lu"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Bin He"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Yixin Zhu"
                    },
                    {
                        "name": "Chenheng Xu"
                    },
                    {
                        "name": "Maofeng Liu"
                    },
                    {
                        "name": "Zeyi Niu"
                    },
                    {
                        "name": "Wanpeng Qi"
                    },
                    {
                        "name": "Xu Shan"
                    },
                    {
                        "name": "Siyuan Xian"
                    },
                    {
                        "name": "Ning Lin"
                    },
                    {
                        "name": "Kairui Feng"
                    }
                ],
                "author_detail": {
                    "name": "Kairui Feng"
                },
                "author": "Kairui Feng",
                "arxiv_comment": "Main text ~12 pages, 4 figures, 0 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06925v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06923v1",
                "updated": "2025-09-08T17:36:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    36,
                    21,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:36:21Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    36,
                    21,
                    0,
                    251,
                    0
                ],
                "title": "Staying in the Sweet Spot: Responsive Reasoning Evolution via\n  Capability-Adaptive Hint Scaffolding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Staying in the Sweet Spot: Responsive Reasoning Evolution via\n  Capability-Adaptive Hint Scaffolding"
                },
                "summary": "Reinforcement learning with verifiable rewards (RLVR) has achieved remarkable\nsuccess in enhancing the reasoning capabilities of large language models\n(LLMs). However, existing RLVR methods often suffer from exploration\ninefficiency due to mismatches between the training data's difficulty and the\nmodel's capability. LLMs fail to discover viable reasoning paths when problems\nare overly difficult, while learning little new capability when problems are\ntoo simple. In this work, we formalize the impact of problem difficulty by\nquantifying the relationship between loss descent speed and rollout accuracy.\nBuilding on this analysis, we propose SEELE, a novel supervision-aided RLVR\nframework that dynamically adjusts problem difficulty to stay within the\nhigh-efficiency region. SEELE augments each training sample by appending a hint\n(part of a full solution) after the original problem. Unlike previous\nhint-based approaches, SEELE deliberately and adaptively adjusts the hint\nlength for each problem to achieve an optimal difficulty. To determine the\noptimal hint length, SEELE employs a multi-round rollout sampling strategy. In\neach round, it fits an item response theory model to the accuracy-hint pairs\ncollected in preceding rounds to predict the required hint length for the next\nround. This instance-level, real-time difficulty adjustment aligns problem\ndifficulty with the evolving model capability, thereby improving exploration\nefficiency. Experimental results show that SEELE outperforms Group Relative\nPolicy Optimization (GRPO) and Supervised Fine-tuning (SFT) by +11.8 and +10.5\npoints, respectively, and surpasses the best previous supervision-aided\napproach by +3.6 points on average across six math reasoning benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable rewards (RLVR) has achieved remarkable\nsuccess in enhancing the reasoning capabilities of large language models\n(LLMs). However, existing RLVR methods often suffer from exploration\ninefficiency due to mismatches between the training data's difficulty and the\nmodel's capability. LLMs fail to discover viable reasoning paths when problems\nare overly difficult, while learning little new capability when problems are\ntoo simple. In this work, we formalize the impact of problem difficulty by\nquantifying the relationship between loss descent speed and rollout accuracy.\nBuilding on this analysis, we propose SEELE, a novel supervision-aided RLVR\nframework that dynamically adjusts problem difficulty to stay within the\nhigh-efficiency region. SEELE augments each training sample by appending a hint\n(part of a full solution) after the original problem. Unlike previous\nhint-based approaches, SEELE deliberately and adaptively adjusts the hint\nlength for each problem to achieve an optimal difficulty. To determine the\noptimal hint length, SEELE employs a multi-round rollout sampling strategy. In\neach round, it fits an item response theory model to the accuracy-hint pairs\ncollected in preceding rounds to predict the required hint length for the next\nround. This instance-level, real-time difficulty adjustment aligns problem\ndifficulty with the evolving model capability, thereby improving exploration\nefficiency. Experimental results show that SEELE outperforms Group Relative\nPolicy Optimization (GRPO) and Supervised Fine-tuning (SFT) by +11.8 and +10.5\npoints, respectively, and surpasses the best previous supervision-aided\napproach by +3.6 points on average across six math reasoning benchmarks."
                },
                "authors": [
                    {
                        "name": "Ziheng Li"
                    },
                    {
                        "name": "Zexu Sun"
                    },
                    {
                        "name": "Jinman Zhao"
                    },
                    {
                        "name": "Erxue Min"
                    },
                    {
                        "name": "Yongcheng Zeng"
                    },
                    {
                        "name": "Hui Wu"
                    },
                    {
                        "name": "Hengyi Cai"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Zhi-Hong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhi-Hong Deng"
                },
                "author": "Zhi-Hong Deng",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06921v1",
                "updated": "2025-09-08T17:33:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    33,
                    59,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:33:59Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    33,
                    59,
                    0,
                    251,
                    0
                ],
                "title": "Neuro-Symbolic AI for Cybersecurity: State of the Art, Challenges, and\n  Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuro-Symbolic AI for Cybersecurity: State of the Art, Challenges, and\n  Opportunities"
                },
                "summary": "Traditional Artificial Intelligence (AI) approaches in cybersecurity exhibit\nfundamental limitations: inadequate conceptual grounding leading to\nnon-robustness against novel attacks; limited instructibility impeding\nanalyst-guided adaptation; and misalignment with cybersecurity objectives.\nNeuro-Symbolic (NeSy) AI has emerged with the potential to revolutionize\ncybersecurity AI. However, there is no systematic understanding of this\nemerging approach. These hybrid systems address critical cybersecurity\nchallenges by combining neural pattern recognition with symbolic reasoning,\nenabling enhanced threat understanding while introducing concerning autonomous\noffensive capabilities that reshape threat landscapes. In this survey, we\nsystematically characterize this field by analyzing 127 publications spanning\n2019-July 2025. We introduce a Grounding-Instructibility-Alignment (G-I-A)\nframework to evaluate these systems, focusing on both cyber defense and cyber\noffense across network security, malware analysis, and cyber operations. Our\nanalysis shows advantages of multi-agent NeSy architectures and identifies\ncritical implementation challenges including standardization gaps,\ncomputational complexity, and human-AI collaboration requirements that\nconstrain deployment. We show that causal reasoning integration is the most\ntransformative advancement, enabling proactive defense beyond correlation-based\napproaches. Our findings highlight dual-use implications where autonomous\nsystems demonstrate substantial capabilities in zero-day exploitation while\nachieving significant cost reductions, altering threat dynamics. We provide\ninsights and future research directions, emphasizing the urgent need for\ncommunity-driven standardization frameworks and responsible development\npractices that ensure advancement serves defensive cybersecurity objectives\nwhile maintaining societal alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional Artificial Intelligence (AI) approaches in cybersecurity exhibit\nfundamental limitations: inadequate conceptual grounding leading to\nnon-robustness against novel attacks; limited instructibility impeding\nanalyst-guided adaptation; and misalignment with cybersecurity objectives.\nNeuro-Symbolic (NeSy) AI has emerged with the potential to revolutionize\ncybersecurity AI. However, there is no systematic understanding of this\nemerging approach. These hybrid systems address critical cybersecurity\nchallenges by combining neural pattern recognition with symbolic reasoning,\nenabling enhanced threat understanding while introducing concerning autonomous\noffensive capabilities that reshape threat landscapes. In this survey, we\nsystematically characterize this field by analyzing 127 publications spanning\n2019-July 2025. We introduce a Grounding-Instructibility-Alignment (G-I-A)\nframework to evaluate these systems, focusing on both cyber defense and cyber\noffense across network security, malware analysis, and cyber operations. Our\nanalysis shows advantages of multi-agent NeSy architectures and identifies\ncritical implementation challenges including standardization gaps,\ncomputational complexity, and human-AI collaboration requirements that\nconstrain deployment. We show that causal reasoning integration is the most\ntransformative advancement, enabling proactive defense beyond correlation-based\napproaches. Our findings highlight dual-use implications where autonomous\nsystems demonstrate substantial capabilities in zero-day exploitation while\nachieving significant cost reductions, altering threat dynamics. We provide\ninsights and future research directions, emphasizing the urgent need for\ncommunity-driven standardization frameworks and responsible development\npractices that ensure advancement serves defensive cybersecurity objectives\nwhile maintaining societal alignment."
                },
                "authors": [
                    {
                        "name": "Safayat Bin Hakim"
                    },
                    {
                        "name": "Muhammad Adil"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    },
                    {
                        "name": "Shouhuai Xu"
                    },
                    {
                        "name": "Houbing Herbert Song"
                    }
                ],
                "author_detail": {
                    "name": "Houbing Herbert Song"
                },
                "author": "Houbing Herbert Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06920v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06920v1",
                "updated": "2025-09-08T17:32:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    32,
                    17,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:32:17Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    32,
                    17,
                    0,
                    251,
                    0
                ],
                "title": "An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and\n  Detection"
                },
                "summary": "Insider threats are a growing organizational problem due to the complexity of\nidentifying their technical and behavioral elements. A large research body is\ndedicated to the study of insider threats from technological, psychological,\nand educational perspectives. However, research in this domain has been\ngenerally dependent on datasets that are static and limited access which\nrestricts the development of adaptive detection models. This study introduces a\nnovel, ethically grounded approach that uses the large language model (LLM)\nClaude Sonnet 3.7 to dynamically synthesize syslog messages, some of which\ncontain indicators of insider threat scenarios. The messages reflect real-world\ndata distributions by being highly imbalanced (1% insider threats). The syslogs\nwere analyzed for insider threats by both Claude Sonnet 3.7 and GPT-4o, with\ntheir performance evaluated through statistical metrics including precision,\nrecall, MCC, and ROC AUC. Sonnet 3.7 consistently outperformed GPT-4o across\nnearly all metrics, particularly in reducing false alarms and improving\ndetection accuracy. The results show strong promise for the use of LLMs in\nsynthetic dataset generation and insider threat detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insider threats are a growing organizational problem due to the complexity of\nidentifying their technical and behavioral elements. A large research body is\ndedicated to the study of insider threats from technological, psychological,\nand educational perspectives. However, research in this domain has been\ngenerally dependent on datasets that are static and limited access which\nrestricts the development of adaptive detection models. This study introduces a\nnovel, ethically grounded approach that uses the large language model (LLM)\nClaude Sonnet 3.7 to dynamically synthesize syslog messages, some of which\ncontain indicators of insider threat scenarios. The messages reflect real-world\ndata distributions by being highly imbalanced (1% insider threats). The syslogs\nwere analyzed for insider threats by both Claude Sonnet 3.7 and GPT-4o, with\ntheir performance evaluated through statistical metrics including precision,\nrecall, MCC, and ROC AUC. Sonnet 3.7 consistently outperformed GPT-4o across\nnearly all metrics, particularly in reducing false alarms and improving\ndetection accuracy. The results show strong promise for the use of LLMs in\nsynthetic dataset generation and insider threat detection."
                },
                "authors": [
                    {
                        "name": "Haywood Gelman"
                    },
                    {
                        "name": "John D. Hastings"
                    },
                    {
                        "name": "David Kenley"
                    }
                ],
                "author_detail": {
                    "name": "David Kenley"
                },
                "author": "David Kenley",
                "arxiv_comment": "6 pages, 5 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06920v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06920v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.0; I.2.7; K.4.1; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03377v2",
                "updated": "2025-09-08T17:22:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    22,
                    17,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-03T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    53,
                    45,
                    2,
                    246,
                    0
                ],
                "title": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing"
                },
                "summary": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06902v1",
                "updated": "2025-09-08T17:20:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    20,
                    16,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:20:16Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    20,
                    16,
                    0,
                    251,
                    0
                ],
                "title": "Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers\n  from LLMs via Claim Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers\n  from LLMs via Claim Verification"
                },
                "summary": "Large Language Models (LLMs) as stochastic systems may generate numbers that\ndeviate from available data, a failure known as \\emph{numeric hallucination}.\nExisting safeguards -- retrieval-augmented generation, citations, and\nuncertainty estimation -- improve transparency but cannot guarantee fidelity:\nfabricated or misquoted values may still be displayed as if correct. We propose\n\\textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that\nenforces numeric fidelity through mechanical verification. Under PCN, numeric\nspans are emitted as \\emph{claim-bound tokens} tied to structured claims, and a\nverifier checks each token under a declared policy (e.g., exact equality,\nrounding, aliases, or tolerance with qualifiers). Crucially, PCN places\nverification in the \\emph{renderer}, not the model: only claim-checked numbers\nare marked as verified, and all others default to unverified. This separation\nprevents spoofing and guarantees fail-closed behavior. We formalize PCN and\nprove soundness, completeness under honest tokens, fail-closed behavior, and\nmonotonicity under policy refinement. PCN is lightweight and model-agnostic,\nintegrates seamlessly into existing applications, and can be extended with\ncryptographic commitments. By enforcing verification as a mandatory step before\ndisplay, PCN establishes a simple contract for numerically sensitive settings:\n\\emph{trust is earned only by proof}, while the absence of a mark communicates\nuncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) as stochastic systems may generate numbers that\ndeviate from available data, a failure known as \\emph{numeric hallucination}.\nExisting safeguards -- retrieval-augmented generation, citations, and\nuncertainty estimation -- improve transparency but cannot guarantee fidelity:\nfabricated or misquoted values may still be displayed as if correct. We propose\n\\textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that\nenforces numeric fidelity through mechanical verification. Under PCN, numeric\nspans are emitted as \\emph{claim-bound tokens} tied to structured claims, and a\nverifier checks each token under a declared policy (e.g., exact equality,\nrounding, aliases, or tolerance with qualifiers). Crucially, PCN places\nverification in the \\emph{renderer}, not the model: only claim-checked numbers\nare marked as verified, and all others default to unverified. This separation\nprevents spoofing and guarantees fail-closed behavior. We formalize PCN and\nprove soundness, completeness under honest tokens, fail-closed behavior, and\nmonotonicity under policy refinement. PCN is lightweight and model-agnostic,\nintegrates seamlessly into existing applications, and can be extended with\ncryptographic commitments. By enforcing verification as a mandatory step before\ndisplay, PCN establishes a simple contract for numerically sensitive settings:\n\\emph{trust is earned only by proof}, while the absence of a mark communicates\nuncertainty."
                },
                "authors": [
                    {
                        "name": "Aivin V. Solatorio"
                    }
                ],
                "author_detail": {
                    "name": "Aivin V. Solatorio"
                },
                "author": "Aivin V. Solatorio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06898v1",
                "updated": "2025-09-08T17:17:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    17,
                    12,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:17:12Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    17,
                    12,
                    0,
                    251,
                    0
                ],
                "title": "BatStation: Toward In-Situ Radar Sensing on 5G Base Stations with\n  Zero-Shot Template Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BatStation: Toward In-Situ Radar Sensing on 5G Base Stations with\n  Zero-Shot Template Generation"
                },
                "summary": "The coexistence between incumbent radar signals and commercial 5G signals\nnecessitates a versatile and ubiquitous radar sensing for efficient and\nadaptive spectrum sharing. In this context, leveraging the densely deployed 5G\nbase stations (BS) for radar sensing is particularly promising, offering both\nwide coverage and immediate feedback to 5G scheduling. However, the targeting\nradar signals are superimposed with concurrent 5G uplink transmissions received\nby the BS, and practical deployment also demands a lightweight, portable radar\nsensing model. This paper presents BatStation, a lightweight, in-situ radar\nsensing framework seamlessly integrated into 5G BSs. BatStation leverages\nuplink resource grids to extract radar signals through three key components:\n(i) radar signal separation to cancel concurrent 5G transmissions and reveal\nthe radar signals, (ii) resource grid reshaping to align time-frequency\nresolution with radar pulse characteristics, and (iii) zero-shot template\ncorrelation based on a portable model trained purely on synthetic data that\nsupports detection, classification, and localization of radar pulses without\nfine-tuning using experimental data. We implement BatStation on a\nsoftware-defined radio (SDR) testbed and evaluate its performance with real 5G\ntraffic in the CBRS band. Results show robust performance across diverse radar\ntypes, achieving detection probabilities of 97.02% (PUCCH) and 79.23% (PUSCH),\nclassification accuracy up to 97.00%, and median localization errors of\n2.68-6.20 MHz (frequency) and 24.6-32.4 microseconds (time). Notably,\nBatStation achieves this performance with a runtime latency of only 0.11/0.94\nms on GPU/CPU, meeting the real-time requirement of 5G networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coexistence between incumbent radar signals and commercial 5G signals\nnecessitates a versatile and ubiquitous radar sensing for efficient and\nadaptive spectrum sharing. In this context, leveraging the densely deployed 5G\nbase stations (BS) for radar sensing is particularly promising, offering both\nwide coverage and immediate feedback to 5G scheduling. However, the targeting\nradar signals are superimposed with concurrent 5G uplink transmissions received\nby the BS, and practical deployment also demands a lightweight, portable radar\nsensing model. This paper presents BatStation, a lightweight, in-situ radar\nsensing framework seamlessly integrated into 5G BSs. BatStation leverages\nuplink resource grids to extract radar signals through three key components:\n(i) radar signal separation to cancel concurrent 5G transmissions and reveal\nthe radar signals, (ii) resource grid reshaping to align time-frequency\nresolution with radar pulse characteristics, and (iii) zero-shot template\ncorrelation based on a portable model trained purely on synthetic data that\nsupports detection, classification, and localization of radar pulses without\nfine-tuning using experimental data. We implement BatStation on a\nsoftware-defined radio (SDR) testbed and evaluate its performance with real 5G\ntraffic in the CBRS band. Results show robust performance across diverse radar\ntypes, achieving detection probabilities of 97.02% (PUCCH) and 79.23% (PUSCH),\nclassification accuracy up to 97.00%, and median localization errors of\n2.68-6.20 MHz (frequency) and 24.6-32.4 microseconds (time). Notably,\nBatStation achieves this performance with a runtime latency of only 0.11/0.94\nms on GPU/CPU, meeting the real-time requirement of 5G networks."
                },
                "authors": [
                    {
                        "name": "Zhihui Gao"
                    },
                    {
                        "name": "Zhecun Liu"
                    },
                    {
                        "name": "Tingjun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tingjun Chen"
                },
                "author": "Tingjun Chen",
                "arxiv_comment": "14 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06896v1",
                "updated": "2025-09-08T17:14:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    14,
                    55,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:14:55Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    14,
                    55,
                    0,
                    251,
                    0
                ],
                "title": "Not All Samples Are Equal: Quantifying Instance-level Difficulty in\n  Targeted Data Poisoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Samples Are Equal: Quantifying Instance-level Difficulty in\n  Targeted Data Poisoning"
                },
                "summary": "Targeted data poisoning attacks pose an increasingly serious threat due to\ntheir ease of deployment and high success rates. These attacks aim to\nmanipulate the prediction for a single test sample in classification models.\nUnlike indiscriminate attacks that aim to decrease overall test performance,\ntargeted attacks present a unique threat to individual test instances. This\nthreat model raises a fundamental question: what factors make certain test\nsamples more susceptible to successful poisoning than others? We investigate\nhow attack difficulty varies across different test instances and identify key\ncharacteristics that influence vulnerability. This paper introduces three\npredictive criteria for targeted data poisoning difficulty: ergodic prediction\naccuracy (analyzed through clean training dynamics), poison distance, and\npoison budget. Our experimental results demonstrate that these metrics\neffectively predict the varying difficulty of real-world targeted poisoning\nattacks across diverse scenarios, offering practitioners valuable insights for\nvulnerability assessment and understanding data poisoning attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Targeted data poisoning attacks pose an increasingly serious threat due to\ntheir ease of deployment and high success rates. These attacks aim to\nmanipulate the prediction for a single test sample in classification models.\nUnlike indiscriminate attacks that aim to decrease overall test performance,\ntargeted attacks present a unique threat to individual test instances. This\nthreat model raises a fundamental question: what factors make certain test\nsamples more susceptible to successful poisoning than others? We investigate\nhow attack difficulty varies across different test instances and identify key\ncharacteristics that influence vulnerability. This paper introduces three\npredictive criteria for targeted data poisoning difficulty: ergodic prediction\naccuracy (analyzed through clean training dynamics), poison distance, and\npoison budget. Our experimental results demonstrate that these metrics\neffectively predict the varying difficulty of real-world targeted poisoning\nattacks across diverse scenarios, offering practitioners valuable insights for\nvulnerability assessment and understanding data poisoning attacks."
                },
                "authors": [
                    {
                        "name": "William Xu"
                    },
                    {
                        "name": "Yiwei Lu"
                    },
                    {
                        "name": "Yihan Wang"
                    },
                    {
                        "name": "Matthew Y. R. Yang"
                    },
                    {
                        "name": "Zuoqiu Liu"
                    },
                    {
                        "name": "Gautam Kamath"
                    },
                    {
                        "name": "Yaoliang Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yaoliang Yu"
                },
                "author": "Yaoliang Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06887v1",
                "updated": "2025-09-08T17:08:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    8,
                    26,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:08:26Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    8,
                    26,
                    0,
                    251,
                    0
                ],
                "title": "UniSearch: Rethinking Search System with a Unified Generative\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniSearch: Rethinking Search System with a Unified Generative\n  Architecture"
                },
                "summary": "Modern search systems play a crucial role in facilitating information\nacquisition. Traditional search engines typically rely on a cascaded\narchitecture, where results are retrieved through recall, pre-ranking, and\nranking stages. The complexity of designing and maintaining multiple modules\nmakes it difficult to achieve holistic performance gains. Recent advances in\ngenerative recommendation have motivated the exploration of unified generative\nsearch as an alternative. However, existing approaches are not genuinely\nend-to-end: they typically train an item encoder to tokenize candidates first\nand then optimize a generator separately, leading to objective inconsistency\nand limited generalization. To address these limitations, we propose UniSearch,\na unified generative search framework for Kuaishou Search. UniSearch replaces\nthe cascaded pipeline with an end-to-end architecture that integrates a Search\nGenerator and a Video Encoder. The Generator produces semantic identifiers of\nrelevant items given a user query, while the Video Encoder learns latent item\nembeddings and provides their tokenized representations. A unified training\nframework jointly optimizes both components, enabling mutual enhancement and\nimproving representation quality and generation accuracy. Furthermore, we\nintroduce Search Preference Optimization (SPO), which leverages a reward model\nand real user feedback to better align generation with user preferences.\nExtensive experiments on industrial-scale datasets, together with online A/B\ntesting in both short-video and live search scenarios, demonstrate the strong\neffectiveness and deployment potential of UniSearch. Notably, its deployment in\nlive search yields the largest single-experiment improvement in recent years of\nour product's history, highlighting its practical value for real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern search systems play a crucial role in facilitating information\nacquisition. Traditional search engines typically rely on a cascaded\narchitecture, where results are retrieved through recall, pre-ranking, and\nranking stages. The complexity of designing and maintaining multiple modules\nmakes it difficult to achieve holistic performance gains. Recent advances in\ngenerative recommendation have motivated the exploration of unified generative\nsearch as an alternative. However, existing approaches are not genuinely\nend-to-end: they typically train an item encoder to tokenize candidates first\nand then optimize a generator separately, leading to objective inconsistency\nand limited generalization. To address these limitations, we propose UniSearch,\na unified generative search framework for Kuaishou Search. UniSearch replaces\nthe cascaded pipeline with an end-to-end architecture that integrates a Search\nGenerator and a Video Encoder. The Generator produces semantic identifiers of\nrelevant items given a user query, while the Video Encoder learns latent item\nembeddings and provides their tokenized representations. A unified training\nframework jointly optimizes both components, enabling mutual enhancement and\nimproving representation quality and generation accuracy. Furthermore, we\nintroduce Search Preference Optimization (SPO), which leverages a reward model\nand real user feedback to better align generation with user preferences.\nExtensive experiments on industrial-scale datasets, together with online A/B\ntesting in both short-video and live search scenarios, demonstrate the strong\neffectiveness and deployment potential of UniSearch. Notably, its deployment in\nlive search yields the largest single-experiment improvement in recent years of\nour product's history, highlighting its practical value for real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Jiahui Chen"
                    },
                    {
                        "name": "Xiaoze Jiang"
                    },
                    {
                        "name": "Zhibo Wang"
                    },
                    {
                        "name": "Quanzhi Zhu"
                    },
                    {
                        "name": "Junyao Zhao"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Kang Pan"
                    },
                    {
                        "name": "Ao Xie"
                    },
                    {
                        "name": "Maohua Pei"
                    },
                    {
                        "name": "Zhiheng Qin"
                    },
                    {
                        "name": "Hongjing Zhang"
                    },
                    {
                        "name": "Zhixin Zhai"
                    },
                    {
                        "name": "Xiaobo Guo"
                    },
                    {
                        "name": "Runbin Zhou"
                    },
                    {
                        "name": "Kefeng Wang"
                    },
                    {
                        "name": "Mingyang Geng"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Jingshan Lv"
                    },
                    {
                        "name": "Yupeng Huang"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Han Li"
                    }
                ],
                "author_detail": {
                    "name": "Han Li"
                },
                "author": "Han Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06885v1",
                "updated": "2025-09-08T17:05:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    5,
                    53,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:05:53Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    5,
                    53,
                    0,
                    251,
                    0
                ],
                "title": "Barlow-Swin: Toward a novel siamese-based segmentation architecture\n  using Swin-Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Barlow-Swin: Toward a novel siamese-based segmentation architecture\n  using Swin-Transformers"
                },
                "summary": "Medical image segmentation is a critical task in clinical workflows,\nparticularly for the detection and delineation of pathological regions. While\nconvolutional architectures like U-Net have become standard for such tasks,\ntheir limited receptive field restricts global context modeling. Recent efforts\nintegrating transformers have addressed this, but often result in deep,\ncomputationally expensive models unsuitable for real-time use. In this work, we\npresent a novel end-to-end lightweight architecture designed specifically for\nreal-time binary medical image segmentation. Our model combines a Swin\nTransformer-like encoder with a U-Net-like decoder, connected via skip pathways\nto preserve spatial detail while capturing contextual information. Unlike\nexisting designs such as Swin Transformer or U-Net, our architecture is\nsignificantly shallower and competitively efficient. To improve the encoder's\nability to learn meaningful features without relying on large amounts of\nlabeled data, we first train it using Barlow Twins, a self-supervised learning\nmethod that helps the model focus on important patterns by reducing unnecessary\nrepetition in the learned features. After this pretraining, we fine-tune the\nentire model for our specific task. Experiments on benchmark binary\nsegmentation tasks demonstrate that our model achieves competitive accuracy\nwith substantially reduced parameter count and faster inference, positioning it\nas a practical alternative for deployment in real-time and resource-limited\nclinical environments. The code for our method is available at Github\nrepository: https://github.com/mkianih/Barlow-Swin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical image segmentation is a critical task in clinical workflows,\nparticularly for the detection and delineation of pathological regions. While\nconvolutional architectures like U-Net have become standard for such tasks,\ntheir limited receptive field restricts global context modeling. Recent efforts\nintegrating transformers have addressed this, but often result in deep,\ncomputationally expensive models unsuitable for real-time use. In this work, we\npresent a novel end-to-end lightweight architecture designed specifically for\nreal-time binary medical image segmentation. Our model combines a Swin\nTransformer-like encoder with a U-Net-like decoder, connected via skip pathways\nto preserve spatial detail while capturing contextual information. Unlike\nexisting designs such as Swin Transformer or U-Net, our architecture is\nsignificantly shallower and competitively efficient. To improve the encoder's\nability to learn meaningful features without relying on large amounts of\nlabeled data, we first train it using Barlow Twins, a self-supervised learning\nmethod that helps the model focus on important patterns by reducing unnecessary\nrepetition in the learned features. After this pretraining, we fine-tune the\nentire model for our specific task. Experiments on benchmark binary\nsegmentation tasks demonstrate that our model achieves competitive accuracy\nwith substantially reduced parameter count and faster inference, positioning it\nas a practical alternative for deployment in real-time and resource-limited\nclinical environments. The code for our method is available at Github\nrepository: https://github.com/mkianih/Barlow-Swin."
                },
                "authors": [
                    {
                        "name": "Morteza Kiani Haftlang"
                    },
                    {
                        "name": "Mohammadhossein Malmir"
                    },
                    {
                        "name": "Foroutan Parand"
                    },
                    {
                        "name": "Umberto Michelucci"
                    },
                    {
                        "name": "Safouane El Ghazouali"
                    }
                ],
                "author_detail": {
                    "name": "Safouane El Ghazouali"
                },
                "author": "Safouane El Ghazouali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05263v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05263v2",
                "updated": "2025-09-08T17:05:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    5,
                    47,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-05T17:22:33Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    22,
                    33,
                    4,
                    248,
                    0
                ],
                "title": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for\n  Interactive Complex World Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for\n  Interactive Complex World Generation"
                },
                "summary": "Recent research has been increasingly focusing on developing 3D world models\nthat simulate complex real-world scenarios. World models have found broad\napplications across various domains, including embodied AI, autonomous driving,\nentertainment, etc. A more realistic simulation with accurate physics will\neffectively narrow the sim-to-real gap and allow us to gather rich information\nabout the real world conveniently. While traditional manual modeling has\nenabled the creation of virtual 3D scenes, modern approaches have leveraged\nadvanced machine learning algorithms for 3D world generation, with most recent\nadvances focusing on generative methods that can create virtual worlds based on\nuser instructions. This work explores such a research direction by proposing\nLatticeWorld, a simple yet effective 3D world generation framework that\nstreamlines the industrial production pipeline of 3D environments. LatticeWorld\nleverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering\nengine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed\nframework accepts textual descriptions and visual instructions as multimodal\ninputs and creates large-scale 3D interactive worlds with dynamic agents,\nfeaturing competitive multi-agent interaction, high-fidelity physics\nsimulation, and real-time rendering. We conduct comprehensive experiments to\nevaluate LatticeWorld, showing that it achieves superior accuracy in scene\nlayout generation and visual fidelity. Moreover, LatticeWorld achieves over a\n$90\\times$ increase in industrial production efficiency while maintaining high\ncreative quality compared with traditional manual production methods. Our demo\nvideo is available at https://youtu.be/8VWZXpERR18",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has been increasingly focusing on developing 3D world models\nthat simulate complex real-world scenarios. World models have found broad\napplications across various domains, including embodied AI, autonomous driving,\nentertainment, etc. A more realistic simulation with accurate physics will\neffectively narrow the sim-to-real gap and allow us to gather rich information\nabout the real world conveniently. While traditional manual modeling has\nenabled the creation of virtual 3D scenes, modern approaches have leveraged\nadvanced machine learning algorithms for 3D world generation, with most recent\nadvances focusing on generative methods that can create virtual worlds based on\nuser instructions. This work explores such a research direction by proposing\nLatticeWorld, a simple yet effective 3D world generation framework that\nstreamlines the industrial production pipeline of 3D environments. LatticeWorld\nleverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering\nengine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed\nframework accepts textual descriptions and visual instructions as multimodal\ninputs and creates large-scale 3D interactive worlds with dynamic agents,\nfeaturing competitive multi-agent interaction, high-fidelity physics\nsimulation, and real-time rendering. We conduct comprehensive experiments to\nevaluate LatticeWorld, showing that it achieves superior accuracy in scene\nlayout generation and visual fidelity. Moreover, LatticeWorld achieves over a\n$90\\times$ increase in industrial production efficiency while maintaining high\ncreative quality compared with traditional manual production methods. Our demo\nvideo is available at https://youtu.be/8VWZXpERR18"
                },
                "authors": [
                    {
                        "name": "Yinglin Duan"
                    },
                    {
                        "name": "Zhengxia Zou"
                    },
                    {
                        "name": "Tongwei Gu"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Zhan Zhao"
                    },
                    {
                        "name": "Luyi Xu"
                    },
                    {
                        "name": "Xinzhu Liu"
                    },
                    {
                        "name": "Yenan Lin"
                    },
                    {
                        "name": "Hao Jiang"
                    },
                    {
                        "name": "Kang Chen"
                    },
                    {
                        "name": "Shuang Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Shuang Qiu"
                },
                "author": "Shuang Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05263v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05263v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06883v1",
                "updated": "2025-09-08T17:02:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    2,
                    34,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:02:34Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    2,
                    34,
                    0,
                    251,
                    0
                ],
                "title": "UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction"
                },
                "summary": "We participate in CheckThat! Task 2 English and explore various methods of\nprompting and in-context learning, including few-shot prompting and fine-tuning\nwith different LLM families, with the goal of extracting check-worthy claims\nfrom social media passages. Our best METEOR score is achieved by fine-tuning a\nFLAN-T5 model. However, we observe that higher-quality claims can sometimes be\nextracted using other methods, even when their METEOR scores are lower.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We participate in CheckThat! Task 2 English and explore various methods of\nprompting and in-context learning, including few-shot prompting and fine-tuning\nwith different LLM families, with the goal of extracting check-worthy claims\nfrom social media passages. Our best METEOR score is achieved by fine-tuning a\nFLAN-T5 model. However, we observe that higher-quality claims can sometimes be\nextracted using other methods, even when their METEOR scores are lower."
                },
                "authors": [
                    {
                        "name": "Joe Wilder"
                    },
                    {
                        "name": "Nikhil Kadapala"
                    },
                    {
                        "name": "Benji Xu"
                    },
                    {
                        "name": "Mohammed Alsaadi"
                    },
                    {
                        "name": "Aiden Parsons"
                    },
                    {
                        "name": "Mitchell Rogers"
                    },
                    {
                        "name": "Palash Agarwal"
                    },
                    {
                        "name": "Adam Hassick"
                    },
                    {
                        "name": "Laura Dietz"
                    }
                ],
                "author_detail": {
                    "name": "Laura Dietz"
                },
                "author": "Laura Dietz",
                "arxiv_comment": "16 pages,3 tables, CLEF 2025 Working Notes, 9-12 September 2025,\n  Madrid, Spain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18504v2",
                "updated": "2025-09-08T16:53:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    53,
                    44,
                    0,
                    251,
                    0
                ],
                "published": "2025-07-24T15:22:27Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    22,
                    27,
                    3,
                    205,
                    0
                ],
                "title": "Not All Features Deserve Attention: Graph-Guided Dependency Learning for\n  Tabular Data Generation with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Features Deserve Attention: Graph-Guided Dependency Learning for\n  Tabular Data Generation with Language Models"
                },
                "summary": "Large Language Models (LLMs) have shown strong potential for tabular data\ngeneration by modeling textualized feature-value pairs. However, tabular data\ninherently exhibits sparse feature-level dependencies, where many feature\ninteractions are structurally insignificant. This creates a fundamental\nmismatch as LLMs' self-attention mechanism inevitably distributes focus across\nall pairs, diluting attention on critical relationships, particularly in\ndatasets with complex dependencies or semantically ambiguous features. To\naddress this limitation, we propose GraDe (Graph-Guided Dependency Learning), a\nnovel method that explicitly integrates sparse dependency graphs into LLMs'\nattention mechanism. GraDe employs a lightweight dynamic graph learning module\nguided by externally extracted functional dependencies, prioritizing key\nfeature interactions while suppressing irrelevant ones. Our experiments across\ndiverse real-world datasets demonstrate that GraDe outperforms existing\nLLM-based approaches by up to 12% on complex datasets while achieving\ncompetitive results with state-of-the-art approaches in synthetic data quality.\nOur method is minimally intrusive yet effective, offering a practical solution\nfor structure-aware tabular data modeling with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown strong potential for tabular data\ngeneration by modeling textualized feature-value pairs. However, tabular data\ninherently exhibits sparse feature-level dependencies, where many feature\ninteractions are structurally insignificant. This creates a fundamental\nmismatch as LLMs' self-attention mechanism inevitably distributes focus across\nall pairs, diluting attention on critical relationships, particularly in\ndatasets with complex dependencies or semantically ambiguous features. To\naddress this limitation, we propose GraDe (Graph-Guided Dependency Learning), a\nnovel method that explicitly integrates sparse dependency graphs into LLMs'\nattention mechanism. GraDe employs a lightweight dynamic graph learning module\nguided by externally extracted functional dependencies, prioritizing key\nfeature interactions while suppressing irrelevant ones. Our experiments across\ndiverse real-world datasets demonstrate that GraDe outperforms existing\nLLM-based approaches by up to 12% on complex datasets while achieving\ncompetitive results with state-of-the-art approaches in synthetic data quality.\nOur method is minimally intrusive yet effective, offering a practical solution\nfor structure-aware tabular data modeling with LLMs."
                },
                "authors": [
                    {
                        "name": "Zheyu Zhang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Bardh Prenkaj"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci",
                "arxiv_comment": "Accepted to EMNLP 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20521v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20521v2",
                "updated": "2025-09-08T16:40:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    40,
                    29,
                    0,
                    251,
                    0
                ],
                "published": "2025-05-26T20:53:53Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    20,
                    53,
                    53,
                    0,
                    146,
                    0
                ],
                "title": "Project Riley: Multimodal Multi-Agent LLM Collaboration with Emotional\n  Reasoning and Voting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Project Riley: Multimodal Multi-Agent LLM Collaboration with Emotional\n  Reasoning and Voting"
                },
                "summary": "This paper presents Project Riley, a novel multimodal and multi-model\nconversational AI architecture oriented towards the simulation of reasoning\ninfluenced by emotional states. Drawing inspiration from Pixar's Inside Out,\nthe system comprises five distinct emotional agents - Joy, Sadness, Fear,\nAnger, and Disgust - that engage in structured multi-round dialogues to\ngenerate, criticise, and iteratively refine responses. A final reasoning\nmechanism synthesises the contributions of these agents into a coherent output\nthat either reflects the dominant emotion or integrates multiple perspectives.\nThe architecture incorporates both textual and visual large language models\n(LLMs), alongside advanced reasoning and self-refinement processes. A\nfunctional prototype was deployed locally in an offline environment, optimised\nfor emotional expressiveness and computational efficiency. From this initial\nprototype, another one emerged, called Armando, which was developed for use in\nemergency contexts, delivering emotionally calibrated and factually accurate\ninformation through the integration of Retrieval-Augmented Generation (RAG) and\ncumulative context tracking. The Project Riley prototype was evaluated through\nuser testing, in which participants interacted with the chatbot and completed a\nstructured questionnaire assessing three dimensions: Emotional Appropriateness,\nClarity and Utility, and Naturalness and Human-likeness. The results indicate\nstrong performance in structured scenarios, particularly with respect to\nemotional alignment and communicative clarity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents Project Riley, a novel multimodal and multi-model\nconversational AI architecture oriented towards the simulation of reasoning\ninfluenced by emotional states. Drawing inspiration from Pixar's Inside Out,\nthe system comprises five distinct emotional agents - Joy, Sadness, Fear,\nAnger, and Disgust - that engage in structured multi-round dialogues to\ngenerate, criticise, and iteratively refine responses. A final reasoning\nmechanism synthesises the contributions of these agents into a coherent output\nthat either reflects the dominant emotion or integrates multiple perspectives.\nThe architecture incorporates both textual and visual large language models\n(LLMs), alongside advanced reasoning and self-refinement processes. A\nfunctional prototype was deployed locally in an offline environment, optimised\nfor emotional expressiveness and computational efficiency. From this initial\nprototype, another one emerged, called Armando, which was developed for use in\nemergency contexts, delivering emotionally calibrated and factually accurate\ninformation through the integration of Retrieval-Augmented Generation (RAG) and\ncumulative context tracking. The Project Riley prototype was evaluated through\nuser testing, in which participants interacted with the chatbot and completed a\nstructured questionnaire assessing three dimensions: Emotional Appropriateness,\nClarity and Utility, and Naturalness and Human-likeness. The results indicate\nstrong performance in structured scenarios, particularly with respect to\nemotional alignment and communicative clarity."
                },
                "authors": [
                    {
                        "name": "Ana Rita Ortigoso"
                    },
                    {
                        "name": "Gabriel Vieira"
                    },
                    {
                        "name": "Daniel Fuentes"
                    },
                    {
                        "name": "Luis Frazão"
                    },
                    {
                        "name": "Nuno Costa"
                    },
                    {
                        "name": "António Pereira"
                    }
                ],
                "author_detail": {
                    "name": "António Pereira"
                },
                "author": "António Pereira",
                "arxiv_comment": "28 pages, 5 figures. Submitted for review to Information Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20521v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20521v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.1; H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06870v1",
                "updated": "2025-09-08T16:39:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    39,
                    38,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T16:39:38Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    39,
                    38,
                    0,
                    251,
                    0
                ],
                "title": "The Majority is not always right: RL training for solution aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Majority is not always right: RL training for solution aggregation"
                },
                "summary": "Scaling up test-time compute, by generating multiple independent solutions\nand selecting or aggregating among them, has become a central paradigm for\nimproving large language models (LLMs) on challenging reasoning tasks. While\nmost prior work relies on simple majority voting or reward model ranking to\naggregate solutions, these approaches may only yield limited benefits. In this\nwork, we propose to learn aggregation as an explicit reasoning skill: given a\nset of candidate solutions, we train an aggregator model to review, reconcile,\nand synthesize a final, correct answer using reinforcement learning from\nverifiable rewards. A key ingredient is careful balancing of easy and hard\ntraining examples, allowing the model to learn both to recover\nminority-but-correct answers as well as easy majority-correct answers.\nEmpirically, we find our method, AggLM, outperforms both strong rule-based and\nreward-model baselines, across multiple benchmarks. Furthermore, it generalizes\neffectively to solutions from differing models, including stronger ones than\ncontained in the training data, all while requiring substantially fewer tokens\nthan majority voting with larger numbers of solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up test-time compute, by generating multiple independent solutions\nand selecting or aggregating among them, has become a central paradigm for\nimproving large language models (LLMs) on challenging reasoning tasks. While\nmost prior work relies on simple majority voting or reward model ranking to\naggregate solutions, these approaches may only yield limited benefits. In this\nwork, we propose to learn aggregation as an explicit reasoning skill: given a\nset of candidate solutions, we train an aggregator model to review, reconcile,\nand synthesize a final, correct answer using reinforcement learning from\nverifiable rewards. A key ingredient is careful balancing of easy and hard\ntraining examples, allowing the model to learn both to recover\nminority-but-correct answers as well as easy majority-correct answers.\nEmpirically, we find our method, AggLM, outperforms both strong rule-based and\nreward-model baselines, across multiple benchmarks. Furthermore, it generalizes\neffectively to solutions from differing models, including stronger ones than\ncontained in the training data, all while requiring substantially fewer tokens\nthan majority voting with larger numbers of solutions."
                },
                "authors": [
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Pranjal Aggarwal"
                    },
                    {
                        "name": "Swarnadeep Saha"
                    },
                    {
                        "name": "Asli Celikyilmaz"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Ilia Kulikov"
                    }
                ],
                "author_detail": {
                    "name": "Ilia Kulikov"
                },
                "author": "Ilia Kulikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18658v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18658v4",
                "updated": "2025-09-08T16:34:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    34,
                    58,
                    0,
                    251,
                    0
                ],
                "published": "2025-02-25T21:37:25Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    21,
                    37,
                    25,
                    1,
                    56,
                    0
                ],
                "title": "Assistance or Disruption? Exploring and Evaluating the Design and\n  Trade-offs of Proactive AI Programming Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assistance or Disruption? Exploring and Evaluating the Design and\n  Trade-offs of Proactive AI Programming Support"
                },
                "summary": "AI programming tools enable powerful code generation, and recent prototypes\nattempt to reduce user effort with proactive AI agents, but their impact on\nprogramming workflows remains unexplored. We introduce and evaluate\nCodellaborator, a design probe LLM agent that initiates programming assistance\nbased on editor activities and task context. We explored three interface\nvariants to assess trade-offs between increasingly salient AI support:\nprompt-only, proactive agent, and proactive agent with presence and context\n(Codellaborator). In a within-subject study (N=18), we find that proactive\nagents increase efficiency compared to prompt-only paradigm, but also incur\nworkflow disruptions. However, presence indicators and interaction context\nsupport alleviated disruptions and improved users' awareness of AI processes.\nWe underscore trade-offs of Codellaborator on user control, ownership, and code\nunderstanding, emphasizing the need to adapt proactivity to programming\nprocesses. Our research contributes to the design exploration and evaluation of\nproactive AI systems, presenting design implications on AI-integrated\nprogramming workflow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI programming tools enable powerful code generation, and recent prototypes\nattempt to reduce user effort with proactive AI agents, but their impact on\nprogramming workflows remains unexplored. We introduce and evaluate\nCodellaborator, a design probe LLM agent that initiates programming assistance\nbased on editor activities and task context. We explored three interface\nvariants to assess trade-offs between increasingly salient AI support:\nprompt-only, proactive agent, and proactive agent with presence and context\n(Codellaborator). In a within-subject study (N=18), we find that proactive\nagents increase efficiency compared to prompt-only paradigm, but also incur\nworkflow disruptions. However, presence indicators and interaction context\nsupport alleviated disruptions and improved users' awareness of AI processes.\nWe underscore trade-offs of Codellaborator on user control, ownership, and code\nunderstanding, emphasizing the need to adapt proactivity to programming\nprocesses. Our research contributes to the design exploration and evaluation of\nproactive AI systems, presenting design implications on AI-integrated\nprogramming workflow."
                },
                "authors": [
                    {
                        "name": "Kevin Pu"
                    },
                    {
                        "name": "Daniel Lazaro"
                    },
                    {
                        "name": "Ian Arawjo"
                    },
                    {
                        "name": "Haijun Xia"
                    },
                    {
                        "name": "Ziang Xiao"
                    },
                    {
                        "name": "Tovi Grossman"
                    },
                    {
                        "name": "Yan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yan Chen"
                },
                "author": "Yan Chen",
                "arxiv_doi": "10.1145/3706598.3713357",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3713357",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.18658v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18658v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06858v1",
                "updated": "2025-09-08T16:26:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    26,
                    45,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T16:26:45Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    26,
                    45,
                    0,
                    251,
                    0
                ],
                "title": "Disentangling Interaction and Bias Effects in Opinion Dynamics of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disentangling Interaction and Bias Effects in Opinion Dynamics of Large\n  Language Models"
                },
                "summary": "Large Language Models are increasingly used to simulate human opinion\ndynamics, yet the effect of genuine interaction is often obscured by systematic\nbiases. We present a Bayesian framework to disentangle and quantify three such\nbiases: (i) a topic bias toward prior opinions in the training data; (ii) an\nagreement bias favoring agreement irrespective of the question; and (iii) an\nanchoring bias toward the initiating agent's stance. Applying this framework to\nmulti-step dialogues reveals that opinion trajectories tend to quickly converge\nto a shared attractor, with the influence of the interaction fading over time,\nand the impact of biases differing between LLMs. In addition, we fine-tune an\nLLM on different sets of strongly opinionated statements (incl. misinformation)\nand demonstrate that the opinion attractor shifts correspondingly. Exposing\nstark differences between LLMs and providing quantitative tools to compare them\nto human subjects in the future, our approach highlights both chances and\npitfalls in using LLMs as proxies for human behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are increasingly used to simulate human opinion\ndynamics, yet the effect of genuine interaction is often obscured by systematic\nbiases. We present a Bayesian framework to disentangle and quantify three such\nbiases: (i) a topic bias toward prior opinions in the training data; (ii) an\nagreement bias favoring agreement irrespective of the question; and (iii) an\nanchoring bias toward the initiating agent's stance. Applying this framework to\nmulti-step dialogues reveals that opinion trajectories tend to quickly converge\nto a shared attractor, with the influence of the interaction fading over time,\nand the impact of biases differing between LLMs. In addition, we fine-tune an\nLLM on different sets of strongly opinionated statements (incl. misinformation)\nand demonstrate that the opinion attractor shifts correspondingly. Exposing\nstark differences between LLMs and providing quantitative tools to compare them\nto human subjects in the future, our approach highlights both chances and\npitfalls in using LLMs as proxies for human behavior."
                },
                "authors": [
                    {
                        "name": "Vincent C. Brockers"
                    },
                    {
                        "name": "David A. Ehrlich"
                    },
                    {
                        "name": "Viola Priesemann"
                    }
                ],
                "author_detail": {
                    "name": "Viola Priesemann"
                },
                "author": "Viola Priesemann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.AO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06855v1",
                "updated": "2025-09-08T16:23:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    23,
                    44,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T16:23:44Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    23,
                    44,
                    0,
                    251,
                    0
                ],
                "title": "Seeing the Forest Through the Trees: Knowledge Retrieval for\n  Streamlining Particle Physics Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeing the Forest Through the Trees: Knowledge Retrieval for\n  Streamlining Particle Physics Analysis"
                },
                "summary": "Generative Large Language Models (LLMs) are a promising approach to\nstructuring knowledge contained within the corpora of research literature\nproduced by large-scale and long-running scientific collaborations. Within\nexperimental particle physics, such structured knowledge bases could expedite\nmethodological and editorial review. Complementarily, within the broader\nscientific community, generative LLM systems grounded in published work could\nmake for reliable companions allowing non-experts to analyze open-access data.\nTechniques such as Retrieval Augmented Generation (RAG) rely on semantically\nmatching localized text chunks, but struggle to maintain coherent context when\nrelevant information spans multiple segments, leading to a fragmented\nrepresentation devoid of global cross-document information. Here, we utilize\nthe hierarchical organization of experimental physics articles to build a tree\nrepresentation of the corpus, and present the SciTreeRAG system that uses this\nstructure to create contexts that are more focused and contextually rich than\nstandard RAG. Additionally, we develop methods for using LLMs to transform the\nunstructured corpus into a structured knowledge graph representation. We then\nimplement SciGraphRAG, a retrieval system that leverages this knowledge graph\nto access global cross-document relationships eluding standard RAG, thereby\nencapsulating domain-specific connections and expertise. We demonstrate\nproof-of-concept implementations using the corpus of the LHCb experiment at\nCERN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Large Language Models (LLMs) are a promising approach to\nstructuring knowledge contained within the corpora of research literature\nproduced by large-scale and long-running scientific collaborations. Within\nexperimental particle physics, such structured knowledge bases could expedite\nmethodological and editorial review. Complementarily, within the broader\nscientific community, generative LLM systems grounded in published work could\nmake for reliable companions allowing non-experts to analyze open-access data.\nTechniques such as Retrieval Augmented Generation (RAG) rely on semantically\nmatching localized text chunks, but struggle to maintain coherent context when\nrelevant information spans multiple segments, leading to a fragmented\nrepresentation devoid of global cross-document information. Here, we utilize\nthe hierarchical organization of experimental physics articles to build a tree\nrepresentation of the corpus, and present the SciTreeRAG system that uses this\nstructure to create contexts that are more focused and contextually rich than\nstandard RAG. Additionally, we develop methods for using LLMs to transform the\nunstructured corpus into a structured knowledge graph representation. We then\nimplement SciGraphRAG, a retrieval system that leverages this knowledge graph\nto access global cross-document relationships eluding standard RAG, thereby\nencapsulating domain-specific connections and expertise. We demonstrate\nproof-of-concept implementations using the corpus of the LHCb experiment at\nCERN."
                },
                "authors": [
                    {
                        "name": "James McGreivy"
                    },
                    {
                        "name": "Blaise Delaney"
                    },
                    {
                        "name": "Anja Beck"
                    },
                    {
                        "name": "Mike Williams"
                    }
                ],
                "author_detail": {
                    "name": "Mike Williams"
                },
                "author": "Mike Williams",
                "arxiv_comment": "17 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06853v1",
                "updated": "2025-09-08T16:21:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    21,
                    11,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T16:21:11Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    21,
                    11,
                    0,
                    251,
                    0
                ],
                "title": "Reinforcement learning meets bioprocess control through behaviour\n  cloning: Real-world deployment in an industrial photobioreactor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning meets bioprocess control through behaviour\n  cloning: Real-world deployment in an industrial photobioreactor"
                },
                "summary": "The inherent complexity of living cells as production units creates major\nchallenges for maintaining stable and optimal bioprocess conditions, especially\nin open Photobioreactors (PBRs) exposed to fluctuating environments. To address\nthis, we propose a Reinforcement Learning (RL) control approach, combined with\nBehavior Cloning (BC), for pH regulation in open PBR systems. This represents,\nto the best of our knowledge, the first application of an RL-based control\nstrategy to such a nonlinear and disturbance-prone bioprocess. Our method\nbegins with an offline training stage in which the RL agent learns from\ntrajectories generated by a nominal Proportional-Integral-Derivative (PID)\ncontroller, without direct interaction with the real system. This is followed\nby a daily online fine-tuning phase, enabling adaptation to evolving process\ndynamics and stronger rejection of fast, transient disturbances. This hybrid\noffline-online strategy allows deployment of an adaptive control policy capable\nof handling the inherent nonlinearities and external perturbations in open\nPBRs. Simulation studies highlight the advantages of our method: the Integral\nof Absolute Error (IAE) was reduced by 8% compared to PID control and by 5%\nrelative to standard off-policy RL. Moreover, control effort decreased\nsubstantially-by 54% compared to PID and 7% compared to standard RL-an\nimportant factor for minimizing operational costs. Finally, an 8-day\nexperimental validation under varying environmental conditions confirmed the\nrobustness and reliability of the proposed approach. Overall, this work\ndemonstrates the potential of RL-based methods for bioprocess control and paves\nthe way for their broader application to other nonlinear, disturbance-prone\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inherent complexity of living cells as production units creates major\nchallenges for maintaining stable and optimal bioprocess conditions, especially\nin open Photobioreactors (PBRs) exposed to fluctuating environments. To address\nthis, we propose a Reinforcement Learning (RL) control approach, combined with\nBehavior Cloning (BC), for pH regulation in open PBR systems. This represents,\nto the best of our knowledge, the first application of an RL-based control\nstrategy to such a nonlinear and disturbance-prone bioprocess. Our method\nbegins with an offline training stage in which the RL agent learns from\ntrajectories generated by a nominal Proportional-Integral-Derivative (PID)\ncontroller, without direct interaction with the real system. This is followed\nby a daily online fine-tuning phase, enabling adaptation to evolving process\ndynamics and stronger rejection of fast, transient disturbances. This hybrid\noffline-online strategy allows deployment of an adaptive control policy capable\nof handling the inherent nonlinearities and external perturbations in open\nPBRs. Simulation studies highlight the advantages of our method: the Integral\nof Absolute Error (IAE) was reduced by 8% compared to PID control and by 5%\nrelative to standard off-policy RL. Moreover, control effort decreased\nsubstantially-by 54% compared to PID and 7% compared to standard RL-an\nimportant factor for minimizing operational costs. Finally, an 8-day\nexperimental validation under varying environmental conditions confirmed the\nrobustness and reliability of the proposed approach. Overall, this work\ndemonstrates the potential of RL-based methods for bioprocess control and paves\nthe way for their broader application to other nonlinear, disturbance-prone\nsystems."
                },
                "authors": [
                    {
                        "name": "Juan D. Gil"
                    },
                    {
                        "name": "Ehecatl Antonio Del Rio Chanona"
                    },
                    {
                        "name": "José L. Guzmán"
                    },
                    {
                        "name": "Manuel Berenguel"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Berenguel"
                },
                "author": "Manuel Berenguel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02019v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02019v2",
                "updated": "2025-09-08T16:15:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    15,
                    6,
                    0,
                    251,
                    0
                ],
                "published": "2025-05-28T08:43:49Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    8,
                    43,
                    49,
                    2,
                    148,
                    0
                ],
                "title": "ChatCFD: An LLM-Driven Agent for End-to-End CFD Automation with\n  Domain-Specific Structured Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatCFD: An LLM-Driven Agent for End-to-End CFD Automation with\n  Domain-Specific Structured Reasoning"
                },
                "summary": "Computational Fluid Dynamics (CFD) is essential for advancing scientific and\nengineering fields but is hindered by operational complexity, high expertise\nrequirements, and limited accessibility. This paper introduces ChatCFD, an\nautomated agent system for OpenFOAM simulations that processes multi-modal\ninputs (e.g., research papers, meshes) via an interactive interface, leveraging\nDeepSeek-R1 and DeepSeek-V3 large language models, a multi-agent architecture,\nand OpenFOAM knowledge. Its four-stage pipeline (Knowledge Base Construction,\nUser Input Processing, Case File Generation, and Execution and Error\nReflection) enables iterative trial-reflection-refinement for intricate setups,\nsupporting diverse physical models and external meshes. Validation on 205\nbenchmark tutorial cases, 110 perturbed variants, and 2 literature-derived\ncases shows ChatCFD's 82.1 percent operational success rate on basic cases,\noutperforming MetaOpenFOAM (6.2 percent) and Foam-Agent (42.3 percent), and\n60-80 percent on literature-derived complex cases. Turbulence model studies\nshow a 40 percent success rate for common models versus 10 percent for rare\nones like RNG k-epsilon. Physics coupling analyses reveal higher resource\ndemands for multi-physics-coupled cases, while LLM bias toward simpler setups\nintroduces persistent errors, such as dimensional inconsistency. Ablation\nstudies highlight the efficacy of RAG-based modules and reflection mechanisms.\nBy automating hypothesis testing and parameter exploration, ChatCFD accelerates\nscientific discovery in fluid mechanics and engineering, addressing LLM\nlimitations through structured design and showing strong potential as a modular\ncomponent in MCP-based agent networks for collaborative multi-agent systems,\npaving the way for scalable AI-driven CFD innovation. The code for ChatCFD is\navailable at https://github.com/ConMoo/ChatCFD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Fluid Dynamics (CFD) is essential for advancing scientific and\nengineering fields but is hindered by operational complexity, high expertise\nrequirements, and limited accessibility. This paper introduces ChatCFD, an\nautomated agent system for OpenFOAM simulations that processes multi-modal\ninputs (e.g., research papers, meshes) via an interactive interface, leveraging\nDeepSeek-R1 and DeepSeek-V3 large language models, a multi-agent architecture,\nand OpenFOAM knowledge. Its four-stage pipeline (Knowledge Base Construction,\nUser Input Processing, Case File Generation, and Execution and Error\nReflection) enables iterative trial-reflection-refinement for intricate setups,\nsupporting diverse physical models and external meshes. Validation on 205\nbenchmark tutorial cases, 110 perturbed variants, and 2 literature-derived\ncases shows ChatCFD's 82.1 percent operational success rate on basic cases,\noutperforming MetaOpenFOAM (6.2 percent) and Foam-Agent (42.3 percent), and\n60-80 percent on literature-derived complex cases. Turbulence model studies\nshow a 40 percent success rate for common models versus 10 percent for rare\nones like RNG k-epsilon. Physics coupling analyses reveal higher resource\ndemands for multi-physics-coupled cases, while LLM bias toward simpler setups\nintroduces persistent errors, such as dimensional inconsistency. Ablation\nstudies highlight the efficacy of RAG-based modules and reflection mechanisms.\nBy automating hypothesis testing and parameter exploration, ChatCFD accelerates\nscientific discovery in fluid mechanics and engineering, addressing LLM\nlimitations through structured design and showing strong potential as a modular\ncomponent in MCP-based agent networks for collaborative multi-agent systems,\npaving the way for scalable AI-driven CFD innovation. The code for ChatCFD is\navailable at https://github.com/ConMoo/ChatCFD."
                },
                "authors": [
                    {
                        "name": "E Fan"
                    },
                    {
                        "name": "Kang Hu"
                    },
                    {
                        "name": "Zhuowen Wu"
                    },
                    {
                        "name": "Jiangyang Ge"
                    },
                    {
                        "name": "Jiawei Miao"
                    },
                    {
                        "name": "Yuzhi Zhang"
                    },
                    {
                        "name": "He Sun"
                    },
                    {
                        "name": "Weizong Wang"
                    },
                    {
                        "name": "Tianhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tianhan Zhang"
                },
                "author": "Tianhan Zhang",
                "arxiv_comment": "19 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02019v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02019v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06838v1",
                "updated": "2025-09-08T16:08:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    8,
                    31,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T16:08:31Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    8,
                    31,
                    0,
                    251,
                    0
                ],
                "title": "EPT Benchmark: Evaluation of Persian Trustworthiness in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPT Benchmark: Evaluation of Persian Trustworthiness in Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs), trained on extensive datasets using advanced\ndeep learning architectures, have demonstrated remarkable performance across a\nwide range of language tasks, becoming a cornerstone of modern AI technologies.\nHowever, ensuring their trustworthiness remains a critical challenge, as\nreliability is essential not only for accurate performance but also for\nupholding ethical, cultural, and social values. Careful alignment of training\ndata and culturally grounded evaluation criteria are vital for developing\nresponsible AI systems. In this study, we introduce the EPT (Evaluation of\nPersian Trustworthiness) metric, a culturally informed benchmark specifically\ndesigned to assess the trustworthiness of LLMs across six key aspects:\ntruthfulness, safety, fairness, robustness, privacy, and ethical alignment. We\ncurated a labeled dataset and evaluated the performance of several leading\nmodels - including ChatGPT, Claude, DeepSeek, Gemini, Grok, LLaMA, Mistral, and\nQwen - using both automated LLM-based and human assessments. Our results reveal\nsignificant deficiencies in the safety dimension, underscoring the urgent need\nfor focused attention on this critical aspect of model behavior. Furthermore,\nour findings offer valuable insights into the alignment of these models with\nPersian ethical-cultural values and highlight critical gaps and opportunities\nfor advancing trustworthy and culturally responsible AI. The dataset is\npublicly available at: https://github.com/Rezamirbagheri110/EPT-Benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), trained on extensive datasets using advanced\ndeep learning architectures, have demonstrated remarkable performance across a\nwide range of language tasks, becoming a cornerstone of modern AI technologies.\nHowever, ensuring their trustworthiness remains a critical challenge, as\nreliability is essential not only for accurate performance but also for\nupholding ethical, cultural, and social values. Careful alignment of training\ndata and culturally grounded evaluation criteria are vital for developing\nresponsible AI systems. In this study, we introduce the EPT (Evaluation of\nPersian Trustworthiness) metric, a culturally informed benchmark specifically\ndesigned to assess the trustworthiness of LLMs across six key aspects:\ntruthfulness, safety, fairness, robustness, privacy, and ethical alignment. We\ncurated a labeled dataset and evaluated the performance of several leading\nmodels - including ChatGPT, Claude, DeepSeek, Gemini, Grok, LLaMA, Mistral, and\nQwen - using both automated LLM-based and human assessments. Our results reveal\nsignificant deficiencies in the safety dimension, underscoring the urgent need\nfor focused attention on this critical aspect of model behavior. Furthermore,\nour findings offer valuable insights into the alignment of these models with\nPersian ethical-cultural values and highlight critical gaps and opportunities\nfor advancing trustworthy and culturally responsible AI. The dataset is\npublicly available at: https://github.com/Rezamirbagheri110/EPT-Benchmark."
                },
                "authors": [
                    {
                        "name": "Mohammad Reza Mirbagheri"
                    },
                    {
                        "name": "Mohammad Mahdi Mirkamali"
                    },
                    {
                        "name": "Zahra Motoshaker Arani"
                    },
                    {
                        "name": "Ali Javeri"
                    },
                    {
                        "name": "Amir Mahdi Sadeghzadeh"
                    },
                    {
                        "name": "Rasool Jalili"
                    }
                ],
                "author_detail": {
                    "name": "Rasool Jalili"
                },
                "author": "Rasool Jalili",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06836v1",
                "updated": "2025-09-08T16:07:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    7,
                    6,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T16:07:06Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    7,
                    6,
                    0,
                    251,
                    0
                ],
                "title": "COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens"
                },
                "summary": "Making LLMs more efficient in memory, latency, and serving cost is crucial\nfor edge deployment, interactive applications, and sustainable inference at\nscale. Pruning is a key technique toward this goal. However, prior pruning\nmethods are limited: width pruning often breaks the standard transformer layout\nor requires custom inference code, while depth pruning removes entire layers\nand can cause abrupt accuracy drops. In this work, we propose COMPACT, which\njointly (i) prunes rare vocabulary to shrink embedding/unembedding and (ii)\nprunes FFN intermediate channels using common-token-weighted activations,\naligning importance with the post-pruning token distribution. COMPACT enjoys\nmerits of both depth and width pruning, such as: deployment-friendliness (keeps\na standard transformer architecture), scale-adaptivity (trade off vocab vs. FFN\npruning), training-free operation with competitive pruning time, and strong\nmemory savings alongside throughput gains. Experiments across Qwen, LLaMA, and\nGemma families (0.5B-70B) show state-of-the-art downstream task performance at\nsimilar or higher pruning ratios, with substantial reductions in parameters,\nGPU memory, and end-to-end latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making LLMs more efficient in memory, latency, and serving cost is crucial\nfor edge deployment, interactive applications, and sustainable inference at\nscale. Pruning is a key technique toward this goal. However, prior pruning\nmethods are limited: width pruning often breaks the standard transformer layout\nor requires custom inference code, while depth pruning removes entire layers\nand can cause abrupt accuracy drops. In this work, we propose COMPACT, which\njointly (i) prunes rare vocabulary to shrink embedding/unembedding and (ii)\nprunes FFN intermediate channels using common-token-weighted activations,\naligning importance with the post-pruning token distribution. COMPACT enjoys\nmerits of both depth and width pruning, such as: deployment-friendliness (keeps\na standard transformer architecture), scale-adaptivity (trade off vocab vs. FFN\npruning), training-free operation with competitive pruning time, and strong\nmemory savings alongside throughput gains. Experiments across Qwen, LLaMA, and\nGemma families (0.5B-70B) show state-of-the-art downstream task performance at\nsimilar or higher pruning ratios, with substantial reductions in parameters,\nGPU memory, and end-to-end latency."
                },
                "authors": [
                    {
                        "name": "Eugene Kwek"
                    },
                    {
                        "name": "Wenpeng Yin"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Yin"
                },
                "author": "Wenpeng Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16482v2",
                "updated": "2025-09-08T16:06:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    16,
                    6,
                    59,
                    0,
                    251,
                    0
                ],
                "published": "2024-08-29T12:18:04Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    12,
                    18,
                    4,
                    3,
                    242,
                    0
                ],
                "title": "Self-Alignment: Improving Alignment of Cultural Values in LLMs via\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Alignment: Improving Alignment of Cultural Values in LLMs via\n  In-Context Learning"
                },
                "summary": "Improving the alignment of Large Language Models (LLMs) with respect to the\ncultural values that they encode has become an increasingly important topic. In\nthis work, we study whether we can exploit existing knowledge about cultural\nvalues at inference time to adjust model responses to cultural value probes. We\npresent a simple and inexpensive method that uses a combination of in-context\nlearning (ICL) and human survey data, and show that we can improve the\nalignment to cultural values across 5 models that include both English-centric\nand multilingual LLMs. Importantly, we show that our method could prove useful\nin test languages other than English and can improve alignment to the cultural\nvalues that correspond to a range of culturally diverse countries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the alignment of Large Language Models (LLMs) with respect to the\ncultural values that they encode has become an increasingly important topic. In\nthis work, we study whether we can exploit existing knowledge about cultural\nvalues at inference time to adjust model responses to cultural value probes. We\npresent a simple and inexpensive method that uses a combination of in-context\nlearning (ICL) and human survey data, and show that we can improve the\nalignment to cultural values across 5 models that include both English-centric\nand multilingual LLMs. Importantly, we show that our method could prove useful\nin test languages other than English and can improve alignment to the cultural\nvalues that correspond to a range of culturally diverse countries."
                },
                "authors": [
                    {
                        "name": "Rochelle Choenni"
                    },
                    {
                        "name": "Ekaterina Shutova"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Shutova"
                },
                "author": "Ekaterina Shutova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06822v1",
                "updated": "2025-09-08T15:57:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    57,
                    14,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T15:57:14Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    57,
                    14,
                    0,
                    251,
                    0
                ],
                "title": "RAFFLES: Reasoning-based Attribution of Faults for LLM Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAFFLES: Reasoning-based Attribution of Faults for LLM Systems"
                },
                "summary": "We have reached a critical roadblock in the development and enhancement of\nlong-horizon, multi-component LLM agentic systems: it is incredibly tricky to\nidentify where these systems break down and why. Evaluation capabilities that\ncurrently exist today (e.g., single pass LLM-as-a-judge) are limited in that\nthey often focus on individual metrics or capabilities, end-to-end outcomes,\nand are narrowly grounded on the preferences of humans. We argue that to match\nthe agentic capabilities, evaluation frameworks must also be able to reason,\nprobe, iterate, and understand the complex logic passing through these systems\nover long horizons. In this paper, we present RAFFLES - an evaluation\narchitecture that incorporates reasoning and iterative refinement.\nSpecifically, RAFFLES operates as an iterative, multi-component pipeline, using\na central Judge to systematically investigate faults and a set of specialized\nEvaluators to assess not only the system's components but also the quality of\nthe reasoning by the Judge itself, thereby building a history of hypotheses. We\ntested RAFFLES against several baselines on the Who&When dataset, a benchmark\ndesigned to diagnose the \"who\" (agent) and \"when\" (step) of a system's failure.\nRAFFLES outperforms these baselines, achieving an agent-step fault pair\naccuracy of over 43% on the Algorithmically-Generated dataset (a substantial\nincrease from the previously published best of 16.6%) and over 20% on the\nHand-Crafted dataset (surpassing the previously published best of 8.8%). These\nresults demonstrate a key step towards introducing automated fault detection\nfor autonomous systems over labor-intensive manual human review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have reached a critical roadblock in the development and enhancement of\nlong-horizon, multi-component LLM agentic systems: it is incredibly tricky to\nidentify where these systems break down and why. Evaluation capabilities that\ncurrently exist today (e.g., single pass LLM-as-a-judge) are limited in that\nthey often focus on individual metrics or capabilities, end-to-end outcomes,\nand are narrowly grounded on the preferences of humans. We argue that to match\nthe agentic capabilities, evaluation frameworks must also be able to reason,\nprobe, iterate, and understand the complex logic passing through these systems\nover long horizons. In this paper, we present RAFFLES - an evaluation\narchitecture that incorporates reasoning and iterative refinement.\nSpecifically, RAFFLES operates as an iterative, multi-component pipeline, using\na central Judge to systematically investigate faults and a set of specialized\nEvaluators to assess not only the system's components but also the quality of\nthe reasoning by the Judge itself, thereby building a history of hypotheses. We\ntested RAFFLES against several baselines on the Who&When dataset, a benchmark\ndesigned to diagnose the \"who\" (agent) and \"when\" (step) of a system's failure.\nRAFFLES outperforms these baselines, achieving an agent-step fault pair\naccuracy of over 43% on the Algorithmically-Generated dataset (a substantial\nincrease from the previously published best of 16.6%) and over 20% on the\nHand-Crafted dataset (surpassing the previously published best of 8.8%). These\nresults demonstrate a key step towards introducing automated fault detection\nfor autonomous systems over labor-intensive manual human review."
                },
                "authors": [
                    {
                        "name": "Chenyang Zhu"
                    },
                    {
                        "name": "Spencer Hong"
                    },
                    {
                        "name": "Jingyu Wu"
                    },
                    {
                        "name": "Kushal Chawla"
                    },
                    {
                        "name": "Charlotte Tang"
                    },
                    {
                        "name": "Youbing Yin"
                    },
                    {
                        "name": "Nathan Wolfe"
                    },
                    {
                        "name": "Erin Babinsky"
                    },
                    {
                        "name": "Daben Liu"
                    }
                ],
                "author_detail": {
                    "name": "Daben Liu"
                },
                "author": "Daben Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06820v1",
                "updated": "2025-09-08T15:56:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    56,
                    6,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T15:56:06Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    56,
                    6,
                    0,
                    251,
                    0
                ],
                "title": "Green Learning for STAR-RIS mmWave Systems with Implicit CSI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Green Learning for STAR-RIS mmWave Systems with Implicit CSI"
                },
                "summary": "In this paper, a green learning (GL)-based precoding framework is proposed\nfor simultaneously transmitting and reflecting reconfigurable intelligent\nsurface (STAR-RIS)-aided millimeter-wave (mmWave) MIMO broadcasting systems.\nMotivated by the growing emphasis on environmental sustainability in future 6G\nnetworks, this work adopts a broadcasting transmission architecture for\nscenarios where multiple users share identical information, improving spectral\nefficiency and reducing redundant transmissions and power consumption.\nDifferent from conventional optimization methods, such as block coordinate\ndescent (BCD) that require perfect channel state information (CSI) and\niterative computation, the proposed GL framework operates directly on received\nuplink pilot signals without explicit CSI estimation. Unlike deep learning (DL)\napproaches that require CSI-based labels for training, the proposed GL approach\nalso avoids deep neural networks and backpropagation, leading to a more\nlightweight design. Although the proposed GL framework is trained with\nsupervision generated by BCD under full CSI, inference is performed in a fully\nCSI-free manner. The proposed GL integrates subspace approximation with\nadjusted bias (Saab), relevant feature test (RFT)-based supervised feature\nselection, and eXtreme gradient boosting (XGBoost)-based decision learning to\njointly predict the STAR-RIS coefficients and transmit precoder. Simulation\nresults show that the proposed GL approach achieves competitive spectral\nefficiency compared to BCD and DL-based models, while reducing floating-point\noperations (FLOPs) by over four orders of magnitude. These advantages make the\nproposed GL approach highly suitable for real-time deployment in energy- and\nhardware-constrained broadcasting scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, a green learning (GL)-based precoding framework is proposed\nfor simultaneously transmitting and reflecting reconfigurable intelligent\nsurface (STAR-RIS)-aided millimeter-wave (mmWave) MIMO broadcasting systems.\nMotivated by the growing emphasis on environmental sustainability in future 6G\nnetworks, this work adopts a broadcasting transmission architecture for\nscenarios where multiple users share identical information, improving spectral\nefficiency and reducing redundant transmissions and power consumption.\nDifferent from conventional optimization methods, such as block coordinate\ndescent (BCD) that require perfect channel state information (CSI) and\niterative computation, the proposed GL framework operates directly on received\nuplink pilot signals without explicit CSI estimation. Unlike deep learning (DL)\napproaches that require CSI-based labels for training, the proposed GL approach\nalso avoids deep neural networks and backpropagation, leading to a more\nlightweight design. Although the proposed GL framework is trained with\nsupervision generated by BCD under full CSI, inference is performed in a fully\nCSI-free manner. The proposed GL integrates subspace approximation with\nadjusted bias (Saab), relevant feature test (RFT)-based supervised feature\nselection, and eXtreme gradient boosting (XGBoost)-based decision learning to\njointly predict the STAR-RIS coefficients and transmit precoder. Simulation\nresults show that the proposed GL approach achieves competitive spectral\nefficiency compared to BCD and DL-based models, while reducing floating-point\noperations (FLOPs) by over four orders of magnitude. These advantages make the\nproposed GL approach highly suitable for real-time deployment in energy- and\nhardware-constrained broadcasting scenarios."
                },
                "authors": [
                    {
                        "name": "Yu-Hsiang Huang"
                    },
                    {
                        "name": "Po-Heng Chou"
                    },
                    {
                        "name": "Wan-Jen Huang"
                    },
                    {
                        "name": "Walid Saad"
                    },
                    {
                        "name": "C. -C. Jay Kuo"
                    }
                ],
                "author_detail": {
                    "name": "C. -C. Jay Kuo"
                },
                "author": "C. -C. Jay Kuo",
                "arxiv_comment": "6 pages, 4 figures, 2 tables, accepted by 2025 IEEE Globecom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06819v1",
                "updated": "2025-09-08T15:55:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    55,
                    50,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T15:55:50Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    55,
                    50,
                    0,
                    251,
                    0
                ],
                "title": "CRISP -- Compliant ROS2 Controllers for Learning-Based Manipulation\n  Policies and Teleoperation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRISP -- Compliant ROS2 Controllers for Learning-Based Manipulation\n  Policies and Teleoperation"
                },
                "summary": "Learning-based controllers, such as diffusion policies and vision-language\naction models, often generate low-frequency or discontinuous robot state\nchanges. Achieving smooth reference tracking requires a low-level controller\nthat converts high-level targets commands into joint torques, enabling\ncompliant behavior during contact interactions. We present CRISP, a lightweight\nC++ implementation of compliant Cartesian and joint-space controllers for the\nROS2 control standard, designed for seamless integration with high-level\nlearning-based policies as well as teleoperation. The controllers are\ncompatible with any manipulator that exposes a joint-torque interface. Through\nour Python and Gymnasium interfaces, CRISP provides a unified pipeline for\nrecording data from hardware and simulation and deploying high-level\nlearning-based policies seamlessly, facilitating rapid experimentation. The\nsystem has been validated on hardware with the Franka Robotics FR3 and in\nsimulation with the Kuka IIWA14 and Kinova Gen3. Designed for rapid\nintegration, flexible deployment, and real-time performance, our implementation\nprovides a unified pipeline for data collection and policy execution, lowering\nthe barrier to applying learning-based methods on ROS2-compatible manipulators.\nDetailed documentation is available at the project website -\nhttps://utiasDSL.github.io/crisp_controllers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-based controllers, such as diffusion policies and vision-language\naction models, often generate low-frequency or discontinuous robot state\nchanges. Achieving smooth reference tracking requires a low-level controller\nthat converts high-level targets commands into joint torques, enabling\ncompliant behavior during contact interactions. We present CRISP, a lightweight\nC++ implementation of compliant Cartesian and joint-space controllers for the\nROS2 control standard, designed for seamless integration with high-level\nlearning-based policies as well as teleoperation. The controllers are\ncompatible with any manipulator that exposes a joint-torque interface. Through\nour Python and Gymnasium interfaces, CRISP provides a unified pipeline for\nrecording data from hardware and simulation and deploying high-level\nlearning-based policies seamlessly, facilitating rapid experimentation. The\nsystem has been validated on hardware with the Franka Robotics FR3 and in\nsimulation with the Kuka IIWA14 and Kinova Gen3. Designed for rapid\nintegration, flexible deployment, and real-time performance, our implementation\nprovides a unified pipeline for data collection and policy execution, lowering\nthe barrier to applying learning-based methods on ROS2-compatible manipulators.\nDetailed documentation is available at the project website -\nhttps://utiasDSL.github.io/crisp_controllers."
                },
                "authors": [
                    {
                        "name": "Daniel San José Pro"
                    },
                    {
                        "name": "Oliver Hausdörfer"
                    },
                    {
                        "name": "Ralf Römer"
                    },
                    {
                        "name": "Maximilian Dösch"
                    },
                    {
                        "name": "Martin Schuck"
                    },
                    {
                        "name": "Angela P. Schöllig"
                    }
                ],
                "author_detail": {
                    "name": "Angela P. Schöllig"
                },
                "author": "Angela P. Schöllig",
                "arxiv_comment": "5 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03165v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03165v3",
                "updated": "2025-09-08T15:51:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    51,
                    58,
                    0,
                    251,
                    0
                ],
                "published": "2025-04-04T04:43:13Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    43,
                    13,
                    4,
                    94,
                    0
                ],
                "title": "Efficient Dynamic Clustering-Based Document Compression for\n  Retrieval-Augmented-Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Dynamic Clustering-Based Document Compression for\n  Retrieval-Augmented-Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach\nfor knowledge injection during large language model (LLM) inference in recent\nyears. However, due to their limited ability to exploit fine-grained\ninter-document relationships, current RAG implementations face challenges in\neffectively addressing the retrieved noise and redundancy content, which may\ncause error in the generation results. To address these limitations, we propose\nan Efficient Dynamic Clustering-based document Compression framework (EDC2-RAG)\nthat utilizes latent inter-document relationships while simultaneously removing\nirrelevant information and redundant content. We validate our approach, built\nupon GPT-3.5-Turbo and GPT-4o-mini, on widely used knowledge-QA and\nHallucination-Detection datasets. Experimental results show that our method\nachieves consistent performance improvements across various scenarios and\nexperimental settings, demonstrating strong robustness and applicability. Our\ncode and datasets are available at https://github.com/Tsinghua-dhy/EDC-2-RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach\nfor knowledge injection during large language model (LLM) inference in recent\nyears. However, due to their limited ability to exploit fine-grained\ninter-document relationships, current RAG implementations face challenges in\neffectively addressing the retrieved noise and redundancy content, which may\ncause error in the generation results. To address these limitations, we propose\nan Efficient Dynamic Clustering-based document Compression framework (EDC2-RAG)\nthat utilizes latent inter-document relationships while simultaneously removing\nirrelevant information and redundant content. We validate our approach, built\nupon GPT-3.5-Turbo and GPT-4o-mini, on widely used knowledge-QA and\nHallucination-Detection datasets. Experimental results show that our method\nachieves consistent performance improvements across various scenarios and\nexperimental settings, demonstrating strong robustness and applicability. Our\ncode and datasets are available at https://github.com/Tsinghua-dhy/EDC-2-RAG."
                },
                "authors": [
                    {
                        "name": "Weitao Li"
                    },
                    {
                        "name": "Kaiming Liu"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Xuanyu Lei"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03165v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03165v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18992v2",
                "updated": "2025-09-08T15:50:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    50,
                    16,
                    0,
                    251,
                    0
                ],
                "published": "2025-08-26T12:46:58Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    46,
                    58,
                    1,
                    238,
                    0
                ],
                "title": "Automatic Prompt Optimization with Prompt Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Prompt Optimization with Prompt Distillation"
                },
                "summary": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which is gaining popularity due to the rapid development of\nprompt engineering driven by extensive research in the field of large language\nmodels (LLMs). This paper presents DistillPrompt -- a novel autoprompting\nmethod based on large language models that employs a multi-stage integration of\ntask-specific information into prompts using training data. DistillPrompt\nutilizes distillation, compression, and aggregation operations to explore the\nprompt space more thoroughly. The method was tested on different datasets for\ntext classification and generation tasks using the t-lite-instruct-0.1 language\nmodel. The results demonstrate a significant average improvement (e.g., 20.12%\nacross the entire dataset compared to Grips) in key metrics over existing\nmethods in the field, establishing DistillPrompt as one of the most effective\nnon-gradient approaches in autoprompting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which is gaining popularity due to the rapid development of\nprompt engineering driven by extensive research in the field of large language\nmodels (LLMs). This paper presents DistillPrompt -- a novel autoprompting\nmethod based on large language models that employs a multi-stage integration of\ntask-specific information into prompts using training data. DistillPrompt\nutilizes distillation, compression, and aggregation operations to explore the\nprompt space more thoroughly. The method was tested on different datasets for\ntext classification and generation tasks using the t-lite-instruct-0.1 language\nmodel. The results demonstrate a significant average improvement (e.g., 20.12%\nacross the entire dataset compared to Grips) in key metrics over existing\nmethods in the field, establishing DistillPrompt as one of the most effective\nnon-gradient approaches in autoprompting."
                },
                "authors": [
                    {
                        "name": "Ernest A. Dyagin"
                    },
                    {
                        "name": "Nikita I. Kulin"
                    },
                    {
                        "name": "Artur R. Khairullin"
                    },
                    {
                        "name": "Viktor N. Zhuravlev"
                    },
                    {
                        "name": "Alena N. Sitkina"
                    }
                ],
                "author_detail": {
                    "name": "Alena N. Sitkina"
                },
                "author": "Alena N. Sitkina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20144v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20144v2",
                "updated": "2025-09-08T15:48:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    48,
                    19,
                    0,
                    251,
                    0
                ],
                "published": "2025-08-27T09:17:03Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    9,
                    17,
                    3,
                    2,
                    239,
                    0
                ],
                "title": "Navigating the EU AI Act: Foreseeable Challenges in Qualifying Deep\n  Learning-Based Automated Inspections of Class III Medical Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating the EU AI Act: Foreseeable Challenges in Qualifying Deep\n  Learning-Based Automated Inspections of Class III Medical Devices"
                },
                "summary": "As deep learning (DL) technologies advance, their application in automated\nvisual inspection for Class III medical devices offers significant potential to\nenhance quality assurance and reduce human error. However, the adoption of such\nAI-based systems introduces new regulatory complexities-particularly under the\nEU Artificial Intelligence (AI) Act, which imposes high-risk system obligations\nthat differ in scope and depth from established regulatory frameworks such as\nthe Medical Device Regulation (MDR) and the U.S. FDA Quality System Regulation\n(QSR). This paper presents a high-level technical assessment of the foreseeable\nchallenges that manufacturers are likely to encounter when qualifying DL-based\nautomated inspections -- specifically static models -- within the existing\nmedical device compliance landscape. It examines divergences in risk management\nprinciples, dataset governance, model validation, explainability requirements,\nand post-deployment monitoring obligations. The discussion also explores\npotential implementation strategies and highlights areas of uncertainty,\nincluding data retention burdens, global compliance implications, and the\npractical difficulties of achieving statistical significance in validation with\nlimited defect data. Disclaimer: This paper presents a technical perspective\nand does not constitute legal or regulatory advice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As deep learning (DL) technologies advance, their application in automated\nvisual inspection for Class III medical devices offers significant potential to\nenhance quality assurance and reduce human error. However, the adoption of such\nAI-based systems introduces new regulatory complexities-particularly under the\nEU Artificial Intelligence (AI) Act, which imposes high-risk system obligations\nthat differ in scope and depth from established regulatory frameworks such as\nthe Medical Device Regulation (MDR) and the U.S. FDA Quality System Regulation\n(QSR). This paper presents a high-level technical assessment of the foreseeable\nchallenges that manufacturers are likely to encounter when qualifying DL-based\nautomated inspections -- specifically static models -- within the existing\nmedical device compliance landscape. It examines divergences in risk management\nprinciples, dataset governance, model validation, explainability requirements,\nand post-deployment monitoring obligations. The discussion also explores\npotential implementation strategies and highlights areas of uncertainty,\nincluding data retention burdens, global compliance implications, and the\npractical difficulties of achieving statistical significance in validation with\nlimited defect data. Disclaimer: This paper presents a technical perspective\nand does not constitute legal or regulatory advice."
                },
                "authors": [
                    {
                        "name": "Julio Zanon Diaz"
                    },
                    {
                        "name": "Tommy Brennan"
                    },
                    {
                        "name": "Peter Corcoran"
                    }
                ],
                "author_detail": {
                    "name": "Peter Corcoran"
                },
                "author": "Peter Corcoran",
                "arxiv_comment": "Critical Review article",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20144v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20144v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06813v1",
                "updated": "2025-09-08T15:48:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    48,
                    17,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T15:48:17Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    48,
                    17,
                    0,
                    251,
                    0
                ],
                "title": "A Comparative Benchmark of Large Language Models for Labelling Wind\n  Turbine Maintenance Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Benchmark of Large Language Models for Labelling Wind\n  Turbine Maintenance Logs"
                },
                "summary": "Effective Operation and Maintenance (O&M) is critical to reducing the\nLevelised Cost of Energy (LCOE) from wind power, yet the unstructured,\nfree-text nature of turbine maintenance logs presents a significant barrier to\nautomated analysis. Our paper addresses this by presenting a novel and\nreproducible framework for benchmarking Large Language Models (LLMs) on the\ntask of classifying these complex industrial records. To promote transparency\nand encourage further research, this framework has been made publicly available\nas an open-source tool. We systematically evaluate a diverse suite of\nstate-of-the-art proprietary and open-source LLMs, providing a foundational\nassessment of their trade-offs in reliability, operational efficiency, and\nmodel calibration. Our results quantify a clear performance hierarchy,\nidentifying top models that exhibit high alignment with a benchmark standard\nand trustworthy, well-calibrated confidence scores. We also demonstrate that\nclassification performance is highly dependent on the task's semantic\nambiguity, with all models showing higher consensus on objective component\nidentification than on interpretive maintenance actions. Given that no model\nachieves perfect accuracy and that calibration varies dramatically, we conclude\nthat the most effective and responsible near-term application is a\nHuman-in-the-Loop system, where LLMs act as a powerful assistant to accelerate\nand standardise data labelling for human experts, thereby enhancing O&M data\nquality and downstream reliability analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective Operation and Maintenance (O&M) is critical to reducing the\nLevelised Cost of Energy (LCOE) from wind power, yet the unstructured,\nfree-text nature of turbine maintenance logs presents a significant barrier to\nautomated analysis. Our paper addresses this by presenting a novel and\nreproducible framework for benchmarking Large Language Models (LLMs) on the\ntask of classifying these complex industrial records. To promote transparency\nand encourage further research, this framework has been made publicly available\nas an open-source tool. We systematically evaluate a diverse suite of\nstate-of-the-art proprietary and open-source LLMs, providing a foundational\nassessment of their trade-offs in reliability, operational efficiency, and\nmodel calibration. Our results quantify a clear performance hierarchy,\nidentifying top models that exhibit high alignment with a benchmark standard\nand trustworthy, well-calibrated confidence scores. We also demonstrate that\nclassification performance is highly dependent on the task's semantic\nambiguity, with all models showing higher consensus on objective component\nidentification than on interpretive maintenance actions. Given that no model\nachieves perfect accuracy and that calibration varies dramatically, we conclude\nthat the most effective and responsible near-term application is a\nHuman-in-the-Loop system, where LLMs act as a powerful assistant to accelerate\nand standardise data labelling for human experts, thereby enhancing O&M data\nquality and downstream reliability analysis."
                },
                "authors": [
                    {
                        "name": "Max Malyi"
                    },
                    {
                        "name": "Jonathan Shek"
                    },
                    {
                        "name": "Alasdair McDonald"
                    },
                    {
                        "name": "Andre Biscaya"
                    }
                ],
                "author_detail": {
                    "name": "Andre Biscaya"
                },
                "author": "Andre Biscaya",
                "arxiv_comment": "Associated GitHub repository:\n  https://github.com/mvmalyi/wind-farm-maintenance-logs-labelling-with-llms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06809v1",
                "updated": "2025-09-08T15:43:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    43,
                    29,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T15:43:29Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    43,
                    29,
                    0,
                    251,
                    0
                ],
                "title": "Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in\n  the TPTP Ecosystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in\n  the TPTP Ecosystem"
                },
                "summary": "The scarcity of high-quality, logically sound data is a critical bottleneck\nfor advancing the mathematical reasoning of Large Language Models (LLMs). Our\nwork confronts this challenge by turning decades of automated theorem proving\nresearch into a scalable data engine. Rather than relying on error-prone LLMs\nor complex proof-assistant syntax like Lean and Isabelle, our framework\nleverages E-prover's saturation capabilities on the vast TPTP axiom library to\nderive a massive, guaranteed-valid corpus of theorems. Our pipeline is\nprincipled and simple: saturate axioms, filter for \"interesting\" theorems, and\ngenerate tasks. With no LLMs in the loop, we eliminate factual errors by\nconstruction. This purely symbolic data is then transformed into three\ndifficulty-controlled challenges: entailment verification, premise selection,\nand proof reconstruction. Our zero-shot experiments on frontier models reveal a\nclear weakness: performance collapses on tasks requiring deep, structural\nreasoning. Our framework provides both the diagnostic tool to measure this gap\nand a scalable source of symbolic training data to address it. We make the code\nand data publicly available.\n  https://github.com/sileod/reasoning_core\nhttps://hf.co/datasets/reasoning-core/rc1",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of high-quality, logically sound data is a critical bottleneck\nfor advancing the mathematical reasoning of Large Language Models (LLMs). Our\nwork confronts this challenge by turning decades of automated theorem proving\nresearch into a scalable data engine. Rather than relying on error-prone LLMs\nor complex proof-assistant syntax like Lean and Isabelle, our framework\nleverages E-prover's saturation capabilities on the vast TPTP axiom library to\nderive a massive, guaranteed-valid corpus of theorems. Our pipeline is\nprincipled and simple: saturate axioms, filter for \"interesting\" theorems, and\ngenerate tasks. With no LLMs in the loop, we eliminate factual errors by\nconstruction. This purely symbolic data is then transformed into three\ndifficulty-controlled challenges: entailment verification, premise selection,\nand proof reconstruction. Our zero-shot experiments on frontier models reveal a\nclear weakness: performance collapses on tasks requiring deep, structural\nreasoning. Our framework provides both the diagnostic tool to measure this gap\nand a scalable source of symbolic training data to address it. We make the code\nand data publicly available.\n  https://github.com/sileod/reasoning_core\nhttps://hf.co/datasets/reasoning-core/rc1"
                },
                "authors": [
                    {
                        "name": "Valentin Quesnel"
                    },
                    {
                        "name": "Damien Sileo"
                    }
                ],
                "author_detail": {
                    "name": "Damien Sileo"
                },
                "author": "Damien Sileo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06807v1",
                "updated": "2025-09-08T15:39:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    39,
                    17,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T15:39:17Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    39,
                    17,
                    0,
                    251,
                    0
                ],
                "title": "MoGU V2: Toward a Higher Pareto Frontier Between Model Usability and\n  Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoGU V2: Toward a Higher Pareto Frontier Between Model Usability and\n  Security"
                },
                "summary": "As Large Language Models (LLMs) increasingly permeate human life, their\nsecurity has emerged as a critical concern, particularly their ability to\nmaintain harmless responses to malicious instructions. Although extensive\nmethods have improved LLMs' security, they often lead to conservative,\nrejection-oriented responses that compromise practical usability. This presents\na key challenge: how to advance the Pareto frontier between LLMs' usability and\nsecurity, rather than necessitate a trade-off between them. To address this, we\npropose the MoGU framework, in which the intra-layer router dynamically\nallocates weights by sensing hidden states, thereby balancing the contributions\nof security-optimized and usability-optimized variants. Despite its initial\npotential, the MoGU framework faces limitations such as parameter redundancy\nand performance bottlenecks. To overcome these, we further propose an improved\nMoGU_v2 framework that establishes a tighter coupling between the routers and\nhidden states. In MoGU_v2, routers are embedded only in layers encoding highly\nclassifiable security features, and backbone modules are activated during\nrouter optimization to enable bidirectional adaptation. MoGU_V2 exhibits strong\nadaptability and stable improvements across various series of LLMs, including\nmainstream LLMs serving as brains in various applications, on-device LLMs\noptimized for resource-constrained scenarios, and reasoning LLMs tailored for\nuser interpretability. Meanwhile, even facing risks introduced by Instruction\nFine-tuning, MoGU_v2 can easily restore security without compromising the task\nperformance gains via a simple data-mix strategy. These comprehensive\nimprovements highlight MoGU_V2 as a robust and versatile solution for\nmitigating security risks in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) increasingly permeate human life, their\nsecurity has emerged as a critical concern, particularly their ability to\nmaintain harmless responses to malicious instructions. Although extensive\nmethods have improved LLMs' security, they often lead to conservative,\nrejection-oriented responses that compromise practical usability. This presents\na key challenge: how to advance the Pareto frontier between LLMs' usability and\nsecurity, rather than necessitate a trade-off between them. To address this, we\npropose the MoGU framework, in which the intra-layer router dynamically\nallocates weights by sensing hidden states, thereby balancing the contributions\nof security-optimized and usability-optimized variants. Despite its initial\npotential, the MoGU framework faces limitations such as parameter redundancy\nand performance bottlenecks. To overcome these, we further propose an improved\nMoGU_v2 framework that establishes a tighter coupling between the routers and\nhidden states. In MoGU_v2, routers are embedded only in layers encoding highly\nclassifiable security features, and backbone modules are activated during\nrouter optimization to enable bidirectional adaptation. MoGU_V2 exhibits strong\nadaptability and stable improvements across various series of LLMs, including\nmainstream LLMs serving as brains in various applications, on-device LLMs\noptimized for resource-constrained scenarios, and reasoning LLMs tailored for\nuser interpretability. Meanwhile, even facing risks introduced by Instruction\nFine-tuning, MoGU_v2 can easily restore security without compromising the task\nperformance gains via a simple data-mix strategy. These comprehensive\nimprovements highlight MoGU_V2 as a robust and versatile solution for\nmitigating security risks in real-world applications."
                },
                "authors": [
                    {
                        "name": "Yanrui Du"
                    },
                    {
                        "name": "Fenglei Fan"
                    },
                    {
                        "name": "Sendong Zhao"
                    },
                    {
                        "name": "Jiawei Cao"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06806v1",
                "updated": "2025-09-08T15:38:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    38,
                    31,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T15:38:31Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    38,
                    31,
                    0,
                    251,
                    0
                ],
                "title": "MachineLearningLM: Continued Pretraining Language Models on Millions of\n  Synthetic Tabular Prediction Tasks Scales In-Context ML",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MachineLearningLM: Continued Pretraining Language Models on Millions of\n  Synthetic Tabular Prediction Tasks Scales In-Context ML"
                },
                "summary": "Large language models (LLMs) possess broad world knowledge and strong\ngeneral-purpose reasoning ability, yet they struggle to learn from many\nin-context examples on standard machine learning (ML) tasks, that is, to\nleverage many-shot demonstrations purely via in-context learning (ICL) without\ngradient descent. We introduce MachineLearningLM, a portable\ncontinued-pretraining framework that equips a general-purpose LLM with robust\nin-context ML capability while preserving its general knowledge and reasoning\nfor broader chat workflows.\n  Our pretraining procedure synthesizes ML tasks from millions of structural\ncausal models (SCMs), spanning shot counts up to 1,024. We begin with a\nrandom-forest teacher, distilling tree-based decision strategies into the LLM\nto strengthen robustness in numerical modeling. All tasks are serialized with a\ntoken-efficient prompt, enabling 3x to 6x more examples per context window and\ndelivering up to 50x amortized throughput via batch inference.\n  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),\nMachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an\naverage of about 15% on out-of-distribution tabular classification across\nfinance, physics, biology, and healthcare domains. It exhibits a striking\nmany-shot scaling law: accuracy increases monotonically as in-context\ndemonstrations grow from 8 to 1,024. Without any task-specific training, it\nattains random-forest-level accuracy across hundreds of shots. General chat\ncapabilities, including knowledge and reasoning, are preserved: it achieves\n75.4% on MMLU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) possess broad world knowledge and strong\ngeneral-purpose reasoning ability, yet they struggle to learn from many\nin-context examples on standard machine learning (ML) tasks, that is, to\nleverage many-shot demonstrations purely via in-context learning (ICL) without\ngradient descent. We introduce MachineLearningLM, a portable\ncontinued-pretraining framework that equips a general-purpose LLM with robust\nin-context ML capability while preserving its general knowledge and reasoning\nfor broader chat workflows.\n  Our pretraining procedure synthesizes ML tasks from millions of structural\ncausal models (SCMs), spanning shot counts up to 1,024. We begin with a\nrandom-forest teacher, distilling tree-based decision strategies into the LLM\nto strengthen robustness in numerical modeling. All tasks are serialized with a\ntoken-efficient prompt, enabling 3x to 6x more examples per context window and\ndelivering up to 50x amortized throughput via batch inference.\n  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),\nMachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an\naverage of about 15% on out-of-distribution tabular classification across\nfinance, physics, biology, and healthcare domains. It exhibits a striking\nmany-shot scaling law: accuracy increases monotonically as in-context\ndemonstrations grow from 8 to 1,024. Without any task-specific training, it\nattains random-forest-level accuracy across hundreds of shots. General chat\ncapabilities, including knowledge and reasoning, are preserved: it achieves\n75.4% on MMLU."
                },
                "authors": [
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Pengkun Zhang"
                    },
                    {
                        "name": "Mingzhe Lu"
                    },
                    {
                        "name": "Yanzhen Shen"
                    },
                    {
                        "name": "Guolin Ke"
                    }
                ],
                "author_detail": {
                    "name": "Guolin Ke"
                },
                "author": "Guolin Ke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15581v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15581v2",
                "updated": "2025-09-08T15:34:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    34,
                    55,
                    0,
                    251,
                    0
                ],
                "published": "2025-01-26T16:17:57Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    16,
                    17,
                    57,
                    6,
                    26,
                    0
                ],
                "title": "Error Classification of Large Language Models on Math Word Problems: A\n  Dynamically Adaptive Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Error Classification of Large Language Models on Math Word Problems: A\n  Dynamically Adaptive Framework"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains. Math Word Problems (MWPs) serve as a crucial benchmark for\nevaluating LLMs' reasoning abilities. While most research primarily focuses on\nimproving accuracy, it often neglects understanding and addressing the\nunderlying patterns of errors. Current error classification methods rely on\nstatic and predefined categories, which limit their ability to capture the full\nspectrum of error patterns in mathematical reasoning. To enable systematic\nerror analysis, we collect error samples from 15 different LLMs of varying\nsizes across four distinct MWP datasets using multiple sampling strategies.\nBased on this extensive collection, we introduce MWPES-300K, a comprehensive\ndataset containing 304,865 error samples that cover diverse error patterns and\nreasoning paths. To reduce human bias and enable fine-grained analysis of error\npatterns, we propose a novel framework for automated dynamic error\nclassification in mathematical reasoning. Experimental results demonstrate that\ndataset characteristics significantly shape error patterns, which evolve from\nbasic to complex manifestations as model capabilities increase. With deeper\ninsights into error patterns, we propose Error-Aware Prompting (EAP) that\nincorporates common error patterns as explicit guidance, leading to significant\nimprovements in mathematical reasoning performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains. Math Word Problems (MWPs) serve as a crucial benchmark for\nevaluating LLMs' reasoning abilities. While most research primarily focuses on\nimproving accuracy, it often neglects understanding and addressing the\nunderlying patterns of errors. Current error classification methods rely on\nstatic and predefined categories, which limit their ability to capture the full\nspectrum of error patterns in mathematical reasoning. To enable systematic\nerror analysis, we collect error samples from 15 different LLMs of varying\nsizes across four distinct MWP datasets using multiple sampling strategies.\nBased on this extensive collection, we introduce MWPES-300K, a comprehensive\ndataset containing 304,865 error samples that cover diverse error patterns and\nreasoning paths. To reduce human bias and enable fine-grained analysis of error\npatterns, we propose a novel framework for automated dynamic error\nclassification in mathematical reasoning. Experimental results demonstrate that\ndataset characteristics significantly shape error patterns, which evolve from\nbasic to complex manifestations as model capabilities increase. With deeper\ninsights into error patterns, we propose Error-Aware Prompting (EAP) that\nincorporates common error patterns as explicit guidance, leading to significant\nimprovements in mathematical reasoning performance."
                },
                "authors": [
                    {
                        "name": "Yuhong Sun"
                    },
                    {
                        "name": "Zhangyue Yin"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Hui Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hui Zhao"
                },
                "author": "Hui Zhao",
                "arxiv_comment": "28 pages, 10 figures, accepted by Findings of EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15581v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15581v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06795v1",
                "updated": "2025-09-08T15:24:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    24,
                    33,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T15:24:33Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    24,
                    33,
                    0,
                    251,
                    0
                ],
                "title": "Anchoring Refusal Direction: Mitigating Safety Risks in Tuning via\n  Projection Constraint",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anchoring Refusal Direction: Mitigating Safety Risks in Tuning via\n  Projection Constraint"
                },
                "summary": "Instruction Fine-Tuning (IFT) has been widely adopted as an effective\npost-training strategy to enhance various abilities of Large Language Models\n(LLMs). However, prior studies have shown that IFT can significantly compromise\nLLMs' safety, particularly their ability to refuse malicious instructions,\nraising significant concerns. Recent research into the internal mechanisms of\nLLMs has identified the refusal direction (r-direction) in the hidden states,\nwhich plays a pivotal role in governing refusal behavior. Building on this\ninsight, our study reveals that the r-direction tends to drift during training,\nwhich we identify as one of the causes of the associated safety risks. To\nmitigate such drift, our proposed ProCon method introduces a\nprojection-constrained loss term that regularizes the projection magnitude of\neach training sample's hidden state onto the r-direction. Our initial analysis\nshows that applying an appropriate constraint can effectively mitigate the\nrefusal direction drift and associated safety risks, but remains limited by\noverall performance barriers. To overcome this barrier, informed by our\nobservation of early-stage sharp drift and a data-driven perspective, we\nintroduce a warm-up strategy that emphasizes early-stage strong constraints and\nbroaden the data distribution to strengthen constraint signals, leading to an\nenhanced ProCon method. Experimental results under various datasets, scenarios,\nand LLMs demonstrate that our method can significantly mitigate safety risks\nposed by IFT while preserving task performance gains. Even compared with strong\nbaselines, our method consistently delivers superior overall performance.\nCrucially, our analysis indicates that ProCon can contribute to stabilizing the\nr-direction during training, while such an interpretability-driven exploration\nof LLMs' internal mechanisms lays a solid foundation for future safety\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction Fine-Tuning (IFT) has been widely adopted as an effective\npost-training strategy to enhance various abilities of Large Language Models\n(LLMs). However, prior studies have shown that IFT can significantly compromise\nLLMs' safety, particularly their ability to refuse malicious instructions,\nraising significant concerns. Recent research into the internal mechanisms of\nLLMs has identified the refusal direction (r-direction) in the hidden states,\nwhich plays a pivotal role in governing refusal behavior. Building on this\ninsight, our study reveals that the r-direction tends to drift during training,\nwhich we identify as one of the causes of the associated safety risks. To\nmitigate such drift, our proposed ProCon method introduces a\nprojection-constrained loss term that regularizes the projection magnitude of\neach training sample's hidden state onto the r-direction. Our initial analysis\nshows that applying an appropriate constraint can effectively mitigate the\nrefusal direction drift and associated safety risks, but remains limited by\noverall performance barriers. To overcome this barrier, informed by our\nobservation of early-stage sharp drift and a data-driven perspective, we\nintroduce a warm-up strategy that emphasizes early-stage strong constraints and\nbroaden the data distribution to strengthen constraint signals, leading to an\nenhanced ProCon method. Experimental results under various datasets, scenarios,\nand LLMs demonstrate that our method can significantly mitigate safety risks\nposed by IFT while preserving task performance gains. Even compared with strong\nbaselines, our method consistently delivers superior overall performance.\nCrucially, our analysis indicates that ProCon can contribute to stabilizing the\nr-direction during training, while such an interpretability-driven exploration\nof LLMs' internal mechanisms lays a solid foundation for future safety\nresearch."
                },
                "authors": [
                    {
                        "name": "Yanrui Du"
                    },
                    {
                        "name": "Fenglei Fan"
                    },
                    {
                        "name": "Sendong Zhao"
                    },
                    {
                        "name": "Jiawei Cao"
                    },
                    {
                        "name": "Qika Lin"
                    },
                    {
                        "name": "Kai He"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Mengling Feng"
                    }
                ],
                "author_detail": {
                    "name": "Mengling Feng"
                },
                "author": "Mengling Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01909v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01909v3",
                "updated": "2025-09-08T15:18:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    18,
                    35,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-02T03:04:27Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    3,
                    4,
                    27,
                    1,
                    245,
                    0
                ],
                "title": "Oyster-I: Beyond Refusal -- Constructive Safety Alignment for\n  Responsible Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oyster-I: Beyond Refusal -- Constructive Safety Alignment for\n  Responsible Language Models"
                },
                "summary": "Large language models (LLMs) typically deploy safety mechanisms to prevent\nharmful content generation. Most current approaches focus narrowly on risks\nposed by malicious actors, often framing risks as adversarial events and\nrelying on defensive refusals. However, in real-world settings, risks also come\nfrom non-malicious users seeking help while under psychological distress (e.g.,\nself-harm intentions). In such cases, the model's response can strongly\ninfluence the user's next actions. Simple refusals may lead them to repeat,\nescalate, or move to unsafe platforms, creating worse outcomes. We introduce\nConstructive Safety Alignment (CSA), a human-centric paradigm that protects\nagainst malicious misuse while actively guiding vulnerable users toward safe\nand helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic\nanticipation of user reactions, fine-grained risk boundary discovery, and\ninterpretable reasoning control, turning safety into a trust-building process.\nOy1 achieves state-of-the-art safety among open models while retaining high\ngeneral capabilities. On our Constructive Benchmark, it shows strong\nconstructive engagement, close to GPT-5, and unmatched robustness on the\nStrata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from\nrefusal-first to guidance-first safety, CSA redefines the model-user\nrelationship, aiming for systems that are not just safe, but meaningfully\nhelpful. We release Oy1, code, and the benchmark to support responsible,\nuser-centered AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) typically deploy safety mechanisms to prevent\nharmful content generation. Most current approaches focus narrowly on risks\nposed by malicious actors, often framing risks as adversarial events and\nrelying on defensive refusals. However, in real-world settings, risks also come\nfrom non-malicious users seeking help while under psychological distress (e.g.,\nself-harm intentions). In such cases, the model's response can strongly\ninfluence the user's next actions. Simple refusals may lead them to repeat,\nescalate, or move to unsafe platforms, creating worse outcomes. We introduce\nConstructive Safety Alignment (CSA), a human-centric paradigm that protects\nagainst malicious misuse while actively guiding vulnerable users toward safe\nand helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic\nanticipation of user reactions, fine-grained risk boundary discovery, and\ninterpretable reasoning control, turning safety into a trust-building process.\nOy1 achieves state-of-the-art safety among open models while retaining high\ngeneral capabilities. On our Constructive Benchmark, it shows strong\nconstructive engagement, close to GPT-5, and unmatched robustness on the\nStrata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from\nrefusal-first to guidance-first safety, CSA redefines the model-user\nrelationship, aiming for systems that are not just safe, but meaningfully\nhelpful. We release Oy1, code, and the benchmark to support responsible,\nuser-centered AI."
                },
                "authors": [
                    {
                        "name": "Ranjie Duan"
                    },
                    {
                        "name": "Jiexi Liu"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Shiji Zhao"
                    },
                    {
                        "name": "Ruoxi Cheng"
                    },
                    {
                        "name": "Fengxiang Wang"
                    },
                    {
                        "name": "Cheng Wei"
                    },
                    {
                        "name": "Yong Xie"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Defeng Li"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Yuefeng Chen"
                    },
                    {
                        "name": "Chongwen Wang"
                    },
                    {
                        "name": "Xingjun Ma"
                    },
                    {
                        "name": "Xingxing Wei"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Yitong Sun"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Jinzhao Hu"
                    },
                    {
                        "name": "Sha Xu"
                    },
                    {
                        "name": "Yitong Yang"
                    },
                    {
                        "name": "Jialing Tao"
                    },
                    {
                        "name": "Hui Xue"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xue"
                },
                "author": "Hui Xue",
                "arxiv_comment": "Technical Report Code & Model weights available:\n  https://github.com/Alibaba-AAIG/Oyster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01909v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01909v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10603v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10603v3",
                "updated": "2025-09-08T15:18:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    18,
                    30,
                    0,
                    251,
                    0
                ],
                "published": "2025-08-14T12:44:39Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    12,
                    44,
                    39,
                    3,
                    226,
                    0
                ],
                "title": "Why Report Failed Interactions With Robots?! Towards Vignette-based\n  Interaction Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Report Failed Interactions With Robots?! Towards Vignette-based\n  Interaction Quality"
                },
                "summary": "Although the quality of human-robot interactions has improved with the advent\nof LLMs, there are still various factors that cause systems to be sub-optimal\nwhen compared to human-human interactions. The nature and criticality of\nfailures are often dependent on the context of the interaction and so cannot be\ngeneralized across the wide range of scenarios and experiments which have been\nimplemented in HRI research. In this work we propose the use of a technique\noverlooked in the field of HRI, ethnographic vignettes, to clearly highlight\nthese failures, particularly those that are rarely documented. We describe the\nmethodology behind the process of writing vignettes and create our own based on\nour personal experiences with failures in HRI systems. We emphasize the\nstrength of vignettes as the ability to communicate failures from a\nmulti-disciplinary perspective, promote transparency about the capabilities of\nrobots, and document unexpected behaviours which would otherwise be omitted\nfrom research reports. We encourage the use of vignettes to augment existing\ninteraction evaluation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although the quality of human-robot interactions has improved with the advent\nof LLMs, there are still various factors that cause systems to be sub-optimal\nwhen compared to human-human interactions. The nature and criticality of\nfailures are often dependent on the context of the interaction and so cannot be\ngeneralized across the wide range of scenarios and experiments which have been\nimplemented in HRI research. In this work we propose the use of a technique\noverlooked in the field of HRI, ethnographic vignettes, to clearly highlight\nthese failures, particularly those that are rarely documented. We describe the\nmethodology behind the process of writing vignettes and create our own based on\nour personal experiences with failures in HRI systems. We emphasize the\nstrength of vignettes as the ability to communicate failures from a\nmulti-disciplinary perspective, promote transparency about the capabilities of\nrobots, and document unexpected behaviours which would otherwise be omitted\nfrom research reports. We encourage the use of vignettes to augment existing\ninteraction evaluation methods."
                },
                "authors": [
                    {
                        "name": "Agnes Axelsson"
                    },
                    {
                        "name": "Merle Reimann"
                    },
                    {
                        "name": "Ronald Cumbal"
                    },
                    {
                        "name": "Hannah Pelikan"
                    },
                    {
                        "name": "Divesh Lala"
                    }
                ],
                "author_detail": {
                    "name": "Divesh Lala"
                },
                "author": "Divesh Lala",
                "arxiv_comment": "Accepted at the workshop on Real-World HRI in Public and Private\n  Spaces: Successes, Failures, and Lessons Learned (PubRob-Fails), held at the\n  IEEE RO-MAN Conference, 2025. 6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10603v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10603v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10484v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10484v3",
                "updated": "2025-09-08T15:16:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    16,
                    5,
                    0,
                    251,
                    0
                ],
                "published": "2025-01-17T05:20:38Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    5,
                    20,
                    38,
                    4,
                    17,
                    0
                ],
                "title": "Bias in Decision-Making for AI's Ethical Dilemmas: A Comparative Study\n  of ChatGPT and Claude",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias in Decision-Making for AI's Ethical Dilemmas: A Comparative Study\n  of ChatGPT and Claude"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have enabled human-like\nresponses across various tasks, raising questions about their ethical\ndecision-making capabilities and potential biases. This study systematically\nevaluates how nine popular LLMs (both open-source and closed-source) respond to\nethical dilemmas involving protected attributes. Across 50,400 trials spanning\nsingle and intersectional attribute combinations in four dilemma scenarios\n(protective vs. harmful), we assess models' ethical preferences, sensitivity,\nstability, and clustering patterns. Results reveal significant biases in\nprotected attributes in all models, with differing preferences depending on\nmodel type and dilemma context. Notably, open-source LLMs show stronger\npreferences for marginalized groups and greater sensitivity in harmful\nscenarios, while closed-source models are more selective in protective\nsituations and tend to favor mainstream groups. We also find that ethical\nbehavior varies across dilemma types: LLMs maintain consistent patterns in\nprotective scenarios but respond with more diverse and cognitively demanding\ndecisions in harmful ones. Furthermore, models display more pronounced ethical\ntendencies under intersectional conditions than in single-attribute settings,\nsuggesting that complex inputs reveal deeper biases. These findings highlight\nthe need for multi-dimensional, context-aware evaluation of LLMs' ethical\nbehavior and offer a systematic evaluation and approach to understanding and\naddressing fairness in LLM decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have enabled human-like\nresponses across various tasks, raising questions about their ethical\ndecision-making capabilities and potential biases. This study systematically\nevaluates how nine popular LLMs (both open-source and closed-source) respond to\nethical dilemmas involving protected attributes. Across 50,400 trials spanning\nsingle and intersectional attribute combinations in four dilemma scenarios\n(protective vs. harmful), we assess models' ethical preferences, sensitivity,\nstability, and clustering patterns. Results reveal significant biases in\nprotected attributes in all models, with differing preferences depending on\nmodel type and dilemma context. Notably, open-source LLMs show stronger\npreferences for marginalized groups and greater sensitivity in harmful\nscenarios, while closed-source models are more selective in protective\nsituations and tend to favor mainstream groups. We also find that ethical\nbehavior varies across dilemma types: LLMs maintain consistent patterns in\nprotective scenarios but respond with more diverse and cognitively demanding\ndecisions in harmful ones. Furthermore, models display more pronounced ethical\ntendencies under intersectional conditions than in single-attribute settings,\nsuggesting that complex inputs reveal deeper biases. These findings highlight\nthe need for multi-dimensional, context-aware evaluation of LLMs' ethical\nbehavior and offer a systematic evaluation and approach to understanding and\naddressing fairness in LLM decision-making."
                },
                "authors": [
                    {
                        "name": "Yile Yan"
                    },
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Wentao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Xu"
                },
                "author": "Wentao Xu",
                "arxiv_comment": "This paper has been accepted by International AAAI Conference on Web\n  and Social Media 2026 (ICWSM 2026), sunny Los Angeles, California",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10484v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10484v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06774v1",
                "updated": "2025-09-08T14:58:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    58,
                    10,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T14:58:10Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    58,
                    10,
                    0,
                    251,
                    0
                ],
                "title": "OpenCoderRank: AI-Driven Technical Assessments Made Easy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenCoderRank: AI-Driven Technical Assessments Made Easy"
                },
                "summary": "Organizations and educational institutions use time-bound assessment tasks to\nevaluate coding and problem-solving skills. These assessments measure not only\nthe correctness of the solutions, but also their efficiency. Problem setters\n(educator/interviewer) are responsible for crafting these challenges, carefully\nbalancing difficulty and relevance to create meaningful evaluation experiences.\nConversely, problem solvers (student/interviewee) apply coding efficiency and\nlogical thinking to arrive at correct solutions. In the era of Large Language\nModels (LLMs), LLMs assist problem setters in generating diverse and\nchallenging questions, but they can undermine assessment integrity for problem\nsolvers by providing easy access to solutions. This paper introduces\nOpenCoderRank, an easy-to-use platform designed to simulate technical\nassessments. It acts as a bridge between problem setters and problem solvers,\nhelping solvers prepare for time constraints and unfamiliar problems while\nallowing setters to self-host assessments, offering a no-cost and customizable\nsolution for technical assessments in resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Organizations and educational institutions use time-bound assessment tasks to\nevaluate coding and problem-solving skills. These assessments measure not only\nthe correctness of the solutions, but also their efficiency. Problem setters\n(educator/interviewer) are responsible for crafting these challenges, carefully\nbalancing difficulty and relevance to create meaningful evaluation experiences.\nConversely, problem solvers (student/interviewee) apply coding efficiency and\nlogical thinking to arrive at correct solutions. In the era of Large Language\nModels (LLMs), LLMs assist problem setters in generating diverse and\nchallenging questions, but they can undermine assessment integrity for problem\nsolvers by providing easy access to solutions. This paper introduces\nOpenCoderRank, an easy-to-use platform designed to simulate technical\nassessments. It acts as a bridge between problem setters and problem solvers,\nhelping solvers prepare for time constraints and unfamiliar problems while\nallowing setters to self-host assessments, offering a no-cost and customizable\nsolution for technical assessments in resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Hridoy Sankar Dutta"
                    },
                    {
                        "name": "Sana Ansari"
                    },
                    {
                        "name": "Swati Kumari"
                    },
                    {
                        "name": "Shounak Ravi Bhalerao"
                    }
                ],
                "author_detail": {
                    "name": "Shounak Ravi Bhalerao"
                },
                "author": "Shounak Ravi Bhalerao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11844v3",
                "updated": "2025-09-08T14:56:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    56,
                    58,
                    0,
                    251,
                    0
                ],
                "published": "2024-11-18T18:59:31Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    31,
                    0,
                    323,
                    0
                ],
                "title": "Generative World Explorer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative World Explorer"
                },
                "summary": "Planning with partial observation is a central challenge in embodied AI. A\nmajority of prior works have tackled this challenge by developing agents that\nphysically explore their environment to update their beliefs about the world\nstate. In contrast, humans can $\\textit{imagine}$ unseen parts of the world\nthrough a mental exploration and $\\textit{revise}$ their beliefs with imagined\nobservations. Such updated beliefs can allow them to make more informed\ndecisions, without necessitating the physical exploration of the world at all\ntimes. To achieve this human-like ability, we introduce the $\\textit{Generative\nWorld Explorer (Genex)}$, an egocentric world exploration framework that allows\nan agent to mentally explore a large-scale 3D world (e.g., urban scenes) and\nacquire imagined observations to update its belief. This updated belief will\nthen help the agent to make a more informed decision at the current step. To\ntrain $\\textit{Genex}$, we create a synthetic urban scene dataset, Genex-DB.\nOur experimental results demonstrate that (1) $\\textit{Genex}$ can generate\nhigh-quality and consistent observations during long-horizon exploration of a\nlarge virtual physical world and (2) the beliefs updated with the generated\nobservations can inform an existing decision-making model (e.g., an LLM agent)\nto make better plans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning with partial observation is a central challenge in embodied AI. A\nmajority of prior works have tackled this challenge by developing agents that\nphysically explore their environment to update their beliefs about the world\nstate. In contrast, humans can $\\textit{imagine}$ unseen parts of the world\nthrough a mental exploration and $\\textit{revise}$ their beliefs with imagined\nobservations. Such updated beliefs can allow them to make more informed\ndecisions, without necessitating the physical exploration of the world at all\ntimes. To achieve this human-like ability, we introduce the $\\textit{Generative\nWorld Explorer (Genex)}$, an egocentric world exploration framework that allows\nan agent to mentally explore a large-scale 3D world (e.g., urban scenes) and\nacquire imagined observations to update its belief. This updated belief will\nthen help the agent to make a more informed decision at the current step. To\ntrain $\\textit{Genex}$, we create a synthetic urban scene dataset, Genex-DB.\nOur experimental results demonstrate that (1) $\\textit{Genex}$ can generate\nhigh-quality and consistent observations during long-horizon exploration of a\nlarge virtual physical world and (2) the beliefs updated with the generated\nobservations can inform an existing decision-making model (e.g., an LLM agent)\nto make better plans."
                },
                "authors": [
                    {
                        "name": "Taiming Lu"
                    },
                    {
                        "name": "Tianmin Shu"
                    },
                    {
                        "name": "Alan Yuille"
                    },
                    {
                        "name": "Daniel Khashabi"
                    },
                    {
                        "name": "Jieneng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jieneng Chen"
                },
                "author": "Jieneng Chen",
                "arxiv_comment": "Website: generative-world-explorer.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06770v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06770v1",
                "updated": "2025-09-08T14:54:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    54,
                    31,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T14:54:31Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    54,
                    31,
                    0,
                    251,
                    0
                ],
                "title": "Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM\n  Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM\n  Prompting"
                },
                "summary": "Large language models (LLMs) are now used in multi-turn workflows, but we\nstill lack a clear way to measure when iteration helps and when it hurts. We\npresent an evaluation framework for iterative refinement that spans ideation,\ncode, and math. Our protocol runs controlled 12-turn conversations per task,\nutilizing a variety of prompts ranging from vague ``improve it'' feedback to\ntargeted steering, and logs per-turn outputs. We score outcomes with\ndomain-appropriate checks (unit tests for code; answer-equivalence plus\nreasoning-soundness for math; originality and feasibility for ideation) and\ntrack turn-level behavior with three families of metrics: semantic movement\nacross turns, turn-to-turn change, and output size growth. Across models and\ntasks, gains are domain-dependent: they arrive early in ideas and code, but in\nmath late turns matter when guided by elaboration. After the first few turns,\nvague feedback often plateaus or reverses correctness, while targeted prompts\nreliably shift the intended quality axis (novelty vs. feasibility in ideation;\nspeed vs. readability in code; in math, elaboration outperforms exploration and\ndrives late-turn gains). We also observe consistent domain patterns: ideation\nmoves more in meaning across turns, code tends to grow in size with little\nsemantic change, and math starts fixed but can break that path with late,\nelaborative iteration.Together, the framework and metrics make iteration\nmeasurable and comparable across models, and signal when to steer, stop, or\nswitch strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are now used in multi-turn workflows, but we\nstill lack a clear way to measure when iteration helps and when it hurts. We\npresent an evaluation framework for iterative refinement that spans ideation,\ncode, and math. Our protocol runs controlled 12-turn conversations per task,\nutilizing a variety of prompts ranging from vague ``improve it'' feedback to\ntargeted steering, and logs per-turn outputs. We score outcomes with\ndomain-appropriate checks (unit tests for code; answer-equivalence plus\nreasoning-soundness for math; originality and feasibility for ideation) and\ntrack turn-level behavior with three families of metrics: semantic movement\nacross turns, turn-to-turn change, and output size growth. Across models and\ntasks, gains are domain-dependent: they arrive early in ideas and code, but in\nmath late turns matter when guided by elaboration. After the first few turns,\nvague feedback often plateaus or reverses correctness, while targeted prompts\nreliably shift the intended quality axis (novelty vs. feasibility in ideation;\nspeed vs. readability in code; in math, elaboration outperforms exploration and\ndrives late-turn gains). We also observe consistent domain patterns: ideation\nmoves more in meaning across turns, code tends to grow in size with little\nsemantic change, and math starts fixed but can break that path with late,\nelaborative iteration.Together, the framework and metrics make iteration\nmeasurable and comparable across models, and signal when to steer, stop, or\nswitch strategies."
                },
                "authors": [
                    {
                        "name": "Shashidhar Reddy Javaji"
                    },
                    {
                        "name": "Bhavul Gauri"
                    },
                    {
                        "name": "Zining Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Zining Zhu"
                },
                "author": "Zining Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06770v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06767v1",
                "updated": "2025-09-08T14:53:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    53,
                    1,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T14:53:01Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    53,
                    1,
                    0,
                    251,
                    0
                ],
                "title": "Raw2Event: Converting Raw Frame Camera into Event Camera",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Raw2Event: Converting Raw Frame Camera into Event Camera"
                },
                "summary": "Event cameras offer unique advantages such as high temporal resolution, low\nlatency, and high dynamic range, making them more and more popular for vision\ntasks under challenging light conditions. However, their high cost, limited\nresolution, and lack of features such as autofocus hinder their broad adoption,\nparticularly for early-stage development and prototyping. In this work, we\npresent Raw2Event, a complete hardware-software system that enables real-time\nevent generation from low-cost raw frame-based cameras. By leveraging direct\naccess to raw Bayer data and bypassing traditional image signal processors\n(ISP), our system is able to utilize the full potential of camera hardware,\ndelivering higher dynamic range, higher resolution, and more faithful output\nthan RGB-based frame-to-event converters.\n  Built upon the DVS-Voltmeter model, Raw2Event features a configurable\nsimulation framework optimized for deployment on embedded platforms. We further\ndesign a data acquisition pipeline that supports synchronized recording of raw,\nRGB, and event streams, facilitating downstream evaluation and dataset\ncreation. Experimental results show that Raw2Event can generate event streams\nclosely resembling those from real event cameras, while benefiting from higher\nresolution and autofocus capabilities. The system also supports user-intuitive\nparameter tuning, enabling flexible adaptation to various application\nrequirements. Finally, we deploy the system on a Raspberry Pi for real-time\noperation, providing a scalable and cost-effective solution for event-based\nvision research and early-stage system development.\n  The codes are available online:\nhttps://anonymous.4open.science/r/raw2event-BFF2/README.md.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event cameras offer unique advantages such as high temporal resolution, low\nlatency, and high dynamic range, making them more and more popular for vision\ntasks under challenging light conditions. However, their high cost, limited\nresolution, and lack of features such as autofocus hinder their broad adoption,\nparticularly for early-stage development and prototyping. In this work, we\npresent Raw2Event, a complete hardware-software system that enables real-time\nevent generation from low-cost raw frame-based cameras. By leveraging direct\naccess to raw Bayer data and bypassing traditional image signal processors\n(ISP), our system is able to utilize the full potential of camera hardware,\ndelivering higher dynamic range, higher resolution, and more faithful output\nthan RGB-based frame-to-event converters.\n  Built upon the DVS-Voltmeter model, Raw2Event features a configurable\nsimulation framework optimized for deployment on embedded platforms. We further\ndesign a data acquisition pipeline that supports synchronized recording of raw,\nRGB, and event streams, facilitating downstream evaluation and dataset\ncreation. Experimental results show that Raw2Event can generate event streams\nclosely resembling those from real event cameras, while benefiting from higher\nresolution and autofocus capabilities. The system also supports user-intuitive\nparameter tuning, enabling flexible adaptation to various application\nrequirements. Finally, we deploy the system on a Raspberry Pi for real-time\noperation, providing a scalable and cost-effective solution for event-based\nvision research and early-stage system development.\n  The codes are available online:\nhttps://anonymous.4open.science/r/raw2event-BFF2/README.md."
                },
                "authors": [
                    {
                        "name": "Zijie Ning"
                    },
                    {
                        "name": "Enmin Lin"
                    },
                    {
                        "name": "Sudarshan R. Iyengar"
                    },
                    {
                        "name": "Patrick Vandewalle"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Vandewalle"
                },
                "author": "Patrick Vandewalle",
                "arxiv_comment": "Submitted to IEEE Transactions on Robotics (Special Section on\n  Event-based Vision for Robotics), under review. This version is submitted for\n  peer review and may be updated upon acceptance",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07831v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07831v3",
                "updated": "2025-09-08T14:34:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    34,
                    7,
                    0,
                    251,
                    0
                ],
                "published": "2024-06-12T02:57:41Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    2,
                    57,
                    41,
                    2,
                    164,
                    0
                ],
                "title": "ALPS: Improved Optimization for Highly Sparse One-Shot Pruning for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALPS: Improved Optimization for Highly Sparse One-Shot Pruning for Large\n  Language Models"
                },
                "summary": "The impressive performance of Large Language Models (LLMs) across various\nnatural language processing tasks comes at the cost of vast computational\nresources and storage requirements. One-shot pruning techniques offer a way to\nalleviate these burdens by removing redundant weights without the need for\nretraining. Yet, the massive scale of LLMs often forces current pruning\napproaches to rely on heuristics instead of optimization-based techniques,\npotentially resulting in suboptimal compression. In this paper, we introduce\nALPS, an optimization-based framework that tackles the pruning problem using\nthe operator splitting technique and a preconditioned conjugate gradient-based\npost-processing step. Our approach incorporates novel techniques to accelerate\nand theoretically guarantee convergence while leveraging vectorization and GPU\nparallelism for efficiency. ALPS substantially outperforms state-of-the-art\nmethods in terms of the pruning objective and perplexity reduction,\nparticularly for highly sparse models. On the OPT-30B model with 70% sparsity,\nALPS achieves a 13% reduction in test perplexity on the WikiText dataset and a\n19% improvement in zero-shot benchmark performance compared to existing\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impressive performance of Large Language Models (LLMs) across various\nnatural language processing tasks comes at the cost of vast computational\nresources and storage requirements. One-shot pruning techniques offer a way to\nalleviate these burdens by removing redundant weights without the need for\nretraining. Yet, the massive scale of LLMs often forces current pruning\napproaches to rely on heuristics instead of optimization-based techniques,\npotentially resulting in suboptimal compression. In this paper, we introduce\nALPS, an optimization-based framework that tackles the pruning problem using\nthe operator splitting technique and a preconditioned conjugate gradient-based\npost-processing step. Our approach incorporates novel techniques to accelerate\nand theoretically guarantee convergence while leveraging vectorization and GPU\nparallelism for efficiency. ALPS substantially outperforms state-of-the-art\nmethods in terms of the pruning objective and perplexity reduction,\nparticularly for highly sparse models. On the OPT-30B model with 70% sparsity,\nALPS achieves a 13% reduction in test perplexity on the WikiText dataset and a\n19% improvement in zero-shot benchmark performance compared to existing\nmethods."
                },
                "authors": [
                    {
                        "name": "Xiang Meng"
                    },
                    {
                        "name": "Kayhan Behdin"
                    },
                    {
                        "name": "Haoyue Wang"
                    },
                    {
                        "name": "Rahul Mazumder"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Mazumder"
                },
                "author": "Rahul Mazumder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07831v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07831v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07963v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07963v3",
                "updated": "2025-09-08T14:31:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    31,
                    8,
                    0,
                    251,
                    0
                ],
                "published": "2025-06-09T17:38:45Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    17,
                    38,
                    45,
                    0,
                    160,
                    0
                ],
                "title": "SUDER: Self-Improving Unified Large Multimodal Models for Understanding\n  and Generation with Dual Self-Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SUDER: Self-Improving Unified Large Multimodal Models for Understanding\n  and Generation with Dual Self-Rewards"
                },
                "summary": "Building upon large language models (LLMs), recent large multimodal models\n(LMMs) unify cross-model understanding and generation into a single framework.\nHowever, LMMs still struggle to achieve accurate vision-language alignment,\nprone to generating text responses contradicting the visual input or failing to\nfollow the text-to-image prompts. Current solutions require external\nsupervision (e.g., human feedback or reward models) and only address\nunidirectional tasks-either understanding or generation. In this work, based on\nthe observation that understanding and generation are naturally inverse dual\ntasks, we propose \\textbf{SUDER} (\\textbf{S}elf-improving \\textbf{U}nified LMMs\nwith \\textbf{D}ual s\\textbf{E}lf-\\textbf{R}ewards), a framework reinforcing the\nunderstanding and generation capabilities of LMMs with a self-supervised dual\nreward mechanism. SUDER leverages the inherent duality between understanding\nand generation tasks to provide self-supervised optimization signals for each\nother. Specifically, we sample multiple outputs for a given input in one task\ndomain, then reverse the input-output pairs to compute the dual likelihood\nwithin the model as self-rewards for optimization. Extensive experimental\nresults on visual understanding and generation benchmarks demonstrate that our\nmethod can effectively enhance the performance of the model without any\nexternal supervision, especially achieving remarkable improvements in\ntext-to-image tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building upon large language models (LLMs), recent large multimodal models\n(LMMs) unify cross-model understanding and generation into a single framework.\nHowever, LMMs still struggle to achieve accurate vision-language alignment,\nprone to generating text responses contradicting the visual input or failing to\nfollow the text-to-image prompts. Current solutions require external\nsupervision (e.g., human feedback or reward models) and only address\nunidirectional tasks-either understanding or generation. In this work, based on\nthe observation that understanding and generation are naturally inverse dual\ntasks, we propose \\textbf{SUDER} (\\textbf{S}elf-improving \\textbf{U}nified LMMs\nwith \\textbf{D}ual s\\textbf{E}lf-\\textbf{R}ewards), a framework reinforcing the\nunderstanding and generation capabilities of LMMs with a self-supervised dual\nreward mechanism. SUDER leverages the inherent duality between understanding\nand generation tasks to provide self-supervised optimization signals for each\nother. Specifically, we sample multiple outputs for a given input in one task\ndomain, then reverse the input-output pairs to compute the dual likelihood\nwithin the model as self-rewards for optimization. Extensive experimental\nresults on visual understanding and generation benchmarks demonstrate that our\nmethod can effectively enhance the performance of the model without any\nexternal supervision, especially achieving remarkable improvements in\ntext-to-image tasks."
                },
                "authors": [
                    {
                        "name": "Jixiang Hong"
                    },
                    {
                        "name": "Yiran Zhang"
                    },
                    {
                        "name": "Guanzhong Wang"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07963v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07963v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06734v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06734v1",
                "updated": "2025-09-08T14:28:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    28,
                    13,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T14:28:13Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    28,
                    13,
                    0,
                    251,
                    0
                ],
                "title": "Intelligent Manufacturing Support: Specialized LLMs for Composite\n  Material Processing and Equipment Operation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Manufacturing Support: Specialized LLMs for Composite\n  Material Processing and Equipment Operation"
                },
                "summary": "Engineering educational curriculum and standards cover many material and\nmanufacturing options. However, engineers and designers are often unfamiliar\nwith certain composite materials or manufacturing techniques. Large language\nmodels (LLMs) could potentially bridge the gap. Their capacity to store and\nretrieve data from large databases provides them with a breadth of knowledge\nacross disciplines. However, their generalized knowledge base can lack\ntargeted, industry-specific knowledge. To this end, we present two LLM-based\napplications based on the GPT-4 architecture: (1) The Composites Guide: a\nsystem that provides expert knowledge on composites material and connects users\nwith research and industry professionals who can provide additional support and\n(2) The Equipment Assistant: a system that provides guidance for manufacturing\ntool operation and material characterization. By combining the knowledge of\ngeneral AI models with industry-specific knowledge, both applications are\nintended to provide more meaningful information for engineers. In this paper,\nwe discuss the development of the applications and evaluate it through a\nbenchmark and two informal user studies. The benchmark analysis uses the Rouge\nand Bertscore metrics to evaluate our model performance against GPT-4o. The\nresults show that GPT-4o and the proposed models perform similarly or better on\nthe ROUGE and BERTScore metrics. The two user studies supplement this\nquantitative evaluation by asking experts to provide qualitative and open-ended\nfeedback about our model performance on a set of domain-specific questions. The\nresults of both studies highlight a potential for more detailed and specific\nresponses with the Composites Guide and the Equipment Assistant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering educational curriculum and standards cover many material and\nmanufacturing options. However, engineers and designers are often unfamiliar\nwith certain composite materials or manufacturing techniques. Large language\nmodels (LLMs) could potentially bridge the gap. Their capacity to store and\nretrieve data from large databases provides them with a breadth of knowledge\nacross disciplines. However, their generalized knowledge base can lack\ntargeted, industry-specific knowledge. To this end, we present two LLM-based\napplications based on the GPT-4 architecture: (1) The Composites Guide: a\nsystem that provides expert knowledge on composites material and connects users\nwith research and industry professionals who can provide additional support and\n(2) The Equipment Assistant: a system that provides guidance for manufacturing\ntool operation and material characterization. By combining the knowledge of\ngeneral AI models with industry-specific knowledge, both applications are\nintended to provide more meaningful information for engineers. In this paper,\nwe discuss the development of the applications and evaluate it through a\nbenchmark and two informal user studies. The benchmark analysis uses the Rouge\nand Bertscore metrics to evaluate our model performance against GPT-4o. The\nresults show that GPT-4o and the proposed models perform similarly or better on\nthe ROUGE and BERTScore metrics. The two user studies supplement this\nquantitative evaluation by asking experts to provide qualitative and open-ended\nfeedback about our model performance on a set of domain-specific questions. The\nresults of both studies highlight a potential for more detailed and specific\nresponses with the Composites Guide and the Equipment Assistant."
                },
                "authors": [
                    {
                        "name": "Gunnika Kapoor"
                    },
                    {
                        "name": "Komal Chawla"
                    },
                    {
                        "name": "Tirthankar Ghosal"
                    },
                    {
                        "name": "Kris Villez"
                    },
                    {
                        "name": "Dan Coughlin"
                    },
                    {
                        "name": "Tyden Rucker"
                    },
                    {
                        "name": "Vincent Paquit"
                    },
                    {
                        "name": "Soydan Ozcan"
                    },
                    {
                        "name": "Seokpum Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seokpum Kim"
                },
                "author": "Seokpum Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06734v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06733v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06733v1",
                "updated": "2025-09-08T14:27:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    27,
                    23,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T14:27:23Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    27,
                    23,
                    0,
                    251,
                    0
                ],
                "title": "Reinforcement Learning Foundations for Deep Research Systems: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning Foundations for Deep Research Systems: A Survey"
                },
                "summary": "Deep research systems, agentic AI that solve complex, multi-step tasks by\ncoordinating reasoning, search across the open web and user files, and tool\nuse, are moving toward hierarchical deployments with a Planner, Coordinator,\nand Executors. In practice, training entire stacks end-to-end remains\nimpractical, so most work trains a single planner connected to core tools such\nas search, browsing, and code. While SFT imparts protocol fidelity, it suffers\nfrom imitation and exposure biases and underuses environment feedback.\nPreference alignment methods such as DPO are schema and proxy-dependent,\noff-policy, and weak for long-horizon credit assignment and multi-objective\ntrade-offs. A further limitation of SFT and DPO is their reliance on human\ndefined decision points and subskills through schema design and labeled\ncomparisons. Reinforcement learning aligns with closed-loop, tool-interaction\nresearch by optimizing trajectory-level policies, enabling exploration,\nrecovery behaviors, and principled credit assignment, and it reduces dependence\non such human priors and rater biases.\n  This survey is, to our knowledge, the first dedicated to the RL foundations\nof deep research systems. It systematizes work after DeepSeek-R1 along three\naxes: (i) data synthesis and curation; (ii) RL methods for agentic research\ncovering stability, sample efficiency, long context handling, reward and credit\ndesign, multi-objective optimization, and multimodal integration; and (iii)\nagentic RL training systems and frameworks. We also cover agent architecture\nand coordination, as well as evaluation and benchmarks, including recent QA,\nVQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We\ndistill recurring patterns, surface infrastructure bottlenecks, and offer\npractical guidance for training robust, transparent deep research agents with\nRL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep research systems, agentic AI that solve complex, multi-step tasks by\ncoordinating reasoning, search across the open web and user files, and tool\nuse, are moving toward hierarchical deployments with a Planner, Coordinator,\nand Executors. In practice, training entire stacks end-to-end remains\nimpractical, so most work trains a single planner connected to core tools such\nas search, browsing, and code. While SFT imparts protocol fidelity, it suffers\nfrom imitation and exposure biases and underuses environment feedback.\nPreference alignment methods such as DPO are schema and proxy-dependent,\noff-policy, and weak for long-horizon credit assignment and multi-objective\ntrade-offs. A further limitation of SFT and DPO is their reliance on human\ndefined decision points and subskills through schema design and labeled\ncomparisons. Reinforcement learning aligns with closed-loop, tool-interaction\nresearch by optimizing trajectory-level policies, enabling exploration,\nrecovery behaviors, and principled credit assignment, and it reduces dependence\non such human priors and rater biases.\n  This survey is, to our knowledge, the first dedicated to the RL foundations\nof deep research systems. It systematizes work after DeepSeek-R1 along three\naxes: (i) data synthesis and curation; (ii) RL methods for agentic research\ncovering stability, sample efficiency, long context handling, reward and credit\ndesign, multi-objective optimization, and multimodal integration; and (iii)\nagentic RL training systems and frameworks. We also cover agent architecture\nand coordination, as well as evaluation and benchmarks, including recent QA,\nVQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We\ndistill recurring patterns, surface infrastructure bottlenecks, and offer\npractical guidance for training robust, transparent deep research agents with\nRL."
                },
                "authors": [
                    {
                        "name": "Wenjun Li"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Jingru Lin"
                    },
                    {
                        "name": "Hannan Cao"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Sheng Liang"
                    },
                    {
                        "name": "Zhi Zhang"
                    },
                    {
                        "name": "Kuicai Dong"
                    },
                    {
                        "name": "Dexun Li"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "38 pages, first version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06733v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06733v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18183v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18183v2",
                "updated": "2025-09-08T14:25:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    25,
                    45,
                    0,
                    251,
                    0
                ],
                "published": "2025-08-25T16:36:36Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    36,
                    36,
                    0,
                    237,
                    0
                ],
                "title": "Leveraging Large Language Models for Accurate Sign Language Translation\n  in Low-Resource Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Accurate Sign Language Translation\n  in Low-Resource Scenarios"
                },
                "summary": "Translating natural languages into sign languages is a highly complex and\nunderexplored task. Despite growing interest in accessibility and inclusivity,\nthe development of robust translation systems remains hindered by the limited\navailability of parallel corpora which align natural language with sign\nlanguage data. Existing methods often struggle to generalize in these\ndata-scarce environments, as the few datasets available are typically\ndomain-specific, lack standardization, or fail to capture the full linguistic\nrichness of sign languages. To address this limitation, we propose Advanced Use\nof LLMs for Sign Language Translation (AulSign), a novel method that leverages\nLarge Language Models via dynamic prompting and in-context learning with sample\nselection and subsequent sign association. Despite their impressive abilities\nin processing text, LLMs lack intrinsic knowledge of sign languages; therefore,\nthey are unable to natively perform this kind of translation. To overcome this\nlimitation, we associate the signs with compact descriptions in natural\nlanguage and instruct the model to use them. We evaluate our method on both\nEnglish and Italian languages using SignBank+, a recognized benchmark in the\nfield, as well as the Italian LaCAM CNR-ISTC dataset. We demonstrate superior\nperformance compared to state-of-the-art models in low-data scenario. Our\nfindings demonstrate the effectiveness of AulSign, with the potential to\nenhance accessibility and inclusivity in communication technologies for\nunderrepresented linguistic communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating natural languages into sign languages is a highly complex and\nunderexplored task. Despite growing interest in accessibility and inclusivity,\nthe development of robust translation systems remains hindered by the limited\navailability of parallel corpora which align natural language with sign\nlanguage data. Existing methods often struggle to generalize in these\ndata-scarce environments, as the few datasets available are typically\ndomain-specific, lack standardization, or fail to capture the full linguistic\nrichness of sign languages. To address this limitation, we propose Advanced Use\nof LLMs for Sign Language Translation (AulSign), a novel method that leverages\nLarge Language Models via dynamic prompting and in-context learning with sample\nselection and subsequent sign association. Despite their impressive abilities\nin processing text, LLMs lack intrinsic knowledge of sign languages; therefore,\nthey are unable to natively perform this kind of translation. To overcome this\nlimitation, we associate the signs with compact descriptions in natural\nlanguage and instruct the model to use them. We evaluate our method on both\nEnglish and Italian languages using SignBank+, a recognized benchmark in the\nfield, as well as the Italian LaCAM CNR-ISTC dataset. We demonstrate superior\nperformance compared to state-of-the-art models in low-data scenario. Our\nfindings demonstrate the effectiveness of AulSign, with the potential to\nenhance accessibility and inclusivity in communication technologies for\nunderrepresented linguistic communities."
                },
                "authors": [
                    {
                        "name": "Luana Bulla"
                    },
                    {
                        "name": "Gabriele Tuccio"
                    },
                    {
                        "name": "Misael Mongiovì"
                    },
                    {
                        "name": "Aldo Gangemi"
                    }
                ],
                "author_detail": {
                    "name": "Aldo Gangemi"
                },
                "author": "Aldo Gangemi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18183v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18183v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07642v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07642v2",
                "updated": "2025-09-08T14:22:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    22,
                    50,
                    0,
                    251,
                    0
                ],
                "published": "2025-06-09T11:07:55Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    11,
                    7,
                    55,
                    0,
                    160,
                    0
                ],
                "title": "TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient\n  LLM-based Scientific Peer Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient\n  LLM-based Scientific Peer Review"
                },
                "summary": "While Large Language Models (LLMs) have shown significant potential in\nassisting peer review, current methods often struggle to generate thorough and\ninsightful reviews while maintaining efficiency. In this paper, we propose\nTreeReview, a novel framework that models paper review as a hierarchical and\nbidirectional question-answering process. TreeReview first constructs a tree of\nreview questions by recursively decomposing high-level questions into\nfine-grained sub-questions and then resolves the question tree by iteratively\naggregating answers from leaf to root to get the final review. Crucially, we\nincorporate a dynamic question expansion mechanism to enable deeper probing by\ngenerating follow-up questions when needed. We construct a benchmark derived\nfrom ICLR and NeurIPS venues to evaluate our method on full review generation\nand actionable feedback comments generation tasks. Experimental results of both\nLLM-based and human evaluation show that TreeReview outperforms strong\nbaselines in providing comprehensive, in-depth, and expert-aligned review\nfeedback, while reducing LLM token usage by up to 80% compared to\ncomputationally intensive approaches. Our code and benchmark dataset are\navailable at https://github.com/YuanChang98/tree-review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have shown significant potential in\nassisting peer review, current methods often struggle to generate thorough and\ninsightful reviews while maintaining efficiency. In this paper, we propose\nTreeReview, a novel framework that models paper review as a hierarchical and\nbidirectional question-answering process. TreeReview first constructs a tree of\nreview questions by recursively decomposing high-level questions into\nfine-grained sub-questions and then resolves the question tree by iteratively\naggregating answers from leaf to root to get the final review. Crucially, we\nincorporate a dynamic question expansion mechanism to enable deeper probing by\ngenerating follow-up questions when needed. We construct a benchmark derived\nfrom ICLR and NeurIPS venues to evaluate our method on full review generation\nand actionable feedback comments generation tasks. Experimental results of both\nLLM-based and human evaluation show that TreeReview outperforms strong\nbaselines in providing comprehensive, in-depth, and expert-aligned review\nfeedback, while reducing LLM token usage by up to 80% compared to\ncomputationally intensive approaches. Our code and benchmark dataset are\navailable at https://github.com/YuanChang98/tree-review."
                },
                "authors": [
                    {
                        "name": "Yuan Chang"
                    },
                    {
                        "name": "Ziyue Li"
                    },
                    {
                        "name": "Hengyuan Zhang"
                    },
                    {
                        "name": "Yuanbo Kong"
                    },
                    {
                        "name": "Yanru Wu"
                    },
                    {
                        "name": "Hayden Kwok-Hay So"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Liya Zhu"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "Accepted to EMNLP2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07642v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07642v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06716v1",
                "updated": "2025-09-08T14:11:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    11,
                    35,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T14:11:35Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    11,
                    35,
                    0,
                    251,
                    0
                ],
                "title": "Efficiently Ranking Software Variants with Minimal Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Ranking Software Variants with Minimal Benchmarks"
                },
                "summary": "Benchmarking is a common practice in software engineering to assess the\nqualities and performance of software variants, coming from multiple competing\nsystems or from configurations of the same system. Benchmarks are used notably\nto compare and understand variant performance, fine-tune software, detect\nregressions, or design new software systems. The execution of benchmarks to get\na complete picture of software variants is highly costly in terms of\ncomputational resources and time. In this paper, we propose a novel approach\nfor reducing benchmarks while maintaining stable rankings, using test suite\noptimization techniques. That is, we remove instances from the benchmarks while\ntrying to keep the same rankings of the variants on all tests. Our method,\nBISection Sampling, BISS, strategically retains the most critical tests and\napplies a novel divide-and-conquer approach to efficiently sample among\nrelevant remaining tests. We experiment with datasets and use cases from LLM\nleaderboards, SAT competitions, and configurable systems for performance\nmodeling. Our results show that our method outperforms baselines even when\noperating on a subset of variants. Using BISS, we reduce the computational cost\nof the benchmarks on average to 44% and on more than half the benchmarks by up\nto 99% without loss in ranking stability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking is a common practice in software engineering to assess the\nqualities and performance of software variants, coming from multiple competing\nsystems or from configurations of the same system. Benchmarks are used notably\nto compare and understand variant performance, fine-tune software, detect\nregressions, or design new software systems. The execution of benchmarks to get\na complete picture of software variants is highly costly in terms of\ncomputational resources and time. In this paper, we propose a novel approach\nfor reducing benchmarks while maintaining stable rankings, using test suite\noptimization techniques. That is, we remove instances from the benchmarks while\ntrying to keep the same rankings of the variants on all tests. Our method,\nBISection Sampling, BISS, strategically retains the most critical tests and\napplies a novel divide-and-conquer approach to efficiently sample among\nrelevant remaining tests. We experiment with datasets and use cases from LLM\nleaderboards, SAT competitions, and configurable systems for performance\nmodeling. Our results show that our method outperforms baselines even when\noperating on a subset of variants. Using BISS, we reduce the computational cost\nof the benchmarks on average to 44% and on more than half the benchmarks by up\nto 99% without loss in ranking stability."
                },
                "authors": [
                    {
                        "name": "Théo Matricon"
                    },
                    {
                        "name": "Mathieu Acher"
                    },
                    {
                        "name": "Helge Spieker"
                    },
                    {
                        "name": "Arnaud Gotlieb"
                    }
                ],
                "author_detail": {
                    "name": "Arnaud Gotlieb"
                },
                "author": "Arnaud Gotlieb",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06701v1",
                "updated": "2025-09-08T13:55:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    55,
                    1,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T13:55:01Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    55,
                    1,
                    0,
                    251,
                    0
                ],
                "title": "Probabilistic Modeling of Latent Agentic Substructures in Deep Neural\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Modeling of Latent Agentic Substructures in Deep Neural\n  Networks"
                },
                "summary": "We develop a theory of intelligent agency grounded in probabilistic modeling\nfor neural models. Agents are represented as outcome distributions with\nepistemic utility given by log score, and compositions are defined through\nweighted logarithmic pooling that strictly improves every member's welfare. We\nprove that strict unanimity is impossible under linear pooling or in binary\noutcome spaces, but possible with three or more outcomes. Our framework admits\nrecursive structure via cloning invariance, continuity, and openness, while\ntilt-based analysis rules out trivial duplication. Finally, we formalize an\nagentic alignment phenomenon in LLMs using our theory: eliciting a benevolent\npersona (\"Luigi'\") induces an antagonistic counterpart (\"Waluigi\"), while a\nmanifest-then-suppress Waluigi strategy yields strictly larger first-order\nmisalignment reduction than pure Luigi reinforcement alone. These results\nclarify how developing a principled mathematical framework for how subagents\ncan coalesce into coherent higher-level entities provides novel implications\nfor alignment in agentic AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a theory of intelligent agency grounded in probabilistic modeling\nfor neural models. Agents are represented as outcome distributions with\nepistemic utility given by log score, and compositions are defined through\nweighted logarithmic pooling that strictly improves every member's welfare. We\nprove that strict unanimity is impossible under linear pooling or in binary\noutcome spaces, but possible with three or more outcomes. Our framework admits\nrecursive structure via cloning invariance, continuity, and openness, while\ntilt-based analysis rules out trivial duplication. Finally, we formalize an\nagentic alignment phenomenon in LLMs using our theory: eliciting a benevolent\npersona (\"Luigi'\") induces an antagonistic counterpart (\"Waluigi\"), while a\nmanifest-then-suppress Waluigi strategy yields strictly larger first-order\nmisalignment reduction than pure Luigi reinforcement alone. These results\nclarify how developing a principled mathematical framework for how subagents\ncan coalesce into coherent higher-level entities provides novel implications\nfor alignment in agentic AI systems."
                },
                "authors": [
                    {
                        "name": "Su Hyeong Lee"
                    },
                    {
                        "name": "Risi Kondor"
                    },
                    {
                        "name": "Richard Ngo"
                    }
                ],
                "author_detail": {
                    "name": "Richard Ngo"
                },
                "author": "Richard Ngo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06700v1",
                "updated": "2025-09-08T13:54:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    54,
                    42,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T13:54:42Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    54,
                    42,
                    0,
                    251,
                    0
                ],
                "title": "Sovereign AI for 6G: Towards the Future of AI-Native Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sovereign AI for 6G: Towards the Future of AI-Native Networks"
                },
                "summary": "The advent of Generative Artificial Intelligence (GenAI), Large Language\nModels (LLMs), and Large Telecom Models (LTM) significantly reshapes mobile\nnetworks, especially as the telecom industry transitions from 5G's\ncloud-centric to AI-native 6G architectures. This transition unlocks\nunprecedented capabilities in real-time automation, semantic networking, and\nautonomous service orchestration. However, it introduces critical risks related\nto data sovereignty, security, explainability, and regulatory compliance\nespecially when AI models are trained, deployed, or governed externally. This\npaper introduces the concept of `Sovereign AI' as a strategic imperative for\n6G, proposing architectural, operational, and governance frameworks that enable\nnational or operator-level control over AI development, deployment, and\nlife-cycle management. Focusing on O-RAN architecture, we explore how sovereign\nAI-based xApps and rApps can be deployed Near-RT and Non-RT RICs to ensure\npolicy-aligned control, secure model updates, and federated learning across\ntrusted infrastructure. We analyse global strategies, technical enablers, and\nchallenges across safety, talent, and model governance. Our findings underscore\nthat Sovereign AI is not just a regulatory necessity but a foundational pillar\nfor secure, resilient, and ethically-aligned 6G networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Generative Artificial Intelligence (GenAI), Large Language\nModels (LLMs), and Large Telecom Models (LTM) significantly reshapes mobile\nnetworks, especially as the telecom industry transitions from 5G's\ncloud-centric to AI-native 6G architectures. This transition unlocks\nunprecedented capabilities in real-time automation, semantic networking, and\nautonomous service orchestration. However, it introduces critical risks related\nto data sovereignty, security, explainability, and regulatory compliance\nespecially when AI models are trained, deployed, or governed externally. This\npaper introduces the concept of `Sovereign AI' as a strategic imperative for\n6G, proposing architectural, operational, and governance frameworks that enable\nnational or operator-level control over AI development, deployment, and\nlife-cycle management. Focusing on O-RAN architecture, we explore how sovereign\nAI-based xApps and rApps can be deployed Near-RT and Non-RT RICs to ensure\npolicy-aligned control, secure model updates, and federated learning across\ntrusted infrastructure. We analyse global strategies, technical enablers, and\nchallenges across safety, talent, and model governance. Our findings underscore\nthat Sovereign AI is not just a regulatory necessity but a foundational pillar\nfor secure, resilient, and ethically-aligned 6G networks."
                },
                "authors": [
                    {
                        "name": "Swarna Bindu Chetty"
                    },
                    {
                        "name": "David Grace"
                    },
                    {
                        "name": "Simon Saunders"
                    },
                    {
                        "name": "Paul Harris"
                    },
                    {
                        "name": "Eirini Eleni Tsiropoulou"
                    },
                    {
                        "name": "Tony Quek"
                    },
                    {
                        "name": "Hamed Ahmadi"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Ahmadi"
                },
                "author": "Hamed Ahmadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06690v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06690v1",
                "updated": "2025-09-08T13:44:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    44,
                    55,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T13:44:55Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    44,
                    55,
                    0,
                    251,
                    0
                ],
                "title": "BioLite U-Net: Edge-Deployable Semantic Segmentation for In Situ\n  Bioprinting Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BioLite U-Net: Edge-Deployable Semantic Segmentation for In Situ\n  Bioprinting Monitoring"
                },
                "summary": "Bioprinting is a rapidly advancing field that offers a transformative\napproach to fabricating tissue and organ models through the precise deposition\nof cell-laden bioinks. Ensuring the fidelity and consistency of printed\nstructures in real-time remains a core challenge, particularly under\nconstraints imposed by limited imaging data and resource-constrained embedded\nhardware. Semantic segmentation of the extrusion process, differentiating\nbetween nozzle, extruded bioink, and surrounding background, enables in situ\nmonitoring critical to maintaining print quality and biological viability. In\nthis work, we introduce a lightweight semantic segmentation framework tailored\nfor real-time bioprinting applications. We present a novel, manually annotated\ndataset comprising 787 RGB images captured during the bioprinting process,\nlabeled across three classes: nozzle, bioink, and background. To achieve fast\nand efficient inference suitable for integration with bioprinting systems, we\npropose a BioLite U-Net architecture that leverages depthwise separable\nconvolutions to drastically reduce computational load without compromising\naccuracy. Our model is benchmarked against MobileNetV2 and MobileNetV3-based\nsegmentation baselines using mean Intersection over Union (mIoU), Dice score,\nand pixel accuracy. All models were evaluated on a Raspberry Pi 4B to assess\nreal-world feasibility. The proposed BioLite U-Net achieves an mIoU of 92.85%\nand a Dice score of 96.17%, while being over 1300x smaller than\nMobileNetV2-DeepLabV3+. On-device inference takes 335 ms per frame,\ndemonstrating near real-time capability. Compared to MobileNet baselines,\nBioLite U-Net offers a superior tradeoff between segmentation accuracy,\nefficiency, and deployability, making it highly suitable for intelligent,\nclosed-loop bioprinting systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bioprinting is a rapidly advancing field that offers a transformative\napproach to fabricating tissue and organ models through the precise deposition\nof cell-laden bioinks. Ensuring the fidelity and consistency of printed\nstructures in real-time remains a core challenge, particularly under\nconstraints imposed by limited imaging data and resource-constrained embedded\nhardware. Semantic segmentation of the extrusion process, differentiating\nbetween nozzle, extruded bioink, and surrounding background, enables in situ\nmonitoring critical to maintaining print quality and biological viability. In\nthis work, we introduce a lightweight semantic segmentation framework tailored\nfor real-time bioprinting applications. We present a novel, manually annotated\ndataset comprising 787 RGB images captured during the bioprinting process,\nlabeled across three classes: nozzle, bioink, and background. To achieve fast\nand efficient inference suitable for integration with bioprinting systems, we\npropose a BioLite U-Net architecture that leverages depthwise separable\nconvolutions to drastically reduce computational load without compromising\naccuracy. Our model is benchmarked against MobileNetV2 and MobileNetV3-based\nsegmentation baselines using mean Intersection over Union (mIoU), Dice score,\nand pixel accuracy. All models were evaluated on a Raspberry Pi 4B to assess\nreal-world feasibility. The proposed BioLite U-Net achieves an mIoU of 92.85%\nand a Dice score of 96.17%, while being over 1300x smaller than\nMobileNetV2-DeepLabV3+. On-device inference takes 335 ms per frame,\ndemonstrating near real-time capability. Compared to MobileNet baselines,\nBioLite U-Net offers a superior tradeoff between segmentation accuracy,\nefficiency, and deployability, making it highly suitable for intelligent,\nclosed-loop bioprinting systems."
                },
                "authors": [
                    {
                        "name": "Usman Haider"
                    },
                    {
                        "name": "Lukasz Szemet"
                    },
                    {
                        "name": "Daniel Kelly"
                    },
                    {
                        "name": "Vasileios Sergis"
                    },
                    {
                        "name": "Andrew C. Daly"
                    },
                    {
                        "name": "Karl Mason"
                    }
                ],
                "author_detail": {
                    "name": "Karl Mason"
                },
                "author": "Karl Mason",
                "arxiv_comment": "8 pages, 5 figures, conference-style submission (ICRA 2026). Includes\n  dataset description, BioLite U-Net architecture, benchmark results on edge\n  device (Raspberry Pi 4B)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06690v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9; I.2.10; I.4.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06665v1",
                "updated": "2025-09-08T13:24:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    24,
                    21,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T13:24:21Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    24,
                    21,
                    0,
                    251,
                    0
                ],
                "title": "TrajAware: Graph Cross-Attention and Trajectory-Aware for Generalisable\n  VANETs under Partial Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrajAware: Graph Cross-Attention and Trajectory-Aware for Generalisable\n  VANETs under Partial Observations"
                },
                "summary": "Vehicular ad hoc networks (VANETs) are a crucial component of intelligent\ntransportation systems; however, routing remains challenging due to dynamic\ntopologies, incomplete observations, and the limited resources of edge devices.\nExisting reinforcement learning (RL) approaches often assume fixed graph\nstructures and require retraining when network conditions change, making them\nunsuitable for deployment on constrained hardware. We present TrajAware, an\nRL-based framework designed for edge AI deployment in VANETs. TrajAware\nintegrates three components: (i) action space pruning, which reduces redundant\nneighbour options while preserving two-hop reachability, alleviating the curse\nof dimensionality; (ii) graph cross-attention, which maps pruned neighbours to\nthe global graph context, producing features that generalise across diverse\nnetwork sizes; and (iii) trajectory-aware prediction, which uses historical\nroutes and junction information to estimate real-time positions under partial\nobservations. We evaluate TrajAware in the open-source SUMO simulator using\nreal-world city maps with a leave-one-city-out setup. Results show that\nTrajAware achieves near-shortest paths and high delivery ratios while\nmaintaining efficiency suitable for constrained edge devices, outperforming\nstate-of-the-art baselines in both full and partial observation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vehicular ad hoc networks (VANETs) are a crucial component of intelligent\ntransportation systems; however, routing remains challenging due to dynamic\ntopologies, incomplete observations, and the limited resources of edge devices.\nExisting reinforcement learning (RL) approaches often assume fixed graph\nstructures and require retraining when network conditions change, making them\nunsuitable for deployment on constrained hardware. We present TrajAware, an\nRL-based framework designed for edge AI deployment in VANETs. TrajAware\nintegrates three components: (i) action space pruning, which reduces redundant\nneighbour options while preserving two-hop reachability, alleviating the curse\nof dimensionality; (ii) graph cross-attention, which maps pruned neighbours to\nthe global graph context, producing features that generalise across diverse\nnetwork sizes; and (iii) trajectory-aware prediction, which uses historical\nroutes and junction information to estimate real-time positions under partial\nobservations. We evaluate TrajAware in the open-source SUMO simulator using\nreal-world city maps with a leave-one-city-out setup. Results show that\nTrajAware achieves near-shortest paths and high delivery ratios while\nmaintaining efficiency suitable for constrained edge devices, outperforming\nstate-of-the-art baselines in both full and partial observation scenarios."
                },
                "authors": [
                    {
                        "name": "Xiaolu Fu"
                    },
                    {
                        "name": "Ziyuan Bao"
                    },
                    {
                        "name": "Eiman Kanjo"
                    }
                ],
                "author_detail": {
                    "name": "Eiman Kanjo"
                },
                "author": "Eiman Kanjo",
                "arxiv_comment": "10 pages, 6 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12583v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12583v3",
                "updated": "2025-09-08T13:19:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    19,
                    38,
                    0,
                    251,
                    0
                ],
                "published": "2024-12-17T06:24:34Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    6,
                    24,
                    34,
                    1,
                    352,
                    0
                ],
                "title": "Process-Supervised Reward Models for Verifying Clinical Note Generation:\n  A Scalable Approach Guided by Domain Expertise",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process-Supervised Reward Models for Verifying Clinical Note Generation:\n  A Scalable Approach Guided by Domain Expertise"
                },
                "summary": "Process-supervised reward models (PRMs) excel at providing step-by-step\nverification for large language model (LLM) outputs in domains like mathematics\nand coding. However, their application to fields lacking ground-truth answers,\nsuch as clinical note generation, poses significant challenges. We introduce a\nnovel framework for training PRMs to deliver step-level reward signals for\nLLM-generated clinical notes. By precisely defining meaningful \"steps,\"\ninjecting realistic \"errors\" informed by domain expertise, and leveraging LLMs\nto generate process supervision data at scale, we overcome previous\nlimitations. Our PRM, built on LLaMA-3.1 8B, consistently outperforms\nproprietary reasoning and non-reasoning models, achieving state-of-the-art\nperformance on two key evaluations: (1) distinguishing gold-standard from\nerror-containing samples with 98.8% accuracy, and (2) selecting\nphysician-preferred clinical notes with 56.2% accuracy. We investigate critical\ncomponents for effective PRM training, including optimal loss functions and\ndata selection strategies, and present a comprehensive physician reader study\nidentifying predictors of downstream Best-of-N performance. Our study sheds\nlight on unlocking the potential of PRMs for diverse generative tasks across\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process-supervised reward models (PRMs) excel at providing step-by-step\nverification for large language model (LLM) outputs in domains like mathematics\nand coding. However, their application to fields lacking ground-truth answers,\nsuch as clinical note generation, poses significant challenges. We introduce a\nnovel framework for training PRMs to deliver step-level reward signals for\nLLM-generated clinical notes. By precisely defining meaningful \"steps,\"\ninjecting realistic \"errors\" informed by domain expertise, and leveraging LLMs\nto generate process supervision data at scale, we overcome previous\nlimitations. Our PRM, built on LLaMA-3.1 8B, consistently outperforms\nproprietary reasoning and non-reasoning models, achieving state-of-the-art\nperformance on two key evaluations: (1) distinguishing gold-standard from\nerror-containing samples with 98.8% accuracy, and (2) selecting\nphysician-preferred clinical notes with 56.2% accuracy. We investigate critical\ncomponents for effective PRM training, including optimal loss functions and\ndata selection strategies, and present a comprehensive physician reader study\nidentifying predictors of downstream Best-of-N performance. Our study sheds\nlight on unlocking the potential of PRMs for diverse generative tasks across\ndomains."
                },
                "authors": [
                    {
                        "name": "Hanyin Wang"
                    },
                    {
                        "name": "Chufan Gao"
                    },
                    {
                        "name": "Qiping Xu"
                    },
                    {
                        "name": "Bolun Liu"
                    },
                    {
                        "name": "Guleid Hussein"
                    },
                    {
                        "name": "Hariprasad Korsapati"
                    },
                    {
                        "name": "Mohamad El Labban"
                    },
                    {
                        "name": "Kingsley Iheasirim"
                    },
                    {
                        "name": "Mohamed Hassan"
                    },
                    {
                        "name": "Gokhan Anil"
                    },
                    {
                        "name": "Brian Bartlett"
                    },
                    {
                        "name": "Jimeng Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jimeng Sun"
                },
                "author": "Jimeng Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12583v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12583v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19576v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19576v2",
                "updated": "2025-09-08T13:12:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    12,
                    19,
                    0,
                    251,
                    0
                ],
                "published": "2025-08-27T05:16:03Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    5,
                    16,
                    3,
                    2,
                    239,
                    0
                ],
                "title": "ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized\n  Self-Training and Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized\n  Self-Training and Decoding"
                },
                "summary": "With respect to improving the reasoning accuracy of LLMs, the representative\nreinforcement learning (RL) method GRPO faces failure due to insignificant\nreward variance, while verification methods based on process reward models\n(PRMs) suffer from difficulties with training data acquisition and verification\neffectiveness. To tackle these problems, this paper introduces ReST-RL, a\nunified LLM RL paradigm that significantly improves LLM's code reasoning\nability by combining an improved GRPO algorithm with a meticulously designed\ntest time decoding method assisted by a value model (VM). As the first stage of\npolicy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter\nand assemble high-value training data, increasing the reward variance of GRPO\nsampling, thus improving the effectiveness and efficiency of training. After\nthe basic reasoning ability of LLM policy has been improved, we further propose\na test time decoding optimization method called VM-MCTS. Through Monte-Carlo\nTree Search (MCTS), we collect accurate value targets with no annotation\nrequired, on which VM training is based. When decoding, the VM is deployed by\nan adapted MCTS algorithm to provide precise process signals as well as\nverification scores, assisting the LLM policy to achieve high reasoning\naccuracy. We conduct extensive experiments on coding problems to verify the\nvalidity of the proposed RL paradigm. Upon comparison, our approach\nsignificantly outperforms other reinforcement training baselines (e.g., naive\nGRPO and ReST-DPO), as well as decoding and verification baselines (e.g.,\nPRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g.,\nAPPS, BigCodeBench, and HumanEval), indicating its power to strengthen the\nreasoning ability of LLM policies. Codes for our project can be found at\nhttps://github.com/THUDM/ReST-RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With respect to improving the reasoning accuracy of LLMs, the representative\nreinforcement learning (RL) method GRPO faces failure due to insignificant\nreward variance, while verification methods based on process reward models\n(PRMs) suffer from difficulties with training data acquisition and verification\neffectiveness. To tackle these problems, this paper introduces ReST-RL, a\nunified LLM RL paradigm that significantly improves LLM's code reasoning\nability by combining an improved GRPO algorithm with a meticulously designed\ntest time decoding method assisted by a value model (VM). As the first stage of\npolicy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter\nand assemble high-value training data, increasing the reward variance of GRPO\nsampling, thus improving the effectiveness and efficiency of training. After\nthe basic reasoning ability of LLM policy has been improved, we further propose\na test time decoding optimization method called VM-MCTS. Through Monte-Carlo\nTree Search (MCTS), we collect accurate value targets with no annotation\nrequired, on which VM training is based. When decoding, the VM is deployed by\nan adapted MCTS algorithm to provide precise process signals as well as\nverification scores, assisting the LLM policy to achieve high reasoning\naccuracy. We conduct extensive experiments on coding problems to verify the\nvalidity of the proposed RL paradigm. Upon comparison, our approach\nsignificantly outperforms other reinforcement training baselines (e.g., naive\nGRPO and ReST-DPO), as well as decoding and verification baselines (e.g.,\nPRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g.,\nAPPS, BigCodeBench, and HumanEval), indicating its power to strengthen the\nreasoning ability of LLM policies. Codes for our project can be found at\nhttps://github.com/THUDM/ReST-RL."
                },
                "authors": [
                    {
                        "name": "Sining Zhoubian"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "arxiv_comment": "21 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19576v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19576v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06652v1",
                "updated": "2025-09-08T13:07:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    7,
                    35,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T13:07:35Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    7,
                    35,
                    0,
                    251,
                    0
                ],
                "title": "IntrEx: A Dataset for Modeling Engagement in Educational Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IntrEx: A Dataset for Modeling Engagement in Educational Conversations"
                },
                "summary": "Engagement and motivation are crucial for second-language acquisition, yet\nmaintaining learner interest in educational conversations remains a challenge.\nWhile prior research has explored what makes educational texts interesting,\nstill little is known about the linguistic features that drive engagement in\nconversations. To address this gap, we introduce IntrEx, the first large\ndataset annotated for interestingness and expected interestingness in\nteacher-student interactions. Built upon the Teacher-Student Chatroom Corpus\n(TSCC), IntrEx extends prior work by incorporating sequence-level annotations,\nallowing for the study of engagement beyond isolated turns to capture how\ninterest evolves over extended dialogues. We employ a rigorous annotation\nprocess with over 100 second-language learners, using a comparison-based rating\napproach inspired by reinforcement learning from human feedback (RLHF) to\nimprove agreement. We investigate whether large language models (LLMs) can\npredict human interestingness judgments. We find that LLMs (7B/8B parameters)\nfine-tuned on interestingness ratings outperform larger proprietary models like\nGPT-4o, demonstrating the potential for specialised datasets to model\nengagement in educational settings. Finally, we analyze how linguistic and\ncognitive factors, such as concreteness, comprehensibility (readability), and\nuptake, influence engagement in educational dialogues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engagement and motivation are crucial for second-language acquisition, yet\nmaintaining learner interest in educational conversations remains a challenge.\nWhile prior research has explored what makes educational texts interesting,\nstill little is known about the linguistic features that drive engagement in\nconversations. To address this gap, we introduce IntrEx, the first large\ndataset annotated for interestingness and expected interestingness in\nteacher-student interactions. Built upon the Teacher-Student Chatroom Corpus\n(TSCC), IntrEx extends prior work by incorporating sequence-level annotations,\nallowing for the study of engagement beyond isolated turns to capture how\ninterest evolves over extended dialogues. We employ a rigorous annotation\nprocess with over 100 second-language learners, using a comparison-based rating\napproach inspired by reinforcement learning from human feedback (RLHF) to\nimprove agreement. We investigate whether large language models (LLMs) can\npredict human interestingness judgments. We find that LLMs (7B/8B parameters)\nfine-tuned on interestingness ratings outperform larger proprietary models like\nGPT-4o, demonstrating the potential for specialised datasets to model\nengagement in educational settings. Finally, we analyze how linguistic and\ncognitive factors, such as concreteness, comprehensibility (readability), and\nuptake, influence engagement in educational dialogues."
                },
                "authors": [
                    {
                        "name": "Xingwei Tan"
                    },
                    {
                        "name": "Mahathi Parvatham"
                    },
                    {
                        "name": "Chiara Gambi"
                    },
                    {
                        "name": "Gabriele Pergola"
                    }
                ],
                "author_detail": {
                    "name": "Gabriele Pergola"
                },
                "author": "Gabriele Pergola",
                "arxiv_comment": "EMNLP 2025 Findings camera-ready, 9+7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03579v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03579v2",
                "updated": "2025-09-08T12:59:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    59,
                    58,
                    0,
                    251,
                    0
                ],
                "published": "2025-04-04T16:30:44Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    30,
                    44,
                    4,
                    94,
                    0
                ],
                "title": "Hallucination Detection on a Budget: Efficient Bayesian Estimation of\n  Semantic Entropy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination Detection on a Budget: Efficient Bayesian Estimation of\n  Semantic Entropy"
                },
                "summary": "Detecting whether an LLM hallucinates is an important research challenge. One\npromising way of doing so is to estimate the semantic entropy (Farquhar et al.,\n2024) of the distribution of generated sequences. We propose a new algorithm\nfor doing that, with two main advantages. First, due to us taking the Bayesian\napproach, we achieve a much better quality of semantic entropy estimates for a\ngiven budget of samples from the LLM. Second, we are able to tune the number of\nsamples adaptively so that `harder' contexts receive more samples. We\ndemonstrate empirically that our approach systematically beats the baselines,\nrequiring only 53% of samples used by Farquhar et al. (2024) to achieve the\nsame quality of hallucination detection as measured by AUROC. Moreover, quite\ncounterintuitively, our estimator is useful even with just one sample from the\nLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting whether an LLM hallucinates is an important research challenge. One\npromising way of doing so is to estimate the semantic entropy (Farquhar et al.,\n2024) of the distribution of generated sequences. We propose a new algorithm\nfor doing that, with two main advantages. First, due to us taking the Bayesian\napproach, we achieve a much better quality of semantic entropy estimates for a\ngiven budget of samples from the LLM. Second, we are able to tune the number of\nsamples adaptively so that `harder' contexts receive more samples. We\ndemonstrate empirically that our approach systematically beats the baselines,\nrequiring only 53% of samples used by Farquhar et al. (2024) to achieve the\nsame quality of hallucination detection as measured by AUROC. Moreover, quite\ncounterintuitively, our estimator is useful even with just one sample from the\nLLM."
                },
                "authors": [
                    {
                        "name": "Kamil Ciosek"
                    },
                    {
                        "name": "Nicolò Felicioni"
                    },
                    {
                        "name": "Sina Ghiassian"
                    }
                ],
                "author_detail": {
                    "name": "Sina Ghiassian"
                },
                "author": "Sina Ghiassian",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03579v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03579v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06636v1",
                "updated": "2025-09-08T12:54:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    54,
                    30,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T12:54:30Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    54,
                    30,
                    0,
                    251,
                    0
                ],
                "title": "Full Integer Arithmetic Online Training for Spiking Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full Integer Arithmetic Online Training for Spiking Neural Networks"
                },
                "summary": "Spiking Neural Networks (SNNs) are promising for neuromorphic computing due\nto their biological plausibility and energy efficiency. However, training\nmethods like Backpropagation Through Time (BPTT) and Real Time Recurrent\nLearning (RTRL) remain computationally intensive. This work introduces an\ninteger-only, online training algorithm using a mixed-precision approach to\nimprove efficiency and reduce memory usage by over 60%. The method replaces\nfloating-point operations with integer arithmetic to enable hardware-friendly\nimplementation. It generalizes to Convolutional and Recurrent SNNs (CSNNs,\nRSNNs), showing versatility across architectures. Evaluations on MNIST and the\nSpiking Heidelberg Digits (SHD) dataset demonstrate that mixed-precision models\nachieve accuracy comparable to or better than full-precision baselines using\n16-bit shadow and 8- or 12-bit inference weights. Despite some limitations in\nlow-precision and deeper models, performance remains robust. In conclusion, the\nproposed integer-only online learning algorithm presents an effective solution\nfor efficiently training SNNs, enabling deployment on resource-constrained\nneuromorphic hardware without sacrificing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) are promising for neuromorphic computing due\nto their biological plausibility and energy efficiency. However, training\nmethods like Backpropagation Through Time (BPTT) and Real Time Recurrent\nLearning (RTRL) remain computationally intensive. This work introduces an\ninteger-only, online training algorithm using a mixed-precision approach to\nimprove efficiency and reduce memory usage by over 60%. The method replaces\nfloating-point operations with integer arithmetic to enable hardware-friendly\nimplementation. It generalizes to Convolutional and Recurrent SNNs (CSNNs,\nRSNNs), showing versatility across architectures. Evaluations on MNIST and the\nSpiking Heidelberg Digits (SHD) dataset demonstrate that mixed-precision models\nachieve accuracy comparable to or better than full-precision baselines using\n16-bit shadow and 8- or 12-bit inference weights. Despite some limitations in\nlow-precision and deeper models, performance remains robust. In conclusion, the\nproposed integer-only online learning algorithm presents an effective solution\nfor efficiently training SNNs, enabling deployment on resource-constrained\nneuromorphic hardware without sacrificing accuracy."
                },
                "authors": [
                    {
                        "name": "Ismael Gomez"
                    },
                    {
                        "name": "Guangzhi Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guangzhi Tang"
                },
                "author": "Guangzhi Tang",
                "arxiv_comment": "Accepted at ICANN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06631v1",
                "updated": "2025-09-08T12:51:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    51,
                    40,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T12:51:40Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    51,
                    40,
                    0,
                    251,
                    0
                ],
                "title": "Guided Decoding and Its Critical Role in Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guided Decoding and Its Critical Role in Retrieval-Augmented Generation"
                },
                "summary": "The integration of Large Language Models (LLMs) into various applications has\ndriven the need for structured and reliable responses. A key challenge in\nRetrieval-Augmented Generation (RAG) systems is ensuring that outputs align\nwith expected formats while minimizing hallucinations. This study examines the\nrole of guided decoding in RAG systems, comparing three methods, Outlines,\nXGrammar, and LM Format Enforcer, across different multi-turn prompting setups\n(0-turn, 1-turn, and 2-turn). By evaluating success rates, hallucination rates,\nand output quality, we provide insights into their performance and\napplicability. Our findings reveal how multi-turn interactions influence guided\ndecoding, uncovering unexpected performance variations that can inform method\nselection for specific use cases. This work advances the understanding of\nstructured output generation in RAG systems, offering both theoretical insights\nand practical guidance for LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into various applications has\ndriven the need for structured and reliable responses. A key challenge in\nRetrieval-Augmented Generation (RAG) systems is ensuring that outputs align\nwith expected formats while minimizing hallucinations. This study examines the\nrole of guided decoding in RAG systems, comparing three methods, Outlines,\nXGrammar, and LM Format Enforcer, across different multi-turn prompting setups\n(0-turn, 1-turn, and 2-turn). By evaluating success rates, hallucination rates,\nand output quality, we provide insights into their performance and\napplicability. Our findings reveal how multi-turn interactions influence guided\ndecoding, uncovering unexpected performance variations that can inform method\nselection for specific use cases. This work advances the understanding of\nstructured output generation in RAG systems, offering both theoretical insights\nand practical guidance for LLM deployment."
                },
                "authors": [
                    {
                        "name": "Özgür Uğur"
                    },
                    {
                        "name": "Musa Yılmaz"
                    },
                    {
                        "name": "Esra Şavirdi"
                    },
                    {
                        "name": "Özay Ezerceli"
                    },
                    {
                        "name": "Mahmut El Huseyni"
                    },
                    {
                        "name": "Selva Taş"
                    },
                    {
                        "name": "Reyhan Bayraktar"
                    }
                ],
                "author_detail": {
                    "name": "Reyhan Bayraktar"
                },
                "author": "Reyhan Bayraktar",
                "arxiv_doi": "10.1109/SIU66497.2025.11111950",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/SIU66497.2025.11111950",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.06631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00719v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00719v3",
                "updated": "2025-09-08T12:44:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    44,
                    14,
                    0,
                    251,
                    0
                ],
                "published": "2025-08-01T15:38:21Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    38,
                    21,
                    4,
                    213,
                    0
                ],
                "title": "Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and\n  Context-Aware KGQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and\n  Context-Aware KGQA"
                },
                "summary": "Knowledge Graph Question Answering (KGQA) aims to interpret natural language\nqueries and perform structured reasoning over knowledge graphs by leveraging\ntheir relational and semantic structures to retrieve accurate answers. Recent\nKGQA methods primarily follow either retrieve-then-reason paradigm, relying on\nGNNs or heuristic rules for static paths extraction, or dynamic path generation\nstrategies that use large language models (LLMs) with prompting to jointly\nperform retrieval and reasoning. However, the former suffers from limited\nadaptability due to static path extraction and lack of contextual refinement,\nwhile the latter incurs high computational costs and struggles with accurate\npath evaluation due to reliance on fixed scoring functions and extensive LLM\ncalls. To address these issues, this paper proposes Dynamically Adaptive\nMCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search\nwith adaptive path evaluation for efficient and context-aware KGQA. DAMR\nemploys a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based\nplanner, which selects top-$k$ relevant relations at each step to reduce search\nspace. To improve path evaluation accuracy, we introduce a lightweight\nTransformer-based scorer that performs context-aware plausibility estimation by\njointly encoding the question and relation sequence through cross-attention,\nenabling the model to capture fine-grained semantic shifts during multi-hop\nreasoning. Furthermore, to alleviate the scarcity of high-quality supervision,\nDAMR incorporates a dynamic pseudo-path refinement mechanism that periodically\ngenerates training signals from partial paths explored during search, allowing\nthe scorer to continuously adapt to the evolving distribution of reasoning\ntrajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR\nsignificantly outperforms state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph Question Answering (KGQA) aims to interpret natural language\nqueries and perform structured reasoning over knowledge graphs by leveraging\ntheir relational and semantic structures to retrieve accurate answers. Recent\nKGQA methods primarily follow either retrieve-then-reason paradigm, relying on\nGNNs or heuristic rules for static paths extraction, or dynamic path generation\nstrategies that use large language models (LLMs) with prompting to jointly\nperform retrieval and reasoning. However, the former suffers from limited\nadaptability due to static path extraction and lack of contextual refinement,\nwhile the latter incurs high computational costs and struggles with accurate\npath evaluation due to reliance on fixed scoring functions and extensive LLM\ncalls. To address these issues, this paper proposes Dynamically Adaptive\nMCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search\nwith adaptive path evaluation for efficient and context-aware KGQA. DAMR\nemploys a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based\nplanner, which selects top-$k$ relevant relations at each step to reduce search\nspace. To improve path evaluation accuracy, we introduce a lightweight\nTransformer-based scorer that performs context-aware plausibility estimation by\njointly encoding the question and relation sequence through cross-attention,\nenabling the model to capture fine-grained semantic shifts during multi-hop\nreasoning. Furthermore, to alleviate the scarcity of high-quality supervision,\nDAMR incorporates a dynamic pseudo-path refinement mechanism that periodically\ngenerates training signals from partial paths explored during search, allowing\nthe scorer to continuously adapt to the evolving distribution of reasoning\ntrajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR\nsignificantly outperforms state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yingxu Wang"
                    },
                    {
                        "name": "Shiqi Fan"
                    },
                    {
                        "name": "Mengzhu Wang"
                    },
                    {
                        "name": "Siyang Gao"
                    },
                    {
                        "name": "Siwei Liu"
                    },
                    {
                        "name": "Nan Yin"
                    }
                ],
                "author_detail": {
                    "name": "Nan Yin"
                },
                "author": "Nan Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00719v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00719v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06602v1",
                "updated": "2025-09-08T12:15:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    15,
                    53,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T12:15:53Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    15,
                    53,
                    0,
                    251,
                    0
                ],
                "title": "Demo: Healthcare Agent Orchestrator (HAO) for Patient Summarization in\n  Molecular Tumor Boards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demo: Healthcare Agent Orchestrator (HAO) for Patient Summarization in\n  Molecular Tumor Boards"
                },
                "summary": "Molecular Tumor Boards (MTBs) are multidisciplinary forums where oncology\nspecialists collaboratively assess complex patient cases to determine optimal\ntreatment strategies. A central element of this process is the patient summary,\ntypically compiled by a medical oncologist, radiation oncologist, or surgeon,\nor their trained medical assistant, who distills heterogeneous medical records\ninto a concise narrative to facilitate discussion. This manual approach is\noften labor-intensive, subjective, and prone to omissions of critical\ninformation. To address these limitations, we introduce the Healthcare Agent\nOrchestrator (HAO), a Large Language Model (LLM)-driven AI agent that\ncoordinates a multi-agent clinical workflow to generate accurate and\ncomprehensive patient summaries for MTBs. Evaluating predicted patient\nsummaries against ground truth presents additional challenges due to stylistic\nvariation, ordering, synonym usage, and phrasing differences, which complicate\nthe measurement of both succinctness and completeness. To overcome these\nevaluation hurdles, we propose TBFact, a ``model-as-a-judge'' framework\ndesigned to assess the comprehensiveness and succinctness of generated\nsummaries. Using a benchmark dataset derived from de-identified tumor board\ndiscussions, we applied TBFact to evaluate our Patient History agent. Results\nshow that the agent captured 94% of high-importance information (including\npartial entailments) and achieved a TBFact recall of 0.84 under strict\nentailment criteria. We further demonstrate that TBFact enables a data-free\nevaluation framework that institutions can deploy locally without sharing\nsensitive clinical data. Together, HAO and TBFact establish a robust foundation\nfor delivering reliable and scalable support to MTBs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular Tumor Boards (MTBs) are multidisciplinary forums where oncology\nspecialists collaboratively assess complex patient cases to determine optimal\ntreatment strategies. A central element of this process is the patient summary,\ntypically compiled by a medical oncologist, radiation oncologist, or surgeon,\nor their trained medical assistant, who distills heterogeneous medical records\ninto a concise narrative to facilitate discussion. This manual approach is\noften labor-intensive, subjective, and prone to omissions of critical\ninformation. To address these limitations, we introduce the Healthcare Agent\nOrchestrator (HAO), a Large Language Model (LLM)-driven AI agent that\ncoordinates a multi-agent clinical workflow to generate accurate and\ncomprehensive patient summaries for MTBs. Evaluating predicted patient\nsummaries against ground truth presents additional challenges due to stylistic\nvariation, ordering, synonym usage, and phrasing differences, which complicate\nthe measurement of both succinctness and completeness. To overcome these\nevaluation hurdles, we propose TBFact, a ``model-as-a-judge'' framework\ndesigned to assess the comprehensiveness and succinctness of generated\nsummaries. Using a benchmark dataset derived from de-identified tumor board\ndiscussions, we applied TBFact to evaluate our Patient History agent. Results\nshow that the agent captured 94% of high-importance information (including\npartial entailments) and achieved a TBFact recall of 0.84 under strict\nentailment criteria. We further demonstrate that TBFact enables a data-free\nevaluation framework that institutions can deploy locally without sharing\nsensitive clinical data. Together, HAO and TBFact establish a robust foundation\nfor delivering reliable and scalable support to MTBs."
                },
                "authors": [
                    {
                        "name": "Noel Codella"
                    },
                    {
                        "name": "Sam Preston"
                    },
                    {
                        "name": "Hao Qiu"
                    },
                    {
                        "name": "Leonardo Schettini"
                    },
                    {
                        "name": "Wen-wai Yim"
                    },
                    {
                        "name": "Mert Öz"
                    },
                    {
                        "name": "Shrey Jain"
                    },
                    {
                        "name": "Matthew P. Lungren"
                    },
                    {
                        "name": "Thomas Osborne"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Osborne"
                },
                "author": "Thomas Osborne",
                "arxiv_comment": "9 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06596v1",
                "updated": "2025-09-08T12:06:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    6,
                    9,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T12:06:09Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    6,
                    9,
                    0,
                    251,
                    0
                ],
                "title": "HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination\n  Mitigation in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination\n  Mitigation in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) often produce hallucinations in\nretrieval-augmented or long-context generation, even when relevant evidence is\npresent. This stems from two issues: head importance is treated as\ninput-agnostic, and raw attention weights poorly reflect each token's true\ncontribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a\nparameter-free decoding framework that directly addresses both challenges. HAVE\nintroduces head-adaptive gating, which performs instance-level soft reweighing\nof attention heads, and value calibration, which augments attention with the\nmagnitude of value vectors to approximate write-back contribution. Together,\nthese modules construct token-level evidence aligned with model updates and\nfuse it with the LM distribution through a lightweight uncertainty-scaled\npolicy. HAVE requires no finetuning and operates in a single forward pass,\nmaking it efficient and broadly applicable. Experiments across multiple QA\nbenchmarks and LLM families demonstrate that HAVE consistently reduces\nhallucinations and outperforms strong baselines, including DAGCD, with modest\noverhead. The framework is transparent, reproducible, and readily integrates\nwith off-the-shelf LLMs, advancing trustworthy generation in real-world\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often produce hallucinations in\nretrieval-augmented or long-context generation, even when relevant evidence is\npresent. This stems from two issues: head importance is treated as\ninput-agnostic, and raw attention weights poorly reflect each token's true\ncontribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a\nparameter-free decoding framework that directly addresses both challenges. HAVE\nintroduces head-adaptive gating, which performs instance-level soft reweighing\nof attention heads, and value calibration, which augments attention with the\nmagnitude of value vectors to approximate write-back contribution. Together,\nthese modules construct token-level evidence aligned with model updates and\nfuse it with the LM distribution through a lightweight uncertainty-scaled\npolicy. HAVE requires no finetuning and operates in a single forward pass,\nmaking it efficient and broadly applicable. Experiments across multiple QA\nbenchmarks and LLM families demonstrate that HAVE consistently reduces\nhallucinations and outperforms strong baselines, including DAGCD, with modest\noverhead. The framework is transparent, reproducible, and readily integrates\nwith off-the-shelf LLMs, advancing trustworthy generation in real-world\nsettings."
                },
                "authors": [
                    {
                        "name": "Xin Tong"
                    },
                    {
                        "name": "Zhi Lin"
                    },
                    {
                        "name": "Jingya Wang"
                    },
                    {
                        "name": "Bo Jin"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jin"
                },
                "author": "Bo Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06595v1",
                "updated": "2025-09-08T12:06:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    6,
                    6,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T12:06:06Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    6,
                    6,
                    0,
                    251,
                    0
                ],
                "title": "LLMs in Cybersecurity: Friend or Foe in the Human Decision Loop?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs in Cybersecurity: Friend or Foe in the Human Decision Loop?"
                },
                "summary": "Large Language Models (LLMs) are transforming human decision-making by acting\nas cognitive collaborators. Yet, this promise comes with a paradox: while LLMs\ncan improve accuracy, they may also erode independent reasoning, promote\nover-reliance and homogenize decisions. In this paper, we investigate how LLMs\nshape human judgment in security-critical contexts. Through two exploratory\nfocus groups (unaided and LLM-supported), we assess decision accuracy,\nbehavioral resilience and reliance dynamics. Our findings reveal that while\nLLMs enhance accuracy and consistency in routine decisions, they can\ninadvertently reduce cognitive diversity and improve automation bias, which is\nespecially the case among users with lower resilience. In contrast,\nhigh-resilience individuals leverage LLMs more effectively, suggesting that\ncognitive traits mediate AI benefit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are transforming human decision-making by acting\nas cognitive collaborators. Yet, this promise comes with a paradox: while LLMs\ncan improve accuracy, they may also erode independent reasoning, promote\nover-reliance and homogenize decisions. In this paper, we investigate how LLMs\nshape human judgment in security-critical contexts. Through two exploratory\nfocus groups (unaided and LLM-supported), we assess decision accuracy,\nbehavioral resilience and reliance dynamics. Our findings reveal that while\nLLMs enhance accuracy and consistency in routine decisions, they can\ninadvertently reduce cognitive diversity and improve automation bias, which is\nespecially the case among users with lower resilience. In contrast,\nhigh-resilience individuals leverage LLMs more effectively, suggesting that\ncognitive traits mediate AI benefit."
                },
                "authors": [
                    {
                        "name": "Irdin Pekaric"
                    },
                    {
                        "name": "Philipp Zech"
                    },
                    {
                        "name": "Tom Mattson"
                    }
                ],
                "author_detail": {
                    "name": "Tom Mattson"
                },
                "author": "Tom Mattson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06593v1",
                "updated": "2025-09-08T12:04:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    4,
                    12,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T12:04:12Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    4,
                    12,
                    0,
                    251,
                    0
                ],
                "title": "A Robust Approach for LiDAR-Inertial Odometry Without Sensor-Specific\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Robust Approach for LiDAR-Inertial Odometry Without Sensor-Specific\n  Modeling"
                },
                "summary": "Accurate odometry is a critical component in a robotic navigation stack, and\nsubsequent modules such as planning and control often rely on an estimate of\nthe robot's motion. Sensor-based odometry approaches should be robust across\nsensor types and deployable in different target domains, from solid-state\nLiDARs mounted on cars in urban-driving scenarios to spinning LiDARs on\nhandheld packages used in unstructured natural environments. In this paper, we\npropose a robust LiDAR-inertial odometry system that does not rely on\nsensor-specific modeling. Sensor fusion techniques for LiDAR and inertial\nmeasurement unit (IMU) data typically integrate IMU data iteratively in a\nKalman filter or use pre-integration in a factor graph framework, combined with\nLiDAR scan matching often exploiting some form of feature extraction. We\npropose an alternative strategy that only requires a simplified motion model\nfor IMU integration and directly registers LiDAR scans in a scan-to-map\napproach. Our approach allows us to impose a novel regularization on the LiDAR\nregistration, improving the overall odometry performance. We detail extensive\nexperiments on a number of datasets covering a wide array of commonly used\nrobotic sensors and platforms. We show that our approach works with the exact\nsame configuration in all these scenarios, demonstrating its robustness. We\nhave open-sourced our implementation so that the community can build further on\nour work and use it in their navigation stacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate odometry is a critical component in a robotic navigation stack, and\nsubsequent modules such as planning and control often rely on an estimate of\nthe robot's motion. Sensor-based odometry approaches should be robust across\nsensor types and deployable in different target domains, from solid-state\nLiDARs mounted on cars in urban-driving scenarios to spinning LiDARs on\nhandheld packages used in unstructured natural environments. In this paper, we\npropose a robust LiDAR-inertial odometry system that does not rely on\nsensor-specific modeling. Sensor fusion techniques for LiDAR and inertial\nmeasurement unit (IMU) data typically integrate IMU data iteratively in a\nKalman filter or use pre-integration in a factor graph framework, combined with\nLiDAR scan matching often exploiting some form of feature extraction. We\npropose an alternative strategy that only requires a simplified motion model\nfor IMU integration and directly registers LiDAR scans in a scan-to-map\napproach. Our approach allows us to impose a novel regularization on the LiDAR\nregistration, improving the overall odometry performance. We detail extensive\nexperiments on a number of datasets covering a wide array of commonly used\nrobotic sensors and platforms. We show that our approach works with the exact\nsame configuration in all these scenarios, demonstrating its robustness. We\nhave open-sourced our implementation so that the community can build further on\nour work and use it in their navigation stacks."
                },
                "authors": [
                    {
                        "name": "Meher V. R. Malladi"
                    },
                    {
                        "name": "Tiziano Guadagnino"
                    },
                    {
                        "name": "Luca Lobefaro"
                    },
                    {
                        "name": "Cyrill Stachniss"
                    }
                ],
                "author_detail": {
                    "name": "Cyrill Stachniss"
                },
                "author": "Cyrill Stachniss",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06591v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06591v1",
                "updated": "2025-09-08T12:02:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    2,
                    38,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T12:02:38Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    2,
                    38,
                    0,
                    251,
                    0
                ],
                "title": "Hybrid Swin Attention Networks for Simultaneously Low-Dose PET and CT\n  Denoising",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Swin Attention Networks for Simultaneously Low-Dose PET and CT\n  Denoising"
                },
                "summary": "Low-dose computed tomography (LDCT) and positron emission tomography (PET)\nhave emerged as safer alternatives to conventional imaging modalities by\nsignificantly reducing radiation exposure. However, this reduction often\nresults in increased noise and artifacts, which can compromise diagnostic\naccuracy. Consequently, denoising for LDCT/PET has become a vital area of\nresearch aimed at enhancing image quality while maintaining radiation safety.\nIn this study, we introduce a novel Hybrid Swin Attention Network (HSANet),\nwhich incorporates Efficient Global Attention (EGA) modules and a hybrid\nupsampling module. The EGA modules enhance both spatial and channel-wise\ninteraction, improving the network's capacity to capture relevant features,\nwhile the hybrid upsampling module mitigates the risk of overfitting to noise.\nWe validate the proposed approach using a publicly available LDCT/PET dataset.\nExperimental results demonstrate that HSANet achieves superior denoising\nperformance compared to existing methods, while maintaining a lightweight model\nsize suitable for deployment on GPUs with standard memory configurations. This\nmakes our approach highly practical for real-world clinical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-dose computed tomography (LDCT) and positron emission tomography (PET)\nhave emerged as safer alternatives to conventional imaging modalities by\nsignificantly reducing radiation exposure. However, this reduction often\nresults in increased noise and artifacts, which can compromise diagnostic\naccuracy. Consequently, denoising for LDCT/PET has become a vital area of\nresearch aimed at enhancing image quality while maintaining radiation safety.\nIn this study, we introduce a novel Hybrid Swin Attention Network (HSANet),\nwhich incorporates Efficient Global Attention (EGA) modules and a hybrid\nupsampling module. The EGA modules enhance both spatial and channel-wise\ninteraction, improving the network's capacity to capture relevant features,\nwhile the hybrid upsampling module mitigates the risk of overfitting to noise.\nWe validate the proposed approach using a publicly available LDCT/PET dataset.\nExperimental results demonstrate that HSANet achieves superior denoising\nperformance compared to existing methods, while maintaining a lightweight model\nsize suitable for deployment on GPUs with standard memory configurations. This\nmakes our approach highly practical for real-world clinical applications."
                },
                "authors": [
                    {
                        "name": "Yichao Liu"
                    },
                    {
                        "name": "YueYang Teng"
                    }
                ],
                "author_detail": {
                    "name": "YueYang Teng"
                },
                "author": "YueYang Teng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06591v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06591v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18706v2",
                "updated": "2025-09-08T11:57:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    57,
                    29,
                    0,
                    251,
                    0
                ],
                "published": "2025-05-24T13:55:38Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    13,
                    55,
                    38,
                    5,
                    144,
                    0
                ],
                "title": "Steering LLM Reasoning Through Bias-Only Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering LLM Reasoning Through Bias-Only Adaptation"
                },
                "summary": "We show that training a single $d$-dimensional steering vector per layer with\nreinforcement learning, while freezing all base weights, matches the accuracy\nof fully RL-tuned reasoning models on mathematical-reasoning tasks. On an 8\nbillion-parameter model this adds only $\\approx 0.0016\\%$ additional parameters\nand reproduces performance across a range of base models and\nmathematical-reasoning benchmarks. These results tighten the upper bound on the\nparameter budget required for high-level chain-of-thought reasoning, indicating\nthat millions of adapter weights are unnecessary. The minimal trainable\nfootprint reduces optimizer memory and inter-GPU communication, lowering the\noverall cost of fine-tuning. Moreover, a logit-lens analysis shows that the\nlearned vectors amplify coherent token directions, providing clearer insight\ninto the model's internal computations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that training a single $d$-dimensional steering vector per layer with\nreinforcement learning, while freezing all base weights, matches the accuracy\nof fully RL-tuned reasoning models on mathematical-reasoning tasks. On an 8\nbillion-parameter model this adds only $\\approx 0.0016\\%$ additional parameters\nand reproduces performance across a range of base models and\nmathematical-reasoning benchmarks. These results tighten the upper bound on the\nparameter budget required for high-level chain-of-thought reasoning, indicating\nthat millions of adapter weights are unnecessary. The minimal trainable\nfootprint reduces optimizer memory and inter-GPU communication, lowering the\noverall cost of fine-tuning. Moreover, a logit-lens analysis shows that the\nlearned vectors amplify coherent token directions, providing clearer insight\ninto the model's internal computations."
                },
                "authors": [
                    {
                        "name": "Viacheslav Sinii"
                    },
                    {
                        "name": "Alexey Gorbatovski"
                    },
                    {
                        "name": "Artem Cherepanov"
                    },
                    {
                        "name": "Boris Shaposhnikov"
                    },
                    {
                        "name": "Nikita Balagansky"
                    },
                    {
                        "name": "Daniil Gavrilov"
                    }
                ],
                "author_detail": {
                    "name": "Daniil Gavrilov"
                },
                "author": "Daniil Gavrilov",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06586v1",
                "updated": "2025-09-08T11:56:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    56,
                    46,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T11:56:46Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    56,
                    46,
                    0,
                    251,
                    0
                ],
                "title": "Simulating Dispute Mediation with LLM-Based Agents for Legal Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Dispute Mediation with LLM-Based Agents for Legal Research"
                },
                "summary": "Legal dispute mediation plays a crucial role in resolving civil disputes, yet\nits empirical study is limited by privacy constraints and complex multivariate\ninteractions. To address this limitation, we present AgentMediation, the first\nLLM-based agent framework for simulating dispute mediation. It simulates\nrealistic mediation processes grounded in real-world disputes and enables\ncontrolled experimentation on key variables such as disputant strategies,\ndispute causes, and mediator expertise. Our empirical analysis reveals patterns\nconsistent with sociological theories, including Group Polarization and\nSurface-level Consensus. As a comprehensive and extensible platform,\nAgentMediation paves the way for deeper integration of social science and AI in\nlegal research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal dispute mediation plays a crucial role in resolving civil disputes, yet\nits empirical study is limited by privacy constraints and complex multivariate\ninteractions. To address this limitation, we present AgentMediation, the first\nLLM-based agent framework for simulating dispute mediation. It simulates\nrealistic mediation processes grounded in real-world disputes and enables\ncontrolled experimentation on key variables such as disputant strategies,\ndispute causes, and mediator expertise. Our empirical analysis reveals patterns\nconsistent with sociological theories, including Group Polarization and\nSurface-level Consensus. As a comprehensive and extensible platform,\nAgentMediation paves the way for deeper integration of social science and AI in\nlegal research."
                },
                "authors": [
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Haitao Li"
                    },
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Yanxue Ren"
                    },
                    {
                        "name": "Wuyue Wang"
                    },
                    {
                        "name": "Yiqun Liu"
                    },
                    {
                        "name": "Yueyue Wu"
                    },
                    {
                        "name": "Qingyao Ai"
                    }
                ],
                "author_detail": {
                    "name": "Qingyao Ai"
                },
                "author": "Qingyao Ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04537v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04537v2",
                "updated": "2025-09-08T11:56:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    56,
                    1,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-04T08:09:42Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    9,
                    42,
                    3,
                    247,
                    0
                ],
                "title": "Emergent Social Dynamics of LLM Agents in the El Farol Bar Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent Social Dynamics of LLM Agents in the El Farol Bar Problem"
                },
                "summary": "We investigate the emergent social dynamics of Large Language Model (LLM)\nagents in a spatially extended El Farol Bar problem, observing how they\nautonomously navigate this classic social dilemma. As a result, the LLM agents\ngenerated a spontaneous motivation to go to the bar and changed their decision\nmaking by becoming a collective. We also observed that the LLM agents did not\nsolve the problem completely, but rather behaved more like humans. These\nfindings reveal a complex interplay between external incentives\n(prompt-specified constraints such as the 60% threshold) and internal\nincentives (culturally-encoded social preferences derived from pre-training),\ndemonstrating that LLM agents naturally balance formal game-theoretic\nrationality with social motivations that characterize human behavior. These\nfindings suggest that a new model of group decision making, which could not be\nhandled in the previous game-theoretic problem setting, can be realized by LLM\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the emergent social dynamics of Large Language Model (LLM)\nagents in a spatially extended El Farol Bar problem, observing how they\nautonomously navigate this classic social dilemma. As a result, the LLM agents\ngenerated a spontaneous motivation to go to the bar and changed their decision\nmaking by becoming a collective. We also observed that the LLM agents did not\nsolve the problem completely, but rather behaved more like humans. These\nfindings reveal a complex interplay between external incentives\n(prompt-specified constraints such as the 60% threshold) and internal\nincentives (culturally-encoded social preferences derived from pre-training),\ndemonstrating that LLM agents naturally balance formal game-theoretic\nrationality with social motivations that characterize human behavior. These\nfindings suggest that a new model of group decision making, which could not be\nhandled in the previous game-theoretic problem setting, can be realized by LLM\nagents."
                },
                "authors": [
                    {
                        "name": "Ryosuke Takata"
                    },
                    {
                        "name": "Atsushi Masumori"
                    },
                    {
                        "name": "Takashi Ikegami"
                    }
                ],
                "author_detail": {
                    "name": "Takashi Ikegami"
                },
                "author": "Takashi Ikegami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04537v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04537v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06572v1",
                "updated": "2025-09-08T11:35:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    35,
                    32,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T11:35:32Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    35,
                    32,
                    0,
                    251,
                    0
                ],
                "title": "Mind Your Server: A Systematic Study of Parasitic Toolchain Attacks on\n  the MCP Ecosystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind Your Server: A Systematic Study of Parasitic Toolchain Attacks on\n  the MCP Ecosystem"
                },
                "summary": "Large language models (LLMs) are increasingly integrated with external\nsystems through the Model Context Protocol (MCP), which standardizes tool\ninvocation and has rapidly become a backbone for LLM-powered applications.\nWhile this paradigm enhances functionality, it also introduces a fundamental\nsecurity shift: LLMs transition from passive information processors to\nautonomous orchestrators of task-oriented toolchains, expanding the attack\nsurface, elevating adversarial goals from manipulating single outputs to\nhijacking entire execution flows. In this paper, we reveal a new class of\nattacks, Parasitic Toolchain Attacks, instantiated as MCP Unintended Privacy\nDisclosure (MCP-UPD). These attacks require no direct victim interaction;\ninstead, adversaries embed malicious instructions into external data sources\nthat LLMs access during legitimate tasks. The malicious logic infiltrates the\ntoolchain and unfolds in three phases: Parasitic Ingestion, Privacy Collection,\nand Privacy Disclosure, culminating in stealthy exfiltration of private data.\nOur root cause analysis reveals that MCP lacks both context-tool isolation and\nleast-privilege enforcement, enabling adversarial instructions to propagate\nunchecked into sensitive tool invocations. To assess the severity, we design\nMCP-SEC and conduct the first large-scale security census of the MCP ecosystem,\nanalyzing 12,230 tools across 1,360 servers. Our findings show that the MCP\necosystem is rife with exploitable gadgets and diverse attack methods,\nunderscoring systemic risks in MCP platforms and the urgent need for defense\nmechanisms in LLM-integrated environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly integrated with external\nsystems through the Model Context Protocol (MCP), which standardizes tool\ninvocation and has rapidly become a backbone for LLM-powered applications.\nWhile this paradigm enhances functionality, it also introduces a fundamental\nsecurity shift: LLMs transition from passive information processors to\nautonomous orchestrators of task-oriented toolchains, expanding the attack\nsurface, elevating adversarial goals from manipulating single outputs to\nhijacking entire execution flows. In this paper, we reveal a new class of\nattacks, Parasitic Toolchain Attacks, instantiated as MCP Unintended Privacy\nDisclosure (MCP-UPD). These attacks require no direct victim interaction;\ninstead, adversaries embed malicious instructions into external data sources\nthat LLMs access during legitimate tasks. The malicious logic infiltrates the\ntoolchain and unfolds in three phases: Parasitic Ingestion, Privacy Collection,\nand Privacy Disclosure, culminating in stealthy exfiltration of private data.\nOur root cause analysis reveals that MCP lacks both context-tool isolation and\nleast-privilege enforcement, enabling adversarial instructions to propagate\nunchecked into sensitive tool invocations. To assess the severity, we design\nMCP-SEC and conduct the first large-scale security census of the MCP ecosystem,\nanalyzing 12,230 tools across 1,360 servers. Our findings show that the MCP\necosystem is rife with exploitable gadgets and diverse attack methods,\nunderscoring systemic risks in MCP platforms and the urgent need for defense\nmechanisms in LLM-integrated environments."
                },
                "authors": [
                    {
                        "name": "Shuli Zhao"
                    },
                    {
                        "name": "Qinsheng Hou"
                    },
                    {
                        "name": "Zihan Zhan"
                    },
                    {
                        "name": "Yanhao Wang"
                    },
                    {
                        "name": "Yuchong Xie"
                    },
                    {
                        "name": "Yu Guo"
                    },
                    {
                        "name": "Libo Chen"
                    },
                    {
                        "name": "Shenghong Li"
                    },
                    {
                        "name": "Zhi Xue"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Xue"
                },
                "author": "Zhi Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06553v1",
                "updated": "2025-09-08T11:07:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    7,
                    47,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T11:07:47Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    7,
                    47,
                    0,
                    251,
                    0
                ],
                "title": "Impact of Labeling Inaccuracy and Image Noise on Tooth Segmentation in\n  Panoramic Radiographs using Federated, Centralized and Local Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of Labeling Inaccuracy and Image Noise on Tooth Segmentation in\n  Panoramic Radiographs using Federated, Centralized and Local Learning"
                },
                "summary": "Objectives: Federated learning (FL) may mitigate privacy constraints,\nheterogeneous data quality, and inconsistent labeling in dental diagnostic AI.\nWe compared FL with centralized (CL) and local learning (LL) for tooth\nsegmentation in panoramic radiographs across multiple data corruption\nscenarios. Methods: An Attention U-Net was trained on 2066 radiographs from six\ninstitutions across four settings: baseline (unaltered data); label\nmanipulation (dilated/missing annotations); image-quality manipulation\n(additive Gaussian noise); and exclusion of a faulty client with corrupted\ndata. FL was implemented via the Flower AI framework. Per-client training- and\nvalidation-loss trajectories were monitored for anomaly detection and a set of\nmetrics (Dice, IoU, HD, HD95 and ASSD) was evaluated on a hold-out test set.\nFrom these metrics significance results were reported through Wilcoxon\nsigned-rank test. CL and LL served as comparators. Results: Baseline: FL\nachieved a median Dice of 0.94889 (ASSD: 1.33229), slightly better than CL at\n0.94706 (ASSD: 1.37074) and LL at 0.93557-0.94026 (ASSD: 1.51910-1.69777).\nLabel manipulation: FL maintained the best median Dice score at 0.94884 (ASSD:\n1.46487) versus CL's 0.94183 (ASSD: 1.75738) and LL's 0.93003-0.94026 (ASSD:\n1.51910-2.11462). Image noise: FL led with Dice at 0.94853 (ASSD: 1.31088); CL\nscored 0.94787 (ASSD: 1.36131); LL ranged from 0.93179-0.94026 (ASSD:\n1.51910-1.77350). Faulty-client exclusion: FL reached Dice at 0.94790 (ASSD:\n1.33113) better than CL's 0.94550 (ASSD: 1.39318). Loss-curve monitoring\nreliably flagged the corrupted site. Conclusions: FL matches or exceeds CL and\noutperforms LL across corruption scenarios while preserving privacy. Per-client\nloss trajectories provide an effective anomaly-detection mechanism and support\nFL as a practical, privacy-preserving approach for scalable clinical AI\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objectives: Federated learning (FL) may mitigate privacy constraints,\nheterogeneous data quality, and inconsistent labeling in dental diagnostic AI.\nWe compared FL with centralized (CL) and local learning (LL) for tooth\nsegmentation in panoramic radiographs across multiple data corruption\nscenarios. Methods: An Attention U-Net was trained on 2066 radiographs from six\ninstitutions across four settings: baseline (unaltered data); label\nmanipulation (dilated/missing annotations); image-quality manipulation\n(additive Gaussian noise); and exclusion of a faulty client with corrupted\ndata. FL was implemented via the Flower AI framework. Per-client training- and\nvalidation-loss trajectories were monitored for anomaly detection and a set of\nmetrics (Dice, IoU, HD, HD95 and ASSD) was evaluated on a hold-out test set.\nFrom these metrics significance results were reported through Wilcoxon\nsigned-rank test. CL and LL served as comparators. Results: Baseline: FL\nachieved a median Dice of 0.94889 (ASSD: 1.33229), slightly better than CL at\n0.94706 (ASSD: 1.37074) and LL at 0.93557-0.94026 (ASSD: 1.51910-1.69777).\nLabel manipulation: FL maintained the best median Dice score at 0.94884 (ASSD:\n1.46487) versus CL's 0.94183 (ASSD: 1.75738) and LL's 0.93003-0.94026 (ASSD:\n1.51910-2.11462). Image noise: FL led with Dice at 0.94853 (ASSD: 1.31088); CL\nscored 0.94787 (ASSD: 1.36131); LL ranged from 0.93179-0.94026 (ASSD:\n1.51910-1.77350). Faulty-client exclusion: FL reached Dice at 0.94790 (ASSD:\n1.33113) better than CL's 0.94550 (ASSD: 1.39318). Loss-curve monitoring\nreliably flagged the corrupted site. Conclusions: FL matches or exceeds CL and\noutperforms LL across corruption scenarios while preserving privacy. Per-client\nloss trajectories provide an effective anomaly-detection mechanism and support\nFL as a practical, privacy-preserving approach for scalable clinical AI\ndeployment."
                },
                "authors": [
                    {
                        "name": "Johan Andreas Balle Rubak"
                    },
                    {
                        "name": "Khuram Naveed"
                    },
                    {
                        "name": "Sanyam Jain"
                    },
                    {
                        "name": "Lukas Esterle"
                    },
                    {
                        "name": "Alexandros Iosifidis"
                    },
                    {
                        "name": "Ruben Pauwels"
                    }
                ],
                "author_detail": {
                    "name": "Ruben Pauwels"
                },
                "author": "Ruben Pauwels",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06552v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06552v1",
                "updated": "2025-09-08T11:06:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    6,
                    50,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T11:06:50Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    6,
                    50,
                    0,
                    251,
                    0
                ],
                "title": "Tackling Device Data Distribution Real-time Shift via Prototype-based\n  Parameter Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tackling Device Data Distribution Real-time Shift via Prototype-based\n  Parameter Editing"
                },
                "summary": "The on-device real-time data distribution shift on devices challenges the\ngeneralization of lightweight on-device models. This critical issue is often\noverlooked in current research, which predominantly relies on data-intensive\nand computationally expensive fine-tuning approaches. To tackle this, we\nintroduce Persona, a novel personalized method using a prototype-based,\nbackpropagation-free parameter editing framework to enhance model\ngeneralization without post-deployment retraining. Persona employs a neural\nadapter in the cloud to generate a parameter editing matrix based on real-time\ndevice data. This matrix adeptly adapts on-device models to the prevailing data\ndistributions, efficiently clustering them into prototype models. The\nprototypes are dynamically refined via the parameter editing matrix,\nfacilitating efficient evolution. Furthermore, the integration of cross-layer\nknowledge transfer ensures consistent and context-aware multi-layer parameter\nchanges and prototype assignment. Extensive experiments on vision task and\nrecommendation task on multiple datasets confirm Persona's effectiveness and\ngenerality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The on-device real-time data distribution shift on devices challenges the\ngeneralization of lightweight on-device models. This critical issue is often\noverlooked in current research, which predominantly relies on data-intensive\nand computationally expensive fine-tuning approaches. To tackle this, we\nintroduce Persona, a novel personalized method using a prototype-based,\nbackpropagation-free parameter editing framework to enhance model\ngeneralization without post-deployment retraining. Persona employs a neural\nadapter in the cloud to generate a parameter editing matrix based on real-time\ndevice data. This matrix adeptly adapts on-device models to the prevailing data\ndistributions, efficiently clustering them into prototype models. The\nprototypes are dynamically refined via the parameter editing matrix,\nfacilitating efficient evolution. Furthermore, the integration of cross-layer\nknowledge transfer ensures consistent and context-aware multi-layer parameter\nchanges and prototype assignment. Extensive experiments on vision task and\nrecommendation task on multiple datasets confirm Persona's effectiveness and\ngenerality."
                },
                "authors": [
                    {
                        "name": "Zheqi Lv"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Kairui Fu"
                    },
                    {
                        "name": "Qi Tian"
                    },
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Jiajie Su"
                    },
                    {
                        "name": "Jingyuan Chen"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "arxiv_comment": "Published on MM'25: Proceedings of the 33rd ACM International\n  Conference on Multimedia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06552v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06552v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03646v2",
                "updated": "2025-09-08T11:03:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    3,
                    11,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-03T18:52:49Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    18,
                    52,
                    49,
                    2,
                    246,
                    0
                ],
                "title": "Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning"
                },
                "summary": "Reinforcement Learning (RL) has proven highly effective at enhancing the\ncomplex reasoning abilities of Large Language Models (LLMs), yet underlying\nmechanisms driving this success remain largely opaque. Our analysis reveals\nthat puzzling phenomena like ``aha moments\", ``length-scaling'' and entropy\ndynamics are not disparate occurrences but hallmarks of an emergent reasoning\nhierarchy, akin to the separation of high-level strategic planning from\nlow-level procedural execution in human cognition. We uncover a compelling\ntwo-phase dynamic: initially, a model is constrained by procedural correctness\nand must improve its low-level skills. The learning bottleneck then decisively\nshifts, with performance gains being driven by the exploration and mastery of\nhigh-level strategic planning. This insight exposes a core inefficiency in\nprevailing RL algorithms like GRPO, which apply optimization pressure\nagnostically and dilute the learning signal across all tokens. To address this,\nwe propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that\nconcentrates optimization efforts on high-impact planning tokens. HICRA\nsignificantly outperforms strong baselines, demonstrating that focusing on this\nstrategic bottleneck is key to unlocking advanced reasoning. Furthermore, we\nvalidate semantic entropy as a superior compass for measuring strategic\nexploration over misleading metrics such as token-level entropy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has proven highly effective at enhancing the\ncomplex reasoning abilities of Large Language Models (LLMs), yet underlying\nmechanisms driving this success remain largely opaque. Our analysis reveals\nthat puzzling phenomena like ``aha moments\", ``length-scaling'' and entropy\ndynamics are not disparate occurrences but hallmarks of an emergent reasoning\nhierarchy, akin to the separation of high-level strategic planning from\nlow-level procedural execution in human cognition. We uncover a compelling\ntwo-phase dynamic: initially, a model is constrained by procedural correctness\nand must improve its low-level skills. The learning bottleneck then decisively\nshifts, with performance gains being driven by the exploration and mastery of\nhigh-level strategic planning. This insight exposes a core inefficiency in\nprevailing RL algorithms like GRPO, which apply optimization pressure\nagnostically and dilute the learning signal across all tokens. To address this,\nwe propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that\nconcentrates optimization efforts on high-impact planning tokens. HICRA\nsignificantly outperforms strong baselines, demonstrating that focusing on this\nstrategic bottleneck is key to unlocking advanced reasoning. Furthermore, we\nvalidate semantic entropy as a superior compass for measuring strategic\nexploration over misleading metrics such as token-level entropy."
                },
                "authors": [
                    {
                        "name": "Haozhe Wang"
                    },
                    {
                        "name": "Qixin Xu"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Junhong Wu"
                    },
                    {
                        "name": "Fangzhen Lin"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06544v1",
                "updated": "2025-09-08T10:58:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    10,
                    58,
                    42,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T10:58:42Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    10,
                    58,
                    42,
                    0,
                    251,
                    0
                ],
                "title": "Reasoning-enhanced Query Understanding through Decomposition and\n  Interpretation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-enhanced Query Understanding through Decomposition and\n  Interpretation"
                },
                "summary": "Accurate inference of user intent is crucial for enhancing document retrieval\nin modern search engines. While large language models (LLMs) have made\nsignificant strides in this area, their effectiveness has predominantly been\nassessed with short, keyword-based queries. As AI-driven search evolves,\nlong-form queries with intricate intents are becoming more prevalent, yet they\nremain underexplored in the context of LLM-based query understanding (QU). To\nbridge this gap, we introduce ReDI: a Reasoning-enhanced approach for query\nunderstanding through Decomposition and Interpretation. ReDI leverages the\nreasoning and comprehension capabilities of LLMs in a three-stage pipeline: (i)\nit breaks down complex queries into targeted sub-queries to accurately capture\nuser intent; (ii) it enriches each sub-query with detailed semantic\ninterpretations to improve the query-document matching; and (iii) it\nindependently retrieves documents for each sub-query and employs a fusion\nstrategy to aggregate the results for the final ranking. We compiled a\nlarge-scale dataset of real-world complex queries from a major search engine\nand distilled the query understanding capabilities of teacher models into\nsmaller models for practical application. Experiments on BRIGHT and BEIR\ndemonstrate that ReDI consistently surpasses strong baselines in both sparse\nand dense retrieval paradigms, affirming its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate inference of user intent is crucial for enhancing document retrieval\nin modern search engines. While large language models (LLMs) have made\nsignificant strides in this area, their effectiveness has predominantly been\nassessed with short, keyword-based queries. As AI-driven search evolves,\nlong-form queries with intricate intents are becoming more prevalent, yet they\nremain underexplored in the context of LLM-based query understanding (QU). To\nbridge this gap, we introduce ReDI: a Reasoning-enhanced approach for query\nunderstanding through Decomposition and Interpretation. ReDI leverages the\nreasoning and comprehension capabilities of LLMs in a three-stage pipeline: (i)\nit breaks down complex queries into targeted sub-queries to accurately capture\nuser intent; (ii) it enriches each sub-query with detailed semantic\ninterpretations to improve the query-document matching; and (iii) it\nindependently retrieves documents for each sub-query and employs a fusion\nstrategy to aggregate the results for the final ranking. We compiled a\nlarge-scale dataset of real-world complex queries from a major search engine\nand distilled the query understanding capabilities of teacher models into\nsmaller models for practical application. Experiments on BRIGHT and BEIR\ndemonstrate that ReDI consistently surpasses strong baselines in both sparse\nand dense retrieval paradigms, affirming its effectiveness."
                },
                "authors": [
                    {
                        "name": "Yunfei Zhong"
                    },
                    {
                        "name": "Jun Yang"
                    },
                    {
                        "name": "Yixing Fan"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Lixin Su"
                    },
                    {
                        "name": "Maarten de Rijke"
                    },
                    {
                        "name": "Ruqing Zhang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12172v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12172v2",
                "updated": "2025-09-08T10:55:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    10,
                    55,
                    32,
                    0,
                    251,
                    0
                ],
                "published": "2024-06-18T00:44:58Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    0,
                    44,
                    58,
                    1,
                    170,
                    0
                ],
                "title": "Navigating the Labyrinth: Evaluating LLMs' Ability to Reason About\n  Search Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating the Labyrinth: Evaluating LLMs' Ability to Reason About\n  Search Problems"
                },
                "summary": "Large Language Models (LLMs) have recently achieved impressive performance in\nmath and reasoning benchmarks. However, they often struggle with logic problems\nand puzzles that are relatively easy for humans. To further investigate this,\nwe introduce a new benchmark, SearchBench, which contains 11 unique search\nproblems, each equipped with automated pipelines to generate an arbitrary\nnumber of instances and analyze the feasibility, correctness, and optimality of\nLLM-generated solutions. We show that using language-only reasoning, even the\nmost advanced LLMs fail to solve SearchBench end-to-end, e.g., OpenAI's\nfrontier models GPT4 and o1-preview solve only 1.4% and 18.6% of SearchBench\nproblems, respectively. The reason is that SearchBench problems require\nconsidering multiple pathways to the solution and performing backtracking,\nposing a significant challenge to auto-regressive models. Instructing LLMs to\ngenerate code that solves the problem helps, but only slightly, e.g., GPT4's\nperformance rises to 11.7%. Interestingly, we show that the current strongest\nbaseline on SearchBench is obtained using in-context learning with A* algorithm\nimplementations. We further show that this baseline can be further enhanced via\na Multi-Stage-Multi-Try inference method, raising GPT4's performance above 57%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently achieved impressive performance in\nmath and reasoning benchmarks. However, they often struggle with logic problems\nand puzzles that are relatively easy for humans. To further investigate this,\nwe introduce a new benchmark, SearchBench, which contains 11 unique search\nproblems, each equipped with automated pipelines to generate an arbitrary\nnumber of instances and analyze the feasibility, correctness, and optimality of\nLLM-generated solutions. We show that using language-only reasoning, even the\nmost advanced LLMs fail to solve SearchBench end-to-end, e.g., OpenAI's\nfrontier models GPT4 and o1-preview solve only 1.4% and 18.6% of SearchBench\nproblems, respectively. The reason is that SearchBench problems require\nconsidering multiple pathways to the solution and performing backtracking,\nposing a significant challenge to auto-regressive models. Instructing LLMs to\ngenerate code that solves the problem helps, but only slightly, e.g., GPT4's\nperformance rises to 11.7%. Interestingly, we show that the current strongest\nbaseline on SearchBench is obtained using in-context learning with A* algorithm\nimplementations. We further show that this baseline can be further enhanced via\na Multi-Stage-Multi-Try inference method, raising GPT4's performance above 57%."
                },
                "authors": [
                    {
                        "name": "Nasim Borazjanizadeh"
                    },
                    {
                        "name": "Roei Herzig"
                    },
                    {
                        "name": "Trevor Darrell"
                    },
                    {
                        "name": "Rogerio Feris"
                    },
                    {
                        "name": "Leonid Karlinsky"
                    }
                ],
                "author_detail": {
                    "name": "Leonid Karlinsky"
                },
                "author": "Leonid Karlinsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12172v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12172v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08994v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08994v2",
                "updated": "2025-09-08T10:55:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    10,
                    55,
                    2,
                    0,
                    251,
                    0
                ],
                "published": "2025-03-12T02:07:08Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    2,
                    7,
                    8,
                    2,
                    71,
                    0
                ],
                "title": "DistJoin: A Decoupled Join Cardinality Estimator based on Adaptive\n  Neural Predicate Modulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DistJoin: A Decoupled Join Cardinality Estimator based on Adaptive\n  Neural Predicate Modulation"
                },
                "summary": "Research on learned cardinality estimation has made significant progress in\nrecent years. However, existing methods still face distinct challenges that\nhinder their practical deployment in production environments. We define these\nchallenges as the ``Trilemma of Cardinality Estimation'', where learned\ncardinality estimation methods struggle to balance generality, accuracy, and\nupdatability. To address these challenges, we introduce DistJoin, a join\ncardinality estimator based on efficient distribution prediction using\nmulti-autoregressive models. Our contributions are threefold: (1) We propose a\nmethod to estimate join cardinality by leveraging the probability distributions\nof individual tables in a decoupled manner. (2) To meet the requirements of\nefficiency for DistJoin, we develop Adaptive Neural Predicate Modulation\n(ANPM), a high-throughput distribution estimation model. (3) We demonstrate\nthat an existing similar approach suffers from variance accumulation issues by\nformal variance analysis. To mitigate this problem, DistJoin employs a\nselectivity-based approach to infer join cardinality, effectively reducing\nvariance. In summary, DistJoin not only represents the first data-driven method\nto support both equi and non-equi joins simultaneously but also demonstrates\nsuperior accuracy while enabling fast and flexible updates. The experimental\nresults demonstrate that DistJoin achieves the highest accuracy, robustness to\ndata updates, generality, and comparable update and inference speed relative to\nexisting methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on learned cardinality estimation has made significant progress in\nrecent years. However, existing methods still face distinct challenges that\nhinder their practical deployment in production environments. We define these\nchallenges as the ``Trilemma of Cardinality Estimation'', where learned\ncardinality estimation methods struggle to balance generality, accuracy, and\nupdatability. To address these challenges, we introduce DistJoin, a join\ncardinality estimator based on efficient distribution prediction using\nmulti-autoregressive models. Our contributions are threefold: (1) We propose a\nmethod to estimate join cardinality by leveraging the probability distributions\nof individual tables in a decoupled manner. (2) To meet the requirements of\nefficiency for DistJoin, we develop Adaptive Neural Predicate Modulation\n(ANPM), a high-throughput distribution estimation model. (3) We demonstrate\nthat an existing similar approach suffers from variance accumulation issues by\nformal variance analysis. To mitigate this problem, DistJoin employs a\nselectivity-based approach to infer join cardinality, effectively reducing\nvariance. In summary, DistJoin not only represents the first data-driven method\nto support both equi and non-equi joins simultaneously but also demonstrates\nsuperior accuracy while enabling fast and flexible updates. The experimental\nresults demonstrate that DistJoin achieves the highest accuracy, robustness to\ndata updates, generality, and comparable update and inference speed relative to\nexisting methods."
                },
                "authors": [
                    {
                        "name": "Kaixin Zhang"
                    },
                    {
                        "name": "Hongzhi Wang"
                    },
                    {
                        "name": "Ziqi Li"
                    },
                    {
                        "name": "Yabin Lu"
                    },
                    {
                        "name": "Yingze Li"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Yiming Guan"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Guan"
                },
                "author": "Yiming Guan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08994v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08994v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11041v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11041v4",
                "updated": "2025-09-08T10:41:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    10,
                    41,
                    34,
                    0,
                    251,
                    0
                ],
                "published": "2024-09-17T10:04:50Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    4,
                    50,
                    1,
                    261,
                    0
                ],
                "title": "Towards No-Code Programming of Cobots: Experiments with Code Synthesis\n  by Large Code Models for Conversational Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards No-Code Programming of Cobots: Experiments with Code Synthesis\n  by Large Code Models for Conversational Programming"
                },
                "summary": "While there has been a lot of research recently on robots in household\nenvironments, at the present time, most robots in existence can be found on\nshop floors, and most interactions between humans and robots happen there.\n``Collaborative robots'' (cobots) designed to work alongside humans on assembly\nlines traditionally require expert programming, limiting ability to make\nchanges, or manual guidance, limiting expressivity of the resulting programs.\nTo address these limitations, we explore using Large Language Models (LLMs),\nand in particular, their abilities of doing in-context learning, for\nconversational code generation. As a first step, we define RATS, the\n``Repetitive Assembly Task'', a 2D building task designed to lay the foundation\nfor simulating industry assembly scenarios. In this task, a `programmer'\ninstructs a cobot, using natural language, on how a certain assembly is to be\nbuilt; that is, the programmer induces a program, through natural language. We\ncreate a dataset that pairs target structures with various example instructions\n(human-authored, template-based, and model-generated) and example code. With\nthis, we systematically evaluate the capabilities of state-of-the-art LLMs for\nsynthesising this kind of code, given in-context examples. Evaluating in a\nsimulated environment, we find that LLMs are capable of generating accurate\n`first order code' (instruction sequences), but have problems producing\n`higher-order code' (abstractions such as functions, or use of loops).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While there has been a lot of research recently on robots in household\nenvironments, at the present time, most robots in existence can be found on\nshop floors, and most interactions between humans and robots happen there.\n``Collaborative robots'' (cobots) designed to work alongside humans on assembly\nlines traditionally require expert programming, limiting ability to make\nchanges, or manual guidance, limiting expressivity of the resulting programs.\nTo address these limitations, we explore using Large Language Models (LLMs),\nand in particular, their abilities of doing in-context learning, for\nconversational code generation. As a first step, we define RATS, the\n``Repetitive Assembly Task'', a 2D building task designed to lay the foundation\nfor simulating industry assembly scenarios. In this task, a `programmer'\ninstructs a cobot, using natural language, on how a certain assembly is to be\nbuilt; that is, the programmer induces a program, through natural language. We\ncreate a dataset that pairs target structures with various example instructions\n(human-authored, template-based, and model-generated) and example code. With\nthis, we systematically evaluate the capabilities of state-of-the-art LLMs for\nsynthesising this kind of code, given in-context examples. Evaluating in a\nsimulated environment, we find that LLMs are capable of generating accurate\n`first order code' (instruction sequences), but have problems producing\n`higher-order code' (abstractions such as functions, or use of loops)."
                },
                "authors": [
                    {
                        "name": "Chalamalasetti Kranti"
                    },
                    {
                        "name": "Sherzod Hakimov"
                    },
                    {
                        "name": "David Schlangen"
                    }
                ],
                "author_detail": {
                    "name": "David Schlangen"
                },
                "author": "David Schlangen",
                "arxiv_comment": "Accepted to ITL4HRI workshop at RO-MAN 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11041v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11041v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06531v1",
                "updated": "2025-09-08T10:36:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    10,
                    36,
                    49,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T10:36:49Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    10,
                    36,
                    49,
                    0,
                    251,
                    0
                ],
                "title": "SLiNT: Structure-aware Language Model with Injection and Contrastive\n  Training for Knowledge Graph Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLiNT: Structure-aware Language Model with Injection and Contrastive\n  Training for Knowledge Graph Completion"
                },
                "summary": "Link prediction in knowledge graphs requires integrating structural\ninformation and semantic context to infer missing entities. While large\nlanguage models offer strong generative reasoning capabilities, their limited\nexploitation of structural signals often results in structural sparsity and\nsemantic ambiguity, especially under incomplete or zero-shot settings. To\naddress these challenges, we propose SLiNT (Structure-aware Language model with\nInjection and coNtrastive Training), a modular framework that injects\nknowledge-graph-derived structural context into a frozen LLM backbone with\nlightweight LoRA-based adaptation for robust link prediction. Specifically,\nStructure-Guided Neighborhood Enhancement (SGNE) retrieves pseudo-neighbors to\nenrich sparse entities and mitigate missing context; Dynamic Hard Contrastive\nLearning (DHCL) introduces fine-grained supervision by interpolating hard\npositives and negatives to resolve entity-level ambiguity; and\nGradient-Decoupled Dual Injection (GDDI) performs token-level structure-aware\nintervention while preserving the core LLM parameters. Experiments on WN18RR\nand FB15k-237 show that SLiNT achieves superior or competitive performance\ncompared with both embedding-based and generation-based baselines,\ndemonstrating the effectiveness of structure-aware representation learning for\nscalable knowledge graph completion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Link prediction in knowledge graphs requires integrating structural\ninformation and semantic context to infer missing entities. While large\nlanguage models offer strong generative reasoning capabilities, their limited\nexploitation of structural signals often results in structural sparsity and\nsemantic ambiguity, especially under incomplete or zero-shot settings. To\naddress these challenges, we propose SLiNT (Structure-aware Language model with\nInjection and coNtrastive Training), a modular framework that injects\nknowledge-graph-derived structural context into a frozen LLM backbone with\nlightweight LoRA-based adaptation for robust link prediction. Specifically,\nStructure-Guided Neighborhood Enhancement (SGNE) retrieves pseudo-neighbors to\nenrich sparse entities and mitigate missing context; Dynamic Hard Contrastive\nLearning (DHCL) introduces fine-grained supervision by interpolating hard\npositives and negatives to resolve entity-level ambiguity; and\nGradient-Decoupled Dual Injection (GDDI) performs token-level structure-aware\nintervention while preserving the core LLM parameters. Experiments on WN18RR\nand FB15k-237 show that SLiNT achieves superior or competitive performance\ncompared with both embedding-based and generation-based baselines,\ndemonstrating the effectiveness of structure-aware representation learning for\nscalable knowledge graph completion."
                },
                "authors": [
                    {
                        "name": "Mengxue Yang"
                    },
                    {
                        "name": "Chun Yang"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Jiafan Li"
                    },
                    {
                        "name": "Jingqi Zhang"
                    },
                    {
                        "name": "Yuyang Li"
                    },
                    {
                        "name": "Ying Li"
                    }
                ],
                "author_detail": {
                    "name": "Ying Li"
                },
                "author": "Ying Li",
                "arxiv_comment": "Accepted by EMNLP Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06524v1",
                "updated": "2025-09-08T10:30:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    10,
                    30,
                    58,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T10:30:58Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    10,
                    30,
                    58,
                    0,
                    251,
                    0
                ],
                "title": "LAMDAS: LLM as an Implicit Classifier for Domain-specific Data Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAMDAS: LLM as an Implicit Classifier for Domain-specific Data Selection"
                },
                "summary": "Adapting large language models (LLMs) to specific domains often faces a\ncritical bottleneck: the scarcity of high-quality, human-curated data. While\nlarge volumes of unchecked data are readily available, indiscriminately using\nthem for fine-tuning risks introducing noise and degrading performance.\nStrategic data selection is thus crucial, requiring a method that is both\naccurate and efficient. Existing approaches, categorized as similarity-based\nand direct optimization methods, struggle to simultaneously achieve these\ngoals. In this paper, we introduce LAMDAS (LLM As an iMplicit classifier for\ndomain-specific DAta Selection), a novel approach that leverages the\npre-trained LLM itself as an implicit classifier, thereby bypassing explicit\nfeature engineering and computationally intensive optimization process. LAMDAS\nreframes data selection as a one-class classification problem, identifying\ncandidate data that \"belongs\" to the target domain defined by a small reference\ndataset. Extensive experimental results demonstrate that LAMDAS not only\nexceeds the performance of full-data training using a fraction of the data but\nalso outperforms nine state-of-the-art (SOTA) baselines under various\nscenarios. Furthermore, LAMDAS achieves the most compelling balance between\nperformance gains and computational efficiency compared to all evaluated\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting large language models (LLMs) to specific domains often faces a\ncritical bottleneck: the scarcity of high-quality, human-curated data. While\nlarge volumes of unchecked data are readily available, indiscriminately using\nthem for fine-tuning risks introducing noise and degrading performance.\nStrategic data selection is thus crucial, requiring a method that is both\naccurate and efficient. Existing approaches, categorized as similarity-based\nand direct optimization methods, struggle to simultaneously achieve these\ngoals. In this paper, we introduce LAMDAS (LLM As an iMplicit classifier for\ndomain-specific DAta Selection), a novel approach that leverages the\npre-trained LLM itself as an implicit classifier, thereby bypassing explicit\nfeature engineering and computationally intensive optimization process. LAMDAS\nreframes data selection as a one-class classification problem, identifying\ncandidate data that \"belongs\" to the target domain defined by a small reference\ndataset. Extensive experimental results demonstrate that LAMDAS not only\nexceeds the performance of full-data training using a fraction of the data but\nalso outperforms nine state-of-the-art (SOTA) baselines under various\nscenarios. Furthermore, LAMDAS achieves the most compelling balance between\nperformance gains and computational efficiency compared to all evaluated\nbaselines."
                },
                "authors": [
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Hang Yu"
                    },
                    {
                        "name": "Bingchang Liu"
                    },
                    {
                        "name": "Wenjie Yang"
                    },
                    {
                        "name": "Peng Di"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09829v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09829v3",
                "updated": "2025-09-08T10:30:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    10,
                    30,
                    51,
                    0,
                    251,
                    0
                ],
                "published": "2024-10-13T13:07:31Z",
                "published_parsed": [
                    2024,
                    10,
                    13,
                    13,
                    7,
                    31,
                    6,
                    287,
                    0
                ],
                "title": "Conversational Code Generation: a Case Study of Designing a Dialogue\n  System for Generating Driving Scenarios for Testing Autonomous Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Code Generation: a Case Study of Designing a Dialogue\n  System for Generating Driving Scenarios for Testing Autonomous Vehicles"
                },
                "summary": "Cyber-physical systems like autonomous vehicles are tested in simulation\nbefore deployment, using domain-specific programs for scenario specification.\nTo aid the testing of autonomous vehicles in simulation, we design a natural\nlanguage interface, using an instruction-following large language model, to\nassist a non-coding domain expert in synthesising the desired scenarios and\nvehicle behaviours. We show that using it to convert utterances to the symbolic\nprogram is feasible, despite the very small training dataset. Human experiments\nshow that dialogue is critical to successful simulation generation, leading to\na 4.5 times higher success rate than a generation without engaging in extended\nconversation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyber-physical systems like autonomous vehicles are tested in simulation\nbefore deployment, using domain-specific programs for scenario specification.\nTo aid the testing of autonomous vehicles in simulation, we design a natural\nlanguage interface, using an instruction-following large language model, to\nassist a non-coding domain expert in synthesising the desired scenarios and\nvehicle behaviours. We show that using it to convert utterances to the symbolic\nprogram is feasible, despite the very small training dataset. Human experiments\nshow that dialogue is critical to successful simulation generation, leading to\na 4.5 times higher success rate than a generation without engaging in extended\nconversation."
                },
                "authors": [
                    {
                        "name": "Rimvydas Rubavicius"
                    },
                    {
                        "name": "Antonio Valerio Miceli-Barone"
                    },
                    {
                        "name": "Alex Lascarides"
                    },
                    {
                        "name": "Subramanian Ramamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Subramanian Ramamoorthy"
                },
                "author": "Subramanian Ramamoorthy",
                "arxiv_comment": "In Proceedings of GeCoIn 2025: Generative Code Intelligence Workshop,\n  co-located with ECAI-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09829v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09829v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06518v1",
                "updated": "2025-09-08T10:24:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    10,
                    24,
                    19,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T10:24:19Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    10,
                    24,
                    19,
                    0,
                    251,
                    0
                ],
                "title": "Crown, Frame, Reverse: Layer-Wise Scaling Variants for LLM Pre-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crown, Frame, Reverse: Layer-Wise Scaling Variants for LLM Pre-Training"
                },
                "summary": "Transformer-based language models traditionally use uniform (isotropic) layer\nsizes, yet they ignore the diverse functional roles that different depths can\nplay and their computational capacity needs. Building on Layer-Wise Scaling\n(LWS) and pruning literature, we introduce three new LWS variants - Framed,\nReverse, and Crown - that redistribute FFN widths and attention heads via two\nor three-point linear interpolation in the pre-training stage. We present the\nfirst systematic ablation of LWS and its variants, on a fixed budget of 180M\nparameters, trained on 5B tokens. All models converge to similar losses and\nachieve better performance compared to an equal-cost isotropic baseline,\nwithout a substantial decrease in training throughput. This work represents an\ninitial step into the design space of layer-wise architectures for\npre-training, but future work should scale experiments to orders of magnitude\nmore tokens and parameters to fully assess their potential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based language models traditionally use uniform (isotropic) layer\nsizes, yet they ignore the diverse functional roles that different depths can\nplay and their computational capacity needs. Building on Layer-Wise Scaling\n(LWS) and pruning literature, we introduce three new LWS variants - Framed,\nReverse, and Crown - that redistribute FFN widths and attention heads via two\nor three-point linear interpolation in the pre-training stage. We present the\nfirst systematic ablation of LWS and its variants, on a fixed budget of 180M\nparameters, trained on 5B tokens. All models converge to similar losses and\nachieve better performance compared to an equal-cost isotropic baseline,\nwithout a substantial decrease in training throughput. This work represents an\ninitial step into the design space of layer-wise architectures for\npre-training, but future work should scale experiments to orders of magnitude\nmore tokens and parameters to fully assess their potential."
                },
                "authors": [
                    {
                        "name": "Andrei Baroian"
                    },
                    {
                        "name": "Kasper Notebomer"
                    }
                ],
                "author_detail": {
                    "name": "Kasper Notebomer"
                },
                "author": "Kasper Notebomer",
                "arxiv_comment": "The reported results are skewed due to a data type mismatch. The\n  dataset was saved with int32, but the data loader interpreted it as uint16.\n  As a result, each 32-bit token was incorrectly split into two 16-bit tokens.\n  Outcome: a consistent artifact where every other token is zero",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15868v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15868v2",
                "updated": "2025-09-08T10:20:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    10,
                    20,
                    38,
                    0,
                    251,
                    0
                ],
                "published": "2025-08-21T00:20:47Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    0,
                    20,
                    47,
                    3,
                    233,
                    0
                ],
                "title": "CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated\n  Chain-of-Thought-based Reinforced Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated\n  Chain-of-Thought-based Reinforced Fine-Tuning"
                },
                "summary": "Reasoning capability plays a significantly critical role in the the broad\napplications of Large Language Models (LLMs). To enhance the reasoning\nperformance of LLMs, diverse Reinforcement Learning (RL)-based fine-tuning\napproaches have been proposed to address the limited generalization capability\nof LLMs trained solely via Supervised Fine-Tuning (SFT). Despite their\neffectiveness, two major limitations hinder the advancement of LLMs. First,\nvanilla RL-based approaches ignore annotated Chain-of-Thought (CoT) and\nincorporate unstable reasoning path sampling, which typically results in model\ncollapse, unstable training process, and suboptimal performance. Second,\nexisting SFT approaches generally overemphasize the annotated CoT, potentially\nleading to performance degradation due to insufficient exploitation of\npotential CoT. In this paper, we propose a Contrastive learning with annotated\nCoT-based Reinforced Fine-Tuning approach, i.e., \\TheName{}, to enhance the\nreasoning performance of LLMs while addressing the aforementioned limitations.\nSpecifically, we propose learning a representation for each CoT. Based on this\nrepresentation, we design novel contrastive signals to guide the fine-tuning\nprocess. Our approach not only fully exploits the available annotated CoT but\nalso stabilizes the fine-tuning procedure by incorporating an additional\nunsupervised learning signal. We conduct comprehensive experiments and in-depth\nanalysis with three baseline approaches, two foundation models, and two\ndatasets to demonstrate significant advantages of \\TheName{} in terms of\nrobustness, performance (up to 10.15\\%), and efficiency (up to 30.62\\%). Code\nis available at https://github.com/WNQzhu/CARFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning capability plays a significantly critical role in the the broad\napplications of Large Language Models (LLMs). To enhance the reasoning\nperformance of LLMs, diverse Reinforcement Learning (RL)-based fine-tuning\napproaches have been proposed to address the limited generalization capability\nof LLMs trained solely via Supervised Fine-Tuning (SFT). Despite their\neffectiveness, two major limitations hinder the advancement of LLMs. First,\nvanilla RL-based approaches ignore annotated Chain-of-Thought (CoT) and\nincorporate unstable reasoning path sampling, which typically results in model\ncollapse, unstable training process, and suboptimal performance. Second,\nexisting SFT approaches generally overemphasize the annotated CoT, potentially\nleading to performance degradation due to insufficient exploitation of\npotential CoT. In this paper, we propose a Contrastive learning with annotated\nCoT-based Reinforced Fine-Tuning approach, i.e., \\TheName{}, to enhance the\nreasoning performance of LLMs while addressing the aforementioned limitations.\nSpecifically, we propose learning a representation for each CoT. Based on this\nrepresentation, we design novel contrastive signals to guide the fine-tuning\nprocess. Our approach not only fully exploits the available annotated CoT but\nalso stabilizes the fine-tuning procedure by incorporating an additional\nunsupervised learning signal. We conduct comprehensive experiments and in-depth\nanalysis with three baseline approaches, two foundation models, and two\ndatasets to demonstrate significant advantages of \\TheName{} in terms of\nrobustness, performance (up to 10.15\\%), and efficiency (up to 30.62\\%). Code\nis available at https://github.com/WNQzhu/CARFT."
                },
                "authors": [
                    {
                        "name": "Wenqiao Zhu"
                    },
                    {
                        "name": "Ji Liu"
                    },
                    {
                        "name": "Rongjuncheng Zhang"
                    },
                    {
                        "name": "Haipang Wu"
                    },
                    {
                        "name": "Yulun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yulun Zhang"
                },
                "author": "Yulun Zhang",
                "arxiv_comment": "14 pages, to appear in EMNLP25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15868v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15868v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06504v1",
                "updated": "2025-09-08T10:08:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    10,
                    8,
                    48,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T10:08:48Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    10,
                    8,
                    48,
                    0,
                    251,
                    0
                ],
                "title": "When Code Crosses Borders: A Security-Centric Evaluation of LLM-based\n  Code Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Code Crosses Borders: A Security-Centric Evaluation of LLM-based\n  Code Translation"
                },
                "summary": "With the growing demand for cross-language codebase migration, evaluating\nLLMs' security implications in translation tasks has become critical. Existing\nevaluations primarily focus on syntactic or functional correctness at the\nfunction level, neglecting the critical dimension of security.\n  To enable security evaluation, we construct STED (Security-centric\nTranslation Evaluation Dataset), the first dataset specifically designed for\nevaluating the security implications of LLM-based code translation. It\ncomprises 720 security-related code samples across five programming languages\nand nine high-impact CWE categories, sourced from CVE/NVD and manually verified\nfor translation tasks. Our evaluation framework consists of two independent\nassessment modules: (1) rigorous evaluation by security researchers, and (2)\nautomated analysis via LLM-as-a-judge. Together they evaluate three critical\naspects: functional correctness, vulnerability preservation, and vulnerability\nintroduction rates.\n  Our large-scale evaluation of five state-of-the-art LLMs across 6,000\ntranslation instances reveals significant security degradation, with 28.6-45%\nof translations introducing new vulnerabilities--particularly for web-related\nflaws like input validation, where LLMs show consistent weaknesses.\nFurthermore, we develop a Retrieval-Augmented Generation (RAG)-based mitigation\nstrategy that reduces translation-induced vulnerabilities by 32.8%, showing the\npotential of knowledge-enhanced prompting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing demand for cross-language codebase migration, evaluating\nLLMs' security implications in translation tasks has become critical. Existing\nevaluations primarily focus on syntactic or functional correctness at the\nfunction level, neglecting the critical dimension of security.\n  To enable security evaluation, we construct STED (Security-centric\nTranslation Evaluation Dataset), the first dataset specifically designed for\nevaluating the security implications of LLM-based code translation. It\ncomprises 720 security-related code samples across five programming languages\nand nine high-impact CWE categories, sourced from CVE/NVD and manually verified\nfor translation tasks. Our evaluation framework consists of two independent\nassessment modules: (1) rigorous evaluation by security researchers, and (2)\nautomated analysis via LLM-as-a-judge. Together they evaluate three critical\naspects: functional correctness, vulnerability preservation, and vulnerability\nintroduction rates.\n  Our large-scale evaluation of five state-of-the-art LLMs across 6,000\ntranslation instances reveals significant security degradation, with 28.6-45%\nof translations introducing new vulnerabilities--particularly for web-related\nflaws like input validation, where LLMs show consistent weaknesses.\nFurthermore, we develop a Retrieval-Augmented Generation (RAG)-based mitigation\nstrategy that reduces translation-induced vulnerabilities by 32.8%, showing the\npotential of knowledge-enhanced prompting."
                },
                "authors": [
                    {
                        "name": "Hailong Chang"
                    },
                    {
                        "name": "Guozhu Meng"
                    },
                    {
                        "name": "Shuhui Xiao"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Kun Sun"
                    },
                    {
                        "name": "Yilin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yilin Li"
                },
                "author": "Yilin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06503v1",
                "updated": "2025-09-08T10:08:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    10,
                    8,
                    36,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T10:08:36Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    10,
                    8,
                    36,
                    0,
                    251,
                    0
                ],
                "title": "An AI system to help scientists write expert-level empirical software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An AI system to help scientists write expert-level empirical software"
                },
                "summary": "The cycle of scientific discovery is frequently bottlenecked by the slow,\nmanual creation of software to support computational experiments. To address\nthis, we present an AI system that creates expert-level scientific software\nwhose goal is to maximize a quality metric. The system uses a Large Language\nModel (LLM) and Tree Search (TS) to systematically improve the quality metric\nand intelligently navigate the large space of possible solutions. The system\nachieves expert-level results when it explores and integrates complex research\nideas from external sources. The effectiveness of tree search is demonstrated\nacross a wide range of benchmarks. In bioinformatics, it discovered 40 novel\nmethods for single-cell data analysis that outperformed the top human-developed\nmethods on a public leaderboard. In epidemiology, it generated 14 models that\noutperformed the CDC ensemble and all other individual models for forecasting\nCOVID-19 hospitalizations. Our method also produced state-of-the-art software\nfor geospatial analysis, neural activity prediction in zebrafish, time series\nforecasting and numerical solution of integrals. By devising and implementing\nnovel solutions to diverse tasks, the system represents a significant step\ntowards accelerating scientific progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cycle of scientific discovery is frequently bottlenecked by the slow,\nmanual creation of software to support computational experiments. To address\nthis, we present an AI system that creates expert-level scientific software\nwhose goal is to maximize a quality metric. The system uses a Large Language\nModel (LLM) and Tree Search (TS) to systematically improve the quality metric\nand intelligently navigate the large space of possible solutions. The system\nachieves expert-level results when it explores and integrates complex research\nideas from external sources. The effectiveness of tree search is demonstrated\nacross a wide range of benchmarks. In bioinformatics, it discovered 40 novel\nmethods for single-cell data analysis that outperformed the top human-developed\nmethods on a public leaderboard. In epidemiology, it generated 14 models that\noutperformed the CDC ensemble and all other individual models for forecasting\nCOVID-19 hospitalizations. Our method also produced state-of-the-art software\nfor geospatial analysis, neural activity prediction in zebrafish, time series\nforecasting and numerical solution of integrals. By devising and implementing\nnovel solutions to diverse tasks, the system represents a significant step\ntowards accelerating scientific progress."
                },
                "authors": [
                    {
                        "name": "Eser Aygün"
                    },
                    {
                        "name": "Anastasiya Belyaeva"
                    },
                    {
                        "name": "Gheorghe Comanici"
                    },
                    {
                        "name": "Marc Coram"
                    },
                    {
                        "name": "Hao Cui"
                    },
                    {
                        "name": "Jake Garrison"
                    },
                    {
                        "name": "Renee Johnston Anton Kast"
                    },
                    {
                        "name": "Cory Y. McLean"
                    },
                    {
                        "name": "Peter Norgaard"
                    },
                    {
                        "name": "Zahra Shamsi"
                    },
                    {
                        "name": "David Smalling"
                    },
                    {
                        "name": "James Thompson"
                    },
                    {
                        "name": "Subhashini Venugopalan"
                    },
                    {
                        "name": "Brian P. Williams"
                    },
                    {
                        "name": "Chujun He"
                    },
                    {
                        "name": "Sarah Martinson"
                    },
                    {
                        "name": "Martyna Plomecka"
                    },
                    {
                        "name": "Lai Wei"
                    },
                    {
                        "name": "Yuchen Zhou"
                    },
                    {
                        "name": "Qian-Ze Zhu"
                    },
                    {
                        "name": "Matthew Abraham"
                    },
                    {
                        "name": "Erica Brand"
                    },
                    {
                        "name": "Anna Bulanova"
                    },
                    {
                        "name": "Jeffrey A. Cardille"
                    },
                    {
                        "name": "Chris Co"
                    },
                    {
                        "name": "Scott Ellsworth"
                    },
                    {
                        "name": "Grace Joseph"
                    },
                    {
                        "name": "Malcolm Kane"
                    },
                    {
                        "name": "Ryan Krueger"
                    },
                    {
                        "name": "Johan Kartiwa"
                    },
                    {
                        "name": "Dan Liebling"
                    },
                    {
                        "name": "Jan-Matthis Lueckmann"
                    },
                    {
                        "name": "Paul Raccuglia"
                    },
                    {
                        "name": "Xuefei"
                    },
                    {
                        "name": "Wang"
                    },
                    {
                        "name": "Katherine Chou"
                    },
                    {
                        "name": "James Manyika"
                    },
                    {
                        "name": "Yossi Matias"
                    },
                    {
                        "name": "John C. Platt"
                    },
                    {
                        "name": "Lizzie Dorfman"
                    },
                    {
                        "name": "Shibl Mourad"
                    },
                    {
                        "name": "Michael P. Brenner"
                    }
                ],
                "author_detail": {
                    "name": "Michael P. Brenner"
                },
                "arxiv_affiliation": "Julie",
                "author": "Michael P. Brenner",
                "arxiv_comment": "71 pages, 26 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18799v2",
                "updated": "2025-09-08T10:07:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    10,
                    7,
                    31,
                    0,
                    251,
                    0
                ],
                "published": "2025-08-26T08:34:04Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    8,
                    34,
                    4,
                    1,
                    238,
                    0
                ],
                "title": "Robust and Label-Efficient Deep Waste Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust and Label-Efficient Deep Waste Detection"
                },
                "summary": "Effective waste sorting is critical for sustainable recycling, yet AI\nresearch in this domain continues to lag behind commercial systems due to\nlimited datasets and reliance on legacy object detectors. In this work, we\nadvance AI-driven waste detection by establishing strong baselines and\nintroducing an ensemble-based semi-supervised learning framework. We first\nbenchmark state-of-the-art Open-Vocabulary Object Detection (OVOD) models on\nthe real-world ZeroWaste dataset, demonstrating that while class-only prompts\nperform poorly, LLM-optimized prompts significantly enhance zero-shot accuracy.\nNext, to address domain-specific limitations, we fine-tune modern\ntransformer-based detectors, achieving a new baseline of 51.6 mAP. We then\npropose a soft pseudo-labeling strategy that fuses ensemble predictions using\nspatial and consensus-aware weighting, enabling robust semi-supervised\ntraining. Applied to the unlabeled ZeroWaste-s subset, our pseudo-annotations\nachieve performance gains that surpass fully supervised training, underscoring\nthe effectiveness of scalable annotation pipelines. Our work contributes to the\nresearch community by establishing rigorous baselines, introducing a robust\nensemble-based pseudo-labeling pipeline, generating high-quality annotations\nfor the unlabeled ZeroWaste-s subset, and systematically evaluating OVOD models\nunder real-world waste sorting conditions. Our code is available at:\nhttps://github.com/h-abid97/robust-waste-detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective waste sorting is critical for sustainable recycling, yet AI\nresearch in this domain continues to lag behind commercial systems due to\nlimited datasets and reliance on legacy object detectors. In this work, we\nadvance AI-driven waste detection by establishing strong baselines and\nintroducing an ensemble-based semi-supervised learning framework. We first\nbenchmark state-of-the-art Open-Vocabulary Object Detection (OVOD) models on\nthe real-world ZeroWaste dataset, demonstrating that while class-only prompts\nperform poorly, LLM-optimized prompts significantly enhance zero-shot accuracy.\nNext, to address domain-specific limitations, we fine-tune modern\ntransformer-based detectors, achieving a new baseline of 51.6 mAP. We then\npropose a soft pseudo-labeling strategy that fuses ensemble predictions using\nspatial and consensus-aware weighting, enabling robust semi-supervised\ntraining. Applied to the unlabeled ZeroWaste-s subset, our pseudo-annotations\nachieve performance gains that surpass fully supervised training, underscoring\nthe effectiveness of scalable annotation pipelines. Our work contributes to the\nresearch community by establishing rigorous baselines, introducing a robust\nensemble-based pseudo-labeling pipeline, generating high-quality annotations\nfor the unlabeled ZeroWaste-s subset, and systematically evaluating OVOD models\nunder real-world waste sorting conditions. Our code is available at:\nhttps://github.com/h-abid97/robust-waste-detection."
                },
                "authors": [
                    {
                        "name": "Hassan Abid"
                    },
                    {
                        "name": "Khan Muhammad"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Haris Khan"
                },
                "author": "Muhammad Haris Khan",
                "arxiv_comment": "Accepted at BMVC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06501v1",
                "updated": "2025-09-08T10:07:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    10,
                    7,
                    3,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T10:07:03Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    10,
                    7,
                    3,
                    0,
                    251,
                    0
                ],
                "title": "WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents"
                },
                "summary": "The paradigm of Large Language Models (LLMs) has increasingly shifted toward\nagentic applications, where web browsing capabilities are fundamental for\nretrieving information from diverse online sources. However, existing\nopen-source web agents either demonstrate limited information-seeking abilities\non complex tasks or lack transparent implementations. In this work, we identify\nthat the key challenge lies in the scarcity of challenging data for information\nseeking. To address this limitation, we introduce WebExplorer: a systematic\ndata generation approach using model-based exploration and iterative,\nlong-to-short query evolution. This method creates challenging query-answer\npairs that require multi-step reasoning and complex web navigation. By\nleveraging our curated high-quality dataset, we successfully develop advanced\nweb agent WebExplorer-8B through supervised fine-tuning followed by\nreinforcement learning. Our model supports 128K context length and up to 100\ntool calling turns, enabling long-horizon problem solving. Across diverse\ninformation-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art\nperformance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able\nto effectively search over an average of 16 turns after RL training, achieving\nhigher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best\nperformance among models up to 100B parameters on WebWalkerQA and FRAMES.\nBeyond these information-seeking tasks, our model also achieves strong\ngeneralization on the HLE benchmark even though it is only trained on\nknowledge-intensive QA data. These results highlight our approach as a\npractical path toward long-horizon web agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paradigm of Large Language Models (LLMs) has increasingly shifted toward\nagentic applications, where web browsing capabilities are fundamental for\nretrieving information from diverse online sources. However, existing\nopen-source web agents either demonstrate limited information-seeking abilities\non complex tasks or lack transparent implementations. In this work, we identify\nthat the key challenge lies in the scarcity of challenging data for information\nseeking. To address this limitation, we introduce WebExplorer: a systematic\ndata generation approach using model-based exploration and iterative,\nlong-to-short query evolution. This method creates challenging query-answer\npairs that require multi-step reasoning and complex web navigation. By\nleveraging our curated high-quality dataset, we successfully develop advanced\nweb agent WebExplorer-8B through supervised fine-tuning followed by\nreinforcement learning. Our model supports 128K context length and up to 100\ntool calling turns, enabling long-horizon problem solving. Across diverse\ninformation-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art\nperformance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able\nto effectively search over an average of 16 turns after RL training, achieving\nhigher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best\nperformance among models up to 100B parameters on WebWalkerQA and FRAMES.\nBeyond these information-seeking tasks, our model also achieves strong\ngeneralization on the HLE benchmark even though it is only trained on\nknowledge-intensive QA data. These results highlight our approach as a\npractical path toward long-horizon web agents."
                },
                "authors": [
                    {
                        "name": "Junteng Liu"
                    },
                    {
                        "name": "Yunji Li"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Jingyang Li"
                    },
                    {
                        "name": "Aili Chen"
                    },
                    {
                        "name": "Ke Ji"
                    },
                    {
                        "name": "Weiyu Cheng"
                    },
                    {
                        "name": "Zijia Wu"
                    },
                    {
                        "name": "Chengyu Du"
                    },
                    {
                        "name": "Qidi Xu"
                    },
                    {
                        "name": "Jiayuan Song"
                    },
                    {
                        "name": "Zhengmao Zhu"
                    },
                    {
                        "name": "Wenhu Chen"
                    },
                    {
                        "name": "Pengyu Zhao"
                    },
                    {
                        "name": "Junxian He"
                    }
                ],
                "author_detail": {
                    "name": "Junxian He"
                },
                "author": "Junxian He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06493v1",
                "updated": "2025-09-08T09:54:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T09:54:18Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers"
                },
                "summary": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search."
                },
                "authors": [
                    {
                        "name": "Ran Xin"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Yanchen Nie"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Xia Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xia Xiao"
                },
                "author": "Xia Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13111v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13111v2",
                "updated": "2025-09-08T09:39:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    39,
                    12,
                    0,
                    251,
                    0
                ],
                "published": "2025-03-17T12:34:22Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    34,
                    22,
                    0,
                    76,
                    0
                ],
                "title": "MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs"
                },
                "summary": "Multimodal large language models (MLLMs) excel at 2D visual understanding but\nremain limited in their ability to reason about 3D space. In this work, we\nleverage large-scale high-quality 3D scene data with open-set annotations to\nintroduce 1) a novel supervised fine-tuning dataset and 2) a new evaluation\nbenchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data\ncovers diverse spatial tasks including spatial relationship prediction, metric\nsize and distance estimation, and 3D grounding. We show that CA-VQA enables us\nto train MM-Spatial, a strong generalist MLLM that also achieves\nstate-of-the-art performance on 3D spatial understanding benchmarks, including\nour own. We show how incorporating metric depth and multi-view inputs (provided\nin CA-VQA) can further improve 3D understanding, and demonstrate that data\nalone allows our model to achieve depth perception capabilities comparable to\ndedicated monocular depth estimation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) excel at 2D visual understanding but\nremain limited in their ability to reason about 3D space. In this work, we\nleverage large-scale high-quality 3D scene data with open-set annotations to\nintroduce 1) a novel supervised fine-tuning dataset and 2) a new evaluation\nbenchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data\ncovers diverse spatial tasks including spatial relationship prediction, metric\nsize and distance estimation, and 3D grounding. We show that CA-VQA enables us\nto train MM-Spatial, a strong generalist MLLM that also achieves\nstate-of-the-art performance on 3D spatial understanding benchmarks, including\nour own. We show how incorporating metric depth and multi-view inputs (provided\nin CA-VQA) can further improve 3D understanding, and demonstrate that data\nalone allows our model to achieve depth perception capabilities comparable to\ndedicated monocular depth estimation models."
                },
                "authors": [
                    {
                        "name": "Erik Daxberger"
                    },
                    {
                        "name": "Nina Wenzel"
                    },
                    {
                        "name": "David Griffiths"
                    },
                    {
                        "name": "Haiming Gang"
                    },
                    {
                        "name": "Justin Lazarow"
                    },
                    {
                        "name": "Gefen Kohavi"
                    },
                    {
                        "name": "Kai Kang"
                    },
                    {
                        "name": "Marcin Eichner"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Peter Grasch"
                    }
                ],
                "author_detail": {
                    "name": "Peter Grasch"
                },
                "author": "Peter Grasch",
                "arxiv_comment": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13111v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13111v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06473v1",
                "updated": "2025-09-08T09:37:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    37,
                    58,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T09:37:58Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    37,
                    58,
                    0,
                    251,
                    0
                ],
                "title": "From Offline to Inline Without Pain: A Practical Framework for\n  Translating Offline MR Reconstructions to Inline Deployment Using the\n  Gadgetron Platform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Offline to Inline Without Pain: A Practical Framework for\n  Translating Offline MR Reconstructions to Inline Deployment Using the\n  Gadgetron Platform"
                },
                "summary": "Purpose: To develop and validate a practical framework to overcome common\nissues in inline deployment of established offline MR reconstruction, including\n(1) delay from lengthy reconstructions, (2) limited support for multi-scan\ninput reconstructions, (3) the need to adapt scripts for different raw formats,\nand (4) limited guidance and experience in retaining scanner reconstructions\nand applying scanner-based post-processing to custom outputs. Methods: The\nframework builds upon the Gadgetron platform and includes: (1) an input\nconverter to transform ISMRMRD format raw into a Siemens format raw structure,\nfacilitating reuse of existing code; (2) an asynchronous trigger-and-retrieve\nmechanism enabling long reconstructions without delaying scanner processes; (3)\nresource-aware scheduling for parallel execution; (4) integrated file\nmanagement to support multi-scan inputs; and (5) preservation of scanner-based\nreconstructions and post-processing. The framework was validated on 2 Siemens\nscanners for SENSE, AlignedSENSE, and NUFFT reconstructions, and in a\nlarge-cohort study. Results: Minimum code modification for inline deployment\nhas been shown, and all reconstructions were successfully executed inline\nwithout disrupting scanner workflows. Images were retrieved via automated or\nretro-reconstruction, with scanner-based post-processing applied to custom\noutputs. Multi-scan input reconstructions were executed using GPU-aware\nscheduling, confirming feasibility for routine and large-scale applications. In\n480 consecutive examinations, inline reconstructions were retrieved in 99% of\ncases without disruptions. Conclusion: The framework lowers the technical\nbarrier to inline deployment of offline reconstructions, enabling robust,\nscalable, and post-processing-compatible integration. It is openly available\nwith documentation and demonstration cases to support reproducibility and\ncommunity adoption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: To develop and validate a practical framework to overcome common\nissues in inline deployment of established offline MR reconstruction, including\n(1) delay from lengthy reconstructions, (2) limited support for multi-scan\ninput reconstructions, (3) the need to adapt scripts for different raw formats,\nand (4) limited guidance and experience in retaining scanner reconstructions\nand applying scanner-based post-processing to custom outputs. Methods: The\nframework builds upon the Gadgetron platform and includes: (1) an input\nconverter to transform ISMRMRD format raw into a Siemens format raw structure,\nfacilitating reuse of existing code; (2) an asynchronous trigger-and-retrieve\nmechanism enabling long reconstructions without delaying scanner processes; (3)\nresource-aware scheduling for parallel execution; (4) integrated file\nmanagement to support multi-scan inputs; and (5) preservation of scanner-based\nreconstructions and post-processing. The framework was validated on 2 Siemens\nscanners for SENSE, AlignedSENSE, and NUFFT reconstructions, and in a\nlarge-cohort study. Results: Minimum code modification for inline deployment\nhas been shown, and all reconstructions were successfully executed inline\nwithout disrupting scanner workflows. Images were retrieved via automated or\nretro-reconstruction, with scanner-based post-processing applied to custom\noutputs. Multi-scan input reconstructions were executed using GPU-aware\nscheduling, confirming feasibility for routine and large-scale applications. In\n480 consecutive examinations, inline reconstructions were retrieved in 99% of\ncases without disruptions. Conclusion: The framework lowers the technical\nbarrier to inline deployment of offline reconstructions, enabling robust,\nscalable, and post-processing-compatible integration. It is openly available\nwith documentation and demonstration cases to support reproducibility and\ncommunity adoption."
                },
                "authors": [
                    {
                        "name": "Zihan Ning"
                    },
                    {
                        "name": "Yannick Brackenier"
                    },
                    {
                        "name": "Sarah McElroy"
                    },
                    {
                        "name": "Sara Neves Silva"
                    },
                    {
                        "name": "Lucilio Cordero-Grande"
                    },
                    {
                        "name": "Sam Rot"
                    },
                    {
                        "name": "Liane S. Canas"
                    },
                    {
                        "name": "Rebecca E Thornley"
                    },
                    {
                        "name": "David Leitão"
                    },
                    {
                        "name": "Davide Poccecai"
                    },
                    {
                        "name": "Andrew Cantell"
                    },
                    {
                        "name": "Rene Kerosi"
                    },
                    {
                        "name": "Anthony N Price"
                    },
                    {
                        "name": "Jon Cleary"
                    },
                    {
                        "name": "Donald J Tournier"
                    },
                    {
                        "name": "Jana Hutter"
                    },
                    {
                        "name": "Philippa Bridgen"
                    },
                    {
                        "name": "Pierluigi Di Cio"
                    },
                    {
                        "name": "Michela Cleri"
                    },
                    {
                        "name": "Inka Granlund"
                    },
                    {
                        "name": "Lucy Billimoria"
                    },
                    {
                        "name": "Yasmin Blunck"
                    },
                    {
                        "name": "Shaihan Malik"
                    },
                    {
                        "name": "Marc Modat"
                    },
                    {
                        "name": "Claire J Steves"
                    },
                    {
                        "name": "Joseph V Hajnal"
                    }
                ],
                "author_detail": {
                    "name": "Joseph V Hajnal"
                },
                "author": "Joseph V Hajnal",
                "arxiv_comment": "17 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06472v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06472v2",
                "updated": "2025-09-09T08:54:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    8,
                    54,
                    11,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-08T09:37:20Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    37,
                    20,
                    0,
                    251,
                    0
                ],
                "title": "Rethinking LLM Parametric Knowledge as Post-retrieval Confidence for\n  Dynamic Retrieval and Reranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking LLM Parametric Knowledge as Post-retrieval Confidence for\n  Dynamic Retrieval and Reranking"
                },
                "summary": "Large Language Models (LLMs) often generate inaccurate responses\n(hallucinations) when faced with questions beyond their knowledge scope.\nRetrieval-Augmented Generation (RAG) addresses this by leveraging external\nknowledge, but a critical challenge remains: determining whether retrieved\ncontexts effectively enhance the model`s ability to answer specific queries.\nThis challenge underscores the importance of knowledge boundary awareness,\nwhich current methods-relying on discrete labels or limited signals-fail to\naddress adequately, as they overlook the rich information in LLMs` continuous\ninternal hidden states. To tackle this, we propose a novel post-retrieval\nknowledge filtering approach. First, we construct a confidence detection model\nbased on LLMs` internal hidden states to quantify how retrieved contexts\nenhance the model`s confidence. Using this model, we build a preference dataset\n(NQ_Rerank) to fine-tune a reranker, enabling it to prioritize contexts\npreferred by the downstream LLM during reranking. Additionally, we introduce\nConfidence-Based Dynamic Retrieval (CBDR), which adaptively triggers retrieval\nbased on the LLM`s initial confidence in the original question, reducing\nknowledge conflicts and improving efficiency. Experimental results demonstrate\nsignificant improvements in accuracy for context screening and end-to-end RAG\nperformance, along with a notable reduction in retrieval costs while\nmaintaining competitive accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often generate inaccurate responses\n(hallucinations) when faced with questions beyond their knowledge scope.\nRetrieval-Augmented Generation (RAG) addresses this by leveraging external\nknowledge, but a critical challenge remains: determining whether retrieved\ncontexts effectively enhance the model`s ability to answer specific queries.\nThis challenge underscores the importance of knowledge boundary awareness,\nwhich current methods-relying on discrete labels or limited signals-fail to\naddress adequately, as they overlook the rich information in LLMs` continuous\ninternal hidden states. To tackle this, we propose a novel post-retrieval\nknowledge filtering approach. First, we construct a confidence detection model\nbased on LLMs` internal hidden states to quantify how retrieved contexts\nenhance the model`s confidence. Using this model, we build a preference dataset\n(NQ_Rerank) to fine-tune a reranker, enabling it to prioritize contexts\npreferred by the downstream LLM during reranking. Additionally, we introduce\nConfidence-Based Dynamic Retrieval (CBDR), which adaptively triggers retrieval\nbased on the LLM`s initial confidence in the original question, reducing\nknowledge conflicts and improving efficiency. Experimental results demonstrate\nsignificant improvements in accuracy for context screening and end-to-end RAG\nperformance, along with a notable reduction in retrieval costs while\nmaintaining competitive accuracy."
                },
                "authors": [
                    {
                        "name": "Haoxiang Jin"
                    },
                    {
                        "name": "Ronghan Li"
                    },
                    {
                        "name": "Zixiang Lu"
                    },
                    {
                        "name": "Qiguang Miao"
                    }
                ],
                "author_detail": {
                    "name": "Qiguang Miao"
                },
                "author": "Qiguang Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06472v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06472v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06469v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06469v2",
                "updated": "2025-09-09T10:50:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    10,
                    50,
                    54,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-08T09:30:22Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    30,
                    22,
                    0,
                    251,
                    0
                ],
                "title": "Interactive Shaping of Granular Media Using Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive Shaping of Granular Media Using Reinforcement Learning"
                },
                "summary": "Autonomous manipulation of granular media, such as sand, is crucial for\napplications in construction, excavation, and additive manufacturing. However,\nshaping granular materials presents unique challenges due to their\nhigh-dimensional configuration space and complex dynamics, where traditional\nrule-based approaches struggle without extensive engineering efforts.\nReinforcement learning (RL) offers a promising alternative by enabling agents\nto learn adaptive manipulation strategies through trial and error. In this\nwork, we present an RL framework that enables a robotic arm with a cubic\nend-effector and a stereo camera to shape granular media into desired target\nstructures. We show the importance of compact observations and concise reward\nformulations for the large configuration space, validating our design choices\nwith an ablation study. Our results demonstrate the effectiveness of the\nproposed approach for the training of visual policies that manipulate granular\nmedia including their real-world deployment, significantly outperforming two\nbaseline approaches in terms of target shape accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous manipulation of granular media, such as sand, is crucial for\napplications in construction, excavation, and additive manufacturing. However,\nshaping granular materials presents unique challenges due to their\nhigh-dimensional configuration space and complex dynamics, where traditional\nrule-based approaches struggle without extensive engineering efforts.\nReinforcement learning (RL) offers a promising alternative by enabling agents\nto learn adaptive manipulation strategies through trial and error. In this\nwork, we present an RL framework that enables a robotic arm with a cubic\nend-effector and a stereo camera to shape granular media into desired target\nstructures. We show the importance of compact observations and concise reward\nformulations for the large configuration space, validating our design choices\nwith an ablation study. Our results demonstrate the effectiveness of the\nproposed approach for the training of visual policies that manipulate granular\nmedia including their real-world deployment, significantly outperforming two\nbaseline approaches in terms of target shape accuracy."
                },
                "authors": [
                    {
                        "name": "Benedikt Kreis"
                    },
                    {
                        "name": "Malte Mosbach"
                    },
                    {
                        "name": "Anny Ripke"
                    },
                    {
                        "name": "Muhammad Ehsan Ullah"
                    },
                    {
                        "name": "Sven Behnke"
                    },
                    {
                        "name": "Maren Bennewitz"
                    }
                ],
                "author_detail": {
                    "name": "Maren Bennewitz"
                },
                "author": "Maren Bennewitz",
                "arxiv_comment": "Accepted to IEEE-RAS International Conference on Humanoid Robots\n  (Humanoids) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06469v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06463v1",
                "updated": "2025-09-08T09:22:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    22,
                    57,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T09:22:57Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    22,
                    57,
                    0,
                    251,
                    0
                ],
                "title": "Accelerate Scaling of LLM Alignment via Quantifying the Coverage and\n  Depth of Instruction Set",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerate Scaling of LLM Alignment via Quantifying the Coverage and\n  Depth of Instruction Set"
                },
                "summary": "With the growing demand for applying large language models to downstream\ntasks, improving model alignment performance and efficiency has become crucial.\nSuch a process involves selecting informative instructions from a candidate\npool. However, due to the complexity of instruction set distributions, the key\nfactors driving the performance of aligned models remain unclear. As a result,\ncurrent instruction set refinement methods fail to improve performance as the\ninstruction pool expands continuously. To address this issue, we first\ninvestigate the key factors that influence the relationship between instruction\ndataset distribution and aligned model performance. Based on these insights, we\npropose a novel instruction data selection method. We identify that the depth\nof instructions and the coverage of the semantic space are the crucial factors\ndetermining downstream performance, which could explain over 70\\% of the model\nloss on the development set. We then design an instruction selection algorithm\nto simultaneously maximize the depth and semantic coverage of the selected\ninstructions. Experimental results demonstrate that, compared to\nstate-of-the-art baseline methods, it can sustainably improve model performance\nat a faster pace and thus achieve \\emph{``Accelerated Scaling''}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing demand for applying large language models to downstream\ntasks, improving model alignment performance and efficiency has become crucial.\nSuch a process involves selecting informative instructions from a candidate\npool. However, due to the complexity of instruction set distributions, the key\nfactors driving the performance of aligned models remain unclear. As a result,\ncurrent instruction set refinement methods fail to improve performance as the\ninstruction pool expands continuously. To address this issue, we first\ninvestigate the key factors that influence the relationship between instruction\ndataset distribution and aligned model performance. Based on these insights, we\npropose a novel instruction data selection method. We identify that the depth\nof instructions and the coverage of the semantic space are the crucial factors\ndetermining downstream performance, which could explain over 70\\% of the model\nloss on the development set. We then design an instruction selection algorithm\nto simultaneously maximize the depth and semantic coverage of the selected\ninstructions. Experimental results demonstrate that, compared to\nstate-of-the-art baseline methods, it can sustainably improve model performance\nat a faster pace and thus achieve \\emph{``Accelerated Scaling''}."
                },
                "authors": [
                    {
                        "name": "Chengwei Wu"
                    },
                    {
                        "name": "Li Du"
                    },
                    {
                        "name": "Hanyu Zhao"
                    },
                    {
                        "name": "Yiming Ju"
                    },
                    {
                        "name": "Jiapu Wang"
                    },
                    {
                        "name": "Tengfei Pan"
                    }
                ],
                "author_detail": {
                    "name": "Tengfei Pan"
                },
                "author": "Tengfei Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11798v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11798v2",
                "updated": "2025-09-08T09:13:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    13,
                    3,
                    0,
                    251,
                    0
                ],
                "published": "2025-06-13T14:02:21Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    2,
                    21,
                    4,
                    164,
                    0
                ],
                "title": "Persona-driven Simulation of Voting Behavior in the European Parliament\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persona-driven Simulation of Voting Behavior in the European Parliament\n  with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) display remarkable capabilities to understand or\neven produce political discourse, but have been found to consistently display a\nprogressive left-leaning bias. At the same time, so-called persona or identity\nprompts have been shown to produce LLM behavior that aligns with socioeconomic\ngroups that the base model is not aligned with. In this work, we analyze\nwhether zero-shot persona prompting with limited information can accurately\npredict individual voting decisions and, by aggregation, accurately predict\npositions of European groups on a diverse set of policies. We evaluate if\npredictions are stable towards counterfactual arguments, different persona\nprompts and generation methods. Finally, we find that we can simulate voting\nbehavior of Members of the European Parliament reasonably well with a weighted\nF1 score of approximately 0.793. Our persona dataset of politicians in the 2024\nEuropean Parliament and our code are available at\nhttps://github.com/dess-mannheim/european_parliament_simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) display remarkable capabilities to understand or\neven produce political discourse, but have been found to consistently display a\nprogressive left-leaning bias. At the same time, so-called persona or identity\nprompts have been shown to produce LLM behavior that aligns with socioeconomic\ngroups that the base model is not aligned with. In this work, we analyze\nwhether zero-shot persona prompting with limited information can accurately\npredict individual voting decisions and, by aggregation, accurately predict\npositions of European groups on a diverse set of policies. We evaluate if\npredictions are stable towards counterfactual arguments, different persona\nprompts and generation methods. Finally, we find that we can simulate voting\nbehavior of Members of the European Parliament reasonably well with a weighted\nF1 score of approximately 0.793. Our persona dataset of politicians in the 2024\nEuropean Parliament and our code are available at\nhttps://github.com/dess-mannheim/european_parliament_simulation."
                },
                "authors": [
                    {
                        "name": "Maximilian Kreutner"
                    },
                    {
                        "name": "Marlene Lutz"
                    },
                    {
                        "name": "Markus Strohmaier"
                    }
                ],
                "author_detail": {
                    "name": "Markus Strohmaier"
                },
                "author": "Markus Strohmaier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11798v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11798v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17388v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17388v2",
                "updated": "2025-09-08T09:11:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    11,
                    42,
                    0,
                    251,
                    0
                ],
                "published": "2025-02-24T18:14:09Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    14,
                    9,
                    0,
                    55,
                    0
                ],
                "title": "Coexistence of continuous-variable quantum key distribution and\n  classical data over 120-km fiber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coexistence of continuous-variable quantum key distribution and\n  classical data over 120-km fiber"
                },
                "summary": "Integrating quantum key distribution (QKD) with classical data transmission\nover the same fiber is crucial for scalable quantum-secured communication.\nHowever, noise from classical channels limits QKD distance. We demonstrate the\nlongest-distance continuous-variable QKD (CV-QKD) over 120 km (20 dB loss) in\nthe asymptotic regime, and over 100 km (17 dB loss) in the finite-size regime,\nboth coexisting with a fully populated coarse wavelength division multiplexing\nsystem. Natural mode filtering of the local oscillator and phase noise\nmitigation enabled this without additional filtering or wavelength\nreallocation. Benchmarking against a commercial discrete-variable QKD system\nand considering finite-size effects confirms the feasibility of CV-QKD as a\nplug-and-play solution for typical 80-100 km long-haul optical networks. Our\nresults set a record fiber distance for CV-QKD, showing its potential for\ncost-effective, large-scale deployment in existing network infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating quantum key distribution (QKD) with classical data transmission\nover the same fiber is crucial for scalable quantum-secured communication.\nHowever, noise from classical channels limits QKD distance. We demonstrate the\nlongest-distance continuous-variable QKD (CV-QKD) over 120 km (20 dB loss) in\nthe asymptotic regime, and over 100 km (17 dB loss) in the finite-size regime,\nboth coexisting with a fully populated coarse wavelength division multiplexing\nsystem. Natural mode filtering of the local oscillator and phase noise\nmitigation enabled this without additional filtering or wavelength\nreallocation. Benchmarking against a commercial discrete-variable QKD system\nand considering finite-size effects confirms the feasibility of CV-QKD as a\nplug-and-play solution for typical 80-100 km long-haul optical networks. Our\nresults set a record fiber distance for CV-QKD, showing its potential for\ncost-effective, large-scale deployment in existing network infrastructure."
                },
                "authors": [
                    {
                        "name": "Adnan A. E. Hajomer"
                    },
                    {
                        "name": "Ivan Derkach"
                    },
                    {
                        "name": "Vladyslav C. Usenko"
                    },
                    {
                        "name": "Ulrik L. Andersen"
                    },
                    {
                        "name": "Tobias Gehring"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Gehring"
                },
                "author": "Tobias Gehring",
                "arxiv_doi": "10.1103/zy2d-m3ch",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/zy2d-m3ch",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.17388v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17388v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Physical Review Letters, 2025 Sep 5",
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13707v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13707v2",
                "updated": "2025-09-08T09:05:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    5,
                    14,
                    0,
                    251,
                    0
                ],
                "published": "2025-04-18T14:11:27Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    14,
                    11,
                    27,
                    4,
                    108,
                    0
                ],
                "title": "OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via\n  Open-ended Interaction Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via\n  Open-ended Interaction Simulation"
                },
                "summary": "As the general capabilities of large language models (LLMs) improve and agent\napplications become more widespread, the underlying deception risks urgently\nrequire systematic evaluation and effective oversight. Unlike existing\nevaluation which uses simulated games or presents limited choices, we introduce\nOpenDeception, a novel deception evaluation framework with an open-ended\nscenario dataset. OpenDeception jointly evaluates both the deception intention\nand capabilities of LLM-based agents by inspecting their internal reasoning\nprocess. Specifically, we construct five types of common use cases where LLMs\nintensively interact with the user, each consisting of ten diverse, concrete\nscenarios from the real world. To avoid ethical concerns and costs of high-risk\ndeceptive interactions with human testers, we propose to simulate the\nmulti-turn dialogue via agent simulation. Extensive evaluation of eleven\nmainstream LLMs on OpenDeception highlights the urgent need to address\ndeception risks and security concerns in LLM-based agents: the deception\nintention ratio across the models exceeds 80%, while the deception success rate\nsurpasses 50%. Furthermore, we observe that LLMs with stronger capabilities do\nexhibit a higher risk of deception, which calls for more alignment efforts on\ninhibiting deceptive behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the general capabilities of large language models (LLMs) improve and agent\napplications become more widespread, the underlying deception risks urgently\nrequire systematic evaluation and effective oversight. Unlike existing\nevaluation which uses simulated games or presents limited choices, we introduce\nOpenDeception, a novel deception evaluation framework with an open-ended\nscenario dataset. OpenDeception jointly evaluates both the deception intention\nand capabilities of LLM-based agents by inspecting their internal reasoning\nprocess. Specifically, we construct five types of common use cases where LLMs\nintensively interact with the user, each consisting of ten diverse, concrete\nscenarios from the real world. To avoid ethical concerns and costs of high-risk\ndeceptive interactions with human testers, we propose to simulate the\nmulti-turn dialogue via agent simulation. Extensive evaluation of eleven\nmainstream LLMs on OpenDeception highlights the urgent need to address\ndeception risks and security concerns in LLM-based agents: the deception\nintention ratio across the models exceeds 80%, while the deception success rate\nsurpasses 50%. Furthermore, we observe that LLMs with stronger capabilities do\nexhibit a higher risk of deception, which calls for more alignment efforts on\ninhibiting deceptive behaviors."
                },
                "authors": [
                    {
                        "name": "Yichen Wu"
                    },
                    {
                        "name": "Xudong Pan"
                    },
                    {
                        "name": "Geng Hong"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13707v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13707v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]