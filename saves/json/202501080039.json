[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.19919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19919v2",
                "updated": "2025-01-06T15:59:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    15,
                    59,
                    23,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-27T20:47:23Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    20,
                    47,
                    23,
                    4,
                    362,
                    0
                ],
                "title": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)"
                },
                "summary": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family."
                },
                "authors": [
                    {
                        "name": "Austin Kaczmarek"
                    },
                    {
                        "name": "Andrea Capa Salinas"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Katja C. Nowack"
                    }
                ],
                "author_detail": {
                    "name": "Katja C. Nowack"
                },
                "author": "Katja C. Nowack",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02803v1",
                "updated": "2025-01-06T06:44:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T06:44:13Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "title": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism"
                },
                "summary": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions."
                },
                "authors": [
                    {
                        "name": "Yimin Tang"
                    },
                    {
                        "name": "Zhenghong Yu"
                    },
                    {
                        "name": "Yi Zheng"
                    },
                    {
                        "name": "T. K. Satish Kumar"
                    },
                    {
                        "name": "Jiaoyang Li"
                    },
                    {
                        "name": "Sven Koenig"
                    }
                ],
                "author_detail": {
                    "name": "Sven Koenig"
                },
                "author": "Sven Koenig",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2403.13421",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v2",
                "updated": "2025-01-06T01:26:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    26,
                    42,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v2",
                "updated": "2025-01-05T14:11:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    14,
                    11,
                    48,
                    6,
                    5,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe"
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Update performance in MLVU-dev and LVBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02524v1",
                "updated": "2025-01-05T12:51:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T12:51:08Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "title": "A Full-System Simulation Framework for CXL-Based SSD Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Full-System Simulation Framework for CXL-Based SSD Memory System"
                },
                "summary": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim."
                },
                "authors": [
                    {
                        "name": "Yaohui Wang"
                    },
                    {
                        "name": "Zicong Wang"
                    },
                    {
                        "name": "Fanfeng Meng"
                    },
                    {
                        "name": "Yanjing Wang"
                    },
                    {
                        "name": "Yang Ou"
                    },
                    {
                        "name": "Lizhou Wu"
                    },
                    {
                        "name": "Wentao Hong"
                    },
                    {
                        "name": "Xuran Ge"
                    },
                    {
                        "name": "Jijun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jijun Cao"
                },
                "author": "Jijun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v1",
                "updated": "2025-01-05T07:41:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "11 pages, 15 figures, and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v1",
                "updated": "2025-01-04T20:59:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01805v1",
                "updated": "2025-01-03T13:32:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T13:32:57Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "title": "End-to-End Long Document Summarization using Gradient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Long Document Summarization using Gradient Caching"
                },
                "summary": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters."
                },
                "authors": [
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Frank Keller"
                    }
                ],
                "author_detail": {
                    "name": "Frank Keller"
                },
                "author": "Frank Keller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01792v1",
                "updated": "2025-01-03T12:51:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T12:51:37Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "title": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching"
                },
                "summary": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache."
                },
                "authors": [
                    {
                        "name": "Sanghyeon Lee"
                    },
                    {
                        "name": "Hongbeen Kim"
                    },
                    {
                        "name": "Soojin Hwang"
                    },
                    {
                        "name": "Guseul Heo"
                    },
                    {
                        "name": "Minwoo Noh"
                    },
                    {
                        "name": "Jaehyuk Huh"
                    }
                ],
                "author_detail": {
                    "name": "Jaehyuk Huh"
                },
                "author": "Jaehyuk Huh",
                "arxiv_comment": "14 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01424v1",
                "updated": "2025-01-02T18:59:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T18:59:44Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "title": "Object-level Visual Prompts for Compositional Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-level Visual Prompts for Compositional Image Generation"
                },
                "summary": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation."
                },
                "authors": [
                    {
                        "name": "Gaurav Parmar"
                    },
                    {
                        "name": "Or Patashnik"
                    },
                    {
                        "name": "Kuan-Chieh Wang"
                    },
                    {
                        "name": "Daniil Ostashev"
                    },
                    {
                        "name": "Srinivasa Narasimhan"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "name": "Daniel Cohen-Or"
                    },
                    {
                        "name": "Kfir Aberman"
                    }
                ],
                "author_detail": {
                    "name": "Kfir Aberman"
                },
                "author": "Kfir Aberman",
                "arxiv_comment": "Project: https://snap-research.github.io/visual-composer/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01039v1",
                "updated": "2025-01-02T03:41:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T03:41:32Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "title": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention"
                },
                "summary": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shivank Nag"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Lu Tian"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v2",
                "updated": "2025-01-02T03:40:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    40,
                    15,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v1",
                "updated": "2025-01-02T02:02:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "code available at http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00946v1",
                "updated": "2025-01-01T20:16:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T20:16:27Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "title": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model"
                },
                "summary": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches."
                },
                "authors": [
                    {
                        "name": "Omid Saghatchian"
                    },
                    {
                        "name": "Atiyeh Gh. Moghadam"
                    },
                    {
                        "name": "Ahmad Nickabadi"
                    }
                ],
                "author_detail": {
                    "name": "Ahmad Nickabadi"
                },
                "author": "Ahmad Nickabadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00799v1",
                "updated": "2025-01-01T10:50:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    10,
                    50,
                    35,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T10:50:35Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    10,
                    50,
                    35,
                    2,
                    1,
                    0
                ],
                "title": "Follow The Sparse Approximate Leader for No-Regret Online Sparse Linear\n  Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow The Sparse Approximate Leader for No-Regret Online Sparse Linear\n  Approximation"
                },
                "summary": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy."
                },
                "authors": [
                    {
                        "name": "Samrat Mukhopadhyay"
                    },
                    {
                        "name": "Debasmita Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Debasmita Mukherjee"
                },
                "author": "Debasmita Mukherjee",
                "arxiv_comment": "12 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21023v2",
                "updated": "2024-12-31T20:40:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    20,
                    40,
                    43,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-30T15:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    46,
                    53,
                    0,
                    365,
                    0
                ],
                "title": "EdgeRAG: Online-Indexed RAG for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeRAG: Online-Indexed RAG for Edge Devices"
                },
                "summary": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory."
                },
                "authors": [
                    {
                        "name": "Korakit Seemakhupt"
                    },
                    {
                        "name": "Sihang Liu"
                    },
                    {
                        "name": "Samira Khan"
                    }
                ],
                "author_detail": {
                    "name": "Samira Khan"
                },
                "author": "Samira Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00375v1",
                "updated": "2024-12-31T09:56:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T09:56:40Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "title": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free"
                },
                "summary": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17."
                },
                "authors": [
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Bang Xiao"
                    },
                    {
                        "name": "Jiayi Tang"
                    },
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v3",
                "updated": "2024-12-31T07:11:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    7,
                    11,
                    0,
                    1,
                    366,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v1",
                "updated": "2024-12-31T05:24:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00243v1",
                "updated": "2024-12-31T03:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T03:19:38Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "title": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition"
                },
                "summary": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}"
                },
                "authors": [
                    {
                        "name": "Edwin Arkel Rios"
                    },
                    {
                        "name": "Jansen Christopher Yuanda"
                    },
                    {
                        "name": "Vincent Leon Ghanz"
                    },
                    {
                        "name": "Cheng-Wei Yu"
                    },
                    {
                        "name": "Bo-Cheng Lai"
                    },
                    {
                        "name": "Min-Chun Hu"
                    }
                ],
                "author_detail": {
                    "name": "Min-Chun Hu"
                },
                "author": "Min-Chun Hu",
                "arxiv_comment": "Accepted to ICASSP 2025. Main: 5 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v1",
                "updated": "2024-12-30T15:33:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: A System for Efficient Annotation of Map Query Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: A System for Efficient Annotation of Map Query Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "13 pages, 35 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v3",
                "updated": "2024-12-30T14:54:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    54,
                    29,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20887v1",
                "updated": "2024-12-30T11:54:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T11:54:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field"
                },
                "summary": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime."
                },
                "authors": [
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Hanbyul Kim"
                    },
                    {
                        "name": "Xinbo Wang"
                    },
                    {
                        "name": "Jianlin Luo"
                    },
                    {
                        "name": "Simone Latini"
                    },
                    {
                        "name": "Dongbin Shin"
                    },
                    {
                        "name": "Jun-Ming Liu"
                    },
                    {
                        "name": "Jing-Feng Li"
                    },
                    {
                        "name": "Angel Rubio"
                    },
                    {
                        "name": "Ce-Wen Nan"
                    },
                    {
                        "name": "Qian Li"
                    }
                ],
                "author_detail": {
                    "name": "Qian Li"
                },
                "author": "Qian Li",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v2",
                "updated": "2024-12-30T05:01:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    1,
                    44,
                    0,
                    365,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20677v1",
                "updated": "2024-12-30T03:05:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T03:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA"
                },
                "summary": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning."
                },
                "authors": [
                    {
                        "name": "Qingyun Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Zengchang Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zengchang Qin"
                },
                "author": "Zengchang Qin",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00068v1",
                "updated": "2024-12-29T17:41:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:41:40Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques"
                },
                "summary": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20524v1",
                "updated": "2024-12-29T17:18:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:18:21Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "title": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation"
                },
                "summary": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes."
                },
                "authors": [
                    {
                        "name": "Anatolij Zubow"
                    },
                    {
                        "name": "Yannik Pilz"
                    },
                    {
                        "name": "Sascha Rsler"
                    },
                    {
                        "name": "Falko Dressler"
                    }
                ],
                "author_detail": {
                    "name": "Falko Dressler"
                },
                "author": "Falko Dressler",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20221v1",
                "updated": "2024-12-28T17:17:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T17:17:03Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "title": "Revisiting Cache Freshness for Emerging Real-Time Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Cache Freshness for Emerging Real-Time Applications"
                },
                "summary": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness."
                },
                "authors": [
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Rishabh Iyer"
                    },
                    {
                        "name": "Scott Shenker"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_doi": "10.1145/3696348.3696858",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696348.3696858",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.20221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "HotNets '24",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20166v1",
                "updated": "2024-12-28T14:38:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T14:38:16Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System"
                },
                "summary": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications."
                },
                "authors": [
                    {
                        "name": "Hyucksung Kwon"
                    },
                    {
                        "name": "Kyungmo Koo"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "Woongkyu Lee"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyungdeok Lee"
                    },
                    {
                        "name": "Yousub Jung"
                    },
                    {
                        "name": "Jaehan Park"
                    },
                    {
                        "name": "Yosub Song"
                    },
                    {
                        "name": "Byeongsu Yang"
                    },
                    {
                        "name": "Haerang Choi"
                    },
                    {
                        "name": "Guhyun Kim"
                    },
                    {
                        "name": "Jongsoon Won"
                    },
                    {
                        "name": "Woojae Shin"
                    },
                    {
                        "name": "Changhyun Kim"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Yongkee Kwon"
                    },
                    {
                        "name": "Ilkon Kim"
                    },
                    {
                        "name": "Euicheol Lim"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20105v1",
                "updated": "2024-12-28T10:17:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T10:17:29Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "title": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming"
                },
                "summary": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference."
                },
                "authors": [
                    {
                        "name": "Jiedong Zhuang"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Ming Dai"
                    },
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Haoji Hu"
                    }
                ],
                "author_detail": {
                    "name": "Haoji Hu"
                },
                "author": "Haoji Hu",
                "arxiv_comment": "Accepted to AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19991v1",
                "updated": "2024-12-28T03:28:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T03:28:52Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "title": "A Robust Federated Learning Framework for Undependable Devices at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Robust Federated Learning Framework for Undependable Devices at Scale"
                },
                "summary": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shilong Wang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Chunming Qiao"
                    },
                    {
                        "name": "Huarong Deng"
                    },
                    {
                        "name": "Qiuye Zheng"
                    },
                    {
                        "name": "Jiantao Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jiantao Gong"
                },
                "author": "Jiantao Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19255v1",
                "updated": "2024-12-26T15:45:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T15:45:45Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "title": "Multi-matrix Factorization Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-matrix Factorization Attention"
                },
                "summary": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    }
                ],
                "author_detail": {
                    "name": "Heung-Yeung Shum"
                },
                "author": "Heung-Yeung Shum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19051v1",
                "updated": "2024-12-26T04:13:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T04:13:52Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "title": "Performance Characterization and Optimizations of Traditional ML\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterization and Optimizations of Traditional ML\n  Applications"
                },
                "summary": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement."
                },
                "authors": [
                    {
                        "name": "Harsh Kumar"
                    },
                    {
                        "name": "R. Govindarajan"
                    }
                ],
                "author_detail": {
                    "name": "R. Govindarajan"
                },
                "author": "R. Govindarajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18960v1",
                "updated": "2024-12-25T18:36:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T18:36:21Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "title": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems"
                },
                "summary": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming."
                },
                "authors": [
                    {
                        "name": "Nader Alfares"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v1",
                "updated": "2024-12-25T14:14:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories"
                },
                "summary": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "23 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18911v1",
                "updated": "2024-12-25T14:00:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:00:14Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Dual Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Dual Feature Caching"
                },
                "summary": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}"
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Haohang Xu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18885v1",
                "updated": "2024-12-25T11:59:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T11:59:17Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "title": "Aspect-oriented Programming with Julia",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-oriented Programming with Julia"
                },
                "summary": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems."
                },
                "authors": [
                    {
                        "name": "Osamu Ishimura"
                    },
                    {
                        "name": "Yoshihide Yoshimoto"
                    }
                ],
                "author_detail": {
                    "name": "Yoshihide Yoshimoto"
                },
                "author": "Yoshihide Yoshimoto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16187v2",
                "updated": "2024-12-24T13:04:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    4,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-13T06:00:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    0,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing"
                },
                "summary": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks."
                },
                "authors": [
                    {
                        "name": "Minghui Liu"
                    },
                    {
                        "name": "Tahseen Rabbani"
                    },
                    {
                        "name": "Tony O'Halloran"
                    },
                    {
                        "name": "Ananth Sankaralingam"
                    },
                    {
                        "name": "Mary-Anne Hartley"
                    },
                    {
                        "name": "Brian Gravelle"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Cornelia Fermller"
                    },
                    {
                        "name": "Yiannis Aloimonos"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Aloimonos"
                },
                "author": "Yiannis Aloimonos",
                "arxiv_comment": "10 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v2",
                "updated": "2024-12-24T00:46:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    0,
                    46,
                    0,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17747v1",
                "updated": "2024-12-23T18:02:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T18:02:25Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "title": "Deliberation in Latent Space via Differentiable Cache Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deliberation in Latent Space via Differentiable Cache Augmentation"
                },
                "summary": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks."
                },
                "authors": [
                    {
                        "name": "Luyang Liu"
                    },
                    {
                        "name": "Jonas Pfeiffer"
                    },
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Arthur Szlam"
                    }
                ],
                "author_detail": {
                    "name": "Arthur Szlam"
                },
                "author": "Arthur Szlam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17685v1",
                "updated": "2024-12-23T16:11:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T16:11:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment"
                },
                "summary": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application."
                },
                "authors": [
                    {
                        "name": "Edward J. Oughton"
                    },
                    {
                        "name": "Evan Alexander Peters"
                    },
                    {
                        "name": "Dennies Bor"
                    },
                    {
                        "name": "Noah Rivera"
                    },
                    {
                        "name": "C. Trevor Gaunt"
                    },
                    {
                        "name": "Robert Weigel"
                    }
                ],
                "author_detail": {
                    "name": "Robert Weigel"
                },
                "author": "Robert Weigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18919v2",
                "updated": "2024-12-23T14:40:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    40,
                    26,
                    0,
                    358,
                    0
                ],
                "published": "2024-05-29T09:22:25Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    9,
                    22,
                    25,
                    2,
                    150,
                    0
                ],
                "title": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN"
                },
                "summary": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Chenyu Wu"
                    },
                    {
                        "name": "Shuai Han"
                    },
                    {
                        "name": "Weixiao Meng"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "14 pages, 13 figures. This work has been accepted by IEEE Internet of\n  Things Journal. It is expanded on our previous research presented at the IEEE\n  Globecom 2024: Q. Chen, C. Wu, S. Han, W. Meng, and T. Q. Quek, \"Exploiting\n  Inter-Satellite Links for In-Flight Connectivity Scheme in Space-Air-Ground\n  Integrated Networks,\" in Proc. GLOBECOM 2024, Cape Town, South Africa, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03408v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03408v3",
                "updated": "2024-12-23T12:55:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    55,
                    21,
                    0,
                    358,
                    0
                ],
                "published": "2024-02-05T15:10:42Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    15,
                    10,
                    42,
                    0,
                    36,
                    0
                ],
                "title": "A Framework for Effective Invocation Methods of Various LLM Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Effective Invocation Methods of Various LLM Services"
                },
                "summary": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research."
                },
                "authors": [
                    {
                        "name": "Can Wang"
                    },
                    {
                        "name": "Dianbo Sui"
                    },
                    {
                        "name": "Bolin Zhang"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Jiabao Kang"
                    },
                    {
                        "name": "Zhidong Qiao"
                    },
                    {
                        "name": "Zhiying Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiying Tu"
                },
                "author": "Zhiying Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03408v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03408v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17464v1",
                "updated": "2024-12-23T10:41:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T10:41:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "CALLIC: Content Adaptive Learning for Lossless Image Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALLIC: Content Adaptive Learning for Lossless Image Compression"
                },
                "summary": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression."
                },
                "authors": [
                    {
                        "name": "Daxin Li"
                    },
                    {
                        "name": "Yuanchao Bai"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Junjun Jiang"
                    },
                    {
                        "name": "Xianming Liu"
                    },
                    {
                        "name": "Wen Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Gao"
                },
                "author": "Wen Gao",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17246v1",
                "updated": "2024-12-23T03:38:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T03:38:46Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "title": "Fast and Live Model Auto Scaling with O(1) Host Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Live Model Auto Scaling with O(1) Host Caching"
                },
                "summary": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling."
                },
                "authors": [
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05831v2",
                "updated": "2024-12-23T02:52:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    2,
                    52,
                    36,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-08T06:37:27Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "title": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval"
                },
                "summary": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval."
                },
                "authors": [
                    {
                        "name": "Shanti Stewart"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Lie Lu"
                    },
                    {
                        "name": "Andrea Fanelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Fanelli"
                },
                "author": "Andrea Fanelli",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17203v1",
                "updated": "2024-12-23T00:46:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T00:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "title": "Agile TLB Prefetching and Prediction Replacement Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agile TLB Prefetching and Prediction Replacement Policy"
                },
                "summary": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management."
                },
                "authors": [
                    {
                        "name": "Melkamu Mersha"
                    },
                    {
                        "name": "Tsion Abay"
                    },
                    {
                        "name": "Mingziem Bitewa"
                    },
                    {
                        "name": "Gedare Bloom"
                    }
                ],
                "author_detail": {
                    "name": "Gedare Bloom"
                },
                "author": "Gedare Bloom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16897v1",
                "updated": "2024-12-22T07:14:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "published": "2024-12-22T07:14:45Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "title": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context"
                },
                "summary": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC"
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Fangjian Liao"
                    },
                    {
                        "name": "Zeqi Ma"
                    },
                    {
                        "name": "Rongchen Zhang"
                    },
                    {
                        "name": "Dongmei Mo"
                    },
                    {
                        "name": "Waikeung Wong"
                    }
                ],
                "author_detail": {
                    "name": "Waikeung Wong"
                },
                "author": "Waikeung Wong",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17565v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17565v3",
                "updated": "2024-12-21T13:55:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    13,
                    55,
                    49,
                    5,
                    356,
                    0
                ],
                "published": "2024-06-25T14:02:08Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    14,
                    2,
                    8,
                    1,
                    177,
                    0
                ],
                "title": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool"
                },
                "summary": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time."
                },
                "authors": [
                    {
                        "name": "Cunchen Hu"
                    },
                    {
                        "name": "Heyang Huang"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Jiang Xu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Sa Wang"
                    },
                    {
                        "name": "Yungang Bao"
                    },
                    {
                        "name": "Ninghui Sun"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17565v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17565v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16585v1",
                "updated": "2024-12-21T11:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T11:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "title": "Parameterized Complexity of Caching in Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameterized Complexity of Caching in Networks"
                },
                "summary": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable."
                },
                "authors": [
                    {
                        "name": "Robert Ganian"
                    },
                    {
                        "name": "Fionn Mc Inerney"
                    },
                    {
                        "name": "Dimitra Tsigkari"
                    }
                ],
                "author_detail": {
                    "name": "Dimitra Tsigkari"
                },
                "author": "Dimitra Tsigkari",
                "arxiv_comment": "A shorter version of this paper will appear in the proceedings of\n  AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v4",
                "updated": "2024-12-21T02:36:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    2,
                    36,
                    3,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16434v1",
                "updated": "2024-12-21T01:48:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T01:48:52Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "title": "SYMPHONY: Improving Memory Management for LLM Inference Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SYMPHONY: Improving Memory Management for LLM Inference Workloads"
                },
                "summary": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile."
                },
                "authors": [
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Anyong Mao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16001v1",
                "updated": "2024-12-20T15:51:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T15:51:42Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "title": "Multi-Strided Access Patterns to Boost Hardware Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Strided Access Patterns to Boost Hardware Prefetching"
                },
                "summary": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future."
                },
                "authors": [
                    {
                        "name": "Miguel O. Blom"
                    },
                    {
                        "name": "Kristian F. D. Rietveld"
                    },
                    {
                        "name": "Rob V. van Nieuwpoort"
                    }
                ],
                "author_detail": {
                    "name": "Rob V. van Nieuwpoort"
                },
                "author": "Rob V. van Nieuwpoort",
                "arxiv_comment": "12 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14485v2",
                "updated": "2024-12-20T15:18:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    18,
                    44,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-19T03:11:33Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    3,
                    11,
                    33,
                    3,
                    354,
                    0
                ],
                "title": "Towards Projected and Incremental Pseudo-Boolean Model Counting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Projected and Incremental Pseudo-Boolean Model Counting"
                },
                "summary": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting."
                },
                "authors": [
                    {
                        "name": "Suwei Yang"
                    },
                    {
                        "name": "Kuldeep S. Meel"
                    }
                ],
                "author_detail": {
                    "name": "Kuldeep S. Meel"
                },
                "author": "Kuldeep S. Meel",
                "arxiv_comment": "To appear in AAAI25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15605v1",
                "updated": "2024-12-20T06:58:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T06:58:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v3",
                "updated": "2024-12-19T23:52:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    23,
                    52,
                    16,
                    3,
                    354,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v2",
                "updated": "2024-12-19T22:34:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    22,
                    34,
                    37,
                    3,
                    354,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jimnez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v1",
                "updated": "2024-12-19T13:28:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v3",
                "updated": "2024-12-19T12:38:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    38,
                    23,
                    3,
                    354,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "In this version, we achieved a nearly lossless acceleration of 1.51\n  times for ToCa on FLUX in the appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14392v1",
                "updated": "2024-12-18T22:52:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:52:12Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "title": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems"
                },
                "summary": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies."
                },
                "authors": [
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14363v1",
                "updated": "2024-12-18T22:01:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:01:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals"
                },
                "summary": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Sayeh Sharify"
                    },
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "14 pages, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v1",
                "updated": "2024-12-18T21:09:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v4",
                "updated": "2024-12-18T17:36:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    36,
                    36,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13779v1",
                "updated": "2024-12-18T12:16:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:16:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization"
                },
                "summary": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13771v1",
                "updated": "2024-12-18T12:07:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:07:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization"
                },
                "summary": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems."
                },
                "authors": [
                    {
                        "name": "Guanghan Li"
                    },
                    {
                        "name": "Xun Zhang"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Yifan Yin"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "arxiv_comment": "7 pages, 3 figures, AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v2",
                "updated": "2024-12-18T09:47:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    47,
                    25,
                    2,
                    353,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13649v1",
                "updated": "2024-12-18T09:27:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T09:27:33Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation"
                },
                "summary": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods."
                },
                "authors": [
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Yilong Lai"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08584v2",
                "updated": "2024-12-18T07:45:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    7,
                    45,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-11T07:24:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification"
                },
                "summary": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs."
                },
                "authors": [
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13509v1",
                "updated": "2024-12-18T05:16:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T05:16:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation"
                },
                "summary": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization."
                },
                "authors": [
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Kaiyuan Hou"
                    },
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12486v2",
                "updated": "2024-12-18T05:08:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    8,
                    39,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-17T02:43:54Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    43,
                    54,
                    1,
                    352,
                    0
                ],
                "title": "Boosting Long-Context Management via Query-Guided Activation Refilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Long-Context Management via Query-Guided Activation Refilling"
                },
                "summary": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Defu Lian"
                    }
                ],
                "author_detail": {
                    "name": "Defu Lian"
                },
                "author": "Defu Lian",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v3",
                "updated": "2024-12-17T14:45:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    45,
                    12,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Code is available at https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12953v1",
                "updated": "2024-12-17T14:34:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:34:51Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "title": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning"
                },
                "summary": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/."
                },
                "authors": [
                    {
                        "name": "Moritz Reuss"
                    },
                    {
                        "name": "Jyothish Pari"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Rudolf Lioutikov"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Lioutikov"
                },
                "author": "Rudolf Lioutikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12798v1",
                "updated": "2024-12-17T11:00:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T11:00:56Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "title": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation"
                },
                "summary": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI."
                },
                "authors": [
                    {
                        "name": "Shiqi Huang"
                    },
                    {
                        "name": "Shuting He"
                    },
                    {
                        "name": "Bihan Wen"
                    }
                ],
                "author_detail": {
                    "name": "Bihan Wen"
                },
                "author": "Bihan Wen",
                "arxiv_comment": "AAAI 2025, code see https://github.com/HuangShiqi128/ZoRI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v1",
                "updated": "2024-12-17T09:20:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v2",
                "updated": "2024-12-17T09:11:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    11,
                    47,
                    1,
                    352,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08585v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08585v3",
                "updated": "2024-12-17T05:40:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    40,
                    9,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-11T18:03:05Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    3,
                    5,
                    2,
                    346,
                    0
                ],
                "title": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs"
                },
                "summary": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "James Hensman"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Victor Ruhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08585v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08585v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12543v1",
                "updated": "2024-12-17T05:09:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T05:09:45Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "title": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks"
                },
                "summary": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks."
                },
                "authors": [
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Tan Li"
                    },
                    {
                        "name": "Hai Liu"
                    },
                    {
                        "name": "Tse-Tin Chan"
                    }
                ],
                "author_detail": {
                    "name": "Tse-Tin Chan"
                },
                "author": "Tse-Tin Chan",
                "arxiv_comment": "8 pages, 8 figures, WiOpt 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12488v1",
                "updated": "2024-12-17T02:44:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T02:44:43Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "title": "A System for Microserving of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A System for Microserving of LLMs"
                },
                "summary": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies."
                },
                "authors": [
                    {
                        "name": "Hongyi Jin"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Charlie F. Ruan"
                    },
                    {
                        "name": "Yingcheng Wang"
                    },
                    {
                        "name": "Todd C. Mowry"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Tianqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianqi Chen"
                },
                "author": "Tianqi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12444v1",
                "updated": "2024-12-17T01:12:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T01:12:35Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers"
                },
                "summary": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency."
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yanyu Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Zhihao Shu"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11828v1",
                "updated": "2024-12-16T14:49:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T14:49:32Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "title": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey"
                },
                "summary": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11741v1",
                "updated": "2024-12-16T13:01:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:01:53Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "title": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation"
                },
                "summary": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments."
                },
                "authors": [
                    {
                        "name": "Hongxuan Zhang"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v1",
                "updated": "2024-12-16T12:28:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11685v1",
                "updated": "2024-12-16T11:55:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T11:55:26Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "title": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning"
                },
                "summary": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU."
                },
                "authors": [
                    {
                        "name": "Xingchi Chen"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    },
                    {
                        "name": "Xuerui Li"
                    },
                    {
                        "name": "Yuying Chen"
                    },
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Wenqi Ren"
                    }
                ],
                "author_detail": {
                    "name": "Wenqi Ren"
                },
                "author": "Wenqi Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14201v1",
                "updated": "2024-12-15T21:02:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T21:02:16Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "title": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models"
                },
                "summary": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint."
                },
                "authors": [
                    {
                        "name": "Boris Ruf"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "arxiv_comment": "Presented at the 18th IEEE International Workshop on Multimedia\n  Technologies for E-Learning (MTEL), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.02388v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.02388v3",
                "updated": "2024-12-15T03:29:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    3,
                    29,
                    54,
                    6,
                    350,
                    0
                ],
                "published": "2023-05-03T19:07:06Z",
                "published_parsed": [
                    2023,
                    5,
                    3,
                    19,
                    7,
                    6,
                    2,
                    123,
                    0
                ],
                "title": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)"
                },
                "summary": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone."
                },
                "authors": [
                    {
                        "name": "Yupeng Tang"
                    },
                    {
                        "name": "Seung-seob Lee"
                    },
                    {
                        "name": "Abhishek Bhattacharjee"
                    },
                    {
                        "name": "Anurag Khandelwal"
                    }
                ],
                "author_detail": {
                    "name": "Anurag Khandelwal"
                },
                "author": "Anurag Khandelwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.02388v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.02388v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11021v1",
                "updated": "2024-12-15T02:30:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T02:30:09Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "title": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array"
                },
                "summary": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works."
                },
                "authors": [
                    {
                        "name": "Xiaobing Ni"
                    },
                    {
                        "name": "Mengke Ge"
                    },
                    {
                        "name": "Jiaheng Ruan"
                    },
                    {
                        "name": "Song Chen"
                    },
                    {
                        "name": "Yi Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Kang"
                },
                "author": "Yi Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15246v1",
                "updated": "2024-12-14T06:47:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    6,
                    47,
                    56,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T06:47:56Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    6,
                    47,
                    56,
                    5,
                    349,
                    0
                ],
                "title": "Accelerating Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Retrieval-Augmented Generation"
                },
                "summary": "An evolving solution to address hallucination and enhance accuracy in large\nlanguage models (LLMs) is Retrieval-Augmented Generation (RAG), which involves\naugmenting LLMs with information retrieved from an external knowledge source,\nsuch as the web. This paper profiles several RAG execution pipelines and\ndemystifies the complex interplay between their retrieval and generation\nphases. We demonstrate that while exact retrieval schemes are expensive, they\ncan reduce inference time compared to approximate retrieval variants because an\nexact retrieval model can send a smaller but more accurate list of documents to\nthe generative model while maintaining the same end-to-end accuracy. This\nobservation motivates the acceleration of the exact nearest neighbor search for\nRAG.\n  In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL\ndevice that implements a scale-out near-memory acceleration architecture with a\nnovel cache-coherent interface between the host CPU and near-memory\naccelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a\n512GB vector database compared with executing the search on Intel Sapphire\nRapids CPUs. This higher search performance translates to 1.7-26.3x lower\nend-to-end inference time for representative RAG applications. IKS is\ninherently a memory expander; its internal DRAM can be disaggregated and used\nfor other applications running on the server to prevent DRAM, which is the most\nexpensive component in today's servers, from being stranded.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An evolving solution to address hallucination and enhance accuracy in large\nlanguage models (LLMs) is Retrieval-Augmented Generation (RAG), which involves\naugmenting LLMs with information retrieved from an external knowledge source,\nsuch as the web. This paper profiles several RAG execution pipelines and\ndemystifies the complex interplay between their retrieval and generation\nphases. We demonstrate that while exact retrieval schemes are expensive, they\ncan reduce inference time compared to approximate retrieval variants because an\nexact retrieval model can send a smaller but more accurate list of documents to\nthe generative model while maintaining the same end-to-end accuracy. This\nobservation motivates the acceleration of the exact nearest neighbor search for\nRAG.\n  In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL\ndevice that implements a scale-out near-memory acceleration architecture with a\nnovel cache-coherent interface between the host CPU and near-memory\naccelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a\n512GB vector database compared with executing the search on Intel Sapphire\nRapids CPUs. This higher search performance translates to 1.7-26.3x lower\nend-to-end inference time for representative RAG applications. IKS is\ninherently a memory expander; its internal DRAM can be disaggregated and used\nfor other applications running on the server to prevent DRAM, which is the most\nexpensive component in today's servers, from being stranded."
                },
                "authors": [
                    {
                        "name": "Derrick Quinn"
                    },
                    {
                        "name": "Mohammad Nouri"
                    },
                    {
                        "name": "Neel Patel"
                    },
                    {
                        "name": "John Salihu"
                    },
                    {
                        "name": "Alireza Salemi"
                    },
                    {
                        "name": "Sukhan Lee"
                    },
                    {
                        "name": "Hamed Zamani"
                    },
                    {
                        "name": "Mohammad Alian"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Alian"
                },
                "author": "Mohammad Alian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10685v1",
                "updated": "2024-12-14T05:20:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T05:20:50Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "title": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs"
                },
                "summary": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm."
                },
                "authors": [
                    {
                        "name": "Baljinder Singh Heera"
                    },
                    {
                        "name": "Shrinivas Petale"
                    },
                    {
                        "name": "Yatindra Nath Singh"
                    },
                    {
                        "name": "Suresh Subramaniam"
                    }
                ],
                "author_detail": {
                    "name": "Suresh Subramaniam"
                },
                "author": "Suresh Subramaniam",
                "arxiv_comment": "The preliminary work was presented at ONDM 2023 conference.\n  https://doi.org/10.23919/ONDM57372.2023.10144866",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v1",
                "updated": "2024-12-13T17:59:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10302v1",
                "updated": "2024-12-13T17:37:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:37:48Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding"
                },
                "summary": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2."
                },
                "authors": [
                    {
                        "name": "Zhiyu Wu"
                    },
                    {
                        "name": "Xiaokang Chen"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Xingchao Liu"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Huazuo Gao"
                    },
                    {
                        "name": "Yiyang Ma"
                    },
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Bingxuan Wang"
                    },
                    {
                        "name": "Zhenda Xie"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Yaofeng Sun"
                    },
                    {
                        "name": "Yukun Li"
                    },
                    {
                        "name": "Yishi Piao"
                    },
                    {
                        "name": "Kang Guan"
                    },
                    {
                        "name": "Aixin Liu"
                    },
                    {
                        "name": "Xin Xie"
                    },
                    {
                        "name": "Yuxiang You"
                    },
                    {
                        "name": "Kai Dong"
                    },
                    {
                        "name": "Xingkai Yu"
                    },
                    {
                        "name": "Haowei Zhang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Yisong Wang"
                    },
                    {
                        "name": "Chong Ruan"
                    }
                ],
                "author_detail": {
                    "name": "Chong Ruan"
                },
                "author": "Chong Ruan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18566v2",
                "updated": "2024-12-13T16:13:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    13,
                    39,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-27T18:09:29Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "title": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software"
                },
                "summary": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software."
                },
                "authors": [
                    {
                        "name": "Oliver Maximilian Zobel"
                    },
                    {
                        "name": "Johannes Maierhofer"
                    },
                    {
                        "name": "Andreas Kstler"
                    },
                    {
                        "name": "Daniel J. Rixen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Rixen"
                },
                "author": "Daniel J. Rixen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v1",
                "updated": "2024-12-13T14:11:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12021v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12021v2",
                "updated": "2024-12-13T14:08:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    8,
                    55,
                    4,
                    348,
                    0
                ],
                "published": "2024-09-18T14:31:33Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "title": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues"
                },
                "summary": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized)."
                },
                "authors": [
                    {
                        "name": "Thore Thieen"
                    },
                    {
                        "name": "Jan Vahrenhold"
                    }
                ],
                "author_detail": {
                    "name": "Jan Vahrenhold"
                },
                "author": "Jan Vahrenhold",
                "arxiv_doi": "10.4230/LIPIcs.ISAAC.2024.55",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ISAAC.2024.55",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12021v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12021v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, full version of the paper in ISAAC 2024; minor changes",
                "arxiv_journal_ref": "Thore Thie{\\ss}en and Jan Vahrenhold. Optimal offline ORAM with\n  perfect security via simple oblivious priority queues. In 35th International\n  Symposium on Algorithms and Computation (ISAAC 2024), 18 pages. 2024",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12178v1",
                "updated": "2024-12-13T02:26:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T02:26:54Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "title": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models"
                },
                "summary": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize compression\nrate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN)\ncomponents, which typically comprise a large proportion of parameters (around\n3/2), ensure that our FFN optimizations would have a better chance of achieving\neffective compression. Moreover, our findings are beneficial to general LLMs\nand are not restricted to ReLU-based models. This work systematically\ninvestigates the tradeoff between enforcing activation sparsity and perplexity\n(accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that\nwe can obtain around 50% of main memory and computing reductions for critical\nFFN components with negligible accuracy degradation. This extra 50% sparsity\ndoes not naturally exist in the current LLMs, which require tuning LLMs'\nactivation outputs by injecting zero-enforcing thresholds. To obtain the\nbenefits of activation sparsity, we provide a guideline for the system\narchitect for LLM prediction and prefetching. The success prediction allows the\nsystem to prefetch the necessary weights while omitting the inactive ones and\ntheir successors, therefore lowering cache and memory pollution and reducing\nLLM execution time on resource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize compression\nrate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN)\ncomponents, which typically comprise a large proportion of parameters (around\n3/2), ensure that our FFN optimizations would have a better chance of achieving\neffective compression. Moreover, our findings are beneficial to general LLMs\nand are not restricted to ReLU-based models. This work systematically\ninvestigates the tradeoff between enforcing activation sparsity and perplexity\n(accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that\nwe can obtain around 50% of main memory and computing reductions for critical\nFFN components with negligible accuracy degradation. This extra 50% sparsity\ndoes not naturally exist in the current LLMs, which require tuning LLMs'\nactivation outputs by injecting zero-enforcing thresholds. To obtain the\nbenefits of activation sparsity, we provide a guideline for the system\narchitect for LLM prediction and prefetching. The success prediction allows the\nsystem to prefetch the necessary weights while omitting the inactive ones and\ntheir successors, therefore lowering cache and memory pollution and reducing\nLLM execution time on resource-constrained edge devices."
                },
                "authors": [
                    {
                        "name": "Nobel Dhar"
                    },
                    {
                        "name": "Bobin Deng"
                    },
                    {
                        "name": "Md Romyull Islam"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Kun Suo"
                    }
                ],
                "author_detail": {
                    "name": "Kun Suo"
                },
                "author": "Kun Suo",
                "arxiv_comment": "Conference submission for IPCCC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09474v1",
                "updated": "2024-12-12T17:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T17:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "title": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance"
                },
                "summary": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments."
                },
                "authors": [
                    {
                        "name": "Md Nurul Absur"
                    },
                    {
                        "name": "Sourya Saha"
                    },
                    {
                        "name": "Sifat Nawrin Nova"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Md Rahat Ul Nasib"
                    }
                ],
                "author_detail": {
                    "name": "Md Rahat Ul Nasib"
                },
                "author": "Md Rahat Ul Nasib",
                "arxiv_comment": "6 Pages, 10 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09416v1",
                "updated": "2024-12-12T16:24:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T16:24:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors"
                },
                "summary": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v3",
                "updated": "2024-12-12T15:39:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    39,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08760v2",
                "updated": "2024-12-12T14:43:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    14,
                    43,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-10-11T12:19:18Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "title": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation"
                },
                "summary": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL."
                },
                "authors": [
                    {
                        "name": "Konstantin Burlachenko"
                    },
                    {
                        "name": "Peter Richtrik"
                    }
                ],
                "author_detail": {
                    "name": "Peter Richtrik"
                },
                "author": "Peter Richtrik",
                "arxiv_comment": "55 pages, 12 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4; C.3; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06282v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06282v3",
                "updated": "2024-12-12T12:24:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    24,
                    18,
                    3,
                    347,
                    0
                ],
                "published": "2024-06-10T14:01:21Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    14,
                    1,
                    21,
                    0,
                    162,
                    0
                ],
                "title": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone"
                },
                "summary": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation."
                },
                "authors": [
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06282v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06282v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v3",
                "updated": "2024-12-12T12:03:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    3,
                    19,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.03226v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03226v1",
                "updated": "2025-01-06T18:59:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    18,
                    59,
                    13,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T18:59:13Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    18,
                    59,
                    13,
                    0,
                    6,
                    0
                ],
                "title": "BoostStep: Boosting mathematical capability of Large Language Models via\n  improved single-step reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BoostStep: Boosting mathematical capability of Large Language Models via\n  improved single-step reasoning"
                },
                "summary": "Cutting-edge large language models (LLMs) demonstrate promising performance\nin solving complex math problems with a divide-and-conquer pipeline and the\nassistance of in-context learning (ICL) examples. However, their potential for\nimprovement is limited by two critical problems within their ICL examples:\ngranularity-mismatch and the ensuing negative-effect noise problem.\nSpecifically, the LLMs are capable of the dividing process yet mostly failed by\ninaccurate reasoning within a few conquer steps, while the ICL examples\nretrieved in question-grained sometimes lack relevant steps for a specific\nchallenging reasoning step. Further, this disconnect may hinder the correct\nreasoning due to its irrelevance. To this end, we focus on improving the\nreasoning quality within each step and present BoostStep. BoostStep aligns the\ngranularity between the retrieving and reasoning on step grained, and provides\nhighly related ICL examples for each reasoning step with a novel `first-try'\nstrategy. BoostStep provides more relevant examples than the coarse\nquestion-grained strategy, enhancing the model reasoning quality within each\nstep steadily. BoostStep is a general and robust reasoning-enhancing method\nthat not only improves standalone reasoning performance but also integrates\nseamlessly with Monte Carlo Tree Search methods (MCTS) to refine both candidate\ngeneration and decision-making. Quantitatively, it improves GPT-4o and\nQwen2.5-Math-72B by 3.6\\% and 2.0\\% respectively on various mathematical\nbenchmarks, and 7.5\\% gain combined with MCTS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cutting-edge large language models (LLMs) demonstrate promising performance\nin solving complex math problems with a divide-and-conquer pipeline and the\nassistance of in-context learning (ICL) examples. However, their potential for\nimprovement is limited by two critical problems within their ICL examples:\ngranularity-mismatch and the ensuing negative-effect noise problem.\nSpecifically, the LLMs are capable of the dividing process yet mostly failed by\ninaccurate reasoning within a few conquer steps, while the ICL examples\nretrieved in question-grained sometimes lack relevant steps for a specific\nchallenging reasoning step. Further, this disconnect may hinder the correct\nreasoning due to its irrelevance. To this end, we focus on improving the\nreasoning quality within each step and present BoostStep. BoostStep aligns the\ngranularity between the retrieving and reasoning on step grained, and provides\nhighly related ICL examples for each reasoning step with a novel `first-try'\nstrategy. BoostStep provides more relevant examples than the coarse\nquestion-grained strategy, enhancing the model reasoning quality within each\nstep steadily. BoostStep is a general and robust reasoning-enhancing method\nthat not only improves standalone reasoning performance but also integrates\nseamlessly with Monte Carlo Tree Search methods (MCTS) to refine both candidate\ngeneration and decision-making. Quantitatively, it improves GPT-4o and\nQwen2.5-Math-72B by 3.6\\% and 2.0\\% respectively on various mathematical\nbenchmarks, and 7.5\\% gain combined with MCTS."
                },
                "authors": [
                    {
                        "name": "Beichen Zhang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Xiaoyi Dong"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Pan Zhang"
                    },
                    {
                        "name": "Haodong Duan"
                    },
                    {
                        "name": "Yuhang Cao"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang",
                "arxiv_comment": "Codes and Data are available at\n  https://github.com/beichenzbc/BoostStep",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03226v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03218v1",
                "updated": "2025-01-06T18:55:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    18,
                    55,
                    10,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T18:55:10Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    18,
                    55,
                    10,
                    0,
                    6,
                    0
                ],
                "title": "Dispider: Enabling Video LLMs with Active Real-Time Interaction via\n  Disentangled Perception, Decision, and Reaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dispider: Enabling Video LLMs with Active Real-Time Interaction via\n  Disentangled Perception, Decision, and Reaction"
                },
                "summary": "Active Real-time interaction with video LLMs introduces a new paradigm for\nhuman-computer interaction, where the model not only understands user intent\nbut also responds while continuously processing streaming video on the fly.\nUnlike offline video LLMs, which analyze the entire video before answering\nquestions, active real-time interaction requires three capabilities: 1)\nPerception: real-time video monitoring and interaction capturing. 2) Decision:\nraising proactive interaction in proper situations, 3) Reaction: continuous\ninteraction with users. However, inherent conflicts exist among the desired\ncapabilities. The Decision and Reaction require a contrary Perception scale and\ngrain, and the autoregressive decoding blocks the real-time Perception and\nDecision during the Reaction. To unify the conflicted capabilities within a\nharmonious system, we present Dispider, a system that disentangles Perception,\nDecision, and Reaction. Dispider features a lightweight proactive streaming\nvideo processing module that tracks the video stream and identifies optimal\nmoments for interaction. Once the interaction is triggered, an asynchronous\ninteraction module provides detailed responses, while the processing module\ncontinues to monitor the video in the meantime. Our disentangled and\nasynchronous design ensures timely, contextually accurate, and computationally\nefficient responses, making Dispider ideal for active real-time interaction for\nlong-duration video streams. Experiments show that Dispider not only maintains\nstrong performance in conventional video QA tasks, but also significantly\nsurpasses previous online models in streaming scenario responses, thereby\nvalidating the effectiveness of our architecture. The code and model are\nreleased at \\url{https://github.com/Mark12Ding/Dispider}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Real-time interaction with video LLMs introduces a new paradigm for\nhuman-computer interaction, where the model not only understands user intent\nbut also responds while continuously processing streaming video on the fly.\nUnlike offline video LLMs, which analyze the entire video before answering\nquestions, active real-time interaction requires three capabilities: 1)\nPerception: real-time video monitoring and interaction capturing. 2) Decision:\nraising proactive interaction in proper situations, 3) Reaction: continuous\ninteraction with users. However, inherent conflicts exist among the desired\ncapabilities. The Decision and Reaction require a contrary Perception scale and\ngrain, and the autoregressive decoding blocks the real-time Perception and\nDecision during the Reaction. To unify the conflicted capabilities within a\nharmonious system, we present Dispider, a system that disentangles Perception,\nDecision, and Reaction. Dispider features a lightweight proactive streaming\nvideo processing module that tracks the video stream and identifies optimal\nmoments for interaction. Once the interaction is triggered, an asynchronous\ninteraction module provides detailed responses, while the processing module\ncontinues to monitor the video in the meantime. Our disentangled and\nasynchronous design ensures timely, contextually accurate, and computationally\nefficient responses, making Dispider ideal for active real-time interaction for\nlong-duration video streams. Experiments show that Dispider not only maintains\nstrong performance in conventional video QA tasks, but also significantly\nsurpasses previous online models in streaming scenario responses, thereby\nvalidating the effectiveness of our architecture. The code and model are\nreleased at \\url{https://github.com/Mark12Ding/Dispider}."
                },
                "authors": [
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Shuangrui Ding"
                    },
                    {
                        "name": "Xiaoyi Dong"
                    },
                    {
                        "name": "Pan Zhang"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Yuhang Cao"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03212v1",
                "updated": "2025-01-06T18:46:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    18,
                    46,
                    53,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T18:46:53Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    18,
                    46,
                    53,
                    0,
                    6,
                    0
                ],
                "title": "Leveraging Explainable AI for LLM Text Attribution: Differentiating\n  Human-Written and Multiple LLMs-Generated Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Explainable AI for LLM Text Attribution: Differentiating\n  Human-Written and Multiple LLMs-Generated Text"
                },
                "summary": "The development of Generative AI Large Language Models (LLMs) raised the\nalarm regarding identifying content produced through generative AI or humans.\nIn one case, issues arise when students heavily rely on such tools in a manner\nthat can affect the development of their writing or coding skills. Other issues\nof plagiarism also apply. This study aims to support efforts to detect and\nidentify textual content generated using LLM tools. We hypothesize that\nLLMs-generated text is detectable by machine learning (ML), and investigate ML\nmodels that can recognize and differentiate texts generated by multiple LLMs\ntools. We leverage several ML and Deep Learning (DL) algorithms such as Random\nForest (RF), and Recurrent Neural Networks (RNN), and utilized Explainable\nArtificial Intelligence (XAI) to understand the important features in\nattribution. Our method is divided into 1) binary classification to\ndifferentiate between human-written and AI-text, and 2) multi classification,\nto differentiate between human-written text and the text generated by the five\ndifferent LLM tools (ChatGPT, LLaMA, Google Bard, Claude, and Perplexity).\nResults show high accuracy in the multi and binary classification. Our model\noutperformed GPTZero with 98.5\\% accuracy to 78.3\\%. Notably, GPTZero was\nunable to recognize about 4.2\\% of the observations, but our model was able to\nrecognize the complete test dataset. XAI results showed that understanding\nfeature importance across different classes enables detailed author/source\nprofiles. Further, aiding in attribution and supporting plagiarism detection by\nhighlighting unique stylistic and structural elements ensuring robust content\noriginality verification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of Generative AI Large Language Models (LLMs) raised the\nalarm regarding identifying content produced through generative AI or humans.\nIn one case, issues arise when students heavily rely on such tools in a manner\nthat can affect the development of their writing or coding skills. Other issues\nof plagiarism also apply. This study aims to support efforts to detect and\nidentify textual content generated using LLM tools. We hypothesize that\nLLMs-generated text is detectable by machine learning (ML), and investigate ML\nmodels that can recognize and differentiate texts generated by multiple LLMs\ntools. We leverage several ML and Deep Learning (DL) algorithms such as Random\nForest (RF), and Recurrent Neural Networks (RNN), and utilized Explainable\nArtificial Intelligence (XAI) to understand the important features in\nattribution. Our method is divided into 1) binary classification to\ndifferentiate between human-written and AI-text, and 2) multi classification,\nto differentiate between human-written text and the text generated by the five\ndifferent LLM tools (ChatGPT, LLaMA, Google Bard, Claude, and Perplexity).\nResults show high accuracy in the multi and binary classification. Our model\noutperformed GPTZero with 98.5\\% accuracy to 78.3\\%. Notably, GPTZero was\nunable to recognize about 4.2\\% of the observations, but our model was able to\nrecognize the complete test dataset. XAI results showed that understanding\nfeature importance across different classes enables detailed author/source\nprofiles. Further, aiding in attribution and supporting plagiarism detection by\nhighlighting unique stylistic and structural elements ensuring robust content\noriginality verification."
                },
                "authors": [
                    {
                        "name": "Ayat Najjar"
                    },
                    {
                        "name": "Huthaifa I. Ashqar"
                    },
                    {
                        "name": "Omar Darwish"
                    },
                    {
                        "name": "Eman Hammad"
                    }
                ],
                "author_detail": {
                    "name": "Eman Hammad"
                },
                "author": "Eman Hammad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03209v1",
                "updated": "2025-01-06T18:41:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    18,
                    41,
                    56,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T18:41:56Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    18,
                    41,
                    56,
                    0,
                    6,
                    0
                ],
                "title": "Local data of elliptic curves under quadratic twist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local data of elliptic curves under quadratic twist"
                },
                "summary": "Let $K$ be the field of fractions of a complete discrete valuation ring with\na perfect residue field. In this article, we investigate how the Tamagawa\nnumber of $E/K$ changes under quadratic twist. To accomplish this, we introduce\nthe notion of a normal model for $E/K$, which is a Weierstrass model satisfying\ncertain conditions that lead one to easily infer the local data of $E/K$. Our\nmain results provide necessary and sufficient conditions on the Weierstrass\ncoefficients of a normal model of $E/K$ to determine the local data of a\nquadratic twist $E^{d}/K$. We note that when the residue field has\ncharacteristic $2$, we only consider the special case $K=\\mathbb{Q}_{2}$. In\nthis setting, we also determine the minimal discriminant valuation and\nconductor exponent of $E$ and $E^d$ from further conditions on the coefficients\nof a normal model for $E$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let $K$ be the field of fractions of a complete discrete valuation ring with\na perfect residue field. In this article, we investigate how the Tamagawa\nnumber of $E/K$ changes under quadratic twist. To accomplish this, we introduce\nthe notion of a normal model for $E/K$, which is a Weierstrass model satisfying\ncertain conditions that lead one to easily infer the local data of $E/K$. Our\nmain results provide necessary and sufficient conditions on the Weierstrass\ncoefficients of a normal model of $E/K$ to determine the local data of a\nquadratic twist $E^{d}/K$. We note that when the residue field has\ncharacteristic $2$, we only consider the special case $K=\\mathbb{Q}_{2}$. In\nthis setting, we also determine the minimal discriminant valuation and\nconductor exponent of $E$ and $E^d$ from further conditions on the coefficients\nof a normal model for $E$."
                },
                "authors": [
                    {
                        "name": "Alexander J. Barrios"
                    },
                    {
                        "name": "Manami Roy"
                    },
                    {
                        "name": "Nandita Sahajpal"
                    },
                    {
                        "name": "Darwin Tallana"
                    },
                    {
                        "name": "Bella Tobin"
                    },
                    {
                        "name": "Hanneke Wiersema"
                    }
                ],
                "author_detail": {
                    "name": "Hanneke Wiersema"
                },
                "author": "Hanneke Wiersema",
                "arxiv_comment": "40 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03203v1",
                "updated": "2025-01-06T18:34:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    18,
                    34,
                    20,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T18:34:20Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    18,
                    34,
                    20,
                    0,
                    6,
                    0
                ],
                "title": "Detecting AI-Generated Text in Educational Content: Leveraging Machine\n  Learning and Explainable AI for Academic Integrity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting AI-Generated Text in Educational Content: Leveraging Machine\n  Learning and Explainable AI for Academic Integrity"
                },
                "summary": "This study seeks to enhance academic integrity by providing tools to detect\nAI-generated content in student work using advanced technologies. The findings\npromote transparency and accountability, helping educators maintain ethical\nstandards and supporting the responsible integration of AI in education. A key\ncontribution of this work is the generation of the CyberHumanAI dataset, which\nhas 1000 observations, 500 of which are written by humans and the other 500\nproduced by ChatGPT. We evaluate various machine learning (ML) and deep\nlearning (DL) algorithms on the CyberHumanAI dataset comparing human-written\nand AI-generated content from Large Language Models (LLMs) (i.e., ChatGPT).\nResults demonstrate that traditional ML algorithms, specifically XGBoost and\nRandom Forest, achieve high performance (83% and 81% accuracies respectively).\nResults also show that classifying shorter content seems to be more challenging\nthan classifying longer content. Further, using Explainable Artificial\nIntelligence (XAI) we identify discriminative features influencing the ML\nmodel's predictions, where human-written content tends to use a practical\nlanguage (e.g., use and allow). Meanwhile AI-generated text is characterized by\nmore abstract and formal terms (e.g., realm and employ). Finally, a comparative\nanalysis with GPTZero show that our narrowly focused, simple, and fine-tuned\nmodel can outperform generalized systems like GPTZero. The proposed model\nachieved approximately 77.5% accuracy compared to GPTZero's 48.5% accuracy when\ntasked to classify Pure AI, Pure Human, and mixed class. GPTZero showed a\ntendency to classify challenging and small-content cases as either mixed or\nunrecognized while our proposed model showed a more balanced performance across\nthe three classes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study seeks to enhance academic integrity by providing tools to detect\nAI-generated content in student work using advanced technologies. The findings\npromote transparency and accountability, helping educators maintain ethical\nstandards and supporting the responsible integration of AI in education. A key\ncontribution of this work is the generation of the CyberHumanAI dataset, which\nhas 1000 observations, 500 of which are written by humans and the other 500\nproduced by ChatGPT. We evaluate various machine learning (ML) and deep\nlearning (DL) algorithms on the CyberHumanAI dataset comparing human-written\nand AI-generated content from Large Language Models (LLMs) (i.e., ChatGPT).\nResults demonstrate that traditional ML algorithms, specifically XGBoost and\nRandom Forest, achieve high performance (83% and 81% accuracies respectively).\nResults also show that classifying shorter content seems to be more challenging\nthan classifying longer content. Further, using Explainable Artificial\nIntelligence (XAI) we identify discriminative features influencing the ML\nmodel's predictions, where human-written content tends to use a practical\nlanguage (e.g., use and allow). Meanwhile AI-generated text is characterized by\nmore abstract and formal terms (e.g., realm and employ). Finally, a comparative\nanalysis with GPTZero show that our narrowly focused, simple, and fine-tuned\nmodel can outperform generalized systems like GPTZero. The proposed model\nachieved approximately 77.5% accuracy compared to GPTZero's 48.5% accuracy when\ntasked to classify Pure AI, Pure Human, and mixed class. GPTZero showed a\ntendency to classify challenging and small-content cases as either mixed or\nunrecognized while our proposed model showed a more balanced performance across\nthe three classes."
                },
                "authors": [
                    {
                        "name": "Ayat A. Najjar"
                    },
                    {
                        "name": "Huthaifa I. Ashqar"
                    },
                    {
                        "name": "Omar A. Darwish"
                    },
                    {
                        "name": "Eman Hammad"
                    }
                ],
                "author_detail": {
                    "name": "Eman Hammad"
                },
                "author": "Eman Hammad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03200v1",
                "updated": "2025-01-06T18:28:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    18,
                    28,
                    4,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T18:28:04Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    18,
                    28,
                    4,
                    0,
                    6,
                    0
                ],
                "title": "The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground\n  Responses to Long-Form Input",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground\n  Responses to Long-Form Input"
                },
                "summary": "We introduce FACTS Grounding, an online leaderboard and associated benchmark\nthat evaluates language models' ability to generate text that is factually\naccurate with respect to given context in the user prompt. In our benchmark,\neach prompt includes a user request and a full document, with a maximum length\nof 32k tokens, requiring long-form responses. The long-form responses are\nrequired to be fully grounded in the provided context document while fulfilling\nthe user request. Models are evaluated using automated judge models in two\nphases: (1) responses are disqualified if they do not fulfill the user request;\n(2) they are judged as accurate if the response is fully grounded in the\nprovided document. The automated judge models were comprehensively evaluated\nagainst a held-out test-set to pick the best prompt template, and the final\nfactuality score is an aggregate of multiple judge models to mitigate\nevaluation bias. The FACTS Grounding leaderboard will be actively maintained\nover time, and contains both public and private splits to allow for external\nparticipation while guarding the integrity of the leaderboard. It can be found\nat https://www.kaggle.com/facts-leaderboard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce FACTS Grounding, an online leaderboard and associated benchmark\nthat evaluates language models' ability to generate text that is factually\naccurate with respect to given context in the user prompt. In our benchmark,\neach prompt includes a user request and a full document, with a maximum length\nof 32k tokens, requiring long-form responses. The long-form responses are\nrequired to be fully grounded in the provided context document while fulfilling\nthe user request. Models are evaluated using automated judge models in two\nphases: (1) responses are disqualified if they do not fulfill the user request;\n(2) they are judged as accurate if the response is fully grounded in the\nprovided document. The automated judge models were comprehensively evaluated\nagainst a held-out test-set to pick the best prompt template, and the final\nfactuality score is an aggregate of multiple judge models to mitigate\nevaluation bias. The FACTS Grounding leaderboard will be actively maintained\nover time, and contains both public and private splits to allow for external\nparticipation while guarding the integrity of the leaderboard. It can be found\nat https://www.kaggle.com/facts-leaderboard."
                },
                "authors": [
                    {
                        "name": "Alon Jacovi"
                    },
                    {
                        "name": "Andrew Wang"
                    },
                    {
                        "name": "Chris Alberti"
                    },
                    {
                        "name": "Connie Tao"
                    },
                    {
                        "name": "Jon Lipovetz"
                    },
                    {
                        "name": "Kate Olszewska"
                    },
                    {
                        "name": "Lukas Haas"
                    },
                    {
                        "name": "Michelle Liu"
                    },
                    {
                        "name": "Nate Keating"
                    },
                    {
                        "name": "Adam Bloniarz"
                    },
                    {
                        "name": "Carl Saroufim"
                    },
                    {
                        "name": "Corey Fry"
                    },
                    {
                        "name": "Dror Marcus"
                    },
                    {
                        "name": "Doron Kukliansky"
                    },
                    {
                        "name": "Gaurav Singh Tomar"
                    },
                    {
                        "name": "James Swirhun"
                    },
                    {
                        "name": "Jinwei Xing"
                    },
                    {
                        "name": "Lily Wang"
                    },
                    {
                        "name": "Madhu Gurumurthy"
                    },
                    {
                        "name": "Michael Aaron"
                    },
                    {
                        "name": "Moran Ambar"
                    },
                    {
                        "name": "Rachana Fellinger"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Zizhao Zhang"
                    },
                    {
                        "name": "Sasha Goldshtein"
                    },
                    {
                        "name": "Dipanjan Das"
                    }
                ],
                "author_detail": {
                    "name": "Dipanjan Das"
                },
                "author": "Dipanjan Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14746v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14746v2",
                "updated": "2025-01-06T18:25:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    18,
                    25,
                    13,
                    0,
                    6,
                    0
                ],
                "published": "2024-02-22T18:06:19Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    18,
                    6,
                    19,
                    3,
                    53,
                    0
                ],
                "title": "Scaling Efficient LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Efficient LLMs"
                },
                "summary": "Trained LLMs are typically sparse in that most of the parameters are zero,\nraising questions on efficiency. In response, we inquire into efficient LLMs,\ni.e. those with the fewest parameters that achieve the desired accuracy on a\ntraining corpus. Specifically, we compare theoretical and empirical estimates\nfor training loss to obtain upper and lower bounds on the number of unique\nsequences in a natural training corpus as a function of its size. Our result\nimplies (1) to double the number of skills represented in a training corpus,\nthe corpus must scale roughly eighteen fold (2) for efficient LLMs, the number\nof parameters N and the size D of a natural training corpus scale as $N \\propto\nD^{0.24} (3) if the number of parameters of an LLM is smaller than the number\nof unique sequences in the training corpus, scaling up can uncover emergent\nskills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trained LLMs are typically sparse in that most of the parameters are zero,\nraising questions on efficiency. In response, we inquire into efficient LLMs,\ni.e. those with the fewest parameters that achieve the desired accuracy on a\ntraining corpus. Specifically, we compare theoretical and empirical estimates\nfor training loss to obtain upper and lower bounds on the number of unique\nsequences in a natural training corpus as a function of its size. Our result\nimplies (1) to double the number of skills represented in a training corpus,\nthe corpus must scale roughly eighteen fold (2) for efficient LLMs, the number\nof parameters N and the size D of a natural training corpus scale as $N \\propto\nD^{0.24} (3) if the number of parameters of an LLM is smaller than the number\nof unique sequences in the training corpus, scaling up can uncover emergent\nskills."
                },
                "authors": [
                    {
                        "name": "B. N. Kausik"
                    }
                ],
                "author_detail": {
                    "name": "B. N. Kausik"
                },
                "author": "B. N. Kausik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14746v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14746v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16133v2",
                "updated": "2025-01-06T18:23:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    18,
                    23,
                    41,
                    0,
                    6,
                    0
                ],
                "published": "2024-11-25T06:48:38Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    6,
                    48,
                    38,
                    0,
                    330,
                    0
                ],
                "title": "Context Awareness Gate For Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Awareness Gate For Retrieval Augmented Generation"
                },
                "summary": "Retrieval Augmented Generation (RAG) has emerged as a widely adopted approach\nto mitigate the limitations of large language models (LLMs) in answering\ndomain-specific questions. Previous research has predominantly focused on\nimproving the accuracy and quality of retrieved data chunks to enhance the\noverall performance of the generation pipeline. However, despite ongoing\nadvancements, the critical issue of retrieving irrelevant information -- which\ncan impair the ability of the model to utilize its internal knowledge\neffectively -- has received minimal attention. In this work, we investigate the\nimpact of retrieving irrelevant information in open-domain question answering,\nhighlighting its significant detrimental effect on the quality of LLM outputs.\nTo address this challenge, we propose the Context Awareness Gate (CAG)\narchitecture, a novel mechanism that dynamically adjusts the LLMs' input prompt\nbased on whether the user query necessitates external context retrieval.\nAdditionally, we introduce the Vector Candidates method, a core mathematical\ncomponent of CAG that is statistical, LLM-independent, and highly scalable. We\nfurther examine the distributions of relationships between contexts and\nquestions, presenting a statistical analysis of these distributions. This\nanalysis can be leveraged to enhance the context retrieval process in Retrieval\nAugmented Generation (RAG) systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) has emerged as a widely adopted approach\nto mitigate the limitations of large language models (LLMs) in answering\ndomain-specific questions. Previous research has predominantly focused on\nimproving the accuracy and quality of retrieved data chunks to enhance the\noverall performance of the generation pipeline. However, despite ongoing\nadvancements, the critical issue of retrieving irrelevant information -- which\ncan impair the ability of the model to utilize its internal knowledge\neffectively -- has received minimal attention. In this work, we investigate the\nimpact of retrieving irrelevant information in open-domain question answering,\nhighlighting its significant detrimental effect on the quality of LLM outputs.\nTo address this challenge, we propose the Context Awareness Gate (CAG)\narchitecture, a novel mechanism that dynamically adjusts the LLMs' input prompt\nbased on whether the user query necessitates external context retrieval.\nAdditionally, we introduce the Vector Candidates method, a core mathematical\ncomponent of CAG that is statistical, LLM-independent, and highly scalable. We\nfurther examine the distributions of relationships between contexts and\nquestions, presenting a statistical analysis of these distributions. This\nanalysis can be leveraged to enhance the context retrieval process in Retrieval\nAugmented Generation (RAG) systems."
                },
                "authors": [
                    {
                        "name": "Mohammad Hassan Heydari"
                    },
                    {
                        "name": "Arshia Hemmat"
                    },
                    {
                        "name": "Erfan Naman"
                    },
                    {
                        "name": "Afsaneh Fatemi"
                    }
                ],
                "author_detail": {
                    "name": "Afsaneh Fatemi"
                },
                "author": "Afsaneh Fatemi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03337v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03337v2",
                "updated": "2025-01-06T17:39:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    17,
                    39,
                    24,
                    0,
                    6,
                    0
                ],
                "published": "2024-10-31T15:35:14Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    35,
                    14,
                    3,
                    305,
                    0
                ],
                "title": "The Systematics and Operational Studies (SOS) Apparatus as a testbed for\n  nEDM@SNS experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Systematics and Operational Studies (SOS) Apparatus as a testbed for\n  nEDM@SNS experiment"
                },
                "summary": "The nEDM experiment at the SNS (nEDM@SNS) is the first measurement of the\nneutron EDM to directly measure the precession frequency of the neutron spin\ndue to magnetic and electric fields. Previous measurements have inferred the\nprecession frequency by measuring the residual polarization of neutrons after a\nlong period of free precession. This difference provides independent\ninformation on potential unknown systematic uncertainties compared to previous\nand on-going measurements. It is precisely because nEDM@SNS is using a number\nof novel techniques, that it is essential to perform detailed studies of these\ntechniques in order to optimize the statistical sensitivity and minimize the\nsystematic uncertainty. The Systematic and Operational Studies Apparatus (SOSA)\nwas one of the main efforts of nEDM@SNS collaboration, concentrated on\ndevelopment of a cryogenic test-bed for learning precise manipulation of spins\nand studing systematic effects of the proposed new technique. The test bed does\nnot need an electric field but designed to have full NMR capability to the same\nlevel as nEDM@SNS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nEDM experiment at the SNS (nEDM@SNS) is the first measurement of the\nneutron EDM to directly measure the precession frequency of the neutron spin\ndue to magnetic and electric fields. Previous measurements have inferred the\nprecession frequency by measuring the residual polarization of neutrons after a\nlong period of free precession. This difference provides independent\ninformation on potential unknown systematic uncertainties compared to previous\nand on-going measurements. It is precisely because nEDM@SNS is using a number\nof novel techniques, that it is essential to perform detailed studies of these\ntechniques in order to optimize the statistical sensitivity and minimize the\nsystematic uncertainty. The Systematic and Operational Studies Apparatus (SOSA)\nwas one of the main efforts of nEDM@SNS collaboration, concentrated on\ndevelopment of a cryogenic test-bed for learning precise manipulation of spins\nand studing systematic effects of the proposed new technique. The test bed does\nnot need an electric field but designed to have full NMR capability to the same\nlevel as nEDM@SNS."
                },
                "authors": [
                    {
                        "name": "V. Cianciolo"
                    },
                    {
                        "name": "R. Golub"
                    },
                    {
                        "name": "B. W. Filippone"
                    },
                    {
                        "name": "P. R. Huffman"
                    },
                    {
                        "name": "K. Leung"
                    },
                    {
                        "name": "E. Korobkina"
                    },
                    {
                        "name": "C. Swank"
                    }
                ],
                "author_detail": {
                    "name": "C. Swank"
                },
                "author": "C. Swank",
                "arxiv_comment": "21 pages, 11 figures. Scientific report written in 2017",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03337v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03337v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16353v2",
                "updated": "2025-01-06T17:37:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    17,
                    37,
                    54,
                    0,
                    6,
                    0
                ],
                "published": "2024-11-25T13:04:28Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    4,
                    28,
                    0,
                    330,
                    0
                ],
                "title": "The Two-Hop Curse: LLMs trained on A$\\rightarrow$B, B$\\rightarrow$C fail\n  to learn A$\\rightarrow$C",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Two-Hop Curse: LLMs trained on A$\\rightarrow$B, B$\\rightarrow$C fail\n  to learn A$\\rightarrow$C"
                },
                "summary": "[Notice: This version is outdated. Recent research contradicts some key\nclaims; we are working on a major revision with more nuanced analysis. Please\nwait for the updated version.]\n  While LLMs excel at multi-hop questions (e.g. \"Who is the spouse of the\nperformer of Imagine?\") when using chain-of-thought reasoning (CoT), they\nstruggle when forced to reason internally (without CoT). Previous work on the\nsize and nature of this gap produced mixed evidence with inconclusive results.\nIn this paper, we introduce a controlled setting for investigating two-hop\nreasoning in LLMs, where the above-chance performance constitutes undeniable\nevidence for latent reasoning. We fine-tune LLMs (including Llama 3 8B Instruct\nand GPT-4o) on fictional facts and confirm that they generalize to answering\ntwo-hop questions about them using CoT. We find that models can perform latent\nreasoning when facts appear together during training or in the prompt. However,\nto our surprise, models completely fail at two-hop reasoning without CoT when\nlearned facts only appear in different documents, achieving chance-level\naccuracy and chance-level test loss. We call this complete failure to compose\nseparately learned facts the Two-Hop Curse. Moreover, we evaluate 9 frontier\nLLMs on real-world facts, finding that models completely fail at two-hop no-CoT\nreasoning for over half of question categories while maintaining partial\nsuccess with CoT across most categories. These results suggest that LLMs lack a\ngeneral capability for latent multi-hop reasoning independent of the question\ntype.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "[Notice: This version is outdated. Recent research contradicts some key\nclaims; we are working on a major revision with more nuanced analysis. Please\nwait for the updated version.]\n  While LLMs excel at multi-hop questions (e.g. \"Who is the spouse of the\nperformer of Imagine?\") when using chain-of-thought reasoning (CoT), they\nstruggle when forced to reason internally (without CoT). Previous work on the\nsize and nature of this gap produced mixed evidence with inconclusive results.\nIn this paper, we introduce a controlled setting for investigating two-hop\nreasoning in LLMs, where the above-chance performance constitutes undeniable\nevidence for latent reasoning. We fine-tune LLMs (including Llama 3 8B Instruct\nand GPT-4o) on fictional facts and confirm that they generalize to answering\ntwo-hop questions about them using CoT. We find that models can perform latent\nreasoning when facts appear together during training or in the prompt. However,\nto our surprise, models completely fail at two-hop reasoning without CoT when\nlearned facts only appear in different documents, achieving chance-level\naccuracy and chance-level test loss. We call this complete failure to compose\nseparately learned facts the Two-Hop Curse. Moreover, we evaluate 9 frontier\nLLMs on real-world facts, finding that models completely fail at two-hop no-CoT\nreasoning for over half of question categories while maintaining partial\nsuccess with CoT across most categories. These results suggest that LLMs lack a\ngeneral capability for latent multi-hop reasoning independent of the question\ntype."
                },
                "authors": [
                    {
                        "name": "Mikita Balesni"
                    },
                    {
                        "name": "Tomek Korbak"
                    },
                    {
                        "name": "Owain Evans"
                    }
                ],
                "author_detail": {
                    "name": "Owain Evans"
                },
                "author": "Owain Evans",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03166v1",
                "updated": "2025-01-06T17:36:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    17,
                    36,
                    9,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T17:36:09Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    17,
                    36,
                    9,
                    0,
                    6,
                    0
                ],
                "title": "Semantic Captioning: Benchmark Dataset and Graph-Aware Few-Shot\n  In-Context Learning for SQL2Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Captioning: Benchmark Dataset and Graph-Aware Few-Shot\n  In-Context Learning for SQL2Text"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in\nvarious NLP tasks, including semantic parsing, which trans lates natural\nlanguage into formal code representations. However, the reverse process,\ntranslating code into natural language, termed semantic captioning, has\nreceived less attention. This task is becoming increasingly important as LLMs\nare integrated into platforms for code generation, security analysis, and\neducational purposes. In this paper, we focus on the captioning of SQL query\n(SQL2Text) to address the critical need for understanding and explaining SQL\nqueries in an era where LLM-generated code poses potential security risks. We\nrepurpose Text2SQL datasets for SQL2Text by introducing an iterative ICL prompt\nusing GPT-4o to generate multiple additional utterances, which enhances the\nrobustness of the datasets for the reverse task. We conduct our experiments\nusing in-context learning (ICL) based on different sample selection methods,\nemphasizing smaller, more computationally efficient LLMs. Our findings\ndemonstrate that leveraging the inherent graph properties of SQL for ICL sample\nselection significantly outperforms random selection by up to 39% on BLEU score\nand provides better results than alternative methods. Dataset and codes are\npublished: \\url{https://github.com/aliwister/ast-icl}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance in\nvarious NLP tasks, including semantic parsing, which trans lates natural\nlanguage into formal code representations. However, the reverse process,\ntranslating code into natural language, termed semantic captioning, has\nreceived less attention. This task is becoming increasingly important as LLMs\nare integrated into platforms for code generation, security analysis, and\neducational purposes. In this paper, we focus on the captioning of SQL query\n(SQL2Text) to address the critical need for understanding and explaining SQL\nqueries in an era where LLM-generated code poses potential security risks. We\nrepurpose Text2SQL datasets for SQL2Text by introducing an iterative ICL prompt\nusing GPT-4o to generate multiple additional utterances, which enhances the\nrobustness of the datasets for the reverse task. We conduct our experiments\nusing in-context learning (ICL) based on different sample selection methods,\nemphasizing smaller, more computationally efficient LLMs. Our findings\ndemonstrate that leveraging the inherent graph properties of SQL for ICL sample\nselection significantly outperforms random selection by up to 39% on BLEU score\nand provides better results than alternative methods. Dataset and codes are\npublished: \\url{https://github.com/aliwister/ast-icl}."
                },
                "authors": [
                    {
                        "name": "Ali Al-Lawati"
                    },
                    {
                        "name": "Jason Lucas"
                    },
                    {
                        "name": "Prasenjit Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Prasenjit Mitra"
                },
                "author": "Prasenjit Mitra",
                "arxiv_comment": "Accepted to COLING'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19363v2",
                "updated": "2025-01-06T17:33:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    17,
                    33,
                    20,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-26T22:06:29Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    22,
                    6,
                    29,
                    3,
                    361,
                    0
                ],
                "title": "Large Language Models for Market Research: A Data-augmentation Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Market Research: A Data-augmentation Approach"
                },
                "summary": "Large Language Models (LLMs) have transformed artificial intelligence by\nexcelling in complex natural language processing tasks. Their ability to\ngenerate human-like text has opened new possibilities for market research,\nparticularly in conjoint analysis, where understanding consumer preferences is\nessential but often resource-intensive. Traditional survey-based methods face\nlimitations in scalability and cost, making LLM-generated data a promising\nalternative. However, while LLMs have the potential to simulate real consumer\nbehavior, recent studies highlight a significant gap between LLM-generated and\nhuman data, with biases introduced when substituting between the two. In this\npaper, we address this gap by proposing a novel statistical data augmentation\napproach that efficiently integrates LLM-generated data with real data in\nconjoint analysis. Our method leverages transfer learning principles to debias\nthe LLM-generated data using a small amount of human data. This results in\nstatistically robust estimators with consistent and asymptotically normal\nproperties, in contrast to naive approaches that simply substitute human data\nwith LLM-generated data, which can exacerbate bias. We validate our framework\nthrough an empirical study on COVID-19 vaccine preferences, demonstrating its\nsuperior ability to reduce estimation error and save data and costs by 24.9% to\n79.8%. In contrast, naive approaches fail to save data due to the inherent\nbiases in LLM-generated data compared to human data. Another empirical study on\nsports car choices validates the robustness of our results. Our findings\nsuggest that while LLM-generated data is not a direct substitute for human\nresponses, it can serve as a valuable complement when used within a robust\nstatistical framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed artificial intelligence by\nexcelling in complex natural language processing tasks. Their ability to\ngenerate human-like text has opened new possibilities for market research,\nparticularly in conjoint analysis, where understanding consumer preferences is\nessential but often resource-intensive. Traditional survey-based methods face\nlimitations in scalability and cost, making LLM-generated data a promising\nalternative. However, while LLMs have the potential to simulate real consumer\nbehavior, recent studies highlight a significant gap between LLM-generated and\nhuman data, with biases introduced when substituting between the two. In this\npaper, we address this gap by proposing a novel statistical data augmentation\napproach that efficiently integrates LLM-generated data with real data in\nconjoint analysis. Our method leverages transfer learning principles to debias\nthe LLM-generated data using a small amount of human data. This results in\nstatistically robust estimators with consistent and asymptotically normal\nproperties, in contrast to naive approaches that simply substitute human data\nwith LLM-generated data, which can exacerbate bias. We validate our framework\nthrough an empirical study on COVID-19 vaccine preferences, demonstrating its\nsuperior ability to reduce estimation error and save data and costs by 24.9% to\n79.8%. In contrast, naive approaches fail to save data due to the inherent\nbiases in LLM-generated data compared to human data. Another empirical study on\nsports car choices validates the robustness of our results. Our findings\nsuggest that while LLM-generated data is not a direct substitute for human\nresponses, it can serve as a valuable complement when used within a robust\nstatistical framework."
                },
                "authors": [
                    {
                        "name": "Mengxin Wang"
                    },
                    {
                        "name": "Dennis J. Zhang"
                    },
                    {
                        "name": "Heng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Heng Zhang"
                },
                "arxiv_affiliation": "W. P. Carey School of Business, Arizona State University",
                "author": "Heng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 90B60, 62F12",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.4; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03151v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03151v1",
                "updated": "2025-01-06T17:18:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    17,
                    18,
                    47,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T17:18:47Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    17,
                    18,
                    47,
                    0,
                    6,
                    0
                ],
                "title": "Large language models for artificial general intelligence (AGI): A\n  survey of foundational principles and approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models for artificial general intelligence (AGI): A\n  survey of foundational principles and approaches"
                },
                "summary": "Generative artificial intelligence (AI) systems based on large-scale\npretrained foundation models (PFMs) such as vision-language models, large\nlanguage models (LLMs), diffusion models and vision-language-action (VLA)\nmodels have demonstrated the ability to solve complex and truly non-trivial AI\nproblems in a wide variety of domains and contexts. Multimodal large language\nmodels (MLLMs), in particular, learn from vast and diverse data sources,\nallowing rich and nuanced representations of the world and, thereby, providing\nextensive capabilities, including the ability to reason, engage in meaningful\ndialog; collaborate with humans and other agents to jointly solve complex\nproblems; and understand social and emotional aspects of humans. Despite this\nimpressive feat, the cognitive abilities of state-of-the-art LLMs trained on\nlarge-scale datasets are still superficial and brittle. Consequently, generic\nLLMs are severely limited in their generalist capabilities. A number of\nfoundational problems -- embodiment, symbol grounding, causality and memory --\nare required to be addressed for LLMs to attain human-level general\nintelligence. These concepts are more aligned with human cognition and provide\nLLMs with inherent human-like cognitive properties that support the realization\nof physically-plausible, semantically meaningful, flexible and more\ngeneralizable knowledge and intelligence. In this work, we discuss the\naforementioned foundational issues and survey state-of-the art approaches for\nimplementing these concepts in LLMs. Specifically, we discuss how the\nprinciples of embodiment, symbol grounding, causality and memory can be\nleveraged toward the attainment of artificial general intelligence (AGI) in an\norganic manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative artificial intelligence (AI) systems based on large-scale\npretrained foundation models (PFMs) such as vision-language models, large\nlanguage models (LLMs), diffusion models and vision-language-action (VLA)\nmodels have demonstrated the ability to solve complex and truly non-trivial AI\nproblems in a wide variety of domains and contexts. Multimodal large language\nmodels (MLLMs), in particular, learn from vast and diverse data sources,\nallowing rich and nuanced representations of the world and, thereby, providing\nextensive capabilities, including the ability to reason, engage in meaningful\ndialog; collaborate with humans and other agents to jointly solve complex\nproblems; and understand social and emotional aspects of humans. Despite this\nimpressive feat, the cognitive abilities of state-of-the-art LLMs trained on\nlarge-scale datasets are still superficial and brittle. Consequently, generic\nLLMs are severely limited in their generalist capabilities. A number of\nfoundational problems -- embodiment, symbol grounding, causality and memory --\nare required to be addressed for LLMs to attain human-level general\nintelligence. These concepts are more aligned with human cognition and provide\nLLMs with inherent human-like cognitive properties that support the realization\nof physically-plausible, semantically meaningful, flexible and more\ngeneralizable knowledge and intelligence. In this work, we discuss the\naforementioned foundational issues and survey state-of-the art approaches for\nimplementing these concepts in LLMs. Specifically, we discuss how the\nprinciples of embodiment, symbol grounding, causality and memory can be\nleveraged toward the attainment of artificial general intelligence (AGI) in an\norganic manner."
                },
                "authors": [
                    {
                        "name": "Alhassan Mumuni"
                    },
                    {
                        "name": "Fuseini Mumuni"
                    }
                ],
                "author_detail": {
                    "name": "Fuseini Mumuni"
                },
                "author": "Fuseini Mumuni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03151v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03151v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03139v1",
                "updated": "2025-01-06T17:01:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    17,
                    1,
                    45,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T17:01:45Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    17,
                    1,
                    45,
                    0,
                    6,
                    0
                ],
                "title": "VicSim: Enhancing Victim Simulation with Emotional and Linguistic\n  Fidelity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VicSim: Enhancing Victim Simulation with Emotional and Linguistic\n  Fidelity"
                },
                "summary": "Scenario-based training has been widely adopted in many public service\nsectors. Recent advancements in Large Language Models (LLMs) have shown promise\nin simulating diverse personas to create these training scenarios. However,\nlittle is known about how LLMs can be developed to simulate victims for\nscenario-based training purposes. In this paper, we introduce VicSim (victim\nsimulator), a novel model that addresses three key dimensions of user\nsimulation: informational faithfulness, emotional dynamics, and language style\n(e.g., grammar usage). We pioneer the integration of scenario-based victim\nmodeling with GAN-based training workflow and key-information-based prompting,\naiming to enhance the realism of simulated victims. Our adversarial training\napproach teaches the discriminator to recognize grammar and emotional cues as\nreliable indicators of synthetic content. According to evaluations by human\nraters, the VicSim model outperforms GPT-4 in terms of human-likeness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scenario-based training has been widely adopted in many public service\nsectors. Recent advancements in Large Language Models (LLMs) have shown promise\nin simulating diverse personas to create these training scenarios. However,\nlittle is known about how LLMs can be developed to simulate victims for\nscenario-based training purposes. In this paper, we introduce VicSim (victim\nsimulator), a novel model that addresses three key dimensions of user\nsimulation: informational faithfulness, emotional dynamics, and language style\n(e.g., grammar usage). We pioneer the integration of scenario-based victim\nmodeling with GAN-based training workflow and key-information-based prompting,\naiming to enhance the realism of simulated victims. Our adversarial training\napproach teaches the discriminator to recognize grammar and emotional cues as\nreliable indicators of synthetic content. According to evaluations by human\nraters, the VicSim model outperforms GPT-4 in terms of human-likeness."
                },
                "authors": [
                    {
                        "name": "Yerong Li"
                    },
                    {
                        "name": "Yiren Liu"
                    },
                    {
                        "name": "Yun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yun Huang"
                },
                "author": "Yun Huang",
                "arxiv_comment": "21 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13147v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13147v3",
                "updated": "2025-01-06T16:49:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    49,
                    55,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-17T18:12:47Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    12,
                    47,
                    1,
                    352,
                    0
                ],
                "title": "Are Your LLMs Capable of Stable Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Your LLMs Capable of Stable Reasoning?"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has demonstrated\nremarkable progress in complex reasoning tasks. However, a significant\ndiscrepancy persists between benchmark performances and real-world\napplications. We identify this gap as primarily stemming from current\nevaluation protocols and metrics, which inadequately capture the full spectrum\nof LLM capabilities, particularly in complex reasoning tasks where both\naccuracy and consistency are crucial. This work makes two key contributions.\nFirst, we introduce G-Pass@k, a novel evaluation metric that provides a\ncontinuous assessment of model performance across multiple sampling attempts,\nquantifying both the model's peak performance potential and its stability.\nSecond, we present LiveMathBench, a dynamic benchmark comprising challenging,\ncontemporary mathematical problems designed to minimize data leakage risks\nduring evaluation. Through extensive experiments using G-Pass@k on\nstate-of-the-art LLMs with LiveMathBench, we provide comprehensive insights\ninto both their maximum capabilities and operational consistency. Our findings\nreveal substantial room for improvement in LLMs' \"realistic\" reasoning\ncapabilities, highlighting the need for more robust evaluation methods. The\nbenchmark and detailed results are available at:\nhttps://github.com/open-compass/GPassK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has demonstrated\nremarkable progress in complex reasoning tasks. However, a significant\ndiscrepancy persists between benchmark performances and real-world\napplications. We identify this gap as primarily stemming from current\nevaluation protocols and metrics, which inadequately capture the full spectrum\nof LLM capabilities, particularly in complex reasoning tasks where both\naccuracy and consistency are crucial. This work makes two key contributions.\nFirst, we introduce G-Pass@k, a novel evaluation metric that provides a\ncontinuous assessment of model performance across multiple sampling attempts,\nquantifying both the model's peak performance potential and its stability.\nSecond, we present LiveMathBench, a dynamic benchmark comprising challenging,\ncontemporary mathematical problems designed to minimize data leakage risks\nduring evaluation. Through extensive experiments using G-Pass@k on\nstate-of-the-art LLMs with LiveMathBench, we provide comprehensive insights\ninto both their maximum capabilities and operational consistency. Our findings\nreveal substantial room for improvement in LLMs' \"realistic\" reasoning\ncapabilities, highlighting the need for more robust evaluation methods. The\nbenchmark and detailed results are available at:\nhttps://github.com/open-compass/GPassK."
                },
                "authors": [
                    {
                        "name": "Junnan Liu"
                    },
                    {
                        "name": "Hongwei Liu"
                    },
                    {
                        "name": "Linchen Xiao"
                    },
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Kuikun Liu"
                    },
                    {
                        "name": "Songyang Gao"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13147v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13147v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03129v1",
                "updated": "2025-01-06T16:47:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    47,
                    24,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T16:47:24Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    47,
                    24,
                    0,
                    6,
                    0
                ],
                "title": "Coarsened confounding for causal effects: a large-sample framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarsened confounding for causal effects: a large-sample framework"
                },
                "summary": "There has been widespread use of causal inference methods for the rigorous\nanalysis of observational studies and to identify policy evaluations. In this\narticle, we consider coarsened exact matching, developed in Iacus et al.\n(2011). While they developed some statistical properties, in this article, we\nstudy the approach using asymptotics based on a superpopulation inferential\nframework. This methodology is generalized to what we termed as coarsened\nconfounding, for which we propose two new algorithms. We develop asymptotic\nresults for the average causal effect estimator as well as providing conditions\nfor consistency. In addition, we provide an asymptotic justification for the\nvariance formulae in Iacus et al. (2011). A bias correction technique is\nproposed, and we apply the proposed methodology to data from two well-known\nobservational studi",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been widespread use of causal inference methods for the rigorous\nanalysis of observational studies and to identify policy evaluations. In this\narticle, we consider coarsened exact matching, developed in Iacus et al.\n(2011). While they developed some statistical properties, in this article, we\nstudy the approach using asymptotics based on a superpopulation inferential\nframework. This methodology is generalized to what we termed as coarsened\nconfounding, for which we propose two new algorithms. We develop asymptotic\nresults for the average causal effect estimator as well as providing conditions\nfor consistency. In addition, we provide an asymptotic justification for the\nvariance formulae in Iacus et al. (2011). A bias correction technique is\nproposed, and we apply the proposed methodology to data from two well-known\nobservational studi"
                },
                "authors": [
                    {
                        "name": "Debashis Ghosh"
                    },
                    {
                        "name": "Lei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Wang"
                },
                "author": "Lei Wang",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2301.00889",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62P10 (Primary) 62P25, 62P15 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.02368v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.02368v2",
                "updated": "2025-01-06T16:41:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    41,
                    9,
                    0,
                    6,
                    0
                ],
                "published": "2023-10-03T18:48:31Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    18,
                    48,
                    31,
                    1,
                    276,
                    0
                ],
                "title": "Reinforcement Learning from Automatic Feedback for High-Quality Unit\n  Test Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Automatic Feedback for High-Quality Unit\n  Test Generation"
                },
                "summary": "Software testing is a crucial aspect of software development, and the\ncreation of high-quality tests that adhere to best practices is essential for\neffective maintenance. Recently, Large Language Models (LLMs) have gained\npopularity for code generation, including the automated creation of test cases.\nHowever, these LLMs are often trained on vast amounts of publicly available\ncode, which may include test cases that do not adhere to best practices and may\neven contain test smells (anti-patterns). To address this issue, we propose a\nnovel technique called Reinforcement Learning from Static Quality Metrics\n(RLSQM). To begin, we analyze the anti-patterns generated by the LLM and show\nthat LLMs can generate undesirable test smells. Thus, we train specific reward\nmodels for each static quality metric, then utilize Proximal Policy\nOptimization (PPO) to train models for optimizing a single quality metric at a\ntime. Furthermore, we amalgamate these rewards into a unified reward model\naimed at capturing different best practices and quality aspects of tests. By\ncomparing RL-trained models with those trained using supervised learning, we\nprovide insights into how reliably utilize RL to improve test generation\nquality and into the effects of various training strategies. Our experimental\nresults demonstrate that the RL-optimized model consistently generated\nhigh-quality test cases compared to the base LLM, improving the model by up to\n21%, and successfully generates nearly 100% syntactically correct code. RLSQM\nalso outperformed GPT-4 on four out of seven metrics. This represents a\nsignificant step towards enhancing the overall efficiency and reliability of\nsoftware testing through Reinforcement Learning and static quality metrics. Our\ndata are available at https://figshare.com/s/ded476c8d4c221222849.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software testing is a crucial aspect of software development, and the\ncreation of high-quality tests that adhere to best practices is essential for\neffective maintenance. Recently, Large Language Models (LLMs) have gained\npopularity for code generation, including the automated creation of test cases.\nHowever, these LLMs are often trained on vast amounts of publicly available\ncode, which may include test cases that do not adhere to best practices and may\neven contain test smells (anti-patterns). To address this issue, we propose a\nnovel technique called Reinforcement Learning from Static Quality Metrics\n(RLSQM). To begin, we analyze the anti-patterns generated by the LLM and show\nthat LLMs can generate undesirable test smells. Thus, we train specific reward\nmodels for each static quality metric, then utilize Proximal Policy\nOptimization (PPO) to train models for optimizing a single quality metric at a\ntime. Furthermore, we amalgamate these rewards into a unified reward model\naimed at capturing different best practices and quality aspects of tests. By\ncomparing RL-trained models with those trained using supervised learning, we\nprovide insights into how reliably utilize RL to improve test generation\nquality and into the effects of various training strategies. Our experimental\nresults demonstrate that the RL-optimized model consistently generated\nhigh-quality test cases compared to the base LLM, improving the model by up to\n21%, and successfully generates nearly 100% syntactically correct code. RLSQM\nalso outperformed GPT-4 on four out of seven metrics. This represents a\nsignificant step towards enhancing the overall efficiency and reliability of\nsoftware testing through Reinforcement Learning and static quality metrics. Our\ndata are available at https://figshare.com/s/ded476c8d4c221222849."
                },
                "authors": [
                    {
                        "name": "Benjamin Steenhoek"
                    },
                    {
                        "name": "Michele Tufano"
                    },
                    {
                        "name": "Neel Sundaresan"
                    },
                    {
                        "name": "Alexey Svyatkovskiy"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Svyatkovskiy"
                },
                "author": "Alexey Svyatkovskiy",
                "arxiv_comment": "Accepted to DeepTest 2025 (ICSE Workshop). Previously this version\n  appeared as arXiv:2412.14308 which was submitted as a new work by accident",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.02368v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.02368v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10606v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10606v2",
                "updated": "2025-01-06T16:35:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    35,
                    59,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-13T23:19:21Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    23,
                    19,
                    21,
                    4,
                    348,
                    0
                ],
                "title": "Do Large Language Models Speak Scientific Workflows?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Speak Scientific Workflows?"
                },
                "summary": "With the advent of large language models (LLMs), there is a growing interest\nin applying LLMs to scientific tasks. In this work, we conduct an experimental\nstudy to explore applicability of LLMs for configuring, annotating,\ntranslating, explaining, and generating scientific workflows. We use 5\ndifferent workflow specific experiments and evaluate several open- and\nclosed-source language models using state-of-the-art workflow systems. Our\nstudies reveal that LLMs often struggle with workflow related tasks due to\ntheir lack of knowledge of scientific workflows. We further observe that the\nperformance of LLMs varies across experiments and workflow systems. Our\nfindings can help workflow developers and users in understanding LLMs\ncapabilities in scientific workflows, and motivate further research applying\nLLMs to workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of large language models (LLMs), there is a growing interest\nin applying LLMs to scientific tasks. In this work, we conduct an experimental\nstudy to explore applicability of LLMs for configuring, annotating,\ntranslating, explaining, and generating scientific workflows. We use 5\ndifferent workflow specific experiments and evaluate several open- and\nclosed-source language models using state-of-the-art workflow systems. Our\nstudies reveal that LLMs often struggle with workflow related tasks due to\ntheir lack of knowledge of scientific workflows. We further observe that the\nperformance of LLMs varies across experiments and workflow systems. Our\nfindings can help workflow developers and users in understanding LLMs\ncapabilities in scientific workflows, and motivate further research applying\nLLMs to workflows."
                },
                "authors": [
                    {
                        "name": "Orcun Yildiz"
                    },
                    {
                        "name": "Tom Peterka"
                    }
                ],
                "author_detail": {
                    "name": "Tom Peterka"
                },
                "author": "Tom Peterka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10606v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10606v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19839v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19839v4",
                "updated": "2025-01-06T16:33:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    33,
                    58,
                    0,
                    6,
                    0
                ],
                "published": "2024-09-30T00:41:51Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    0,
                    41,
                    51,
                    0,
                    274,
                    0
                ],
                "title": "ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities"
                },
                "summary": "Forecasts of future events are essential inputs into informed\ndecision-making. Machine learning (ML) systems have the potential to deliver\nforecasts at scale, but there is no framework for evaluating the accuracy of ML\nsystems on a standardized set of forecasting questions. To address this gap, we\nintroduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML\nsystems on an automatically generated and regularly updated set of 1,000\nforecasting questions. To avoid any possibility of data leakage, ForecastBench\nis comprised solely of questions about future events that have no known answer\nat the time of submission. We quantify the capabilities of current ML systems\nby collecting forecasts from expert (human) forecasters, the general public,\nand LLMs on a random subset of questions from the benchmark ($N=200$). While\nLLMs have achieved super-human performance on many benchmarks, they perform\nless well here: expert forecasters outperform the top-performing LLM (p-value\n$<0.01$). We display system and human scores in a public leaderboard at\nwww.forecastbench.org.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasts of future events are essential inputs into informed\ndecision-making. Machine learning (ML) systems have the potential to deliver\nforecasts at scale, but there is no framework for evaluating the accuracy of ML\nsystems on a standardized set of forecasting questions. To address this gap, we\nintroduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML\nsystems on an automatically generated and regularly updated set of 1,000\nforecasting questions. To avoid any possibility of data leakage, ForecastBench\nis comprised solely of questions about future events that have no known answer\nat the time of submission. We quantify the capabilities of current ML systems\nby collecting forecasts from expert (human) forecasters, the general public,\nand LLMs on a random subset of questions from the benchmark ($N=200$). While\nLLMs have achieved super-human performance on many benchmarks, they perform\nless well here: expert forecasters outperform the top-performing LLM (p-value\n$<0.01$). We display system and human scores in a public leaderboard at\nwww.forecastbench.org."
                },
                "authors": [
                    {
                        "name": "Ezra Karger"
                    },
                    {
                        "name": "Houtan Bastani"
                    },
                    {
                        "name": "Chen Yueh-Han"
                    },
                    {
                        "name": "Zachary Jacobs"
                    },
                    {
                        "name": "Danny Halawi"
                    },
                    {
                        "name": "Fred Zhang"
                    },
                    {
                        "name": "Philip E. Tetlock"
                    }
                ],
                "author_detail": {
                    "name": "Philip E. Tetlock"
                },
                "author": "Philip E. Tetlock",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19839v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19839v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14308v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14308v2",
                "updated": "2025-01-06T16:31:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    31,
                    18,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-18T20:20:01Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    20,
                    20,
                    1,
                    2,
                    353,
                    0
                ],
                "title": "Reinforcement Learning from Automatic Feedback for High-Quality Unit\n  Test Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Automatic Feedback for High-Quality Unit\n  Test Generation"
                },
                "summary": "Software testing is a crucial but time-consuming aspect of software\ndevelopment, and recently, Large Language Models (LLMs) have gained popularity\nfor automated test case generation. However, because LLMs are trained on vast\namounts of open-source code, they often generate test cases that do not adhere\nto best practices and may even contain test smells (anti-patterns). To address\nthis issue, we propose Reinforcement Learning from Static Quality Metrics\n(RLSQM), wherein we utilize Reinforcement Learning to generate high-quality\nunit tests based on static analysis-based quality metrics. First, we analyzed\nLLM-generated tests and show that LLMs frequently do generate undesirable test\nsmells -- up to 37% of the time. Then, we implemented lightweight static\nanalysis-based reward model and trained LLMs using this reward model to\noptimize for five code quality metrics. Our experimental results demonstrate\nthat the RL-optimized Codex model consistently generated higher-quality test\ncases than the base LLM, improving quality metrics by up to 23%, and generated\nnearly 100% syntactically-correct code. RLSQM also outperformed GPT-4 on all\ncode quality metrics, in spite of training a substantially cheaper Codex model.\nWe provide insights into how reliably utilize RL to improve test generation\nquality and show that RLSQM is a significant step towards enhancing the overall\nefficiency and reliability of automated software testing. Our data are\navailable at https://doi.org/10.6084/m9.figshare.25983166.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software testing is a crucial but time-consuming aspect of software\ndevelopment, and recently, Large Language Models (LLMs) have gained popularity\nfor automated test case generation. However, because LLMs are trained on vast\namounts of open-source code, they often generate test cases that do not adhere\nto best practices and may even contain test smells (anti-patterns). To address\nthis issue, we propose Reinforcement Learning from Static Quality Metrics\n(RLSQM), wherein we utilize Reinforcement Learning to generate high-quality\nunit tests based on static analysis-based quality metrics. First, we analyzed\nLLM-generated tests and show that LLMs frequently do generate undesirable test\nsmells -- up to 37% of the time. Then, we implemented lightweight static\nanalysis-based reward model and trained LLMs using this reward model to\noptimize for five code quality metrics. Our experimental results demonstrate\nthat the RL-optimized Codex model consistently generated higher-quality test\ncases than the base LLM, improving quality metrics by up to 23%, and generated\nnearly 100% syntactically-correct code. RLSQM also outperformed GPT-4 on all\ncode quality metrics, in spite of training a substantially cheaper Codex model.\nWe provide insights into how reliably utilize RL to improve test generation\nquality and show that RLSQM is a significant step towards enhancing the overall\nefficiency and reliability of automated software testing. Our data are\navailable at https://doi.org/10.6084/m9.figshare.25983166."
                },
                "authors": [
                    {
                        "name": "Benjamin Steenhoek"
                    },
                    {
                        "name": "Michele Tufano"
                    },
                    {
                        "name": "Neel Sundaresan"
                    },
                    {
                        "name": "Alexey Svyatkovskiy"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Svyatkovskiy"
                },
                "author": "Alexey Svyatkovskiy",
                "arxiv_comment": "This work was intended as a replacement of arXiv:2310.02368 and any\n  subsequent updates will appear there",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14308v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14308v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19155v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19155v2",
                "updated": "2025-01-06T16:30:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    30,
                    35,
                    0,
                    6,
                    0
                ],
                "published": "2024-10-24T20:49:22Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    20,
                    49,
                    22,
                    3,
                    298,
                    0
                ],
                "title": "Lived Experience Not Found: LLMs Struggle to Align with Experts on\n  Addressing Adverse Drug Reactions from Psychiatric Medication Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lived Experience Not Found: LLMs Struggle to Align with Experts on\n  Addressing Adverse Drug Reactions from Psychiatric Medication Use"
                },
                "summary": "Adverse Drug Reactions (ADRs) from psychiatric medications are the leading\ncause of hospitalizations among mental health patients. With healthcare systems\nand online communities facing limitations in resolving ADR-related issues,\nLarge Language Models (LLMs) have the potential to fill this gap. Despite the\nincreasing capabilities of LLMs, past research has not explored their\ncapabilities in detecting ADRs related to psychiatric medications or in\nproviding effective harm reduction strategies. To address this, we introduce\nthe Psych-ADR benchmark and the Adverse Drug Reaction Response Assessment\n(ADRA) framework to systematically evaluate LLM performance in detecting ADR\nexpressions and delivering expert-aligned mitigation strategies. Our analyses\nshow that LLMs struggle with understanding the nuances of ADRs and\ndifferentiating between types of ADRs. While LLMs align with experts in terms\nof expressed emotions and tone of the text, their responses are more complex,\nharder to read, and only 70.86% aligned with expert strategies. Furthermore,\nthey provide less actionable advice by a margin of 12.32% on average. Our work\nprovides a comprehensive benchmark and evaluation framework for assessing LLMs\nin strategy-driven tasks within high-risk domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adverse Drug Reactions (ADRs) from psychiatric medications are the leading\ncause of hospitalizations among mental health patients. With healthcare systems\nand online communities facing limitations in resolving ADR-related issues,\nLarge Language Models (LLMs) have the potential to fill this gap. Despite the\nincreasing capabilities of LLMs, past research has not explored their\ncapabilities in detecting ADRs related to psychiatric medications or in\nproviding effective harm reduction strategies. To address this, we introduce\nthe Psych-ADR benchmark and the Adverse Drug Reaction Response Assessment\n(ADRA) framework to systematically evaluate LLM performance in detecting ADR\nexpressions and delivering expert-aligned mitigation strategies. Our analyses\nshow that LLMs struggle with understanding the nuances of ADRs and\ndifferentiating between types of ADRs. While LLMs align with experts in terms\nof expressed emotions and tone of the text, their responses are more complex,\nharder to read, and only 70.86% aligned with expert strategies. Furthermore,\nthey provide less actionable advice by a margin of 12.32% on average. Our work\nprovides a comprehensive benchmark and evaluation framework for assessing LLMs\nin strategy-driven tasks within high-risk domains."
                },
                "authors": [
                    {
                        "name": "Mohit Chandra"
                    },
                    {
                        "name": "Siddharth Sriraman"
                    },
                    {
                        "name": "Gaurav Verma"
                    },
                    {
                        "name": "Harneet Singh Khanuja"
                    },
                    {
                        "name": "Jose Suarez Campayo"
                    },
                    {
                        "name": "Zihang Li"
                    },
                    {
                        "name": "Michael L. Birnbaum"
                    },
                    {
                        "name": "Munmun De Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Munmun De Choudhury"
                },
                "author": "Munmun De Choudhury",
                "arxiv_comment": "30 pages, 8 figures, 16 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19155v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19155v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12196v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12196v4",
                "updated": "2025-01-06T16:29:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    29,
                    32,
                    0,
                    6,
                    0
                ],
                "published": "2024-03-18T19:10:12Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    19,
                    10,
                    12,
                    0,
                    78,
                    0
                ],
                "title": "Leveraging Large Language Models to Detect npm Malicious Packages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models to Detect npm Malicious Packages"
                },
                "summary": "Existing malicious code detection techniques demand the integration of\nmultiple tools to detect different malware patterns, often suffering from high\nmisclassification rates. Therefore, malicious code detection techniques could\nbe enhanced by adopting advanced, more automated approaches to achieve high\naccuracy and a low misclassification rate. The goal of this study is to aid\nsecurity analysts in detecting malicious packages by empirically studying the\neffectiveness of Large Language Models (LLMs) in detecting malicious code. We\npresent SocketAI, a malicious code review workflow to detect malicious code. To\nevaluate the effectiveness of SocketAI, we leverage a benchmark dataset of\n5,115 npm packages, of which 2,180 packages have malicious code. We conducted a\nbaseline comparison of GPT-3 and GPT-4 models with the state-of-the-art CodeQL\nstatic analysis tool, using 39 custom CodeQL rules developed in prior research\nto detect malicious Javascript code. We also compare the effectiveness of\nstatic analysis as a pre-screener with SocketAI workflow, measuring the number\nof files that need to be analyzed. and the associated costs. Additionally, we\nperformed a qualitative study to understand the types of malicious activities\ndetected or missed by our workflow. Our baseline comparison demonstrates a 16%\nand 9% improvement over static analysis in precision and F1 scores,\nrespectively. GPT-4 achieves higher accuracy with 99% precision and 97% F1\nscores, while GPT-3 offers a more cost-effective balance at 91% precision and\n94% F1 scores. Pre-screening files with a static analyzer reduces the number of\nfiles requiring LLM analysis by 77.9% and decreases costs by 60.9% for GPT-3\nand 76.1% for GPT-4. Our qualitative analysis identified data theft, execution\nof arbitrary code, and suspicious domain categories as the top detected\nmalicious packages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing malicious code detection techniques demand the integration of\nmultiple tools to detect different malware patterns, often suffering from high\nmisclassification rates. Therefore, malicious code detection techniques could\nbe enhanced by adopting advanced, more automated approaches to achieve high\naccuracy and a low misclassification rate. The goal of this study is to aid\nsecurity analysts in detecting malicious packages by empirically studying the\neffectiveness of Large Language Models (LLMs) in detecting malicious code. We\npresent SocketAI, a malicious code review workflow to detect malicious code. To\nevaluate the effectiveness of SocketAI, we leverage a benchmark dataset of\n5,115 npm packages, of which 2,180 packages have malicious code. We conducted a\nbaseline comparison of GPT-3 and GPT-4 models with the state-of-the-art CodeQL\nstatic analysis tool, using 39 custom CodeQL rules developed in prior research\nto detect malicious Javascript code. We also compare the effectiveness of\nstatic analysis as a pre-screener with SocketAI workflow, measuring the number\nof files that need to be analyzed. and the associated costs. Additionally, we\nperformed a qualitative study to understand the types of malicious activities\ndetected or missed by our workflow. Our baseline comparison demonstrates a 16%\nand 9% improvement over static analysis in precision and F1 scores,\nrespectively. GPT-4 achieves higher accuracy with 99% precision and 97% F1\nscores, while GPT-3 offers a more cost-effective balance at 91% precision and\n94% F1 scores. Pre-screening files with a static analyzer reduces the number of\nfiles requiring LLM analysis by 77.9% and decreases costs by 60.9% for GPT-3\nand 76.1% for GPT-4. Our qualitative analysis identified data theft, execution\nof arbitrary code, and suspicious domain categories as the top detected\nmalicious packages."
                },
                "authors": [
                    {
                        "name": "Nusrat Zahan"
                    },
                    {
                        "name": "Philipp Burckhardt"
                    },
                    {
                        "name": "Mikola Lysenko"
                    },
                    {
                        "name": "Feross Aboukhadijeh"
                    },
                    {
                        "name": "Laurie Williams"
                    }
                ],
                "author_detail": {
                    "name": "Laurie Williams"
                },
                "author": "Laurie Williams",
                "arxiv_comment": "13 pages, 2 Figure, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12196v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12196v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03120v1",
                "updated": "2025-01-06T16:28:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    28,
                    47,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T16:28:47Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    28,
                    47,
                    0,
                    6,
                    0
                ],
                "title": "CAT: Content-Adaptive Image Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAT: Content-Adaptive Image Tokenization"
                },
                "summary": "Most existing image tokenizers encode images into a fixed number of tokens or\npatches, overlooking the inherent variability in image complexity. To address\nthis, we introduce Content-Adaptive Tokenizer (CAT), which dynamically adjusts\nrepresentation capacity based on the image content and encodes simpler images\ninto fewer tokens. We design a caption-based evaluation system that leverages\nlarge language models (LLMs) to predict content complexity and determine the\noptimal compression ratio for a given image, taking into account factors\ncritical to human perception. Trained on images with diverse compression\nratios, CAT demonstrates robust performance in image reconstruction. We also\nutilize its variable-length latent representations to train Diffusion\nTransformers (DiTs) for ImageNet generation. By optimizing token allocation,\nCAT improves the FID score over fixed-ratio baselines trained with the same\nflops and boosts the inference throughput by 18.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most existing image tokenizers encode images into a fixed number of tokens or\npatches, overlooking the inherent variability in image complexity. To address\nthis, we introduce Content-Adaptive Tokenizer (CAT), which dynamically adjusts\nrepresentation capacity based on the image content and encodes simpler images\ninto fewer tokens. We design a caption-based evaluation system that leverages\nlarge language models (LLMs) to predict content complexity and determine the\noptimal compression ratio for a given image, taking into account factors\ncritical to human perception. Trained on images with diverse compression\nratios, CAT demonstrates robust performance in image reconstruction. We also\nutilize its variable-length latent representations to train Diffusion\nTransformers (DiTs) for ImageNet generation. By optimizing token allocation,\nCAT improves the FID score over fixed-ratio baselines trained with the same\nflops and boosts the inference throughput by 18.5%."
                },
                "authors": [
                    {
                        "name": "Junhong Shen"
                    },
                    {
                        "name": "Kushal Tirumala"
                    },
                    {
                        "name": "Michihiro Yasunaga"
                    },
                    {
                        "name": "Ishan Misra"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "name": "Lili Yu"
                    },
                    {
                        "name": "Chunting Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Chunting Zhou"
                },
                "author": "Chunting Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03119v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03119v1",
                "updated": "2025-01-06T16:27:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    27,
                    53,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T16:27:53Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    27,
                    53,
                    0,
                    6,
                    0
                ],
                "title": "From Models to Network Topologies: A Topology Inference Attack in\n  Decentralized Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Models to Network Topologies: A Topology Inference Attack in\n  Decentralized Federated Learning"
                },
                "summary": "Federated Learning (FL) is widely recognized as a privacy-preserving machine\nlearning paradigm due to its model-sharing mechanism that avoids direct data\nexchange. However, model training inevitably leaves exploitable traces that can\nbe used to infer sensitive information. In Decentralized FL (DFL), the overlay\ntopology significantly influences its models' convergence, robustness, and\nsecurity. This study explores the feasibility of inferring the overlay topology\nof DFL systems based solely on model behavior, introducing a novel Topology\nInference Attack. A taxonomy of topology inference attacks is proposed,\ncategorizing them by the attacker's capabilities and knowledge. Practical\nattack strategies are developed for different scenarios, and quantitative\nexperiments are conducted to identify key factors influencing the attack\neffectiveness. Experimental results demonstrate that analyzing only the public\nmodels of individual nodes can accurately infer the DFL topology, underscoring\nthe risk of sensitive information leakage in DFL systems. This finding offers\nvaluable insights for improving privacy preservation in decentralized learning\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is widely recognized as a privacy-preserving machine\nlearning paradigm due to its model-sharing mechanism that avoids direct data\nexchange. However, model training inevitably leaves exploitable traces that can\nbe used to infer sensitive information. In Decentralized FL (DFL), the overlay\ntopology significantly influences its models' convergence, robustness, and\nsecurity. This study explores the feasibility of inferring the overlay topology\nof DFL systems based solely on model behavior, introducing a novel Topology\nInference Attack. A taxonomy of topology inference attacks is proposed,\ncategorizing them by the attacker's capabilities and knowledge. Practical\nattack strategies are developed for different scenarios, and quantitative\nexperiments are conducted to identify key factors influencing the attack\neffectiveness. Experimental results demonstrate that analyzing only the public\nmodels of individual nodes can accurately infer the DFL topology, underscoring\nthe risk of sensitive information leakage in DFL systems. This finding offers\nvaluable insights for improving privacy preservation in decentralized learning\nenvironments."
                },
                "authors": [
                    {
                        "name": "Chao Feng"
                    },
                    {
                        "name": "Yuanzhe Gao"
                    },
                    {
                        "name": "Alberto Huertas Celdran"
                    },
                    {
                        "name": "Gerome Bovet"
                    },
                    {
                        "name": "Burkhard Stiller"
                    }
                ],
                "author_detail": {
                    "name": "Burkhard Stiller"
                },
                "author": "Burkhard Stiller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03119v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03119v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03112v1",
                "updated": "2025-01-06T16:20:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    20,
                    44,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T16:20:44Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    20,
                    44,
                    0,
                    6,
                    0
                ],
                "title": "LangFair: A Python Package for Assessing Bias and Fairness in Large\n  Language Model Use Cases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LangFair: A Python Package for Assessing Bias and Fairness in Large\n  Language Model Use Cases"
                },
                "summary": "Large Language Models (LLMs) have been observed to exhibit bias in numerous\nways, potentially creating or worsening outcomes for specific groups identified\nby protected attributes such as sex, race, sexual orientation, or age. To help\naddress this gap, we introduce LangFair, an open-source Python package that\naims to equip LLM practitioners with the tools to evaluate bias and fairness\nrisks relevant to their specific use cases. The package offers functionality to\neasily generate evaluation datasets, comprised of LLM responses to\nuse-case-specific prompts, and subsequently calculate applicable metrics for\nthe practitioner's use case. To guide in metric selection, LangFair offers an\nactionable decision framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been observed to exhibit bias in numerous\nways, potentially creating or worsening outcomes for specific groups identified\nby protected attributes such as sex, race, sexual orientation, or age. To help\naddress this gap, we introduce LangFair, an open-source Python package that\naims to equip LLM practitioners with the tools to evaluate bias and fairness\nrisks relevant to their specific use cases. The package offers functionality to\neasily generate evaluation datasets, comprised of LLM responses to\nuse-case-specific prompts, and subsequently calculate applicable metrics for\nthe practitioner's use case. To guide in metric selection, LangFair offers an\nactionable decision framework."
                },
                "authors": [
                    {
                        "name": "Dylan Bouchard"
                    },
                    {
                        "name": "Mohit Singh Chauhan"
                    },
                    {
                        "name": "David Skarbrevik"
                    },
                    {
                        "name": "Viren Bajaj"
                    },
                    {
                        "name": "Zeya Ahmad"
                    }
                ],
                "author_detail": {
                    "name": "Zeya Ahmad"
                },
                "author": "Zeya Ahmad",
                "arxiv_comment": "Journal of Open Source Software; LangFair repository:\n  https://github.com/cvs-health/langfair",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22733v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22733v3",
                "updated": "2025-01-06T16:19:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    19,
                    15,
                    0,
                    6,
                    0
                ],
                "published": "2024-10-30T06:39:27Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    6,
                    39,
                    27,
                    2,
                    304,
                    0
                ],
                "title": "ETO:Efficient Transformer-based Local Feature Matching by Organizing\n  Multiple Homography Hypotheses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETO:Efficient Transformer-based Local Feature Matching by Organizing\n  Multiple Homography Hypotheses"
                },
                "summary": "We tackle the efficiency problem of learning local feature matching. Recent\nadvancements have given rise to purely CNN-based and transformer-based\napproaches, each augmented with deep learning techniques. While CNN-based\nmethods often excel in matching speed, transformer-based methods tend to\nprovide more accurate matches. We propose an efficient transformer-based\nnetwork architecture for local feature matching. This technique is built on\nconstructing multiple homography hypotheses to approximate the continuous\ncorrespondence in the real world and uni-directional cross-attention to\naccelerate the refinement. On the YFCC100M dataset, our matching accuracy is\ncompetitive with LoFTR, a state-of-the-art transformer-based architecture,\nwhile the inference speed is boosted to 4 times, even outperforming the\nCNN-based methods. Comprehensive evaluations on other open datasets such as\nMegadepth, ScanNet, and HPatches demonstrate our method's efficacy,\nhighlighting its potential to significantly enhance a wide array of downstream\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We tackle the efficiency problem of learning local feature matching. Recent\nadvancements have given rise to purely CNN-based and transformer-based\napproaches, each augmented with deep learning techniques. While CNN-based\nmethods often excel in matching speed, transformer-based methods tend to\nprovide more accurate matches. We propose an efficient transformer-based\nnetwork architecture for local feature matching. This technique is built on\nconstructing multiple homography hypotheses to approximate the continuous\ncorrespondence in the real world and uni-directional cross-attention to\naccelerate the refinement. On the YFCC100M dataset, our matching accuracy is\ncompetitive with LoFTR, a state-of-the-art transformer-based architecture,\nwhile the inference speed is boosted to 4 times, even outperforming the\nCNN-based methods. Comprehensive evaluations on other open datasets such as\nMegadepth, ScanNet, and HPatches demonstrate our method's efficacy,\nhighlighting its potential to significantly enhance a wide array of downstream\napplications."
                },
                "authors": [
                    {
                        "name": "Junjie Ni"
                    },
                    {
                        "name": "Guofeng Zhang"
                    },
                    {
                        "name": "Guanglin Li"
                    },
                    {
                        "name": "Yijin Li"
                    },
                    {
                        "name": "Xinyang Liu"
                    },
                    {
                        "name": "Zhaoyang Huang"
                    },
                    {
                        "name": "Hujun Bao"
                    }
                ],
                "author_detail": {
                    "name": "Hujun Bao"
                },
                "author": "Hujun Bao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22733v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22733v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03628v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03628v2",
                "updated": "2025-01-06T15:37:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    15,
                    37,
                    48,
                    0,
                    6,
                    0
                ],
                "published": "2024-06-05T21:24:26Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    21,
                    24,
                    26,
                    2,
                    157,
                    0
                ],
                "title": "Synthetic Oversampling: Theory and A Practical Approach Using LLMs to\n  Address Data Imbalance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Oversampling: Theory and A Practical Approach Using LLMs to\n  Address Data Imbalance"
                },
                "summary": "Imbalanced classification and spurious correlation are common challenges in\ndata science and machine learning. Both issues are linked to data imbalance,\nwith certain groups of data samples significantly underrepresented, which in\nturn would compromise the accuracy, robustness and generalizability of the\nlearned models. Recent advances have proposed leveraging the flexibility and\ngenerative capabilities of large language models (LLMs), typically built on\ntransformer architectures, to generate synthetic samples and to augment the\nobserved data. In the context of imbalanced data, LLMs are used to oversample\nunderrepresented groups and have shown promising improvements. However, there\nis a clear lack of theoretical understanding of such synthetic data approaches.\nIn this article, we develop novel theoretical foundations to systematically\nstudy the roles of synthetic samples in addressing imbalanced classification\nand spurious correlation. Specifically, we first explicitly quantify the\nbenefits of synthetic oversampling. Next, we analyze the scaling dynamics in\nsynthetic data augmentation, and derive the corresponding scaling law. Finally,\nwe demonstrate the capacity of transformer models to generate high-quality\nsynthetic samples. We further conduct extensive numerical experiments to\nvalidate the efficacy of the LLM-based synthetic oversampling and augmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imbalanced classification and spurious correlation are common challenges in\ndata science and machine learning. Both issues are linked to data imbalance,\nwith certain groups of data samples significantly underrepresented, which in\nturn would compromise the accuracy, robustness and generalizability of the\nlearned models. Recent advances have proposed leveraging the flexibility and\ngenerative capabilities of large language models (LLMs), typically built on\ntransformer architectures, to generate synthetic samples and to augment the\nobserved data. In the context of imbalanced data, LLMs are used to oversample\nunderrepresented groups and have shown promising improvements. However, there\nis a clear lack of theoretical understanding of such synthetic data approaches.\nIn this article, we develop novel theoretical foundations to systematically\nstudy the roles of synthetic samples in addressing imbalanced classification\nand spurious correlation. Specifically, we first explicitly quantify the\nbenefits of synthetic oversampling. Next, we analyze the scaling dynamics in\nsynthetic data augmentation, and derive the corresponding scaling law. Finally,\nwe demonstrate the capacity of transformer models to generate high-quality\nsynthetic samples. We further conduct extensive numerical experiments to\nvalidate the efficacy of the LLM-based synthetic oversampling and augmentation."
                },
                "authors": [
                    {
                        "name": "Ryumei Nakada"
                    },
                    {
                        "name": "Yichen Xu"
                    },
                    {
                        "name": "Lexin Li"
                    },
                    {
                        "name": "Linjun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linjun Zhang"
                },
                "author": "Linjun Zhang",
                "arxiv_comment": "82 pages, 28 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03628v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03628v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03073v1",
                "updated": "2025-01-06T15:10:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    15,
                    10,
                    22,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T15:10:22Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    15,
                    10,
                    22,
                    0,
                    6,
                    0
                ],
                "title": "Retrieval-Augmented TLAPS Proof Generation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented TLAPS Proof Generation with Large Language Models"
                },
                "summary": "We present a novel approach to automated proof generation for the TLA+ Proof\nSystem (TLAPS) using Large Language Models (LLMs). Our method combines two key\ncomponents: a sub-proof obligation generation phase that breaks down complex\nproof obligations into simpler sub-obligations, and a proof generation phase\nthat leverages Retrieval-Augmented Generation with verified proof examples. We\nevaluate our approach using proof obligations from varying complexity levels of\nproof obligations, spanning from fundamental arithmetic properties to the\nproperties of algorithms. Our experiments demonstrate that while the method\nsuccessfully generates valid proofs for intermediate-complexity obligations, it\nfaces limitations with more complex theorems. These results indicate that our\napproach can effectively assist in proof development for certain classes of\nproperties, contributing to the broader goal of integrating LLMs into formal\nverification workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach to automated proof generation for the TLA+ Proof\nSystem (TLAPS) using Large Language Models (LLMs). Our method combines two key\ncomponents: a sub-proof obligation generation phase that breaks down complex\nproof obligations into simpler sub-obligations, and a proof generation phase\nthat leverages Retrieval-Augmented Generation with verified proof examples. We\nevaluate our approach using proof obligations from varying complexity levels of\nproof obligations, spanning from fundamental arithmetic properties to the\nproperties of algorithms. Our experiments demonstrate that while the method\nsuccessfully generates valid proofs for intermediate-complexity obligations, it\nfaces limitations with more complex theorems. These results indicate that our\napproach can effectively assist in proof development for certain classes of\nproperties, contributing to the broader goal of integrating LLMs into formal\nverification workflows."
                },
                "authors": [
                    {
                        "name": "Yuhao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yuhao Zhou"
                },
                "author": "Yuhao Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00722v2",
                "updated": "2025-01-06T15:09:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    15,
                    9,
                    6,
                    0,
                    6,
                    0
                ],
                "published": "2024-08-01T17:15:13Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    17,
                    15,
                    13,
                    3,
                    214,
                    0
                ],
                "title": "Pathway to Secure and Trustworthy ZSM for LLMs: Attacks, Defense, and\n  Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pathway to Secure and Trustworthy ZSM for LLMs: Attacks, Defense, and\n  Opportunities"
                },
                "summary": "Recently, large language models (LLMs) have been gaining a lot of interest\ndue to their adaptability and extensibility in emerging applications, including\ncommunication networks. It is anticipated that ZSM networks will be able to\nsupport LLMs as a service, as they provide ultra reliable low-latency\ncommunications and closed loop massive connectivity. However, LLMs are\nvulnerable to data and model privacy issues that affect the trustworthiness of\nLLMs to be deployed for user-based services. In this paper, we explore the\nsecurity vulnerabilities associated with fine-tuning LLMs in ZSM networks, in\nparticular the membership inference attack. We define the characteristics of an\nattack network that can perform a membership inference attack if the attacker\nhas access to the fine-tuned model for the downstream task. We show that the\nmembership inference attacks are effective for any downstream task, which can\nlead to a personal data breach when using LLM as a service. The experimental\nresults show that the attack success rate of maximum 92% can be achieved on\nnamed entity recognition task. Based on the experimental analysis, we discuss\npossible defense mechanisms and present possible research directions to make\nthe LLMs more trustworthy in the context of ZSM networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have been gaining a lot of interest\ndue to their adaptability and extensibility in emerging applications, including\ncommunication networks. It is anticipated that ZSM networks will be able to\nsupport LLMs as a service, as they provide ultra reliable low-latency\ncommunications and closed loop massive connectivity. However, LLMs are\nvulnerable to data and model privacy issues that affect the trustworthiness of\nLLMs to be deployed for user-based services. In this paper, we explore the\nsecurity vulnerabilities associated with fine-tuning LLMs in ZSM networks, in\nparticular the membership inference attack. We define the characteristics of an\nattack network that can perform a membership inference attack if the attacker\nhas access to the fine-tuned model for the downstream task. We show that the\nmembership inference attacks are effective for any downstream task, which can\nlead to a personal data breach when using LLM as a service. The experimental\nresults show that the attack success rate of maximum 92% can be achieved on\nnamed entity recognition task. Based on the experimental analysis, we discuss\npossible defense mechanisms and present possible research directions to make\nthe LLMs more trustworthy in the context of ZSM networks."
                },
                "authors": [
                    {
                        "name": "Sunder Ali Khowaja"
                    },
                    {
                        "name": "Parus Khuwaja"
                    },
                    {
                        "name": "Kapal Dev"
                    },
                    {
                        "name": "Hussam Al Hamadi"
                    },
                    {
                        "name": "Engin Zeydan"
                    }
                ],
                "author_detail": {
                    "name": "Engin Zeydan"
                },
                "author": "Engin Zeydan",
                "arxiv_comment": "7 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03067v1",
                "updated": "2025-01-06T15:04:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    15,
                    4,
                    45,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T15:04:45Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    15,
                    4,
                    45,
                    0,
                    6,
                    0
                ],
                "title": "Design and implementation of tools to build an ontology of Security\n  Requirements for Internet of Medical Things",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and implementation of tools to build an ontology of Security\n  Requirements for Internet of Medical Things"
                },
                "summary": "When developing devices, architectures and services for the Internet of\nMedical Things (IoMT) world, manufacturers or integrators must be aware of the\nsecurity requirements expressed by both laws and specifications. To provide\ntools guiding through these requirements and to assure a third party of the\ncorrect compliance, an ontology charting the relevant laws and specifications\n(for the European context) is very useful. We here address the development of\nthis ontology. Due to the very high number and size of the considered\nspecification documents, we have put in place a methodology and tools to\nsimplify the transition from natural text to an ontology. The first step is a\nmanual highlighting of relevant concepts in the corpus, then a manual\ntranslation to XML/XSD is operated. We have developed a tool allowing us to\nconvert this semi-structured data into an ontology. Because the different\nspecifications use similar but different wording, our approach favors the\ncreation of similar instances in the ontology. To improve the ontology\nsimplification through instance merging, we consider the use of LLMs. The\nresponses of the LLMs are compared against our manually defined correct\nresponses. The quality of the responses of the automated system does not prove\nto be good enough to be trusted blindly, and should only be used as a starting\npoint for a manual correction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When developing devices, architectures and services for the Internet of\nMedical Things (IoMT) world, manufacturers or integrators must be aware of the\nsecurity requirements expressed by both laws and specifications. To provide\ntools guiding through these requirements and to assure a third party of the\ncorrect compliance, an ontology charting the relevant laws and specifications\n(for the European context) is very useful. We here address the development of\nthis ontology. Due to the very high number and size of the considered\nspecification documents, we have put in place a methodology and tools to\nsimplify the transition from natural text to an ontology. The first step is a\nmanual highlighting of relevant concepts in the corpus, then a manual\ntranslation to XML/XSD is operated. We have developed a tool allowing us to\nconvert this semi-structured data into an ontology. Because the different\nspecifications use similar but different wording, our approach favors the\ncreation of similar instances in the ontology. To improve the ontology\nsimplification through instance merging, we consider the use of LLMs. The\nresponses of the LLMs are compared against our manually defined correct\nresponses. The quality of the responses of the automated system does not prove\nto be good enough to be trusted blindly, and should only be used as a starting\npoint for a manual correction."
                },
                "authors": [
                    {
                        "name": "Daniel Naro"
                    },
                    {
                        "name": "Jaime Delgado"
                    },
                    {
                        "name": "Silvia Llorente"
                    },
                    {
                        "name": "Amanda Palomo"
                    }
                ],
                "author_detail": {
                    "name": "Amanda Palomo"
                },
                "author": "Amanda Palomo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03066v1",
                "updated": "2025-01-06T15:04:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    15,
                    4,
                    42,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T15:04:42Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    15,
                    4,
                    42,
                    0,
                    6,
                    0
                ],
                "title": "Shock and SEP Modeling Study for the 5 September 2022 SEP Event",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shock and SEP Modeling Study for the 5 September 2022 SEP Event"
                },
                "summary": "On September 5, 2022, during Parker Solar Probe's (PSP) 13th encounter, a\nfast shock wave and a related solar energetic particle (SEP) event were\nobserved as the spacecraft approached the perihelion of its orbit. Observations\nfrom the Integrated Science Investigation of the Sun (ISOIS) instrument suite\nshow that SEPs arrived at the spacecraft with a significant delay from the\nonset of the parent solar eruption and that the first arriving SEPs exhibited\nan Inverse Velocity Dispersion (IVD) for energetic protons above $\\sim$1~MeV.\nUtilizing data from multiple spacecraft we investigate the eruption dynamics\nand shock wave propagation. Our analysis includes 3D shock modeling and SEP\ntransport simulations to examine the origins of this SEP event and explore the\ncauses of the delayed SEP onset and the observed IVD. The data-driven SEP\nsimulation reproduces the SEP event onset observed at PSP, its evolving energy\nspectrum and the IVD. This IVD is attributed to a relatively slow, ongoing\nparticle acceleration process occurring at the flank of the expanding shock\nwave intercepted by PSP. This has significant implications for the role of\nshocks in the release of SEPs at widespread events and for methods used to\ninfer the SEP release times. Furthermore, the match between the simulation and\nobservations worsens when cross-field diffusion is considered, indicating that\nSEP diffusion had a minor effect on this event. These findings underscore the\ncomplexity of SEP events and emphasize the need for advanced modelling\napproaches to better understand the role of shock waves and other physical\nprocesses in SEP acceleration and release.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On September 5, 2022, during Parker Solar Probe's (PSP) 13th encounter, a\nfast shock wave and a related solar energetic particle (SEP) event were\nobserved as the spacecraft approached the perihelion of its orbit. Observations\nfrom the Integrated Science Investigation of the Sun (ISOIS) instrument suite\nshow that SEPs arrived at the spacecraft with a significant delay from the\nonset of the parent solar eruption and that the first arriving SEPs exhibited\nan Inverse Velocity Dispersion (IVD) for energetic protons above $\\sim$1~MeV.\nUtilizing data from multiple spacecraft we investigate the eruption dynamics\nand shock wave propagation. Our analysis includes 3D shock modeling and SEP\ntransport simulations to examine the origins of this SEP event and explore the\ncauses of the delayed SEP onset and the observed IVD. The data-driven SEP\nsimulation reproduces the SEP event onset observed at PSP, its evolving energy\nspectrum and the IVD. This IVD is attributed to a relatively slow, ongoing\nparticle acceleration process occurring at the flank of the expanding shock\nwave intercepted by PSP. This has significant implications for the role of\nshocks in the release of SEPs at widespread events and for methods used to\ninfer the SEP release times. Furthermore, the match between the simulation and\nobservations worsens when cross-field diffusion is considered, indicating that\nSEP diffusion had a minor effect on this event. These findings underscore the\ncomplexity of SEP events and emphasize the need for advanced modelling\napproaches to better understand the role of shock waves and other physical\nprocesses in SEP acceleration and release."
                },
                "authors": [
                    {
                        "name": "A. Kouloumvakos"
                    },
                    {
                        "name": "N. Wijsen"
                    },
                    {
                        "name": "I. C. Jebaraj"
                    },
                    {
                        "name": "A. Afanasiev"
                    },
                    {
                        "name": "D. Lario"
                    },
                    {
                        "name": "C. M. S. Cohen"
                    },
                    {
                        "name": "P. Riley"
                    },
                    {
                        "name": "D. G. Mitchell"
                    },
                    {
                        "name": "Z. Ding"
                    },
                    {
                        "name": "A. Vourlidas"
                    },
                    {
                        "name": "J. Giacalone"
                    },
                    {
                        "name": "X. Chen"
                    },
                    {
                        "name": "M. E. Hill"
                    }
                ],
                "author_detail": {
                    "name": "M. E. Hill"
                },
                "author": "M. E. Hill",
                "arxiv_doi": "10.3847/1538-4357/ada0be",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ada0be",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.03066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 11 figures, Accepted in The Astrophysical Journal",
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.13969v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.13969v4",
                "updated": "2025-01-06T14:59:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    14,
                    59,
                    12,
                    0,
                    6,
                    0
                ],
                "published": "2023-11-23T12:32:50Z",
                "published_parsed": [
                    2023,
                    11,
                    23,
                    12,
                    32,
                    50,
                    3,
                    327,
                    0
                ],
                "title": "Was Javert right to be suspicious? Unpacking treatment effect\n  heterogeneity of alternative sentences on time-to-recidivism in Brazil",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Was Javert right to be suspicious? Unpacking treatment effect\n  heterogeneity of alternative sentences on time-to-recidivism in Brazil"
                },
                "summary": "This paper presents econometric tools to unpack the treatment effect\nheterogeneity of punishing misdemeanor offenses on time-to-recidivism. We show\nhow one can identify, estimate, and make inferences on the distributional,\nquantile, and average marginal treatment effects in setups where the treatment\nselection is endogenous and the outcome of interest, usually a duration\nvariable, is potentially right-censored. We explore our proposed econometric\nmethodology to evaluate the effect of fines and community service sentences as\na form of punishment on time-to-recidivism in the State of S\\~ao Paulo, Brazil,\nbetween 2010 and 2019, leveraging the as-if random assignment of judges to\ncases. Our results highlight substantial treatment effect heterogeneity that\nother tools are not meant to capture. For instance, we find that people whom\nmost judges would punish take longer to recidivate as a consequence of the\npunishment, while people who would be punished only by strict judges recidivate\nat an earlier date than if they were not punished.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents econometric tools to unpack the treatment effect\nheterogeneity of punishing misdemeanor offenses on time-to-recidivism. We show\nhow one can identify, estimate, and make inferences on the distributional,\nquantile, and average marginal treatment effects in setups where the treatment\nselection is endogenous and the outcome of interest, usually a duration\nvariable, is potentially right-censored. We explore our proposed econometric\nmethodology to evaluate the effect of fines and community service sentences as\na form of punishment on time-to-recidivism in the State of S\\~ao Paulo, Brazil,\nbetween 2010 and 2019, leveraging the as-if random assignment of judges to\ncases. Our results highlight substantial treatment effect heterogeneity that\nother tools are not meant to capture. For instance, we find that people whom\nmost judges would punish take longer to recidivate as a consequence of the\npunishment, while people who would be punished only by strict judges recidivate\nat an earlier date than if they were not punished."
                },
                "authors": [
                    {
                        "name": "Santiago Acerenza"
                    },
                    {
                        "name": "Vitor Possebom"
                    },
                    {
                        "name": "Pedro H. C. Sant'Anna"
                    }
                ],
                "author_detail": {
                    "name": "Pedro H. C. Sant'Anna"
                },
                "author": "Pedro H. C. Sant'Anna",
                "arxiv_comment": "More detailed empirical analysis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.13969v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.13969v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19047v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19047v4",
                "updated": "2025-01-06T14:43:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    14,
                    43,
                    25,
                    0,
                    6,
                    0
                ],
                "published": "2024-02-29T11:20:16Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    11,
                    20,
                    16,
                    3,
                    60,
                    0
                ],
                "title": "Theoretical Foundations of Deep Selective State-Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theoretical Foundations of Deep Selective State-Space Models"
                },
                "summary": "Structured state-space models (SSMs) such as S4, stemming from the seminal\nwork of Gu et al., are gaining popularity as effective approaches for modeling\nsequential data. Deep SSMs demonstrate outstanding performance across a diverse\nset of domains, at a reduced training and inference cost compared to\nattention-based transformers. Recent developments show that if the linear\nrecurrence powering SSMs allows for multiplicative interactions between inputs\nand hidden states (e.g. GateLoop, Mamba, GLA), then the resulting architecture\ncan surpass in both in accuracy and efficiency attention-powered foundation\nmodels trained on text, at scales of billion parameters. In this paper, we give\ntheoretical grounding to this recent finding using tools from Rough Path\nTheory: we show that when random linear recurrences are equipped with simple\ninput-controlled transitions (selectivity mechanism), then the hidden state is\nprovably a low-dimensional projection of a powerful mathematical object called\nthe signature of the input -- capturing non-linear interactions between tokens\nat distinct timescales. Our theory not only motivates the success of modern\nselective state-space models such as Mamba but also provides a solid framework\nto understand the expressive power of future SSM variants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured state-space models (SSMs) such as S4, stemming from the seminal\nwork of Gu et al., are gaining popularity as effective approaches for modeling\nsequential data. Deep SSMs demonstrate outstanding performance across a diverse\nset of domains, at a reduced training and inference cost compared to\nattention-based transformers. Recent developments show that if the linear\nrecurrence powering SSMs allows for multiplicative interactions between inputs\nand hidden states (e.g. GateLoop, Mamba, GLA), then the resulting architecture\ncan surpass in both in accuracy and efficiency attention-powered foundation\nmodels trained on text, at scales of billion parameters. In this paper, we give\ntheoretical grounding to this recent finding using tools from Rough Path\nTheory: we show that when random linear recurrences are equipped with simple\ninput-controlled transitions (selectivity mechanism), then the hidden state is\nprovably a low-dimensional projection of a powerful mathematical object called\nthe signature of the input -- capturing non-linear interactions between tokens\nat distinct timescales. Our theory not only motivates the success of modern\nselective state-space models such as Mamba but also provides a solid framework\nto understand the expressive power of future SSM variants."
                },
                "authors": [
                    {
                        "name": "Nicola Muca Cirone"
                    },
                    {
                        "name": "Antonio Orvieto"
                    },
                    {
                        "name": "Benjamin Walker"
                    },
                    {
                        "name": "Cristopher Salvi"
                    },
                    {
                        "name": "Terry Lyons"
                    }
                ],
                "author_detail": {
                    "name": "Terry Lyons"
                },
                "author": "Terry Lyons",
                "arxiv_comment": "Fina NeurIPS Camera Ready Version w/ minor edits",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19047v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19047v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03045v1",
                "updated": "2025-01-06T14:32:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    14,
                    32,
                    24,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T14:32:24Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    14,
                    32,
                    24,
                    0,
                    6,
                    0
                ],
                "title": "Single-Channel Distance-Based Source Separation for Mobile GPU in\n  Outdoor and Indoor Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-Channel Distance-Based Source Separation for Mobile GPU in\n  Outdoor and Indoor Environments"
                },
                "summary": "This study emphasizes the significance of exploring distance-based source\nseparation (DSS) in outdoor environments. Unlike existing studies that\nprimarily focus on indoor settings, the proposed model is designed to capture\nthe unique characteristics of outdoor audio sources. It incorporates advanced\ntechniques, including a two-stage conformer block, a linear relation-aware\nself-attention (RSA), and a TensorFlow Lite GPU delegate. While the linear RSA\nmay not capture physical cues as explicitly as the quadratic RSA, the linear\nRSA enhances the model's context awareness, leading to improved performance on\nthe DSS that requires an understanding of physical cues in outdoor and indoor\nenvironments. The experimental results demonstrated that the proposed model\novercomes the limitations of existing approaches and considerably enhances\nenergy efficiency and real-time inference speed on mobile devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study emphasizes the significance of exploring distance-based source\nseparation (DSS) in outdoor environments. Unlike existing studies that\nprimarily focus on indoor settings, the proposed model is designed to capture\nthe unique characteristics of outdoor audio sources. It incorporates advanced\ntechniques, including a two-stage conformer block, a linear relation-aware\nself-attention (RSA), and a TensorFlow Lite GPU delegate. While the linear RSA\nmay not capture physical cues as explicitly as the quadratic RSA, the linear\nRSA enhances the model's context awareness, leading to improved performance on\nthe DSS that requires an understanding of physical cues in outdoor and indoor\nenvironments. The experimental results demonstrated that the proposed model\novercomes the limitations of existing approaches and considerably enhances\nenergy efficiency and real-time inference speed on mobile devices."
                },
                "authors": [
                    {
                        "name": "Hanbin Bae"
                    },
                    {
                        "name": "Byungjun Kang"
                    },
                    {
                        "name": "Jiwon Kim"
                    },
                    {
                        "name": "Jaeyong Hwang"
                    },
                    {
                        "name": "Hosang Sung"
                    },
                    {
                        "name": "Hoon-Young Cho"
                    }
                ],
                "author_detail": {
                    "name": "Hoon-Young Cho"
                },
                "author": "Hoon-Young Cho",
                "arxiv_comment": "Accepted by ICASSP2025. \\c{opyright} 2025 IEEE. Personal use of this\n  material is permitted. Permission from IEEE must be obtained for all other\n  uses, in any current or future media, including reprinting/republishing this\n  material for advertising or promotional purposes, creating new collective\n  works, for resale or redistribution to servers or lists, or reuse of any\n  copyrighted component",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.09721v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.09721v2",
                "updated": "2025-01-06T14:31:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    14,
                    31,
                    39,
                    0,
                    6,
                    0
                ],
                "published": "2023-08-12T13:31:02Z",
                "published_parsed": [
                    2023,
                    8,
                    12,
                    13,
                    31,
                    2,
                    5,
                    224,
                    0
                ],
                "title": "A new solution and concrete implementation steps for Artificial General\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new solution and concrete implementation steps for Artificial General\n  Intelligence"
                },
                "summary": "In this paper, we propose a new approach to building a artificial general\nintelligence with self awareness, which includes: (1) a new method to implement\nattention mechanisms; (2) a way to give machines self-demands; (3) how to form\na value evaluation system compatible with the network; (4) a way to create the\nworld models; (5) how to realize a top-down, hierarchical thinking\ndecision-making chain; (6) a way to achieve general decision-making and\nresponse capabilities; (7) a way for a machine to directly obtain human\nexperience through language. In the paper, we first analyze some of the\nshortcomings of current LLMs (Large Language Model) and propose ideas for\nimprovement. Then we analyze why our scheme can solve the above problems and\nprovide detailed steps for implementing our scheme. In chapter 4, we have\npresented a step-by-step mplementation roadmap. And in chapter 5, we have\npresented a specific implementation demonstration. In chapter 6, we analyze the\nadvantages and disadvantages of our scheme and propose further research\ndirections. In this article, we have put forward how to create genuine\nartificial general intelligence step by step. It can handle data of all\nmodalities in a unified form and can directly understand the experience that\nhumans already possess through language, thus avoiding the problem that\nreinforcement learning is required for every decision-making process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a new approach to building a artificial general\nintelligence with self awareness, which includes: (1) a new method to implement\nattention mechanisms; (2) a way to give machines self-demands; (3) how to form\na value evaluation system compatible with the network; (4) a way to create the\nworld models; (5) how to realize a top-down, hierarchical thinking\ndecision-making chain; (6) a way to achieve general decision-making and\nresponse capabilities; (7) a way for a machine to directly obtain human\nexperience through language. In the paper, we first analyze some of the\nshortcomings of current LLMs (Large Language Model) and propose ideas for\nimprovement. Then we analyze why our scheme can solve the above problems and\nprovide detailed steps for implementing our scheme. In chapter 4, we have\npresented a step-by-step mplementation roadmap. And in chapter 5, we have\npresented a specific implementation demonstration. In chapter 6, we analyze the\nadvantages and disadvantages of our scheme and propose further research\ndirections. In this article, we have put forward how to create genuine\nartificial general intelligence step by step. It can handle data of all\nmodalities in a unified form and can directly understand the experience that\nhumans already possess through language, thus avoiding the problem that\nreinforcement learning is required for every decision-making process."
                },
                "authors": [
                    {
                        "name": "Yongcong Chen"
                    },
                    {
                        "name": "Ting Zeng"
                    },
                    {
                        "name": "Xingyue Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xingyue Chen"
                },
                "author": "Xingyue Chen",
                "arxiv_comment": "25 pages, 2 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.09721v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.09721v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03040v1",
                "updated": "2025-01-06T14:27:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    14,
                    27,
                    41,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T14:27:41Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    14,
                    27,
                    41,
                    0,
                    6,
                    0
                ],
                "title": "ChronoSense: Exploring Temporal Understanding in Large Language Models\n  with Time Intervals of Events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChronoSense: Exploring Temporal Understanding in Large Language Models\n  with Time Intervals of Events"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success in various NLP\ntasks, yet they still face significant challenges in reasoning and arithmetic.\nTemporal reasoning, a critical component of natural language understanding, has\nraised increasing research attention. However, comprehensive testing of Allen's\ninterval relations (e.g., before, after, during) -- a fundamental framework for\ntemporal relationships -- remains underexplored. To fill this gap, we present\nChronoSense, a new benchmark for evaluating LLMs' temporal understanding. It\nincludes 16 tasks, focusing on identifying the Allen relation between two\ntemporal events and temporal arithmetic, using both abstract events and\nreal-world data from Wikidata. We assess the performance of seven recent LLMs\nusing this benchmark and the results indicate that models handle Allen\nrelations, even symmetrical ones, quite differently. Moreover, the findings\nsuggest that the models may rely on memorization to answer time-related\nquestions. Overall, the models' low performance highlights the need for\nimproved temporal understanding in LLMs and ChronoSense offers a robust\nframework for future research in this area. Our dataset and the source code are\navailable at https://github.com/duyguislakoglu/chronosense.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success in various NLP\ntasks, yet they still face significant challenges in reasoning and arithmetic.\nTemporal reasoning, a critical component of natural language understanding, has\nraised increasing research attention. However, comprehensive testing of Allen's\ninterval relations (e.g., before, after, during) -- a fundamental framework for\ntemporal relationships -- remains underexplored. To fill this gap, we present\nChronoSense, a new benchmark for evaluating LLMs' temporal understanding. It\nincludes 16 tasks, focusing on identifying the Allen relation between two\ntemporal events and temporal arithmetic, using both abstract events and\nreal-world data from Wikidata. We assess the performance of seven recent LLMs\nusing this benchmark and the results indicate that models handle Allen\nrelations, even symmetrical ones, quite differently. Moreover, the findings\nsuggest that the models may rely on memorization to answer time-related\nquestions. Overall, the models' low performance highlights the need for\nimproved temporal understanding in LLMs and ChronoSense offers a robust\nframework for future research in this area. Our dataset and the source code are\navailable at https://github.com/duyguislakoglu/chronosense."
                },
                "authors": [
                    {
                        "name": "Duygu Sezen Islakoglu"
                    },
                    {
                        "name": "Jan-Christoph Kalo"
                    }
                ],
                "author_detail": {
                    "name": "Jan-Christoph Kalo"
                },
                "author": "Jan-Christoph Kalo",
                "arxiv_comment": "14 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03035v1",
                "updated": "2025-01-06T14:23:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    14,
                    23,
                    2,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T14:23:02Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    14,
                    23,
                    2,
                    0,
                    6,
                    0
                ],
                "title": "Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization\n  Degradation for Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization\n  Degradation for Mathematical Reasoning"
                },
                "summary": "Large language models have achieved significant advancements in complex\nmathematical reasoning benchmarks, such as MATH. However, their substantial\ncomputational requirements present challenges for practical deployment. Model\nquantization has emerged as an effective strategy to reduce memory usage and\ncomputational costs by employing lower precision and bit-width representations.\nIn this study, we systematically evaluate the impact of quantization on\nmathematical reasoning tasks. We introduce a multidimensional evaluation\nframework that qualitatively assesses specific capability dimensions and\nconduct quantitative analyses on the step-by-step outputs of various\nquantization methods. Our results demonstrate that quantization differentially\naffects numerical computation and reasoning planning abilities, identifying key\nareas where quantized models experience performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have achieved significant advancements in complex\nmathematical reasoning benchmarks, such as MATH. However, their substantial\ncomputational requirements present challenges for practical deployment. Model\nquantization has emerged as an effective strategy to reduce memory usage and\ncomputational costs by employing lower precision and bit-width representations.\nIn this study, we systematically evaluate the impact of quantization on\nmathematical reasoning tasks. We introduce a multidimensional evaluation\nframework that qualitatively assesses specific capability dimensions and\nconduct quantitative analyses on the step-by-step outputs of various\nquantization methods. Our results demonstrate that quantization differentially\naffects numerical computation and reasoning planning abilities, identifying key\nareas where quantized models experience performance degradation."
                },
                "authors": [
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Yupeng Su"
                    },
                    {
                        "name": "Runming Yang"
                    },
                    {
                        "name": "Zhongwei Xie"
                    },
                    {
                        "name": "Ngai Wong"
                    },
                    {
                        "name": "Hongxia Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hongxia Yang"
                },
                "author": "Hongxia Yang",
                "arxiv_comment": "4 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07349v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07349v3",
                "updated": "2025-01-06T14:19:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    14,
                    19,
                    10,
                    0,
                    6,
                    0
                ],
                "published": "2024-10-09T18:01:43Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    18,
                    1,
                    43,
                    2,
                    283,
                    0
                ],
                "title": "Psi-GAN: A power-spectrum-informed generative adversarial network for\n  the emulation of large-scale structure maps across cosmologies and redshifts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Psi-GAN: A power-spectrum-informed generative adversarial network for\n  the emulation of large-scale structure maps across cosmologies and redshifts"
                },
                "summary": "Simulations of the dark matter distribution throughout the Universe are\nessential in order to analyse data from cosmological surveys. $N$-body\nsimulations are computationally expensive, and many cheaper alternatives (such\nas lognormal random fields) fail to reproduce accurate statistics of the\nsmaller, non-linear scales. In this work, we present \\textsc{Psi-GAN}\n(\\textbf{P}ower-\\textbf{s}pectrum-\\textbf{i}nformed \\textbf{G}enerative\n\\textbf{A}dversarial \\textbf{N}etwork), a machine learning model which takes a\ntwo-dimensional lognormal dark matter density field and transforms it into a\nmore realistic field. We construct \\textsc{Psi-GAN} so that it is continuously\nconditional, and can therefore generate realistic realisations of the dark\nmatter density field across a range of cosmologies and redshifts in $z \\in [0,\n3]$. We train \\textsc{Psi-GAN} as a generative adversarial network on $2\\,000$\nsimulation boxes from the Quijote simulation suite. We use a novel critic\narchitecture that utilises the power spectrum as the basis for discrimination\nbetween real and generated samples. \\textsc{Psi-GAN} shows agreement with\n$N$-body simulations over a range of redshifts and cosmologies, consistently\noutperforming the lognormal approximation on all tests of non-linear structure,\nsuch as being able to reproduce both the power spectrum up to wavenumbers of\n$1~h~\\mathrm{Mpc}^{-1}$, and the bispectra of target $N$-body simulations to\nwithin ${\\sim}5$ per cent. Our improved ability to model non-linear structure\nshould allow more robust constraints on cosmological parameters when used in\ntechniques such as simulation-based inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulations of the dark matter distribution throughout the Universe are\nessential in order to analyse data from cosmological surveys. $N$-body\nsimulations are computationally expensive, and many cheaper alternatives (such\nas lognormal random fields) fail to reproduce accurate statistics of the\nsmaller, non-linear scales. In this work, we present \\textsc{Psi-GAN}\n(\\textbf{P}ower-\\textbf{s}pectrum-\\textbf{i}nformed \\textbf{G}enerative\n\\textbf{A}dversarial \\textbf{N}etwork), a machine learning model which takes a\ntwo-dimensional lognormal dark matter density field and transforms it into a\nmore realistic field. We construct \\textsc{Psi-GAN} so that it is continuously\nconditional, and can therefore generate realistic realisations of the dark\nmatter density field across a range of cosmologies and redshifts in $z \\in [0,\n3]$. We train \\textsc{Psi-GAN} as a generative adversarial network on $2\\,000$\nsimulation boxes from the Quijote simulation suite. We use a novel critic\narchitecture that utilises the power spectrum as the basis for discrimination\nbetween real and generated samples. \\textsc{Psi-GAN} shows agreement with\n$N$-body simulations over a range of redshifts and cosmologies, consistently\noutperforming the lognormal approximation on all tests of non-linear structure,\nsuch as being able to reproduce both the power spectrum up to wavenumbers of\n$1~h~\\mathrm{Mpc}^{-1}$, and the bispectra of target $N$-body simulations to\nwithin ${\\sim}5$ per cent. Our improved ability to model non-linear structure\nshould allow more robust constraints on cosmological parameters when used in\ntechniques such as simulation-based inference."
                },
                "authors": [
                    {
                        "name": "Prabh Bhambra"
                    },
                    {
                        "name": "Benjamin Joachimi"
                    },
                    {
                        "name": "Ofer Lahav"
                    },
                    {
                        "name": "Davide Piras"
                    }
                ],
                "author_detail": {
                    "name": "Davide Piras"
                },
                "author": "Davide Piras",
                "arxiv_doi": "10.1093/mnras/stae2810",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/stae2810",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.07349v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07349v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "20 pages, 11 figures, 3 tables, 1 appendix. Accepted for publication\n  by Monthly Notices of the Royal Astronomical Society",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.09830v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.09830v3",
                "updated": "2025-01-06T13:43:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    13,
                    43,
                    9,
                    0,
                    6,
                    0
                ],
                "published": "2023-11-16T11:55:27Z",
                "published_parsed": [
                    2023,
                    11,
                    16,
                    11,
                    55,
                    27,
                    3,
                    320,
                    0
                ],
                "title": "Automating the Generation of Prompts for LLM-based Action Choice in PDDL\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating the Generation of Prompts for LLM-based Action Choice in PDDL\n  Planning"
                },
                "summary": "Large language models (LLMs) have revolutionized a large variety of NLP\ntasks. An active debate is to what extent they can do reasoning and planning.\nPrior work has assessed the latter in the specific context of PDDL planning,\nbased on manually converting three PDDL domains into natural language (NL)\nprompts. Here we automate this conversion step, showing how to leverage an LLM\nto automatically generate NL prompts from PDDL input. Our automatically\ngenerated NL prompts result in similar LLM-planning performance as the previous\nmanually generated ones. Beyond this, the automation enables us to run much\nlarger experiments, providing for the first time a broad evaluation of LLM\nplanning performance in PDDL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized a large variety of NLP\ntasks. An active debate is to what extent they can do reasoning and planning.\nPrior work has assessed the latter in the specific context of PDDL planning,\nbased on manually converting three PDDL domains into natural language (NL)\nprompts. Here we automate this conversion step, showing how to leverage an LLM\nto automatically generate NL prompts from PDDL input. Our automatically\ngenerated NL prompts result in similar LLM-planning performance as the previous\nmanually generated ones. Beyond this, the automation enables us to run much\nlarger experiments, providing for the first time a broad evaluation of LLM\nplanning performance in PDDL."
                },
                "authors": [
                    {
                        "name": "Katharina Stein"
                    },
                    {
                        "name": "Daniel Fier"
                    },
                    {
                        "name": "Jrg Hoffmann"
                    },
                    {
                        "name": "Alexander Koller"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Koller"
                },
                "author": "Alexander Koller",
                "arxiv_comment": "Latest Version of the paper previously called \"AutoPlanBench:\n  Automatically generating benchmarks for LLM planners from PDDL\"; Added\n  extended experiments; newer gpt4 model",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.09830v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.09830v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03012v1",
                "updated": "2025-01-06T13:37:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    13,
                    37,
                    13,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T13:37:13Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    13,
                    37,
                    13,
                    0,
                    6,
                    0
                ],
                "title": "Analyzing Fine-tuning Representation Shift for Multimodal LLMs Steering\n  alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Fine-tuning Representation Shift for Multimodal LLMs Steering\n  alignment"
                },
                "summary": "Multimodal LLMs have reached remarkable levels of proficiency in\nunderstanding multimodal inputs, driving extensive research to develop\nincreasingly powerful models. However, much less attention has been paid to\nunderstanding and explaining the underlying mechanisms of these models. Most\nexisting explainability research examines these models only in their final\nstates, overlooking the dynamic representational shifts that occur during\ntraining. In this work, we systematically analyze the evolution of hidden state\nrepresentations to reveal how fine-tuning alters the internal structure of a\nmodel to specialize in new multimodal tasks. Using a concept-based approach, we\nmap hidden states to interpretable visual and textual concepts, enabling us to\ntrace changes in encoded concepts across modalities as training progresses. We\nalso demonstrate the use of shift vectors to capture these concepts changes.\nThese shift vectors allow us to recover fine-tuned concepts by shifting those\nin the original model. Finally, we explore the practical impact of our findings\non model steering, showing that we can adjust multimodal LLMs behaviors without\nany training, such as modifying answer types, captions style, or biasing the\nmodel toward specific responses. Our work sheds light on how multimodal\nrepresentations evolve through fine-tuning and offers a new perspective for\ninterpreting model adaptation in multimodal tasks. The code for this project is\npublicly available at https://github.com/mshukor/xl-vlms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLMs have reached remarkable levels of proficiency in\nunderstanding multimodal inputs, driving extensive research to develop\nincreasingly powerful models. However, much less attention has been paid to\nunderstanding and explaining the underlying mechanisms of these models. Most\nexisting explainability research examines these models only in their final\nstates, overlooking the dynamic representational shifts that occur during\ntraining. In this work, we systematically analyze the evolution of hidden state\nrepresentations to reveal how fine-tuning alters the internal structure of a\nmodel to specialize in new multimodal tasks. Using a concept-based approach, we\nmap hidden states to interpretable visual and textual concepts, enabling us to\ntrace changes in encoded concepts across modalities as training progresses. We\nalso demonstrate the use of shift vectors to capture these concepts changes.\nThese shift vectors allow us to recover fine-tuned concepts by shifting those\nin the original model. Finally, we explore the practical impact of our findings\non model steering, showing that we can adjust multimodal LLMs behaviors without\nany training, such as modifying answer types, captions style, or biasing the\nmodel toward specific responses. Our work sheds light on how multimodal\nrepresentations evolve through fine-tuning and offers a new perspective for\ninterpreting model adaptation in multimodal tasks. The code for this project is\npublicly available at https://github.com/mshukor/xl-vlms."
                },
                "authors": [
                    {
                        "name": "Pegah Khayatan"
                    },
                    {
                        "name": "Mustafa Shukor"
                    },
                    {
                        "name": "Jayneel Parekh"
                    },
                    {
                        "name": "Matthieu Cord"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Cord"
                },
                "author": "Matthieu Cord",
                "arxiv_comment": "The first three authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.01162v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.01162v3",
                "updated": "2025-01-06T13:17:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    13,
                    17,
                    33,
                    0,
                    6,
                    0
                ],
                "published": "2023-12-02T15:52:24Z",
                "published_parsed": [
                    2023,
                    12,
                    2,
                    15,
                    52,
                    24,
                    5,
                    336,
                    0
                ],
                "title": "Inference on many jumps in nonparametric panel regression models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on many jumps in nonparametric panel regression models"
                },
                "summary": "We investigate the significance of change-points within fully nonparametric\nregression contexts, with a particular focus on panel data where data\ngeneration processes vary across units, and error terms may display complex\ndependency structures. In our setting the threshold effect depends on one\nspecific covariate, and we permit the true nonparametric regression to vary\nbased on additional (latent) variables. We propose two uniform testing\nprocedures: one to assess the existence of change-points and another to\nevaluate the uniformity of such effects across units. Our approach involves\nderiving a straightforward analytical expression to approximate the\nvariance-covariance structure of change-point effects under general dependency\nconditions. Notably, when Gaussian approximations are made to these test\nstatistics, the intricate dependency structures within the data can be safely\ndisregarded owing to the localized nature of the statistics. This finding bears\nsignificant implications for obtaining critical values. Through extensive\nsimulations, we demonstrate that our tests exhibit excellent control over size\nand reasonable power performance in finite samples, irrespective of strong\ncross-sectional and weak serial dependency within the data. Furthermore,\napplying our tests to two datasets reveals the existence of significant\nnonsmooth effects in both cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the significance of change-points within fully nonparametric\nregression contexts, with a particular focus on panel data where data\ngeneration processes vary across units, and error terms may display complex\ndependency structures. In our setting the threshold effect depends on one\nspecific covariate, and we permit the true nonparametric regression to vary\nbased on additional (latent) variables. We propose two uniform testing\nprocedures: one to assess the existence of change-points and another to\nevaluate the uniformity of such effects across units. Our approach involves\nderiving a straightforward analytical expression to approximate the\nvariance-covariance structure of change-point effects under general dependency\nconditions. Notably, when Gaussian approximations are made to these test\nstatistics, the intricate dependency structures within the data can be safely\ndisregarded owing to the localized nature of the statistics. This finding bears\nsignificant implications for obtaining critical values. Through extensive\nsimulations, we demonstrate that our tests exhibit excellent control over size\nand reasonable power performance in finite samples, irrespective of strong\ncross-sectional and weak serial dependency within the data. Furthermore,\napplying our tests to two datasets reveals the existence of significant\nnonsmooth effects in both cases."
                },
                "authors": [
                    {
                        "name": "Likai Chen"
                    },
                    {
                        "name": "Georg Keilbar"
                    },
                    {
                        "name": "Liangjun Su"
                    },
                    {
                        "name": "Weining Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weining Wang"
                },
                "author": "Weining Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.01162v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.01162v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02997v1",
                "updated": "2025-01-06T13:14:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    13,
                    14,
                    34,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T13:14:34Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    13,
                    14,
                    34,
                    0,
                    6,
                    0
                ],
                "title": "CALM: Curiosity-Driven Auditing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALM: Curiosity-Driven Auditing for Large Language Models"
                },
                "summary": "Auditing Large Language Models (LLMs) is a crucial and challenging task. In\nthis study, we focus on auditing black-box LLMs without access to their\nparameters, only to the provided service. We treat this type of auditing as a\nblack-box optimization problem where the goal is to automatically uncover\ninput-output pairs of the target LLMs that exhibit illegal, immoral, or unsafe\nbehaviors. For instance, we may seek a non-toxic input that the target LLM\nresponds to with a toxic output or an input that induces the hallucinative\nresponse from the target LLM containing politically sensitive individuals. This\nblack-box optimization is challenging due to the scarcity of feasible points,\nthe discrete nature of the prompt space, and the large search space. To address\nthese challenges, we propose Curiosity-Driven Auditing for Large Language\nModels (CALM), which uses intrinsically motivated reinforcement learning to\nfinetune an LLM as the auditor agent to uncover potential harmful and biased\ninput-output pairs of the target LLM. CALM successfully identifies derogatory\ncompletions involving celebrities and uncovers inputs that elicit specific\nnames under the black-box setting. This work offers a promising direction for\nauditing black-box LLMs. Our code is available at\nhttps://github.com/x-zheng16/CALM.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing Large Language Models (LLMs) is a crucial and challenging task. In\nthis study, we focus on auditing black-box LLMs without access to their\nparameters, only to the provided service. We treat this type of auditing as a\nblack-box optimization problem where the goal is to automatically uncover\ninput-output pairs of the target LLMs that exhibit illegal, immoral, or unsafe\nbehaviors. For instance, we may seek a non-toxic input that the target LLM\nresponds to with a toxic output or an input that induces the hallucinative\nresponse from the target LLM containing politically sensitive individuals. This\nblack-box optimization is challenging due to the scarcity of feasible points,\nthe discrete nature of the prompt space, and the large search space. To address\nthese challenges, we propose Curiosity-Driven Auditing for Large Language\nModels (CALM), which uses intrinsically motivated reinforcement learning to\nfinetune an LLM as the auditor agent to uncover potential harmful and biased\ninput-output pairs of the target LLM. CALM successfully identifies derogatory\ncompletions involving celebrities and uncovers inputs that elicit specific\nnames under the black-box setting. This work offers a promising direction for\nauditing black-box LLMs. Our code is available at\nhttps://github.com/x-zheng16/CALM.git."
                },
                "authors": [
                    {
                        "name": "Xiang Zheng"
                    },
                    {
                        "name": "Longxiang Wang"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Xingjun Ma"
                    },
                    {
                        "name": "Chao Shen"
                    },
                    {
                        "name": "Cong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Cong Wang"
                },
                "author": "Cong Wang",
                "arxiv_comment": "Accepted by AAAI 2025 AI Alignment Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02996v1",
                "updated": "2025-01-06T13:14:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    13,
                    14,
                    31,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T13:14:31Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    13,
                    14,
                    31,
                    0,
                    6,
                    0
                ],
                "title": "Comparing the Extrinsic Orbital Hall Effect in Centrosymmetric and\n  Noncentrosymmetric Systems: Insights from Bilayer Transition Metal\n  Dichalcogenides",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing the Extrinsic Orbital Hall Effect in Centrosymmetric and\n  Noncentrosymmetric Systems: Insights from Bilayer Transition Metal\n  Dichalcogenides"
                },
                "summary": "Both intrinsic and extrinsic orbital Hall effects (OHE) in bilayer transition\nmetal dichalcogenides (TMDs) are investigated in the presence of short-range\ndisorder using quantum kinetic theory. Bilayer TMDs provide an ideal platform\nto study the effects of inversion symmetry breaking on transport properties due\nto their unique structural and electronic characteristics. While bilayer TMDs\nare naturally inversion symmetric, applying a finite gate voltage to create a\nbias between the layers effectively breaks this symmetry. Our findings reveal\nthat slightly away from the band edges, the extrinsic OHE becomes the dominant\ncontribution in both inversion-symmetric and asymmetric cases, with its\nprominence increasing significantly as the Fermi energy rises. Furthermore, we\ndemonstrate that breaking inversion symmetry greatly enhances the extrinsic\nOHE. This enhancement arises from the fundamentally distinct behavior of\norbital angular momentum (OAM) in centrosymmetric systems, where intraband\ncomponents vanish due to symmetry constraints. As a result, in centrosymmetric\nsystems, only the off-diagonal components of the density matrix contribute to\nthe extrinsic OHE. In contrast, in noncentrosymmetric systems, both diagonal\nand off-diagonal components play a role. Our study suggests that in\nexperimentally relevant, highly doped systems, the OHE is predominantly\nextrinsic in nature, regardless of whether the system is centrosymmetric or\nnoncentrosymmetric. Importantly, we infer that even a weakly breaking of\ninversion symmetry can lead to a dramatic enhancement of the OHE, a finding\nwith significant implications for experimental investigations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Both intrinsic and extrinsic orbital Hall effects (OHE) in bilayer transition\nmetal dichalcogenides (TMDs) are investigated in the presence of short-range\ndisorder using quantum kinetic theory. Bilayer TMDs provide an ideal platform\nto study the effects of inversion symmetry breaking on transport properties due\nto their unique structural and electronic characteristics. While bilayer TMDs\nare naturally inversion symmetric, applying a finite gate voltage to create a\nbias between the layers effectively breaks this symmetry. Our findings reveal\nthat slightly away from the band edges, the extrinsic OHE becomes the dominant\ncontribution in both inversion-symmetric and asymmetric cases, with its\nprominence increasing significantly as the Fermi energy rises. Furthermore, we\ndemonstrate that breaking inversion symmetry greatly enhances the extrinsic\nOHE. This enhancement arises from the fundamentally distinct behavior of\norbital angular momentum (OAM) in centrosymmetric systems, where intraband\ncomponents vanish due to symmetry constraints. As a result, in centrosymmetric\nsystems, only the off-diagonal components of the density matrix contribute to\nthe extrinsic OHE. In contrast, in noncentrosymmetric systems, both diagonal\nand off-diagonal components play a role. Our study suggests that in\nexperimentally relevant, highly doped systems, the OHE is predominantly\nextrinsic in nature, regardless of whether the system is centrosymmetric or\nnoncentrosymmetric. Importantly, we infer that even a weakly breaking of\ninversion symmetry can lead to a dramatic enhancement of the OHE, a finding\nwith significant implications for experimental investigations."
                },
                "authors": [
                    {
                        "name": "Azadeh Faridi"
                    },
                    {
                        "name": "Reza Asgari"
                    }
                ],
                "author_detail": {
                    "name": "Reza Asgari"
                },
                "author": "Reza Asgari",
                "arxiv_comment": "12 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02979v1",
                "updated": "2025-01-06T12:42:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    12,
                    42,
                    54,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T12:42:54Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    12,
                    42,
                    54,
                    0,
                    6,
                    0
                ],
                "title": "Registering Source Tokens to Target Language Spaces in Multilingual\n  Neural Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Registering Source Tokens to Target Language Spaces in Multilingual\n  Neural Machine Translation"
                },
                "summary": "The multilingual neural machine translation (MNMT) enables arbitrary\ntranslations across multiple languages by training a model with limited\nparameters using parallel data only. However, the performance of such MNMT\nmodels still lags behind that of large language models (LLMs), limiting their\npracticality. In this work, we address this limitation by introducing\nregistering to achieve the new state-of-the-art of decoder-only MNMT models.\nSpecifically, we insert a set of artificial tokens specifying the target\nlanguage, called registers, into the input sequence between the source and\ntarget tokens. By modifying the attention mask, the target token generation\nonly pays attention to the activation of registers, representing the source\ntokens in the target language space. Experiments on EC-40, a large-scale\nbenchmark, show that our method outperforms related methods driven by\noptimizing multilingual representations. We further scale up and collect 9.3\nbillion sentence pairs across 24 languages from public datasets to pre-train\ntwo models, namely MITRE (multilingual translation with registers). One of\nthem, MITRE-913M, outperforms NLLB-3.3B, achieves comparable performance with\ncommercial LLMs, and shows strong adaptability in fine-tuning. Finally, we\nopen-source our models to facilitate further research and development in MNMT:\nhttps://github.com/zhiqu22/mitre.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The multilingual neural machine translation (MNMT) enables arbitrary\ntranslations across multiple languages by training a model with limited\nparameters using parallel data only. However, the performance of such MNMT\nmodels still lags behind that of large language models (LLMs), limiting their\npracticality. In this work, we address this limitation by introducing\nregistering to achieve the new state-of-the-art of decoder-only MNMT models.\nSpecifically, we insert a set of artificial tokens specifying the target\nlanguage, called registers, into the input sequence between the source and\ntarget tokens. By modifying the attention mask, the target token generation\nonly pays attention to the activation of registers, representing the source\ntokens in the target language space. Experiments on EC-40, a large-scale\nbenchmark, show that our method outperforms related methods driven by\noptimizing multilingual representations. We further scale up and collect 9.3\nbillion sentence pairs across 24 languages from public datasets to pre-train\ntwo models, namely MITRE (multilingual translation with registers). One of\nthem, MITRE-913M, outperforms NLLB-3.3B, achieves comparable performance with\ncommercial LLMs, and shows strong adaptability in fine-tuning. Finally, we\nopen-source our models to facilitate further research and development in MNMT:\nhttps://github.com/zhiqu22/mitre."
                },
                "authors": [
                    {
                        "name": "Zhi Qu"
                    },
                    {
                        "name": "Yiran Wang"
                    },
                    {
                        "name": "Jiannan Mao"
                    },
                    {
                        "name": "Chenchen Ding"
                    },
                    {
                        "name": "Hideki Tanaka"
                    },
                    {
                        "name": "Masao Utiyama"
                    },
                    {
                        "name": "Taro Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Taro Watanabe"
                },
                "author": "Taro Watanabe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02968v1",
                "updated": "2025-01-06T12:24:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    12,
                    24,
                    57,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T12:24:57Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    12,
                    24,
                    57,
                    0,
                    6,
                    0
                ],
                "title": "FlipedRAG: Black-Box Opinion Manipulation Attacks to Retrieval-Augmented\n  Generation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlipedRAG: Black-Box Opinion Manipulation Attacks to Retrieval-Augmented\n  Generation of Large Language Models"
                },
                "summary": "Retrieval-Augmented Generation (RAG) addresses hallucination and real-time\nconstraints by dynamically retrieving relevant information from a knowledge\ndatabase to supplement the LLMs' input. When presented with a query, RAG\nselects the most semantically similar texts from its knowledge bases and uses\nthem as context for the LLMs to generate more accurate responses. RAG also\ncreates a new attack surface, especially since RAG databases are frequently\nsourced from public domains. While existing studies have predominantly focused\non optimizing RAG's performance and efficiency, emerging research has begun\naddressing the security concerns associated with RAG. However, these works have\nsome limitations, typically focusing on either white-box methodologies or\nheuristic-based black-box attacks. Furthermore, prior research has mainly\ntargeted simple factoid question answering, which is neither practically\nchallenging nor resistant to correction. In this paper, we unveil a more\nrealistic and threatening scenario: opinion manipulation for controversial\ntopics against RAG. Particularly, we propose a novel RAG black-box attack\nmethod, termed FlipedRAG, which is transfer-based. By leveraging instruction\nengineering, we obtain partial retrieval model outputs from black-box RAG\nsystem, facilitating the training of surrogate models to enhance the\neffectiveness of opinion manipulation attack. Extensive experimental results\nconfirms that our approach significantly enhances the average success rate of\nopinion manipulation by 16.7%. It achieves an average of a 50% directional\nchange in the opinion polarity of RAG responses across four themes.\nAdditionally, it induces a 20% shift in user cognition. Furthermore, we discuss\nthe efficacy of potential defense mechanisms and conclude that they are\ninsufficient in mitigating this type of attack, highlighting the urgent need to\ndevelop novel defensive strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) addresses hallucination and real-time\nconstraints by dynamically retrieving relevant information from a knowledge\ndatabase to supplement the LLMs' input. When presented with a query, RAG\nselects the most semantically similar texts from its knowledge bases and uses\nthem as context for the LLMs to generate more accurate responses. RAG also\ncreates a new attack surface, especially since RAG databases are frequently\nsourced from public domains. While existing studies have predominantly focused\non optimizing RAG's performance and efficiency, emerging research has begun\naddressing the security concerns associated with RAG. However, these works have\nsome limitations, typically focusing on either white-box methodologies or\nheuristic-based black-box attacks. Furthermore, prior research has mainly\ntargeted simple factoid question answering, which is neither practically\nchallenging nor resistant to correction. In this paper, we unveil a more\nrealistic and threatening scenario: opinion manipulation for controversial\ntopics against RAG. Particularly, we propose a novel RAG black-box attack\nmethod, termed FlipedRAG, which is transfer-based. By leveraging instruction\nengineering, we obtain partial retrieval model outputs from black-box RAG\nsystem, facilitating the training of surrogate models to enhance the\neffectiveness of opinion manipulation attack. Extensive experimental results\nconfirms that our approach significantly enhances the average success rate of\nopinion manipulation by 16.7%. It achieves an average of a 50% directional\nchange in the opinion polarity of RAG responses across four themes.\nAdditionally, it induces a 20% shift in user cognition. Furthermore, we discuss\nthe efficacy of potential defense mechanisms and conclude that they are\ninsufficient in mitigating this type of attack, highlighting the urgent need to\ndevelop novel defensive strategies."
                },
                "authors": [
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Yuyang Gong"
                    },
                    {
                        "name": "Miaokun Chen"
                    },
                    {
                        "name": "Haotan Liu"
                    },
                    {
                        "name": "Qikai Cheng"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Wei Lu"
                    },
                    {
                        "name": "Xiaozhong Liu"
                    },
                    {
                        "name": "Jiawei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Liu"
                },
                "author": "Jiawei Liu",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2407.13757",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20790v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20790v2",
                "updated": "2025-01-06T12:17:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    12,
                    17,
                    43,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-30T08:12:17Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    8,
                    12,
                    17,
                    0,
                    365,
                    0
                ],
                "title": "Frequency-Masked Embedding Inference: A Non-Contrastive Approach for\n  Time Series Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frequency-Masked Embedding Inference: A Non-Contrastive Approach for\n  Time Series Representation Learning"
                },
                "summary": "Contrastive learning underpins most current self-supervised time series\nrepresentation methods. The strategy for constructing positive and negative\nsample pairs significantly affects the final representation quality. However,\ndue to the continuous nature of time series semantics, the modeling approach of\ncontrastive learning struggles to accommodate the characteristics of time\nseries data. This results in issues such as difficulties in constructing hard\nnegative samples and the potential introduction of inappropriate biases during\npositive sample construction. Although some recent works have developed several\nscientific strategies for constructing positive and negative sample pairs with\nimproved effectiveness, they remain constrained by the contrastive learning\nframework. To fundamentally overcome the limitations of contrastive learning,\nthis paper introduces Frequency-masked Embedding Inference (FEI), a novel\nnon-contrastive method that completely eliminates the need for positive and\nnegative samples. The proposed FEI constructs 2 inference branches based on a\nprompting strategy: 1) Using frequency masking as prompts to infer the\nembedding representation of the target series with missing frequency bands in\nthe embedding space, and 2) Using the target series as prompts to infer its\nfrequency masking embedding. In this way, FEI enables continuous semantic\nrelationship modeling for time series. Experiments on 8 widely used time series\ndatasets for classification and regression tasks, using linear evaluation and\nend-to-end fine-tuning, show that FEI significantly outperforms existing\ncontrastive-based methods in terms of generalization. This study provides new\ninsights into self-supervised representation learning for time series. The code\nis available at\nhttps://github.com/USTBInnovationPark/Frequency-masked-Embedding-Inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive learning underpins most current self-supervised time series\nrepresentation methods. The strategy for constructing positive and negative\nsample pairs significantly affects the final representation quality. However,\ndue to the continuous nature of time series semantics, the modeling approach of\ncontrastive learning struggles to accommodate the characteristics of time\nseries data. This results in issues such as difficulties in constructing hard\nnegative samples and the potential introduction of inappropriate biases during\npositive sample construction. Although some recent works have developed several\nscientific strategies for constructing positive and negative sample pairs with\nimproved effectiveness, they remain constrained by the contrastive learning\nframework. To fundamentally overcome the limitations of contrastive learning,\nthis paper introduces Frequency-masked Embedding Inference (FEI), a novel\nnon-contrastive method that completely eliminates the need for positive and\nnegative samples. The proposed FEI constructs 2 inference branches based on a\nprompting strategy: 1) Using frequency masking as prompts to infer the\nembedding representation of the target series with missing frequency bands in\nthe embedding space, and 2) Using the target series as prompts to infer its\nfrequency masking embedding. In this way, FEI enables continuous semantic\nrelationship modeling for time series. Experiments on 8 widely used time series\ndatasets for classification and regression tasks, using linear evaluation and\nend-to-end fine-tuning, show that FEI significantly outperforms existing\ncontrastive-based methods in terms of generalization. This study provides new\ninsights into self-supervised representation learning for time series. The code\nis available at\nhttps://github.com/USTBInnovationPark/Frequency-masked-Embedding-Inference."
                },
                "authors": [
                    {
                        "name": "En Fu"
                    },
                    {
                        "name": "Yanyan Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yanyan Hu"
                },
                "author": "Yanyan Hu",
                "arxiv_comment": "This paper has been accepted by AAAI-2025 main track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20790v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20790v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02961v1",
                "updated": "2025-01-06T12:09:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    12,
                    9,
                    8,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T12:09:08Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    12,
                    9,
                    8,
                    0,
                    6,
                    0
                ],
                "title": "A Point Process Model for Optimizing Repeated Personalized Action\n  Delivery to Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Point Process Model for Optimizing Repeated Personalized Action\n  Delivery to Users"
                },
                "summary": "This paper provides a formalism for an important class of causal inference\nproblems inspired by user-advertiser interaction in online advertiser. Then\nthis formalism is specialized to an extension of temporal marked point\nprocesses and the neural point processes are suggested as practical solutions\nto some interesting special cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides a formalism for an important class of causal inference\nproblems inspired by user-advertiser interaction in online advertiser. Then\nthis formalism is specialized to an extension of temporal marked point\nprocesses and the neural point processes are suggested as practical solutions\nto some interesting special cases."
                },
                "authors": [
                    {
                        "name": "Alexander Merkov"
                    },
                    {
                        "name": "David Rohde"
                    }
                ],
                "author_detail": {
                    "name": "David Rohde"
                },
                "author": "David Rohde",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62M45",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02956v1",
                "updated": "2025-01-06T12:02:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    12,
                    2,
                    40,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T12:02:40Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    12,
                    2,
                    40,
                    0,
                    6,
                    0
                ],
                "title": "Evolution of the Srsic Index up to z=2.5 from JWST and HST",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolution of the Srsic Index up to z=2.5 from JWST and HST"
                },
                "summary": "The James Webb Space Telescope (JWST) is unveiling the rest-frame near-IR\nstructure of galaxies. We measure the evolution with redshift of the rest-frame\noptical and near-IR S\\'ersic index ($n$), and examine the dependence on stellar\nmass and star-formation activity across the redshift range $0.5\\leq z\\leq2.5$.\nWe infer rest-frame near-IR S\\'ersic profiles for $\\approx 15.000$ galaxies in\npublicly available NIRCam imaging mosaics from the COSMOS-Web and PRIMER\nsurveys. We augment these with rest-frame optical S\\'ersic indices, previously\nmeasured from HST imaging mosaics. The median S\\'ersic index evolves slowly or\nnot at all with redshift, except for very high-mass galaxies ($M_\\star >\n10^{11}~{\\text{M}}_\\odot$), which show an increase from $n\\approx 2.5$ to\n$n\\approx 4$ at $z<1$. High-mass galaxies have higher $n$ than lower-mass\ngalaxies ($M_\\star=10^{9.5}~{\\text{M}}_\\odot$) at all redshifts, with a\nstronger dependence in the rest-frame near-IR than in the rest-frame optical at\n$z>1$. This wavelength dependence is caused by star-forming galaxies that have\nlower optical than near-IR $n$ at z>1 (but not at z<1). Both at optical and\nnear-IR wavelengths, star-forming galaxies have lower $n$ than quiescent\ngalaxies, fortifying the connection between star-formation activity and radial\nstellar mass distribution. At $z>1$ the median near-IR $n$ varies strongly with\nstar formation activity, but not with stellar mass. The scatter in near-IR $n$\nis higher in the green valley (0.25 dex) than on the star-forming sequence and\namong quiescent galaxies (0.18 dex) -- this trend is not seen in the optical\nbecause dust and young stars contribute to the variety in optical light\nprofiles. Our newly measured rest-frame near-IR radial light profiles motivate\nfuture comparisons with radial stellar mass profiles of simulated galaxies as a\nstringent constraint on processes that govern galaxy formation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The James Webb Space Telescope (JWST) is unveiling the rest-frame near-IR\nstructure of galaxies. We measure the evolution with redshift of the rest-frame\noptical and near-IR S\\'ersic index ($n$), and examine the dependence on stellar\nmass and star-formation activity across the redshift range $0.5\\leq z\\leq2.5$.\nWe infer rest-frame near-IR S\\'ersic profiles for $\\approx 15.000$ galaxies in\npublicly available NIRCam imaging mosaics from the COSMOS-Web and PRIMER\nsurveys. We augment these with rest-frame optical S\\'ersic indices, previously\nmeasured from HST imaging mosaics. The median S\\'ersic index evolves slowly or\nnot at all with redshift, except for very high-mass galaxies ($M_\\star >\n10^{11}~{\\text{M}}_\\odot$), which show an increase from $n\\approx 2.5$ to\n$n\\approx 4$ at $z<1$. High-mass galaxies have higher $n$ than lower-mass\ngalaxies ($M_\\star=10^{9.5}~{\\text{M}}_\\odot$) at all redshifts, with a\nstronger dependence in the rest-frame near-IR than in the rest-frame optical at\n$z>1$. This wavelength dependence is caused by star-forming galaxies that have\nlower optical than near-IR $n$ at z>1 (but not at z<1). Both at optical and\nnear-IR wavelengths, star-forming galaxies have lower $n$ than quiescent\ngalaxies, fortifying the connection between star-formation activity and radial\nstellar mass distribution. At $z>1$ the median near-IR $n$ varies strongly with\nstar formation activity, but not with stellar mass. The scatter in near-IR $n$\nis higher in the green valley (0.25 dex) than on the star-forming sequence and\namong quiescent galaxies (0.18 dex) -- this trend is not seen in the optical\nbecause dust and young stars contribute to the variety in optical light\nprofiles. Our newly measured rest-frame near-IR radial light profiles motivate\nfuture comparisons with radial stellar mass profiles of simulated galaxies as a\nstringent constraint on processes that govern galaxy formation."
                },
                "authors": [
                    {
                        "name": "Marco Martorano"
                    },
                    {
                        "name": "Arjen van der Wel"
                    },
                    {
                        "name": "Maarten Baes"
                    },
                    {
                        "name": "Eric F. Bell"
                    },
                    {
                        "name": "Gabriel Brammer"
                    },
                    {
                        "name": "Marijn Franx"
                    },
                    {
                        "name": "Andrea Gebek"
                    },
                    {
                        "name": "Sharon E. Meidt"
                    },
                    {
                        "name": "Tim B. Miller"
                    },
                    {
                        "name": "Erica Nelson"
                    },
                    {
                        "name": "Angelos Nersesian"
                    },
                    {
                        "name": "Sedona H. Price"
                    },
                    {
                        "name": "Pieter van Dokkum"
                    },
                    {
                        "name": "Katherine Whitaker"
                    },
                    {
                        "name": "Stijn Wuyts"
                    }
                ],
                "author_detail": {
                    "name": "Stijn Wuyts"
                },
                "author": "Stijn Wuyts",
                "arxiv_comment": "Accepted for publication on A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02955v1",
                "updated": "2025-01-06T11:57:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    11,
                    57,
                    38,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T11:57:38Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    11,
                    57,
                    38,
                    0,
                    6,
                    0
                ],
                "title": "MotionBench: Benchmarking and Improving Fine-grained Video Motion\n  Understanding for Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MotionBench: Benchmarking and Improving Fine-grained Video Motion\n  Understanding for Vision Language Models"
                },
                "summary": "In recent years, vision language models (VLMs) have made significant\nadvancements in video understanding. However, a crucial capability -\nfine-grained motion comprehension - remains under-explored in current\nbenchmarks. To address this gap, we propose MotionBench, a comprehensive\nevaluation benchmark designed to assess the fine-grained motion comprehension\nof video understanding models. MotionBench evaluates models' motion-level\nperception through six primary categories of motion-oriented question types and\nincludes data collected from diverse sources, ensuring a broad representation\nof real-world video content. Experimental results reveal that existing VLMs\nperform poorly in understanding fine-grained motions. To enhance VLM's ability\nto perceive fine-grained motion within a limited sequence length of LLM, we\nconduct extensive experiments reviewing VLM architectures optimized for video\nfeature compression and propose a novel and efficient Through-Encoder (TE)\nFusion method. Experiments show that higher frame rate inputs and TE Fusion\nyield improvements in motion understanding, yet there is still substantial room\nfor enhancement. Our benchmark aims to guide and motivate the development of\nmore capable video understanding models, emphasizing the importance of\nfine-grained motion comprehension. Project page: https://motion-bench.github.io .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, vision language models (VLMs) have made significant\nadvancements in video understanding. However, a crucial capability -\nfine-grained motion comprehension - remains under-explored in current\nbenchmarks. To address this gap, we propose MotionBench, a comprehensive\nevaluation benchmark designed to assess the fine-grained motion comprehension\nof video understanding models. MotionBench evaluates models' motion-level\nperception through six primary categories of motion-oriented question types and\nincludes data collected from diverse sources, ensuring a broad representation\nof real-world video content. Experimental results reveal that existing VLMs\nperform poorly in understanding fine-grained motions. To enhance VLM's ability\nto perceive fine-grained motion within a limited sequence length of LLM, we\nconduct extensive experiments reviewing VLM architectures optimized for video\nfeature compression and propose a novel and efficient Through-Encoder (TE)\nFusion method. Experiments show that higher frame rate inputs and TE Fusion\nyield improvements in motion understanding, yet there is still substantial room\nfor enhancement. Our benchmark aims to guide and motivate the development of\nmore capable video understanding models, emphasizing the importance of\nfine-grained motion comprehension. Project page: https://motion-bench.github.io ."
                },
                "authors": [
                    {
                        "name": "Wenyi Hong"
                    },
                    {
                        "name": "Yean Cheng"
                    },
                    {
                        "name": "Zhuoyi Yang"
                    },
                    {
                        "name": "Weihan Wang"
                    },
                    {
                        "name": "Lefan Wang"
                    },
                    {
                        "name": "Xiaotao Gu"
                    },
                    {
                        "name": "Shiyu Huang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02934v1",
                "updated": "2025-01-06T11:20:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    11,
                    20,
                    0,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T11:20:00Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    11,
                    20,
                    0,
                    0,
                    6,
                    0
                ],
                "title": "A Bayesian Approach for Discovering Time- Delayed Differential Equation\n  from Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bayesian Approach for Discovering Time- Delayed Differential Equation\n  from Data"
                },
                "summary": "Time-delayed differential equations (TDDEs) are widely used to model complex\ndynamic systems where future states depend on past states with a delay.\nHowever, inferring the underlying TDDEs from observed data remains a\nchallenging problem due to the inherent nonlinearity, uncertainty, and noise in\nreal-world systems. Conventional equation discovery methods often exhibit\nlimitations when dealing with large time delays, relying on deterministic\ntechniques or optimization-based approaches that may struggle with scalability\nand robustness. In this paper, we present BayTiDe - Bayesian Approach for\nDiscovering Time-Delayed Differential Equations from Data, that is capable of\nidentifying arbitrarily large values of time delay to an accuracy that is\ndirectly proportional to the resolution of the data input to it. BayTiDe\nleverages Bayesian inference combined with a sparsity-promoting discontinuous\nspike-and-slab prior to accurately identify time-delayed differential\nequations. The approach accommodates arbitrarily large time delays with\naccuracy proportional to the input data resolution, while efficiently narrowing\nthe search space to achieve significant computational savings. We demonstrate\nthe efficiency and robustness of BayTiDe through a range of numerical examples,\nvalidating its ability to recover delayed differential equations from noisy\ndata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-delayed differential equations (TDDEs) are widely used to model complex\ndynamic systems where future states depend on past states with a delay.\nHowever, inferring the underlying TDDEs from observed data remains a\nchallenging problem due to the inherent nonlinearity, uncertainty, and noise in\nreal-world systems. Conventional equation discovery methods often exhibit\nlimitations when dealing with large time delays, relying on deterministic\ntechniques or optimization-based approaches that may struggle with scalability\nand robustness. In this paper, we present BayTiDe - Bayesian Approach for\nDiscovering Time-Delayed Differential Equations from Data, that is capable of\nidentifying arbitrarily large values of time delay to an accuracy that is\ndirectly proportional to the resolution of the data input to it. BayTiDe\nleverages Bayesian inference combined with a sparsity-promoting discontinuous\nspike-and-slab prior to accurately identify time-delayed differential\nequations. The approach accommodates arbitrarily large time delays with\naccuracy proportional to the input data resolution, while efficiently narrowing\nthe search space to achieve significant computational savings. We demonstrate\nthe efficiency and robustness of BayTiDe through a range of numerical examples,\nvalidating its ability to recover delayed differential equations from noisy\ndata."
                },
                "authors": [
                    {
                        "name": "Debangshu Chowdhury"
                    },
                    {
                        "name": "Souvik Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Souvik Chakraborty"
                },
                "author": "Souvik Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.13100v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.13100v2",
                "updated": "2025-01-06T11:15:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    11,
                    15,
                    11,
                    0,
                    6,
                    0
                ],
                "published": "2023-12-20T15:18:51Z",
                "published_parsed": [
                    2023,
                    12,
                    20,
                    15,
                    18,
                    51,
                    2,
                    354,
                    0
                ],
                "title": "SEER-ZSL: Semantic Encoder-Enhanced Representations for Generalized\n  Zero-Shot Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEER-ZSL: Semantic Encoder-Enhanced Representations for Generalized\n  Zero-Shot Learning"
                },
                "summary": "Zero-Shot Learning (ZSL) presents the challenge of identifying categories not\nseen during training. This task is crucial in domains where it is costly,\nprohibited, or simply not feasible to collect training data. ZSL depends on a\nmapping between the visual space and available semantic information. Prior\nworks learn a mapping between spaces that can be exploited during inference. We\ncontend, however, that the disparity between meticulously curated semantic\nspaces and the inherently noisy nature of real-world data remains a substantial\nand unresolved challenge. In this paper, we address this by introducing a\nSemantic Encoder-Enhanced Representations for Zero-Shot Learning (SEER-ZSL). We\npropose a hybrid strategy to address the generalization gap. First, we aim to\ndistill meaningful semantic information using a probabilistic encoder,\nenhancing the semantic consistency and robustness. Second, we distill the\nvisual space by exploiting the learned data distribution through an\nadversarially trained generator. Finally, we align the distilled information,\nenabling a mapping of unseen categories onto the true data manifold. We\ndemonstrate empirically that this approach yields a model that outperforms the\nstate-of-the-art benchmarks in terms of both generalization and benchmarks\nacross diverse settings with small, medium, and large datasets. The complete\ncode is available on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Learning (ZSL) presents the challenge of identifying categories not\nseen during training. This task is crucial in domains where it is costly,\nprohibited, or simply not feasible to collect training data. ZSL depends on a\nmapping between the visual space and available semantic information. Prior\nworks learn a mapping between spaces that can be exploited during inference. We\ncontend, however, that the disparity between meticulously curated semantic\nspaces and the inherently noisy nature of real-world data remains a substantial\nand unresolved challenge. In this paper, we address this by introducing a\nSemantic Encoder-Enhanced Representations for Zero-Shot Learning (SEER-ZSL). We\npropose a hybrid strategy to address the generalization gap. First, we aim to\ndistill meaningful semantic information using a probabilistic encoder,\nenhancing the semantic consistency and robustness. Second, we distill the\nvisual space by exploiting the learned data distribution through an\nadversarially trained generator. Finally, we align the distilled information,\nenabling a mapping of unseen categories onto the true data manifold. We\ndemonstrate empirically that this approach yields a model that outperforms the\nstate-of-the-art benchmarks in terms of both generalization and benchmarks\nacross diverse settings with small, medium, and large datasets. The complete\ncode is available on GitHub."
                },
                "authors": [
                    {
                        "name": "William Heyden"
                    },
                    {
                        "name": "Habib Ullah"
                    },
                    {
                        "name": "M. Salman Siddiqui"
                    },
                    {
                        "name": "Fadi Al Machot"
                    }
                ],
                "author_detail": {
                    "name": "Fadi Al Machot"
                },
                "author": "Fadi Al Machot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.13100v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.13100v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09472v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09472v2",
                "updated": "2025-01-06T10:52:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    10,
                    52,
                    2,
                    0,
                    6,
                    0
                ],
                "published": "2024-10-12T10:21:00Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    21,
                    0,
                    5,
                    286,
                    0
                ],
                "title": "DRCap: Decoding CLAP Latents with Retrieval-Augmented Generation for\n  Zero-shot Audio Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRCap: Decoding CLAP Latents with Retrieval-Augmented Generation for\n  Zero-shot Audio Captioning"
                },
                "summary": "While automated audio captioning (AAC) has made notable progress, traditional\nfully supervised AAC models still face two critical challenges: the need for\nexpensive audio-text pair data for training and performance degradation when\ntransferring across domains. To overcome these limitations, we present DRCap, a\ndata-efficient and flexible zero-shot audio captioning system that requires\ntext-only data for training and can quickly adapt to new domains without\nadditional fine-tuning. DRCap integrates a contrastive language-audio\npre-training (CLAP) model and a large-language model (LLM) as its backbone.\nDuring training, the model predicts the ground-truth caption with a fixed text\nencoder from CLAP, whereas, during inference, the text encoder is replaced with\nthe audio encoder to generate captions for audio clips in a zero-shot manner.\nTo mitigate the modality gap of the CLAP model, we use both the projection\nstrategy from the encoder side and the retrieval-augmented generation strategy\nfrom the decoder side. Specifically, audio embeddings are first projected onto\na text embedding support to absorb extensive semantic information within the\njoint multi-modal space of CLAP. At the same time, similar captions retrieved\nfrom a datastore are fed as prompts to instruct the LLM, incorporating external\nknowledge to take full advantage of its strong generative capability.\nConditioned on both the projected CLAP embedding and the retrieved similar\ncaptions, the model is able to produce a more accurate and semantically rich\ntextual description. By tailoring the text embedding support and the caption\ndatastore to the target domain, DRCap acquires a robust ability to adapt to new\ndomains in a training-free manner. Experimental results demonstrate that DRCap\noutperforms all other zero-shot models in in-domain scenarios and achieves\nstate-of-the-art performance in cross-domain scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While automated audio captioning (AAC) has made notable progress, traditional\nfully supervised AAC models still face two critical challenges: the need for\nexpensive audio-text pair data for training and performance degradation when\ntransferring across domains. To overcome these limitations, we present DRCap, a\ndata-efficient and flexible zero-shot audio captioning system that requires\ntext-only data for training and can quickly adapt to new domains without\nadditional fine-tuning. DRCap integrates a contrastive language-audio\npre-training (CLAP) model and a large-language model (LLM) as its backbone.\nDuring training, the model predicts the ground-truth caption with a fixed text\nencoder from CLAP, whereas, during inference, the text encoder is replaced with\nthe audio encoder to generate captions for audio clips in a zero-shot manner.\nTo mitigate the modality gap of the CLAP model, we use both the projection\nstrategy from the encoder side and the retrieval-augmented generation strategy\nfrom the decoder side. Specifically, audio embeddings are first projected onto\na text embedding support to absorb extensive semantic information within the\njoint multi-modal space of CLAP. At the same time, similar captions retrieved\nfrom a datastore are fed as prompts to instruct the LLM, incorporating external\nknowledge to take full advantage of its strong generative capability.\nConditioned on both the projected CLAP embedding and the retrieved similar\ncaptions, the model is able to produce a more accurate and semantically rich\ntextual description. By tailoring the text embedding support and the caption\ndatastore to the target domain, DRCap acquires a robust ability to adapt to new\ndomains in a training-free manner. Experimental results demonstrate that DRCap\noutperforms all other zero-shot models in in-domain scenarios and achieves\nstate-of-the-art performance in cross-domain scenarios."
                },
                "authors": [
                    {
                        "name": "Xiquan Li"
                    },
                    {
                        "name": "Wenxi Chen"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Xuenan Xu"
                    },
                    {
                        "name": "Yuzhe Liang"
                    },
                    {
                        "name": "Zhisheng Zheng"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Xie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xie Chen"
                },
                "author": "Xie Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09472v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09472v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02905v1",
                "updated": "2025-01-06T10:29:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    10,
                    29,
                    38,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T10:29:38Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    10,
                    29,
                    38,
                    0,
                    6,
                    0
                ],
                "title": "Skillful High-Resolution Ensemble Precipitation Forecasting with an\n  Integrated Deep Learning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skillful High-Resolution Ensemble Precipitation Forecasting with an\n  Integrated Deep Learning Framework"
                },
                "summary": "High-resolution precipitation forecasts are crucial for providing accurate\nweather prediction and supporting effective responses to extreme weather\nevents. Traditional numerical models struggle with stochastic subgrid-scale\nprocesses, while recent deep learning models often produce blurry results. To\naddress these challenges, we propose a physics-inspired deep learning framework\nfor high-resolution (0.05\\textdegree{} $\\times$ 0.05\\textdegree{}) ensemble\nprecipitation forecasting. Trained on ERA5 and CMPA high-resolution\nprecipitation datasets, the framework integrates deterministic and\nprobabilistic components. The deterministic model, based on a 3D\nSwinTransformer, captures average precipitation at mesoscale resolution and\nincorporates strategies to enhance performance, particularly for moderate to\nheavy rainfall. The probabilistic model employs conditional diffusion in latent\nspace to account for uncertainties in residual precipitation at convective\nscales. During inference, ensemble members are generated by repeatedly sampling\nlatent variables, enabling the model to represent precipitation uncertainty.\nOur model significantly enhances spatial resolution and forecast accuracy. Rank\nhistogram shows that the ensemble system is reliable and unbiased. In a case\nstudy of heavy precipitation in southern China, the model outputs align more\nclosely with observed precipitation distributions than ERA5, demonstrating\nsuperior capability in capturing extreme precipitation events. Additionally,\n5-day real-time forecasts show good performance in terms of CSI scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-resolution precipitation forecasts are crucial for providing accurate\nweather prediction and supporting effective responses to extreme weather\nevents. Traditional numerical models struggle with stochastic subgrid-scale\nprocesses, while recent deep learning models often produce blurry results. To\naddress these challenges, we propose a physics-inspired deep learning framework\nfor high-resolution (0.05\\textdegree{} $\\times$ 0.05\\textdegree{}) ensemble\nprecipitation forecasting. Trained on ERA5 and CMPA high-resolution\nprecipitation datasets, the framework integrates deterministic and\nprobabilistic components. The deterministic model, based on a 3D\nSwinTransformer, captures average precipitation at mesoscale resolution and\nincorporates strategies to enhance performance, particularly for moderate to\nheavy rainfall. The probabilistic model employs conditional diffusion in latent\nspace to account for uncertainties in residual precipitation at convective\nscales. During inference, ensemble members are generated by repeatedly sampling\nlatent variables, enabling the model to represent precipitation uncertainty.\nOur model significantly enhances spatial resolution and forecast accuracy. Rank\nhistogram shows that the ensemble system is reliable and unbiased. In a case\nstudy of heavy precipitation in southern China, the model outputs align more\nclosely with observed precipitation distributions than ERA5, demonstrating\nsuperior capability in capturing extreme precipitation events. Additionally,\n5-day real-time forecasts show good performance in terms of CSI scores."
                },
                "authors": [
                    {
                        "name": "Shuangshuang He"
                    },
                    {
                        "name": "Hongli Liang"
                    },
                    {
                        "name": "Yuanting Zhang"
                    },
                    {
                        "name": "Xingyuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Xingyuan Yuan"
                },
                "author": "Xingyuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02902v1",
                "updated": "2025-01-06T10:26:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    10,
                    26,
                    16,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T10:26:16Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    10,
                    26,
                    16,
                    0,
                    6,
                    0
                ],
                "title": "Sim-to-Real Transfer for Mobile Robots with Reinforcement Learning: from\n  NVIDIA Isaac Sim to Gazebo and Real ROS 2 Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sim-to-Real Transfer for Mobile Robots with Reinforcement Learning: from\n  NVIDIA Isaac Sim to Gazebo and Real ROS 2 Robots"
                },
                "summary": "Unprecedented agility and dexterous manipulation have been demonstrated with\ncontrollers based on deep reinforcement learning (RL), with a significant\nimpact on legged and humanoid robots. Modern tooling and simulation platforms,\nsuch as NVIDIA Isaac Sim, have been enabling such advances. This article\nfocuses on demonstrating the applications of Isaac in local planning and\nobstacle avoidance as one of the most fundamental ways in which a mobile robot\ninteracts with its environments. Although there is extensive research on\nproprioception-based RL policies, the article highlights less standardized and\nreproducible approaches to exteroception. At the same time, the article aims to\nprovide a base framework for end-to-end local navigation policies and how a\ncustom robot can be trained in such simulation environment. We benchmark\nend-to-end policies with the state-of-the-art Nav2, navigation stack in Robot\nOperating System (ROS). We also cover the sim-to-real transfer process by\ndemonstrating zero-shot transferability of policies trained in the Isaac\nsimulator to real-world robots. This is further evidenced by the tests with\ndifferent simulated robots, which show the generalization of the learned\npolicy. Finally, the benchmarks demonstrate comparable performance to Nav2,\nopening the door to quick deployment of state-of-the-art end-to-end local\nplanners for custom robot platforms, but importantly furthering the\npossibilities by expanding the state and action spaces or task definitions for\nmore complex missions. Overall, with this article we introduce the most\nimportant steps, and aspects to consider, in deploying RL policies for local\npath planning and obstacle avoidance with Isaac Sim training, Gazebo testing,\nand ROS 2 for real-time inference in real robots. The code is available at\nhttps://github.com/sahars93/RL-Navigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unprecedented agility and dexterous manipulation have been demonstrated with\ncontrollers based on deep reinforcement learning (RL), with a significant\nimpact on legged and humanoid robots. Modern tooling and simulation platforms,\nsuch as NVIDIA Isaac Sim, have been enabling such advances. This article\nfocuses on demonstrating the applications of Isaac in local planning and\nobstacle avoidance as one of the most fundamental ways in which a mobile robot\ninteracts with its environments. Although there is extensive research on\nproprioception-based RL policies, the article highlights less standardized and\nreproducible approaches to exteroception. At the same time, the article aims to\nprovide a base framework for end-to-end local navigation policies and how a\ncustom robot can be trained in such simulation environment. We benchmark\nend-to-end policies with the state-of-the-art Nav2, navigation stack in Robot\nOperating System (ROS). We also cover the sim-to-real transfer process by\ndemonstrating zero-shot transferability of policies trained in the Isaac\nsimulator to real-world robots. This is further evidenced by the tests with\ndifferent simulated robots, which show the generalization of the learned\npolicy. Finally, the benchmarks demonstrate comparable performance to Nav2,\nopening the door to quick deployment of state-of-the-art end-to-end local\nplanners for custom robot platforms, but importantly furthering the\npossibilities by expanding the state and action spaces or task definitions for\nmore complex missions. Overall, with this article we introduce the most\nimportant steps, and aspects to consider, in deploying RL policies for local\npath planning and obstacle avoidance with Isaac Sim training, Gazebo testing,\nand ROS 2 for real-time inference in real robots. The code is available at\nhttps://github.com/sahars93/RL-Navigation."
                },
                "authors": [
                    {
                        "name": "Sahar Salimpour"
                    },
                    {
                        "name": "Jorge Pea-Queralta"
                    },
                    {
                        "name": "Diego Paez-Granados"
                    },
                    {
                        "name": "Jukka Heikkonen"
                    },
                    {
                        "name": "Tomi Westerlund"
                    }
                ],
                "author_detail": {
                    "name": "Tomi Westerlund"
                },
                "author": "Tomi Westerlund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02901v1",
                "updated": "2025-01-06T10:25:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    10,
                    25,
                    28,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T10:25:28Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    10,
                    25,
                    28,
                    0,
                    6,
                    0
                ],
                "title": "DeCon: Detecting Incorrect Assertions via Postconditions Generated by a\n  Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeCon: Detecting Incorrect Assertions via Postconditions Generated by a\n  Large Language Model"
                },
                "summary": "Recently, given the docstring for the target problem and the target function\nsignature, large language models (LLMs) have been used not only to generate\nsource code, but also to generate test cases, consisting of test inputs and\nassertions (e.g., in the form of checking an actual output against the expected\noutput). However, as shown by our empirical study on assertions generated by\nfour LLMs for the HumanEval benchmark, over 62% of the generated assertions are\nincorrect (i.e., failed on the ground-truth problem solution). To detect\nincorrect assertions (given the docstring and the target function signature\nalong with a sample of example inputs and outputs), in this paper, we propose a\nnew approach named DeCon to effectively detect incorrect assertions via\nLLM-generated postconditions for the target problem (a postcondition is a\npredicate that must always be true just after the execution of the ground-truth\nproblem solution). Our approach requires a small set of I/O examples (i.e., a\nsample of example inputs and outputs) for the target problem (e.g., the I/O\nexamples included in the docstring for a target problem in HumanEval). We use\nthe given I/O examples to filter out those LLM-generated postconditions that\nare violated by at least one given I/O example. We then use the remaining\npostconditions to detect incorrect assertions as those assertions that violate\nat least one remaining postcondition. Experimental results show that DeCon can\ndetect averagely more than 64% (63% and 65.5% detected by GPT-3.5 and GPT-4,\nrespectively) incorrect assertions generated by four state-of-the-art LLMs, and\nDeCon can also improve the effectiveness of these LLMs in code generation by 4%\nin terms of Pass@1. In addition, although DeCon might filter out correct\nassertions, the fault-finding ability of the remaining correct assertions\ndecreases only slightly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, given the docstring for the target problem and the target function\nsignature, large language models (LLMs) have been used not only to generate\nsource code, but also to generate test cases, consisting of test inputs and\nassertions (e.g., in the form of checking an actual output against the expected\noutput). However, as shown by our empirical study on assertions generated by\nfour LLMs for the HumanEval benchmark, over 62% of the generated assertions are\nincorrect (i.e., failed on the ground-truth problem solution). To detect\nincorrect assertions (given the docstring and the target function signature\nalong with a sample of example inputs and outputs), in this paper, we propose a\nnew approach named DeCon to effectively detect incorrect assertions via\nLLM-generated postconditions for the target problem (a postcondition is a\npredicate that must always be true just after the execution of the ground-truth\nproblem solution). Our approach requires a small set of I/O examples (i.e., a\nsample of example inputs and outputs) for the target problem (e.g., the I/O\nexamples included in the docstring for a target problem in HumanEval). We use\nthe given I/O examples to filter out those LLM-generated postconditions that\nare violated by at least one given I/O example. We then use the remaining\npostconditions to detect incorrect assertions as those assertions that violate\nat least one remaining postcondition. Experimental results show that DeCon can\ndetect averagely more than 64% (63% and 65.5% detected by GPT-3.5 and GPT-4,\nrespectively) incorrect assertions generated by four state-of-the-art LLMs, and\nDeCon can also improve the effectiveness of these LLMs in code generation by 4%\nin terms of Pass@1. In addition, although DeCon might filter out correct\nassertions, the fault-finding ability of the remaining correct assertions\ndecreases only slightly."
                },
                "authors": [
                    {
                        "name": "Hao Yu"
                    },
                    {
                        "name": "Tianyu Chen"
                    },
                    {
                        "name": "Jiaming Huang"
                    },
                    {
                        "name": "Zongyang Li"
                    },
                    {
                        "name": "Dezhi Ran"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Assaf Marron"
                    },
                    {
                        "name": "David Harel"
                    },
                    {
                        "name": "Yuan Xie"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02893v1",
                "updated": "2025-01-06T10:15:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    10,
                    15,
                    21,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T10:15:21Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    10,
                    15,
                    21,
                    0,
                    6,
                    0
                ],
                "title": "A Volumetric Approach to Privacy of Dynamical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Volumetric Approach to Privacy of Dynamical Systems"
                },
                "summary": "Information-theoretic metrics, such as mutual information, have been widely\nused to evaluate privacy leakage in dynamic systems. However, these approaches\nare typically limited to stochastic systems and face computational challenges.\nIn this paper, we introduce a novel volumetric framework for analyzing privacy\nin systems affected by unknown but bounded noise. Our model considers a dynamic\nsystem comprising public and private states, where an observation set of the\npublic state is released. An adversary utilizes the observed public state to\ninfer an uncertainty set of the private state, referred to as the inference\nattack. We define the evolution dynamics of these inference attacks and\nquantify the privacy level of the private state using the volume of its\nuncertainty sets. For linear scalar systems, we derive an explicit formulation\nof the uncertainty set. For multi-dimensional linear systems, we develop an\napproximate computation method leveraging interval analysis. We investigate the\nproperties of the proposed volumetric privacy measure and demonstrate that it\nis bounded by the information gain derived from the observation set.\nFurthermore, we propose an optimization approach to designing privacy filter\nusing randomization and linear programming based on the proposed privacy\nmeasure. The effectiveness of the optimal privacy filter design is evaluated\nthrough a production-inventory case study, illustrating its robustness against\nthe inference attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information-theoretic metrics, such as mutual information, have been widely\nused to evaluate privacy leakage in dynamic systems. However, these approaches\nare typically limited to stochastic systems and face computational challenges.\nIn this paper, we introduce a novel volumetric framework for analyzing privacy\nin systems affected by unknown but bounded noise. Our model considers a dynamic\nsystem comprising public and private states, where an observation set of the\npublic state is released. An adversary utilizes the observed public state to\ninfer an uncertainty set of the private state, referred to as the inference\nattack. We define the evolution dynamics of these inference attacks and\nquantify the privacy level of the private state using the volume of its\nuncertainty sets. For linear scalar systems, we derive an explicit formulation\nof the uncertainty set. For multi-dimensional linear systems, we develop an\napproximate computation method leveraging interval analysis. We investigate the\nproperties of the proposed volumetric privacy measure and demonstrate that it\nis bounded by the information gain derived from the observation set.\nFurthermore, we propose an optimization approach to designing privacy filter\nusing randomization and linear programming based on the proposed privacy\nmeasure. The effectiveness of the optimal privacy filter design is evaluated\nthrough a production-inventory case study, illustrating its robustness against\nthe inference attack."
                },
                "authors": [
                    {
                        "name": "Chuanghong Weng"
                    },
                    {
                        "name": "Ehsan Nekouei"
                    }
                ],
                "author_detail": {
                    "name": "Ehsan Nekouei"
                },
                "author": "Ehsan Nekouei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02885v1",
                "updated": "2025-01-06T09:55:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    9,
                    55,
                    55,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T09:55:55Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    9,
                    55,
                    55,
                    0,
                    6,
                    0
                ],
                "title": "MDP3: A Training-free Approach for List-wise Frame Selection in\n  Video-LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MDP3: A Training-free Approach for List-wise Frame Selection in\n  Video-LLMs"
                },
                "summary": "Video large language models (Video-LLMs) have made significant progress in\nunderstanding videos. However, processing multiple frames leads to lengthy\nvisual token sequences, presenting challenges such as the limited context\nlength cannot accommodate the entire video, and the inclusion of irrelevant\nframes hinders visual perception. Hence, effective frame selection is crucial.\nThis paper emphasizes that frame selection should follow three key principles:\nquery relevance, list-wise diversity, and sequentiality. Existing methods, such\nas uniform frame sampling and query-frame matching, do not capture all of these\nprinciples. Thus, we propose Markov decision determinantal point process with\ndynamic programming (MDP3) for frame selection, a training-free and\nmodel-agnostic method that can be seamlessly integrated into existing\nVideo-LLMs. Our method first estimates frame similarities conditioned on the\nquery using a conditional Gaussian kernel within the reproducing kernel Hilbert\nspace~(RKHS). We then apply the determinantal point process~(DPP) to the\nsimilarity matrix to capture both query relevance and list-wise diversity. To\nincorporate sequentiality, we segment the video and apply DPP within each\nsegment, conditioned on the preceding segment selection, modeled as a Markov\ndecision process~(MDP) for allocating selection sizes across segments.\nTheoretically, MDP3 provides a \\((1 - 1/e)\\)-approximate solution to the\nNP-hard list-wise frame selection problem with pseudo-polynomial time\ncomplexity, demonstrating its efficiency. Empirically, MDP3 significantly\noutperforms existing methods, verifying its effectiveness and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (Video-LLMs) have made significant progress in\nunderstanding videos. However, processing multiple frames leads to lengthy\nvisual token sequences, presenting challenges such as the limited context\nlength cannot accommodate the entire video, and the inclusion of irrelevant\nframes hinders visual perception. Hence, effective frame selection is crucial.\nThis paper emphasizes that frame selection should follow three key principles:\nquery relevance, list-wise diversity, and sequentiality. Existing methods, such\nas uniform frame sampling and query-frame matching, do not capture all of these\nprinciples. Thus, we propose Markov decision determinantal point process with\ndynamic programming (MDP3) for frame selection, a training-free and\nmodel-agnostic method that can be seamlessly integrated into existing\nVideo-LLMs. Our method first estimates frame similarities conditioned on the\nquery using a conditional Gaussian kernel within the reproducing kernel Hilbert\nspace~(RKHS). We then apply the determinantal point process~(DPP) to the\nsimilarity matrix to capture both query relevance and list-wise diversity. To\nincorporate sequentiality, we segment the video and apply DPP within each\nsegment, conditioned on the preceding segment selection, modeled as a Markov\ndecision process~(MDP) for allocating selection sizes across segments.\nTheoretically, MDP3 provides a \\((1 - 1/e)\\)-approximate solution to the\nNP-hard list-wise frame selection problem with pseudo-polynomial time\ncomplexity, demonstrating its efficiency. Empirically, MDP3 significantly\noutperforms existing methods, verifying its effectiveness and robustness."
                },
                "authors": [
                    {
                        "name": "Hui Sun"
                    },
                    {
                        "name": "Shiyin Lu"
                    },
                    {
                        "name": "Huanyu Wang"
                    },
                    {
                        "name": "Qing-Guo Chen"
                    },
                    {
                        "name": "Zhao Xu"
                    },
                    {
                        "name": "Weihua Luo"
                    },
                    {
                        "name": "Kaifu Zhang"
                    },
                    {
                        "name": "Ming Li"
                    }
                ],
                "author_detail": {
                    "name": "Ming Li"
                },
                "author": "Ming Li",
                "arxiv_comment": "24 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13432v2",
                "updated": "2025-01-06T09:27:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    9,
                    27,
                    0,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-18T02:07:21Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    2,
                    7,
                    21,
                    2,
                    353,
                    0
                ],
                "title": "Large Language Model Enhanced Recommender Systems: Taxonomy, Trend,\n  Application and Future",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Enhanced Recommender Systems: Taxonomy, Trend,\n  Application and Future"
                },
                "summary": "Large Language Model (LLM) has transformative potential in various domains,\nincluding recommender systems (RS). There have been a handful of research that\nfocuses on empowering the RS by LLM. However, previous efforts mainly focus on\nLLM as RS, which may face the challenge of intolerant inference costs by LLM.\nRecently, the integration of LLM into RS, known as LLM-Enhanced Recommender\nSystems (LLMERS), has garnered significant interest due to its potential to\naddress latency and memory constraints in real-world applications. This paper\npresents a comprehensive survey of the latest research efforts aimed at\nleveraging LLM to enhance RS capabilities. We identify a critical shift in the\nfield with the move towards incorporating LLM into the online system, notably\nby avoiding their use during inference. Our survey categorizes the existing\nLLMERS approaches into three primary types based on the component of the RS\nmodel being augmented: Knowledge Enhancement, Interaction Enhancement, and\nModel Enhancement. We provide an in-depth analysis of each category, discussing\nthe methodologies, challenges, and contributions of recent studies.\nFurthermore, we highlight several promising research directions that could\nfurther advance the field of LLMERS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) has transformative potential in various domains,\nincluding recommender systems (RS). There have been a handful of research that\nfocuses on empowering the RS by LLM. However, previous efforts mainly focus on\nLLM as RS, which may face the challenge of intolerant inference costs by LLM.\nRecently, the integration of LLM into RS, known as LLM-Enhanced Recommender\nSystems (LLMERS), has garnered significant interest due to its potential to\naddress latency and memory constraints in real-world applications. This paper\npresents a comprehensive survey of the latest research efforts aimed at\nleveraging LLM to enhance RS capabilities. We identify a critical shift in the\nfield with the move towards incorporating LLM into the online system, notably\nby avoiding their use during inference. Our survey categorizes the existing\nLLMERS approaches into three primary types based on the component of the RS\nmodel being augmented: Knowledge Enhancement, Interaction Enhancement, and\nModel Enhancement. We provide an in-depth analysis of each category, discussing\nthe methodologies, challenges, and contributions of recent studies.\nFurthermore, we highlight several promising research directions that could\nfurther advance the field of LLMERS."
                },
                "authors": [
                    {
                        "name": "Qidong Liu"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Yejing Wang"
                    },
                    {
                        "name": "Zijian Zhang"
                    },
                    {
                        "name": "Yuqi Sun"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Maolin Wang"
                    },
                    {
                        "name": "Pengyue Jia"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Feng Tian"
                    }
                ],
                "author_detail": {
                    "name": "Feng Tian"
                },
                "author": "Feng Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02869v1",
                "updated": "2025-01-06T09:22:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    9,
                    22,
                    36,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T09:22:36Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    9,
                    22,
                    36,
                    0,
                    6,
                    0
                ],
                "title": "IIMedGPT: Promoting Large Language Model Capabilities of Medical Tasks\n  by Efficient Human Preference Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IIMedGPT: Promoting Large Language Model Capabilities of Medical Tasks\n  by Efficient Human Preference Alignment"
                },
                "summary": "Recent researches of large language models(LLM), which is pre-trained on\nmassive general-purpose corpora, have achieved breakthroughs in responding\nhuman queries. However, these methods face challenges including limited data\ninsufficiency to support extensive pre-training and can not align responses\nwith users' instructions. To address these issues, we introduce a medical\ninstruction dataset, CMedINS, containing six medical instructions derived from\nactual medical tasks, which effectively fine-tunes LLM in conjunction with\nother data. Subsequently, We launch our medical model, IIMedGPT, employing an\nefficient preference alignment method, Direct preference Optimization(DPO). The\nresults show that our final model outperforms existing medical models in\nmedical dialogue.Datsets, Code and model checkpoints will be released upon\nacceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent researches of large language models(LLM), which is pre-trained on\nmassive general-purpose corpora, have achieved breakthroughs in responding\nhuman queries. However, these methods face challenges including limited data\ninsufficiency to support extensive pre-training and can not align responses\nwith users' instructions. To address these issues, we introduce a medical\ninstruction dataset, CMedINS, containing six medical instructions derived from\nactual medical tasks, which effectively fine-tunes LLM in conjunction with\nother data. Subsequently, We launch our medical model, IIMedGPT, employing an\nefficient preference alignment method, Direct preference Optimization(DPO). The\nresults show that our final model outperforms existing medical models in\nmedical dialogue.Datsets, Code and model checkpoints will be released upon\nacceptance."
                },
                "authors": [
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Zheng Chang"
                    },
                    {
                        "name": "Wentao Cai"
                    },
                    {
                        "name": "MengXing Ren"
                    },
                    {
                        "name": "Kang Yuan"
                    },
                    {
                        "name": "Yining Sun"
                    },
                    {
                        "name": "Zenghui Ding"
                    }
                ],
                "author_detail": {
                    "name": "Zenghui Ding"
                },
                "author": "Zenghui Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.08982v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.08982v4",
                "updated": "2025-01-06T08:44:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    8,
                    44,
                    2,
                    0,
                    6,
                    0
                ],
                "published": "2023-09-16T12:46:44Z",
                "published_parsed": [
                    2023,
                    9,
                    16,
                    12,
                    46,
                    44,
                    5,
                    259,
                    0
                ],
                "title": "Least squares estimation in nonstationary nonlinear cohort panels with\n  learning from experience",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Least squares estimation in nonstationary nonlinear cohort panels with\n  learning from experience"
                },
                "summary": "We discuss techniques of estimation and inference for nonstationary nonlinear\ncohort panels with learning from experience, showing, inter alia, the\nconsistency and asymptotic normality of the nonlinear least squares estimator\nused in empirical practice. Potential pitfalls for hypothesis testing are\nidentified and solutions proposed. Monte Carlo simulations verify the\nproperties of the estimator and corresponding test statistics in finite\nsamples, while an application to a panel of survey expectations demonstrates\nthe usefulness of the theory developed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We discuss techniques of estimation and inference for nonstationary nonlinear\ncohort panels with learning from experience, showing, inter alia, the\nconsistency and asymptotic normality of the nonlinear least squares estimator\nused in empirical practice. Potential pitfalls for hypothesis testing are\nidentified and solutions proposed. Monte Carlo simulations verify the\nproperties of the estimator and corresponding test statistics in finite\nsamples, while an application to a panel of survey expectations demonstrates\nthe usefulness of the theory developed."
                },
                "authors": [
                    {
                        "name": "Alexander Mayer"
                    },
                    {
                        "name": "Michael Massmann"
                    }
                ],
                "author_detail": {
                    "name": "Michael Massmann"
                },
                "author": "Michael Massmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.08982v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.08982v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02844v1",
                "updated": "2025-01-06T08:43:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    8,
                    43,
                    31,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T08:43:31Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    8,
                    43,
                    31,
                    0,
                    6,
                    0
                ],
                "title": "Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text\n  Classification"
                },
                "summary": "Text classification is a fundamental task in natural language processing,\npivotal to various applications such as query optimization, data integration,\nand schema matching. While neural network-based models, such as CNN and BERT,\nhave demonstrated remarkable performance in text classification, their\neffectiveness heavily relies on abundant labeled training data. This dependency\nmakes these models less effective in dynamic few-shot text classification,\nwhere labeled data is scarce, and target labels frequently evolve based on\napplication needs. Recently, large language models (LLMs) have shown promise\ndue to their extensive pretraining and contextual understanding. Current\napproaches provide LLMs with text inputs, candidate labels, and additional side\ninformation (e.g., descriptions) to predict text labels. However, their\neffectiveness is hindered by the increased input size and the noise introduced\nthrough side information processing. To address these limitations, we propose a\ngraph-based online retrieval-augmented generation framework, namely GORAG, for\ndynamic few-shot text classification. GORAG constructs and maintains an\nadaptive information graph by extracting side information across all target\ntexts, rather than treating each input independently. It employs a weighted\nedge mechanism to prioritize the importance and reliability of extracted\ninformation and dynamically retrieves relevant context using a minimum-cost\nspanning tree tailored for each text input. Empirical evaluations demonstrate\nthat GORAG outperforms existing approaches by providing more comprehensive and\naccurate contextual information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text classification is a fundamental task in natural language processing,\npivotal to various applications such as query optimization, data integration,\nand schema matching. While neural network-based models, such as CNN and BERT,\nhave demonstrated remarkable performance in text classification, their\neffectiveness heavily relies on abundant labeled training data. This dependency\nmakes these models less effective in dynamic few-shot text classification,\nwhere labeled data is scarce, and target labels frequently evolve based on\napplication needs. Recently, large language models (LLMs) have shown promise\ndue to their extensive pretraining and contextual understanding. Current\napproaches provide LLMs with text inputs, candidate labels, and additional side\ninformation (e.g., descriptions) to predict text labels. However, their\neffectiveness is hindered by the increased input size and the noise introduced\nthrough side information processing. To address these limitations, we propose a\ngraph-based online retrieval-augmented generation framework, namely GORAG, for\ndynamic few-shot text classification. GORAG constructs and maintains an\nadaptive information graph by extracting side information across all target\ntexts, rather than treating each input independently. It employs a weighted\nedge mechanism to prioritize the importance and reliability of extracted\ninformation and dynamically retrieves relevant context using a minimum-cost\nspanning tree tailored for each text input. Empirical evaluations demonstrate\nthat GORAG outperforms existing approaches by providing more comprehensive and\naccurate contextual information."
                },
                "authors": [
                    {
                        "name": "Yubo Wang"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Fei Teng"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02825v1",
                "updated": "2025-01-06T07:57:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    7,
                    57,
                    51,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T07:57:51Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    7,
                    57,
                    51,
                    0,
                    6,
                    0
                ],
                "title": "Randomly Sampled Language Reasoning Problems Reveal Limits of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomly Sampled Language Reasoning Problems Reveal Limits of LLMs"
                },
                "summary": "Can LLMs pick up language structure from examples? Evidence in prior work\nseems to indicate yes, as pretrained models repeatedly demonstrate the ability\nto adapt to new language structures and vocabularies. However, this line of\nresearch typically considers languages that are present within common\npretraining datasets, or otherwise share notable similarities with these seen\nlanguages. In contrast, in this work we attempt to measure models' language\nunderstanding capacity while circumventing the risk of dataset recall. We\nparameterize large families of language tasks recognized by deterministic\nfinite automata (DFAs), and can thus sample novel language reasoning problems\nto fairly evaulate LLMs regardless of training data. We find that, even in the\nstrikingly simple setting of 3-state DFAs, LLMs underperform unparameterized\nngram models on both language recognition and synthesis tasks. These results\nsuggest that LLMs struggle to match the ability of basic language models in\nrecognizing and reasoning over languages that are sufficiently distinct from\nthe ones they see at training time, underscoring the distinction between\nlearning individual languages and possessing a general theory of language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs pick up language structure from examples? Evidence in prior work\nseems to indicate yes, as pretrained models repeatedly demonstrate the ability\nto adapt to new language structures and vocabularies. However, this line of\nresearch typically considers languages that are present within common\npretraining datasets, or otherwise share notable similarities with these seen\nlanguages. In contrast, in this work we attempt to measure models' language\nunderstanding capacity while circumventing the risk of dataset recall. We\nparameterize large families of language tasks recognized by deterministic\nfinite automata (DFAs), and can thus sample novel language reasoning problems\nto fairly evaulate LLMs regardless of training data. We find that, even in the\nstrikingly simple setting of 3-state DFAs, LLMs underperform unparameterized\nngram models on both language recognition and synthesis tasks. These results\nsuggest that LLMs struggle to match the ability of basic language models in\nrecognizing and reasoning over languages that are sufficiently distinct from\nthe ones they see at training time, underscoring the distinction between\nlearning individual languages and possessing a general theory of language."
                },
                "authors": [
                    {
                        "name": "Kavi Gupta"
                    },
                    {
                        "name": "Kate Sanders"
                    },
                    {
                        "name": "Armando Solar-Lezama"
                    }
                ],
                "author_detail": {
                    "name": "Armando Solar-Lezama"
                },
                "author": "Armando Solar-Lezama",
                "arxiv_comment": "8 pages, 3 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18537v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18537v2",
                "updated": "2025-01-06T07:42:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    7,
                    42,
                    20,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-24T16:38:04Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    38,
                    4,
                    1,
                    359,
                    0
                ],
                "title": "Harnessing Large Language Models for Knowledge Graph Question Answering\n  via Adaptive Multi-Aspect Retrieval-Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Large Language Models for Knowledge Graph Question Answering\n  via Adaptive Multi-Aspect Retrieval-Augmentation"
                },
                "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities, yet\nstruggle with hallucination and outdated knowledge when tasked with complex\nknowledge reasoning, resulting in factually incorrect outputs. Previous studies\nhave attempted to mitigate it by retrieving factual knowledge from large-scale\nknowledge graphs (KGs) to assist LLMs in logical reasoning and prediction of\nanswers. However, this kind of approach often introduces noise and irrelevant\ndata, especially in situations with extensive context from multiple knowledge\naspects. In this way, LLM attention can be potentially mislead from question\nand relevant information. In our study, we introduce an Adaptive Multi-Aspect\nRetrieval-augmented over KGs (Amar) framework. This method retrieves knowledge\nincluding entities, relations, and subgraphs, and converts each piece of\nretrieved text into prompt embeddings. The Amar framework comprises two key\nsub-components: 1) a self-alignment module that aligns commonalities among\nentities, relations, and subgraphs to enhance retrieved text, thereby reducing\nnoise interference; 2) a relevance gating module that employs a soft gate to\nlearn the relevance score between question and multi-aspect retrieved data, to\ndetermine which information should be used to enhance LLMs' output, or even\nfiltered altogether. Our method has achieved state-of-the-art performance on\ntwo common datasets, WebQSP and CWQ, showing a 1.9\\% improvement in accuracy\nover its best competitor and a 6.6\\% improvement in logical form generation\nover a method that directly uses retrieved text as context prompts. These\nresults demonstrate the effectiveness of Amar in improving the reasoning of\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate remarkable capabilities, yet\nstruggle with hallucination and outdated knowledge when tasked with complex\nknowledge reasoning, resulting in factually incorrect outputs. Previous studies\nhave attempted to mitigate it by retrieving factual knowledge from large-scale\nknowledge graphs (KGs) to assist LLMs in logical reasoning and prediction of\nanswers. However, this kind of approach often introduces noise and irrelevant\ndata, especially in situations with extensive context from multiple knowledge\naspects. In this way, LLM attention can be potentially mislead from question\nand relevant information. In our study, we introduce an Adaptive Multi-Aspect\nRetrieval-augmented over KGs (Amar) framework. This method retrieves knowledge\nincluding entities, relations, and subgraphs, and converts each piece of\nretrieved text into prompt embeddings. The Amar framework comprises two key\nsub-components: 1) a self-alignment module that aligns commonalities among\nentities, relations, and subgraphs to enhance retrieved text, thereby reducing\nnoise interference; 2) a relevance gating module that employs a soft gate to\nlearn the relevance score between question and multi-aspect retrieved data, to\ndetermine which information should be used to enhance LLMs' output, or even\nfiltered altogether. Our method has achieved state-of-the-art performance on\ntwo common datasets, WebQSP and CWQ, showing a 1.9\\% improvement in accuracy\nover its best competitor and a 6.6\\% improvement in logical form generation\nover a method that directly uses retrieved text as context prompts. These\nresults demonstrate the effectiveness of Amar in improving the reasoning of\nLLMs."
                },
                "authors": [
                    {
                        "name": "Derong Xu"
                    },
                    {
                        "name": "Xinhang Li"
                    },
                    {
                        "name": "Ziheng Zhang"
                    },
                    {
                        "name": "Zhenxi Lin"
                    },
                    {
                        "name": "Zhihong Zhu"
                    },
                    {
                        "name": "Zhi Zheng"
                    },
                    {
                        "name": "Xian Wu"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "arxiv_comment": "Accepted by AAAI'2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18537v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18537v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10238v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10238v2",
                "updated": "2025-01-06T07:39:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    7,
                    39,
                    41,
                    0,
                    6,
                    0
                ],
                "published": "2024-10-14T07:56:51Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    7,
                    56,
                    51,
                    0,
                    288,
                    0
                ],
                "title": "ForgeryGPT: Multimodal Large Language Model For Explainable Image\n  Forgery Detection and Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ForgeryGPT: Multimodal Large Language Model For Explainable Image\n  Forgery Detection and Localization"
                },
                "summary": "Multimodal Large Language Models (MLLMs), such as GPT4o, have shown strong\ncapabilities in visual reasoning and explanation generation. However, despite\nthese strengths, they face significant challenges in the increasingly critical\ntask of Image Forgery Detection and Localization (IFDL). Moreover, existing\nIFDL methods are typically limited to the learning of low-level\nsemantic-agnostic clues and merely provide a single outcome judgment. To tackle\nthese issues, we propose ForgeryGPT, a novel framework that advances the IFDL\ntask by capturing high-order forensics knowledge correlations of forged images\nfrom diverse linguistic feature spaces, while enabling explainable generation\nand interactive dialogue through a newly customized Large Language Model (LLM)\narchitecture. Specifically, ForgeryGPT enhances traditional LLMs by integrating\nthe Mask-Aware Forgery Extractor, which enables the excavating of precise\nforgery mask information from input images and facilitating pixel-level\nunderstanding of tampering artifacts. The Mask-Aware Forgery Extractor consists\nof a Forgery Localization Expert (FL-Expert) and a Mask Encoder, where the\nFL-Expert is augmented with an Object-agnostic Forgery Prompt and a\nVocabulary-enhanced Vision Encoder, allowing for effectively capturing of\nmulti-scale fine-grained forgery details. To enhance its performance, we\nimplement a three-stage training strategy, supported by our designed Mask-Text\nAlignment and IFDL Task-Specific Instruction Tuning datasets, which align\nvision-language modalities and improve forgery detection and\ninstruction-following capabilities. Extensive experiments demonstrate the\neffectiveness of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs), such as GPT4o, have shown strong\ncapabilities in visual reasoning and explanation generation. However, despite\nthese strengths, they face significant challenges in the increasingly critical\ntask of Image Forgery Detection and Localization (IFDL). Moreover, existing\nIFDL methods are typically limited to the learning of low-level\nsemantic-agnostic clues and merely provide a single outcome judgment. To tackle\nthese issues, we propose ForgeryGPT, a novel framework that advances the IFDL\ntask by capturing high-order forensics knowledge correlations of forged images\nfrom diverse linguistic feature spaces, while enabling explainable generation\nand interactive dialogue through a newly customized Large Language Model (LLM)\narchitecture. Specifically, ForgeryGPT enhances traditional LLMs by integrating\nthe Mask-Aware Forgery Extractor, which enables the excavating of precise\nforgery mask information from input images and facilitating pixel-level\nunderstanding of tampering artifacts. The Mask-Aware Forgery Extractor consists\nof a Forgery Localization Expert (FL-Expert) and a Mask Encoder, where the\nFL-Expert is augmented with an Object-agnostic Forgery Prompt and a\nVocabulary-enhanced Vision Encoder, allowing for effectively capturing of\nmulti-scale fine-grained forgery details. To enhance its performance, we\nimplement a three-stage training strategy, supported by our designed Mask-Text\nAlignment and IFDL Task-Specific Instruction Tuning datasets, which align\nvision-language modalities and improve forgery detection and\ninstruction-following capabilities. Extensive experiments demonstrate the\neffectiveness of the proposed method."
                },
                "authors": [
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Fanrui Zhang"
                    },
                    {
                        "name": "Jiaying Zhu"
                    },
                    {
                        "name": "Esther Sun"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Zheng-Jun Zha"
                    }
                ],
                "author_detail": {
                    "name": "Zheng-Jun Zha"
                },
                "author": "Zheng-Jun Zha",
                "arxiv_comment": "16 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10238v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20787v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20787v3",
                "updated": "2025-01-06T07:22:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    7,
                    22,
                    50,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-30T08:11:54Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    8,
                    11,
                    54,
                    0,
                    365,
                    0
                ],
                "title": "SecBench: A Comprehensive Multi-Dimensional Benchmarking Dataset for\n  LLMs in Cybersecurity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SecBench: A Comprehensive Multi-Dimensional Benchmarking Dataset for\n  LLMs in Cybersecurity"
                },
                "summary": "Evaluating Large Language Models (LLMs) is crucial for understanding their\ncapabilities and limitations across various applications, including natural\nlanguage processing and code generation. Existing benchmarks like MMLU, C-Eval,\nand HumanEval assess general LLM performance but lack focus on specific expert\ndomains such as cybersecurity. Previous attempts to create cybersecurity\ndatasets have faced limitations, including insufficient data volume and a\nreliance on multiple-choice questions (MCQs). To address these gaps, we propose\nSecBench, a multi-dimensional benchmarking dataset designed to evaluate LLMs in\nthe cybersecurity domain. SecBench includes questions in various formats (MCQs\nand short-answer questions (SAQs)), at different capability levels (Knowledge\nRetention and Logical Reasoning), in multiple languages (Chinese and English),\nand across various sub-domains. The dataset was constructed by collecting\nhigh-quality data from open sources and organizing a Cybersecurity Question\nDesign Contest, resulting in 44,823 MCQs and 3,087 SAQs. Particularly, we used\nthe powerful while cost-effective LLMs to (1). label the data and (2).\nconstructing a grading agent for automatic evaluation of SAQs. Benchmarking\nresults on 16 SOTA LLMs demonstrate the usability of SecBench, which is\narguably the largest and most comprehensive benchmark dataset for LLMs in\ncybersecurity. More information about SecBench can be found at our website, and\nthe dataset can be accessed via the artifact link.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models (LLMs) is crucial for understanding their\ncapabilities and limitations across various applications, including natural\nlanguage processing and code generation. Existing benchmarks like MMLU, C-Eval,\nand HumanEval assess general LLM performance but lack focus on specific expert\ndomains such as cybersecurity. Previous attempts to create cybersecurity\ndatasets have faced limitations, including insufficient data volume and a\nreliance on multiple-choice questions (MCQs). To address these gaps, we propose\nSecBench, a multi-dimensional benchmarking dataset designed to evaluate LLMs in\nthe cybersecurity domain. SecBench includes questions in various formats (MCQs\nand short-answer questions (SAQs)), at different capability levels (Knowledge\nRetention and Logical Reasoning), in multiple languages (Chinese and English),\nand across various sub-domains. The dataset was constructed by collecting\nhigh-quality data from open sources and organizing a Cybersecurity Question\nDesign Contest, resulting in 44,823 MCQs and 3,087 SAQs. Particularly, we used\nthe powerful while cost-effective LLMs to (1). label the data and (2).\nconstructing a grading agent for automatic evaluation of SAQs. Benchmarking\nresults on 16 SOTA LLMs demonstrate the usability of SecBench, which is\narguably the largest and most comprehensive benchmark dataset for LLMs in\ncybersecurity. More information about SecBench can be found at our website, and\nthe dataset can be accessed via the artifact link."
                },
                "authors": [
                    {
                        "name": "Pengfei Jing"
                    },
                    {
                        "name": "Mengyun Tang"
                    },
                    {
                        "name": "Xiaorong Shi"
                    },
                    {
                        "name": "Xing Zheng"
                    },
                    {
                        "name": "Sen Nie"
                    },
                    {
                        "name": "Shi Wu"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Xiapu Luo"
                    }
                ],
                "author_detail": {
                    "name": "Xiapu Luo"
                },
                "author": "Xiapu Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20787v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20787v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02809v1",
                "updated": "2025-01-06T07:13:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    7,
                    13,
                    10,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T07:13:10Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    7,
                    13,
                    10,
                    0,
                    6,
                    0
                ],
                "title": "Theoretical Data-Driven MobilePosenet: Lightweight Neural Network for\n  Accurate Calibration-Free 5-DOF Magnet Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theoretical Data-Driven MobilePosenet: Lightweight Neural Network for\n  Accurate Calibration-Free 5-DOF Magnet Localization"
                },
                "summary": "Permanent magnet tracking using the external sensor array is crucial for the\naccurate localization of wireless capsule endoscope robots. Traditional\ntracking algorithms, based on the magnetic dipole model and Levenberg-Marquardt\n(LM) algorithm, face challenges related to computational delays and the need\nfor initial position estimation. More recently proposed neural network-based\napproaches often require extensive hardware calibration and real-world data\ncollection, which are time-consuming and labor-intensive. To address these\nchallenges, we propose MobilePosenet, a lightweight neural network architecture\nthat leverages depthwise separable convolutions to minimize computational cost\nand a channel attention mechanism to enhance localization accuracy. Besides,\nthe inputs to the network integrate the sensors' coordinate information and\nrandom noise, compensating for the discrepancies between the theoretical model\nand the actual magnetic fields and thus allowing MobilePosenet to be trained\nentirely on theoretical data. Experimental evaluations conducted in a \\(90\n\\times 90 \\times 80\\) mm workspace demonstrate that MobilePosenet exhibits\nexcellent 5-DOF localization accuracy ($1.54 \\pm 1.03$ mm and $2.24 \\pm\n1.84^{\\circ}$) and inference speed (0.9 ms) against state-of-the-art methods\ntrained on real-world data. Since network training relies solely on theoretical\ndata, MobilePosenet can eliminate the hardware calibration and real-world data\ncollection process, improving the generalizability of this permanent magnet\nlocalization method and the potential for rapid adoption in different clinical\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Permanent magnet tracking using the external sensor array is crucial for the\naccurate localization of wireless capsule endoscope robots. Traditional\ntracking algorithms, based on the magnetic dipole model and Levenberg-Marquardt\n(LM) algorithm, face challenges related to computational delays and the need\nfor initial position estimation. More recently proposed neural network-based\napproaches often require extensive hardware calibration and real-world data\ncollection, which are time-consuming and labor-intensive. To address these\nchallenges, we propose MobilePosenet, a lightweight neural network architecture\nthat leverages depthwise separable convolutions to minimize computational cost\nand a channel attention mechanism to enhance localization accuracy. Besides,\nthe inputs to the network integrate the sensors' coordinate information and\nrandom noise, compensating for the discrepancies between the theoretical model\nand the actual magnetic fields and thus allowing MobilePosenet to be trained\nentirely on theoretical data. Experimental evaluations conducted in a \\(90\n\\times 90 \\times 80\\) mm workspace demonstrate that MobilePosenet exhibits\nexcellent 5-DOF localization accuracy ($1.54 \\pm 1.03$ mm and $2.24 \\pm\n1.84^{\\circ}$) and inference speed (0.9 ms) against state-of-the-art methods\ntrained on real-world data. Since network training relies solely on theoretical\ndata, MobilePosenet can eliminate the hardware calibration and real-world data\ncollection process, improving the generalizability of this permanent magnet\nlocalization method and the potential for rapid adoption in different clinical\nsettings."
                },
                "authors": [
                    {
                        "name": "Wenxuan Xie"
                    },
                    {
                        "name": "Yuelin Zhang"
                    },
                    {
                        "name": "Jiwei Shan"
                    },
                    {
                        "name": "Hongzhe Sun"
                    },
                    {
                        "name": "Jiewen Tan"
                    },
                    {
                        "name": "Shing Shin Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Shing Shin Cheng"
                },
                "author": "Shing Shin Cheng",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02808v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02808v1",
                "updated": "2025-01-06T07:11:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    7,
                    11,
                    34,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T07:11:34Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    7,
                    11,
                    34,
                    0,
                    6,
                    0
                ],
                "title": "DarkFarseer: Inductive Spatio-temporal Kriging via Hidden Style\n  Enhancement and Sparsity-Noise Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DarkFarseer: Inductive Spatio-temporal Kriging via Hidden Style\n  Enhancement and Sparsity-Noise Mitigation"
                },
                "summary": "With the rapid growth of the Internet of Things and Cyber-Physical Systems,\nwidespread sensor deployment has become essential. However, the high costs of\nbuilding sensor networks limit their scale and coverage, making fine-grained\ndeployment challenging. Inductive Spatio-Temporal Kriging (ISK) addresses this\nissue by introducing virtual sensors. Based on graph neural networks (GNNs)\nextracting the relationships between physical and virtual sensors, ISK can\ninfer the measurements of virtual sensors from physical sensors. However,\ncurrent ISK methods rely on conventional message-passing mechanisms and network\narchitectures, without effectively extracting spatio-temporal features of\nphysical sensors and focusing on representing virtual sensors. Additionally,\nexisting graph construction methods face issues of sparse and noisy\nconnections, destroying ISK performance. To address these issues, we propose\nDarkFarseer, a novel ISK framework with three key components. First, we propose\nthe Neighbor Hidden Style Enhancement module with a style transfer strategy to\nenhance the representation of virtual nodes in a temporal-then-spatial manner\nto better extract the spatial relationships between physical and virtual nodes.\nSecond, we propose Virtual-Component Contrastive Learning, which aims to enrich\nthe node representation by establishing the association between the patterns of\nvirtual nodes and the regional patterns within graph components. Lastly, we\ndesign a Similarity-Based Graph Denoising Strategy, which reduces the\nconnectivity strength of noisy connections around virtual nodes and their\nneighbors based on their temporal information and regional spatial patterns.\nExtensive experiments demonstrate that DarkFarseer significantly outperforms\nexisting ISK methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of the Internet of Things and Cyber-Physical Systems,\nwidespread sensor deployment has become essential. However, the high costs of\nbuilding sensor networks limit their scale and coverage, making fine-grained\ndeployment challenging. Inductive Spatio-Temporal Kriging (ISK) addresses this\nissue by introducing virtual sensors. Based on graph neural networks (GNNs)\nextracting the relationships between physical and virtual sensors, ISK can\ninfer the measurements of virtual sensors from physical sensors. However,\ncurrent ISK methods rely on conventional message-passing mechanisms and network\narchitectures, without effectively extracting spatio-temporal features of\nphysical sensors and focusing on representing virtual sensors. Additionally,\nexisting graph construction methods face issues of sparse and noisy\nconnections, destroying ISK performance. To address these issues, we propose\nDarkFarseer, a novel ISK framework with three key components. First, we propose\nthe Neighbor Hidden Style Enhancement module with a style transfer strategy to\nenhance the representation of virtual nodes in a temporal-then-spatial manner\nto better extract the spatial relationships between physical and virtual nodes.\nSecond, we propose Virtual-Component Contrastive Learning, which aims to enrich\nthe node representation by establishing the association between the patterns of\nvirtual nodes and the regional patterns within graph components. Lastly, we\ndesign a Similarity-Based Graph Denoising Strategy, which reduces the\nconnectivity strength of noisy connections around virtual nodes and their\nneighbors based on their temporal information and regional spatial patterns.\nExtensive experiments demonstrate that DarkFarseer significantly outperforms\nexisting ISK methods."
                },
                "authors": [
                    {
                        "name": "Zhuoxuan Liang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Dalin Zhang"
                    },
                    {
                        "name": "Yidan Chen"
                    },
                    {
                        "name": "Zhihong Wang"
                    },
                    {
                        "name": "Xiangping Zheng"
                    },
                    {
                        "name": "Moustafa Youssef"
                    }
                ],
                "author_detail": {
                    "name": "Moustafa Youssef"
                },
                "author": "Moustafa Youssef",
                "arxiv_comment": "TKDE (Under Review)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02808v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02808v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11533v2",
                "updated": "2025-01-06T06:39:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    39,
                    22,
                    0,
                    6,
                    0
                ],
                "published": "2024-05-19T12:24:30Z",
                "published_parsed": [
                    2024,
                    5,
                    19,
                    12,
                    24,
                    30,
                    6,
                    140,
                    0
                ],
                "title": "Hierarchical Selective Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Selective Classification"
                },
                "summary": "Deploying deep neural networks for risk-sensitive tasks necessitates an\nuncertainty estimation mechanism. This paper introduces hierarchical selective\nclassification, extending selective classification to a hierarchical setting.\nOur approach leverages the inherent structure of class relationships, enabling\nmodels to reduce the specificity of their predictions when faced with\nuncertainty. In this paper, we first formalize hierarchical risk and coverage,\nand introduce hierarchical risk-coverage curves. Next, we develop algorithms\nfor hierarchical selective classification (which we refer to as \"inference\nrules\"), and propose an efficient algorithm that guarantees a target accuracy\nconstraint with high probability. Lastly, we conduct extensive empirical\nstudies on over a thousand ImageNet classifiers, revealing that training\nregimes such as CLIP, pretraining on ImageNet21k and knowledge distillation\nboost hierarchical selective performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying deep neural networks for risk-sensitive tasks necessitates an\nuncertainty estimation mechanism. This paper introduces hierarchical selective\nclassification, extending selective classification to a hierarchical setting.\nOur approach leverages the inherent structure of class relationships, enabling\nmodels to reduce the specificity of their predictions when faced with\nuncertainty. In this paper, we first formalize hierarchical risk and coverage,\nand introduce hierarchical risk-coverage curves. Next, we develop algorithms\nfor hierarchical selective classification (which we refer to as \"inference\nrules\"), and propose an efficient algorithm that guarantees a target accuracy\nconstraint with high probability. Lastly, we conduct extensive empirical\nstudies on over a thousand ImageNet classifiers, revealing that training\nregimes such as CLIP, pretraining on ImageNet21k and knowledge distillation\nboost hierarchical selective performance."
                },
                "authors": [
                    {
                        "name": "Shani Goren"
                    },
                    {
                        "name": "Ido Galil"
                    },
                    {
                        "name": "Ran El-Yaniv"
                    }
                ],
                "author_detail": {
                    "name": "Ran El-Yaniv"
                },
                "author": "Ran El-Yaniv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16398v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16398v3",
                "updated": "2025-01-06T06:35:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    35,
                    49,
                    0,
                    6,
                    0
                ],
                "published": "2024-02-26T08:47:35Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    8,
                    47,
                    35,
                    0,
                    57,
                    0
                ],
                "title": "AsynEVO: Asynchronous Event-Driven Visual Odometry for Pure Event\n  Streams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsynEVO: Asynchronous Event-Driven Visual Odometry for Pure Event\n  Streams"
                },
                "summary": "Event cameras are bio-inspired vision sensors that asynchronously measure\nper-pixel brightness changes.The high-temporal resolution and asynchronicity of\nevent cameras offer great potential for estimating robot motion states. Recent\nworks have adopted the continuous-time estimation methods to exploit the\ninherent nature of event cameras. However, existing methods either have poor\nruntime performance or neglect the high-temporal resolution of event cameras.\nTo alleviate it, an Asynchronous Event-driven Visual Odometry (AsynEVO) based\non sparse Gaussian Process (GP) regression is proposed to efficiently infer the\nmotion trajectory from pure event streams. Concretely, an asynchronous frontend\npipeline is designed to adapt event-driven feature tracking and manage feature\ntrajectories; a parallel dynamic sliding-window backend is presented within the\nframework of sparse GP regression on $SE(3)$. Notably, a dynamic\nmarginalization strategy is employed to ensure the consistency and sparsity of\nthis GP regression. Experiments conducted on public datasets and real-world\nscenarios demonstrate that AsynEVO achieves competitive precision and superior\nrobustness compared to the state-of-the-art.The experiment in the\nrepeated-texture scenario indicates that the high-temporal resolution of\nAsynEVO plays a vital role in the estimation of high-speed movement.\nFurthermore, we show that the computational efficiency of AsynEVO significantly\noutperforms the incremental method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event cameras are bio-inspired vision sensors that asynchronously measure\nper-pixel brightness changes.The high-temporal resolution and asynchronicity of\nevent cameras offer great potential for estimating robot motion states. Recent\nworks have adopted the continuous-time estimation methods to exploit the\ninherent nature of event cameras. However, existing methods either have poor\nruntime performance or neglect the high-temporal resolution of event cameras.\nTo alleviate it, an Asynchronous Event-driven Visual Odometry (AsynEVO) based\non sparse Gaussian Process (GP) regression is proposed to efficiently infer the\nmotion trajectory from pure event streams. Concretely, an asynchronous frontend\npipeline is designed to adapt event-driven feature tracking and manage feature\ntrajectories; a parallel dynamic sliding-window backend is presented within the\nframework of sparse GP regression on $SE(3)$. Notably, a dynamic\nmarginalization strategy is employed to ensure the consistency and sparsity of\nthis GP regression. Experiments conducted on public datasets and real-world\nscenarios demonstrate that AsynEVO achieves competitive precision and superior\nrobustness compared to the state-of-the-art.The experiment in the\nrepeated-texture scenario indicates that the high-temporal resolution of\nAsynEVO plays a vital role in the estimation of high-speed movement.\nFurthermore, we show that the computational efficiency of AsynEVO significantly\noutperforms the incremental method."
                },
                "authors": [
                    {
                        "name": "Zhixiang Wang"
                    },
                    {
                        "name": "Xudong Li"
                    },
                    {
                        "name": "Yizhai Zhang"
                    },
                    {
                        "name": "Panfeng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Panfeng Huang"
                },
                "author": "Panfeng Huang",
                "arxiv_comment": "Submitted to IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16398v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16398v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11932v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11932v3",
                "updated": "2025-01-06T06:33:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    33,
                    51,
                    0,
                    6,
                    0
                ],
                "published": "2024-04-18T06:20:50Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    6,
                    20,
                    50,
                    3,
                    109,
                    0
                ],
                "title": "CrossIn: An Efficient Instruction Tuning Approach for Cross-Lingual\n  Knowledge Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CrossIn: An Efficient Instruction Tuning Approach for Cross-Lingual\n  Knowledge Alignment"
                },
                "summary": "Multilingual proficiency presents a significant challenge for large language\nmodels (LLMs). English-centric models are usually suboptimal in other\nlanguages, particularly those that are linguistically distant from English.\nThis performance discrepancy mainly stems from the imbalanced distribution of\ntraining data across languages during pre-training and instruction tuning\nstages. To address this problem, we propose a novel approach called CrossIn,\nwhich utilizes a mixed composition of cross-lingual instruction tuning data.\nOur method leverages the compressed representation shared by various languages\nto efficiently enhance the model's task-solving capabilities and multilingual\nproficiency within a single process. In addition, we introduce a multi-task and\nmulti-faceted benchmark to evaluate the effectiveness of CrossIn. Experimental\nresults demonstrate that our method substantially improves performance across\ntasks and languages, and we provide extensive insights into the impact of\ncross-lingual data volume and the integration of translation data on enhancing\nmultilingual consistency and accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual proficiency presents a significant challenge for large language\nmodels (LLMs). English-centric models are usually suboptimal in other\nlanguages, particularly those that are linguistically distant from English.\nThis performance discrepancy mainly stems from the imbalanced distribution of\ntraining data across languages during pre-training and instruction tuning\nstages. To address this problem, we propose a novel approach called CrossIn,\nwhich utilizes a mixed composition of cross-lingual instruction tuning data.\nOur method leverages the compressed representation shared by various languages\nto efficiently enhance the model's task-solving capabilities and multilingual\nproficiency within a single process. In addition, we introduce a multi-task and\nmulti-faceted benchmark to evaluate the effectiveness of CrossIn. Experimental\nresults demonstrate that our method substantially improves performance across\ntasks and languages, and we provide extensive insights into the impact of\ncross-lingual data volume and the integration of translation data on enhancing\nmultilingual consistency and accuracy."
                },
                "authors": [
                    {
                        "name": "Geyu Lin"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Zhengyuan Liu"
                    },
                    {
                        "name": "Nancy F. Chen"
                    }
                ],
                "author_detail": {
                    "name": "Nancy F. Chen"
                },
                "author": "Nancy F. Chen",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11932v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11932v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.09402v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.09402v2",
                "updated": "2025-01-06T06:32:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    32,
                    9,
                    0,
                    6,
                    0
                ],
                "published": "2023-12-14T23:48:51Z",
                "published_parsed": [
                    2023,
                    12,
                    14,
                    23,
                    48,
                    51,
                    3,
                    348,
                    0
                ],
                "title": "CERN for AI: A Theoretical Framework for Autonomous Simulation-Based\n  Artificial Intelligence Testing and Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CERN for AI: A Theoretical Framework for Autonomous Simulation-Based\n  Artificial Intelligence Testing and Alignment"
                },
                "summary": "This paper explores the potential of a multidisciplinary approach to testing\nand aligning artificial intelligence (AI), specifically focusing on large\nlanguage models (LLMs). Due to the rapid development and wide application of\nLLMs, challenges such as ethical alignment, controllability, and predictability\nof these models emerged as global risks. This study investigates an innovative\nsimulation-based multi-agent system within a virtual reality framework that\nreplicates the real-world environment. The framework is populated by automated\n'digital citizens,' simulating complex social structures and interactions to\nexamine and optimize AI. Application of various theories from the fields of\nsociology, social psychology, computer science, physics, biology, and economics\ndemonstrates the possibility of a more human-aligned and socially responsible\nAI. The purpose of such a digital environment is to provide a dynamic platform\nwhere advanced AI agents can interact and make independent decisions, thereby\nmimicking realistic scenarios. The actors in this digital city, operated by the\nLLMs, serve as the primary agents, exhibiting high degrees of autonomy. While\nthis approach shows immense potential, there are notable challenges and\nlimitations, most significantly the unpredictable nature of real-world social\ndynamics. This research endeavors to contribute to the development and\nrefinement of AI, emphasizing the integration of social, ethical, and\ntheoretical dimensions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of a multidisciplinary approach to testing\nand aligning artificial intelligence (AI), specifically focusing on large\nlanguage models (LLMs). Due to the rapid development and wide application of\nLLMs, challenges such as ethical alignment, controllability, and predictability\nof these models emerged as global risks. This study investigates an innovative\nsimulation-based multi-agent system within a virtual reality framework that\nreplicates the real-world environment. The framework is populated by automated\n'digital citizens,' simulating complex social structures and interactions to\nexamine and optimize AI. Application of various theories from the fields of\nsociology, social psychology, computer science, physics, biology, and economics\ndemonstrates the possibility of a more human-aligned and socially responsible\nAI. The purpose of such a digital environment is to provide a dynamic platform\nwhere advanced AI agents can interact and make independent decisions, thereby\nmimicking realistic scenarios. The actors in this digital city, operated by the\nLLMs, serve as the primary agents, exhibiting high degrees of autonomy. While\nthis approach shows immense potential, there are notable challenges and\nlimitations, most significantly the unpredictable nature of real-world social\ndynamics. This research endeavors to contribute to the development and\nrefinement of AI, emphasizing the integration of social, ethical, and\ntheoretical dimensions for future research."
                },
                "authors": [
                    {
                        "name": "Ljubisa Bojic"
                    },
                    {
                        "name": "Matteo Cinelli"
                    },
                    {
                        "name": "Dubravko Culibrk"
                    },
                    {
                        "name": "Boris Delibasic"
                    }
                ],
                "author_detail": {
                    "name": "Boris Delibasic"
                },
                "author": "Boris Delibasic",
                "arxiv_doi": "10.1186/s40309-024-00238-0",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1186/s40309-024-00238-0",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.09402v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.09402v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "32 pages, 4 figures, 2 tables",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02795v1",
                "updated": "2025-01-06T06:29:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    29,
                    55,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T06:29:55Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    29,
                    55,
                    0,
                    6,
                    0
                ],
                "title": "InfiFusion: A Unified Framework for Enhanced Cross-Model Reasoning via\n  LLM Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiFusion: A Unified Framework for Enhanced Cross-Model Reasoning via\n  LLM Fusion"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong performance across\nvarious reasoning tasks, yet building a single model that consistently excels\nacross all domains remains challenging. This paper addresses this problem by\nexploring strategies to integrate multiple domain-specialized models into an\nefficient pivot model.We propose two fusion strategies to combine the strengths\nof multiple LLMs: (1) a pairwise, multi-step fusion approach that sequentially\ndistills each source model into the pivot model, followed by a weight merging\nstep to integrate the distilled models into the final model. This method\nachieves strong performance but requires substantial training effort; and (2) a\nunified fusion approach that aggregates all source models' outputs\nsimultaneously.To improve the fusion process, we introduce a novel\nRate-Skewness Adaptive Fusion (RSAF) technique, which dynamically adjusts top-K\nratios during parameter merging for enhanced flexibility and\nstability.Furthermore, we propose an uncertainty-based weighting method for the\nunified approach, which dynamically balances the contributions of source models\nand outperforms other logits/distribution ensemble methods.We achieved accuracy\nimprovements of 9.27%, 8.80%, and 8.89% on the GSM8K, MATH, and HumanEval\ntasks, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong performance across\nvarious reasoning tasks, yet building a single model that consistently excels\nacross all domains remains challenging. This paper addresses this problem by\nexploring strategies to integrate multiple domain-specialized models into an\nefficient pivot model.We propose two fusion strategies to combine the strengths\nof multiple LLMs: (1) a pairwise, multi-step fusion approach that sequentially\ndistills each source model into the pivot model, followed by a weight merging\nstep to integrate the distilled models into the final model. This method\nachieves strong performance but requires substantial training effort; and (2) a\nunified fusion approach that aggregates all source models' outputs\nsimultaneously.To improve the fusion process, we introduce a novel\nRate-Skewness Adaptive Fusion (RSAF) technique, which dynamically adjusts top-K\nratios during parameter merging for enhanced flexibility and\nstability.Furthermore, we propose an uncertainty-based weighting method for the\nunified approach, which dynamically balances the contributions of source models\nand outperforms other logits/distribution ensemble methods.We achieved accuracy\nimprovements of 9.27%, 8.80%, and 8.89% on the GSM8K, MATH, and HumanEval\ntasks, respectively."
                },
                "authors": [
                    {
                        "name": "Zhaoyi Yan"
                    },
                    {
                        "name": "Zhijie Sang"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Yuhao Fu"
                    },
                    {
                        "name": "Baoyi He"
                    },
                    {
                        "name": "Qi Zhou"
                    },
                    {
                        "name": "Yining Di"
                    },
                    {
                        "name": "Chunlin Ji"
                    },
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Fei Wu"
                    },
                    {
                        "name": "Hongxia Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hongxia Yang"
                },
                "author": "Hongxia Yang",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03658v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03658v2",
                "updated": "2025-01-06T06:01:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    1,
                    17,
                    0,
                    6,
                    0
                ],
                "published": "2024-02-06T03:14:46Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    3,
                    14,
                    46,
                    1,
                    37,
                    0
                ],
                "title": "Sentiment-enhanced Graph-based Sarcasm Explanation in Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentiment-enhanced Graph-based Sarcasm Explanation in Dialogue"
                },
                "summary": "Sarcasm Explanation in Dialogue (SED) is a new yet challenging task, which\naims to generate a natural language explanation for the given sarcastic\ndialogue that involves multiple modalities (\\ie utterance, video, and audio).\nAlthough existing studies have achieved great success based on the generative\npretrained language model BART, they overlook exploiting the sentiments\nresiding in the utterance, video and audio, which play important roles in\nreflecting sarcasm that essentially involves subtle sentiment contrasts.\nNevertheless, it is non-trivial to incorporate sentiments for boosting SED\nperformance, due to three main challenges: 1) diverse effects of utterance\ntokens on sentiments; 2) gap between video-audio sentiment signals and the\nembedding space of BART; and 3) various relations among utterances, utterance\nsentiments, and video-audio sentiments. To tackle these challenges, we propose\na novel sEntiment-enhanceD Graph-based multimodal sarcasm Explanation\nframework, named EDGE. In particular, we first propose a lexicon-guided\nutterance sentiment inference module, where a heuristic utterance sentiment\nrefinement strategy is devised. We then develop a module named Joint Cross\nAttention-based Sentiment Inference (JCA-SI) by extending the multimodal\nsentiment analysis model JCA to derive the joint sentiment label for each\nvideo-audio clip. Thereafter, we devise a context-sentiment graph to\ncomprehensively model the semantic relations among the utterances, utterance\nsentiments, and video-audio sentiments, to facilitate sarcasm explanation\ngeneration. Extensive experiments on the publicly released dataset WITS verify\nthe superiority of our model over cutting-edge methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sarcasm Explanation in Dialogue (SED) is a new yet challenging task, which\naims to generate a natural language explanation for the given sarcastic\ndialogue that involves multiple modalities (\\ie utterance, video, and audio).\nAlthough existing studies have achieved great success based on the generative\npretrained language model BART, they overlook exploiting the sentiments\nresiding in the utterance, video and audio, which play important roles in\nreflecting sarcasm that essentially involves subtle sentiment contrasts.\nNevertheless, it is non-trivial to incorporate sentiments for boosting SED\nperformance, due to three main challenges: 1) diverse effects of utterance\ntokens on sentiments; 2) gap between video-audio sentiment signals and the\nembedding space of BART; and 3) various relations among utterances, utterance\nsentiments, and video-audio sentiments. To tackle these challenges, we propose\na novel sEntiment-enhanceD Graph-based multimodal sarcasm Explanation\nframework, named EDGE. In particular, we first propose a lexicon-guided\nutterance sentiment inference module, where a heuristic utterance sentiment\nrefinement strategy is devised. We then develop a module named Joint Cross\nAttention-based Sentiment Inference (JCA-SI) by extending the multimodal\nsentiment analysis model JCA to derive the joint sentiment label for each\nvideo-audio clip. Thereafter, we devise a context-sentiment graph to\ncomprehensively model the semantic relations among the utterances, utterance\nsentiments, and video-audio sentiments, to facilitate sarcasm explanation\ngeneration. Extensive experiments on the publicly released dataset WITS verify\nthe superiority of our model over cutting-edge methods."
                },
                "authors": [
                    {
                        "name": "Kun Ouyang"
                    },
                    {
                        "name": "Liqiang Jing"
                    },
                    {
                        "name": "Xuemeng Song"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Yupeng Hu"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "This paper got accepted by IEEE TMM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03658v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03658v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02784v1",
                "updated": "2025-01-06T06:00:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    0,
                    38,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T06:00:38Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    0,
                    38,
                    0,
                    6,
                    0
                ],
                "title": "Overcoming Quantum Metrology Singularity through Sequential Measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overcoming Quantum Metrology Singularity through Sequential Measurements"
                },
                "summary": "The simultaneous estimation of multiple unknown parameters is the most\ngeneral scenario in quantum sensing. Quantum multi-parameter estimation theory\nprovides fundamental bounds on the achievable precision of simultaneous\nestimation. However, these bounds can become singular (no finite bound exists)\nin multi-parameter sensing due to parameter interdependencies, limited probe\naccessibility, and insufficient measurement outcomes. Here, we address the\nsingularity issue in quantum sensing through a simple mechanism based on a\nsequential measurement strategy. This sensing scheme overcomes the singularity\nconstraint and enables the simultaneous estimation of multiple parameters with\na local and fixed measurement throughout the sensing protocol. This is because\nsequential measurements, involving consecutive steps of local measurements\nfollowed by probe evolution, inherently produce correlated measurement data\nthat grows exponentially with the number of sequential measurements. Finally,\nthrough two different examples, namely a strongly correlated probe and a\nlight-matter system, we demonstrate how such singularities are reflected when\ninferring the unknown parameters through Bayesian estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The simultaneous estimation of multiple unknown parameters is the most\ngeneral scenario in quantum sensing. Quantum multi-parameter estimation theory\nprovides fundamental bounds on the achievable precision of simultaneous\nestimation. However, these bounds can become singular (no finite bound exists)\nin multi-parameter sensing due to parameter interdependencies, limited probe\naccessibility, and insufficient measurement outcomes. Here, we address the\nsingularity issue in quantum sensing through a simple mechanism based on a\nsequential measurement strategy. This sensing scheme overcomes the singularity\nconstraint and enables the simultaneous estimation of multiple parameters with\na local and fixed measurement throughout the sensing protocol. This is because\nsequential measurements, involving consecutive steps of local measurements\nfollowed by probe evolution, inherently produce correlated measurement data\nthat grows exponentially with the number of sequential measurements. Finally,\nthrough two different examples, namely a strongly correlated probe and a\nlight-matter system, we demonstrate how such singularities are reflected when\ninferring the unknown parameters through Bayesian estimation."
                },
                "authors": [
                    {
                        "name": "Yaoling Yang"
                    },
                    {
                        "name": "Victor Montenegro"
                    },
                    {
                        "name": "Abolfazl Bayat"
                    }
                ],
                "author_detail": {
                    "name": "Abolfazl Bayat"
                },
                "author": "Abolfazl Bayat",
                "arxiv_comment": "5 pages main text, 4 figures. Feedback is welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16594v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16594v5",
                "updated": "2025-01-06T05:53:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    5,
                    53,
                    18,
                    0,
                    6,
                    0
                ],
                "published": "2024-11-25T17:28:44Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    17,
                    28,
                    44,
                    0,
                    330,
                    0
                ],
                "title": "From Generation to Judgment: Opportunities and Challenges of\n  LLM-as-a-judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Generation to Judgment: Opportunities and Challenges of\n  LLM-as-a-judge"
                },
                "summary": "Assessment and evaluation have long been critical challenges in artificial\nintelligence (AI) and natural language processing (NLP). However, traditional\nmethods, whether matching-based or embedding-based, often fall short of judging\nsubtle attributes and delivering satisfactory results. Recent advancements in\nLarge Language Models (LLMs) inspire the \"LLM-as-a-judge\" paradigm, where LLMs\nare leveraged to perform scoring, ranking, or selection across various tasks\nand applications. This paper provides a comprehensive survey of LLM-based\njudgment and assessment, offering an in-depth overview to advance this emerging\nfield. We begin by giving detailed definitions from both input and output\nperspectives. Then we introduce a comprehensive taxonomy to explore\nLLM-as-a-judge from three dimensions: what to judge, how to judge and where to\njudge. Finally, we compile benchmarks for evaluating LLM-as-a-judge and\nhighlight key challenges and promising directions, aiming to provide valuable\ninsights and inspire future research in this promising research area. Paper\nlist and more resources about LLM-as-a-judge can be found at\n\\url{https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge} and\n\\url{https://llm-as-a-judge.github.io}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessment and evaluation have long been critical challenges in artificial\nintelligence (AI) and natural language processing (NLP). However, traditional\nmethods, whether matching-based or embedding-based, often fall short of judging\nsubtle attributes and delivering satisfactory results. Recent advancements in\nLarge Language Models (LLMs) inspire the \"LLM-as-a-judge\" paradigm, where LLMs\nare leveraged to perform scoring, ranking, or selection across various tasks\nand applications. This paper provides a comprehensive survey of LLM-based\njudgment and assessment, offering an in-depth overview to advance this emerging\nfield. We begin by giving detailed definitions from both input and output\nperspectives. Then we introduce a comprehensive taxonomy to explore\nLLM-as-a-judge from three dimensions: what to judge, how to judge and where to\njudge. Finally, we compile benchmarks for evaluating LLM-as-a-judge and\nhighlight key challenges and promising directions, aiming to provide valuable\ninsights and inspire future research in this promising research area. Paper\nlist and more resources about LLM-as-a-judge can be found at\n\\url{https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge} and\n\\url{https://llm-as-a-judge.github.io}."
                },
                "authors": [
                    {
                        "name": "Dawei Li"
                    },
                    {
                        "name": "Bohan Jiang"
                    },
                    {
                        "name": "Liangjie Huang"
                    },
                    {
                        "name": "Alimohammad Beigi"
                    },
                    {
                        "name": "Chengshuai Zhao"
                    },
                    {
                        "name": "Zhen Tan"
                    },
                    {
                        "name": "Amrita Bhattacharjee"
                    },
                    {
                        "name": "Yuxuan Jiang"
                    },
                    {
                        "name": "Canyu Chen"
                    },
                    {
                        "name": "Tianhao Wu"
                    },
                    {
                        "name": "Kai Shu"
                    },
                    {
                        "name": "Lu Cheng"
                    },
                    {
                        "name": "Huan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huan Liu"
                },
                "author": "Huan Liu",
                "arxiv_comment": "v5: add new citations; 36 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16594v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16594v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20024v2",
                "updated": "2025-01-06T04:34:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    4,
                    34,
                    16,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-28T05:01:26Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    5,
                    1,
                    26,
                    5,
                    363,
                    0
                ],
                "title": "BaiJia: A Large-Scale Role-Playing Agent Corpus of Chinese Historical\n  Characters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BaiJia: A Large-Scale Role-Playing Agent Corpus of Chinese Historical\n  Characters"
                },
                "summary": "We introduce a comprehensive large-scale role-playing agent corpus, termed\nBaiJia, that comprises various Chinese historical characters. This corpus is\nnoteworthy for being the pioneering compilation of low-resource data that can\nbe utilized in large language models (LLMs) to engage in AI-driven historical\nrole-playing agents. BaiJia addresses the challenges in terms of fragmented\nhistorical textual records in different forms and modalities, integrating\nvarious characters' information, including their biographical, literary, family\nrelations, historical events, and so on. We conduct extensive experiments to\ndemonstrate the effectiveness of our BaiJia agent corpus in bolstering the\nrole-playing abilities of various foundational LLMs, and promoting the\ndevelopment and assessment of LLMs in the context of historical role-playing\ntasks. The agent corpus is available at baijia.online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a comprehensive large-scale role-playing agent corpus, termed\nBaiJia, that comprises various Chinese historical characters. This corpus is\nnoteworthy for being the pioneering compilation of low-resource data that can\nbe utilized in large language models (LLMs) to engage in AI-driven historical\nrole-playing agents. BaiJia addresses the challenges in terms of fragmented\nhistorical textual records in different forms and modalities, integrating\nvarious characters' information, including their biographical, literary, family\nrelations, historical events, and so on. We conduct extensive experiments to\ndemonstrate the effectiveness of our BaiJia agent corpus in bolstering the\nrole-playing abilities of various foundational LLMs, and promoting the\ndevelopment and assessment of LLMs in the context of historical role-playing\ntasks. The agent corpus is available at baijia.online."
                },
                "authors": [
                    {
                        "name": "Ting Bai"
                    },
                    {
                        "name": "Jiazheng Kang"
                    },
                    {
                        "name": "Jiayang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Jiayang Fan"
                },
                "author": "Jiayang Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02751v1",
                "updated": "2025-01-06T03:58:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    3,
                    58,
                    31,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T03:58:31Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    3,
                    58,
                    31,
                    0,
                    6,
                    0
                ],
                "title": "Ultrasound-QBench: Can LLMs Aid in Quality Assessment of Ultrasound\n  Imaging?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultrasound-QBench: Can LLMs Aid in Quality Assessment of Ultrasound\n  Imaging?"
                },
                "summary": "With the dramatic upsurge in the volume of ultrasound examinations,\nlow-quality ultrasound imaging has gradually increased due to variations in\noperator proficiency and imaging circumstances, imposing a severe burden on\ndiagnosis accuracy and even entailing the risk of restarting the diagnosis in\ncritical cases. To assist clinicians in selecting high-quality ultrasound\nimages and ensuring accurate diagnoses, we introduce Ultrasound-QBench, a\ncomprehensive benchmark that systematically evaluates multimodal large language\nmodels (MLLMs) on quality assessment tasks of ultrasound images.\nUltrasound-QBench establishes two datasets collected from diverse sources:\nIVUSQA, consisting of 7,709 images, and CardiacUltraQA, containing 3,863\nimages. These images encompassing common ultrasound imaging artifacts are\nannotated by professional ultrasound experts and classified into three quality\nlevels: high, medium, and low. To better evaluate MLLMs, we decompose the\nquality assessment task into three dimensionalities: qualitative\nclassification, quantitative scoring, and comparative assessment. The\nevaluation of 7 open-source MLLMs as well as 1 proprietary MLLMs demonstrates\nthat MLLMs possess preliminary capabilities for low-level visual tasks in\nultrasound image quality classification. We hope this benchmark will inspire\nthe research community to delve deeper into uncovering and enhancing the\nuntapped potential of MLLMs for medical imaging tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the dramatic upsurge in the volume of ultrasound examinations,\nlow-quality ultrasound imaging has gradually increased due to variations in\noperator proficiency and imaging circumstances, imposing a severe burden on\ndiagnosis accuracy and even entailing the risk of restarting the diagnosis in\ncritical cases. To assist clinicians in selecting high-quality ultrasound\nimages and ensuring accurate diagnoses, we introduce Ultrasound-QBench, a\ncomprehensive benchmark that systematically evaluates multimodal large language\nmodels (MLLMs) on quality assessment tasks of ultrasound images.\nUltrasound-QBench establishes two datasets collected from diverse sources:\nIVUSQA, consisting of 7,709 images, and CardiacUltraQA, containing 3,863\nimages. These images encompassing common ultrasound imaging artifacts are\nannotated by professional ultrasound experts and classified into three quality\nlevels: high, medium, and low. To better evaluate MLLMs, we decompose the\nquality assessment task into three dimensionalities: qualitative\nclassification, quantitative scoring, and comparative assessment. The\nevaluation of 7 open-source MLLMs as well as 1 proprietary MLLMs demonstrates\nthat MLLMs possess preliminary capabilities for low-level visual tasks in\nultrasound image quality classification. We hope this benchmark will inspire\nthe research community to delve deeper into uncovering and enhancing the\nuntapped potential of MLLMs for medical imaging tasks."
                },
                "authors": [
                    {
                        "name": "Hongyi Miao"
                    },
                    {
                        "name": "Jun Jia"
                    },
                    {
                        "name": "Yankun Cao"
                    },
                    {
                        "name": "Yingjie Zhou"
                    },
                    {
                        "name": "Yanwei Jiang"
                    },
                    {
                        "name": "Zhi Liu"
                    },
                    {
                        "name": "Guangtao Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Guangtao Zhai"
                },
                "author": "Guangtao Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12470v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12470v3",
                "updated": "2025-01-06T03:48:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    3,
                    48,
                    27,
                    0,
                    6,
                    0
                ],
                "published": "2024-08-22T15:10:56Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    10,
                    56,
                    3,
                    235,
                    0
                ],
                "title": "DLCRec: A Novel Approach for Managing Diversity in LLM-Based Recommender\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DLCRec: A Novel Approach for Managing Diversity in LLM-Based Recommender\n  Systems"
                },
                "summary": "The integration of Large Language Models (LLMs) into recommender systems has\nled to substantial performance improvements. However, this often comes at the\ncost of diminished recommendation diversity, which can negatively impact user\nsatisfaction. To address this issue, controllable recommendation has emerged as\na promising approach, allowing users to specify their preferences and receive\nrecommendations that meet their diverse needs. Despite its potential, existing\ncontrollable recommender systems frequently rely on simplistic mechanisms, such\nas a single prompt, to regulate diversity-an approach that falls short of\ncapturing the full complexity of user preferences. In response to these\nlimitations, we propose DLCRec, a novel framework designed to enable\nfine-grained control over diversity in LLM-based recommendations. Unlike\ntraditional methods, DLCRec adopts a fine-grained task decomposition strategy,\nbreaking down the recommendation process into three sequential sub-tasks: genre\nprediction, genre filling, and item prediction. These sub-tasks are trained\nindependently and inferred sequentially according to user-defined control\nnumbers, ensuring more precise control over diversity. Furthermore, the\nscarcity and uneven distribution of diversity-related user behavior data pose\nsignificant challenges for fine-tuning. To overcome these obstacles, we\nintroduce two data augmentation techniques that enhance the model's robustness\nto noisy and out-of-distribution data. These techniques expose the model to a\nbroader range of patterns, improving its adaptability in generating\nrecommendations with varying levels of diversity. Our extensive empirical\nevaluation demonstrates that DLCRec not only provides precise control over\ndiversity but also outperforms state-of-the-art baselines across multiple\nrecommendation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into recommender systems has\nled to substantial performance improvements. However, this often comes at the\ncost of diminished recommendation diversity, which can negatively impact user\nsatisfaction. To address this issue, controllable recommendation has emerged as\na promising approach, allowing users to specify their preferences and receive\nrecommendations that meet their diverse needs. Despite its potential, existing\ncontrollable recommender systems frequently rely on simplistic mechanisms, such\nas a single prompt, to regulate diversity-an approach that falls short of\ncapturing the full complexity of user preferences. In response to these\nlimitations, we propose DLCRec, a novel framework designed to enable\nfine-grained control over diversity in LLM-based recommendations. Unlike\ntraditional methods, DLCRec adopts a fine-grained task decomposition strategy,\nbreaking down the recommendation process into three sequential sub-tasks: genre\nprediction, genre filling, and item prediction. These sub-tasks are trained\nindependently and inferred sequentially according to user-defined control\nnumbers, ensuring more precise control over diversity. Furthermore, the\nscarcity and uneven distribution of diversity-related user behavior data pose\nsignificant challenges for fine-tuning. To overcome these obstacles, we\nintroduce two data augmentation techniques that enhance the model's robustness\nto noisy and out-of-distribution data. These techniques expose the model to a\nbroader range of patterns, improving its adaptability in generating\nrecommendations with varying levels of diversity. Our extensive empirical\nevaluation demonstrates that DLCRec not only provides precise control over\ndiversity but also outperforms state-of-the-art baselines across multiple\nrecommendation scenarios."
                },
                "authors": [
                    {
                        "name": "Jiaju Chen"
                    },
                    {
                        "name": "Chongming Gao"
                    },
                    {
                        "name": "Shuai Yuan"
                    },
                    {
                        "name": "Shuchang Liu"
                    },
                    {
                        "name": "Qingpeng Cai"
                    },
                    {
                        "name": "Peng Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Jiang"
                },
                "author": "Peng Jiang",
                "arxiv_comment": "Accepted by WSDM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12470v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12470v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14743v2",
                "updated": "2025-01-06T03:18:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    3,
                    18,
                    15,
                    0,
                    6,
                    0
                ],
                "published": "2024-09-23T06:41:52Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    6,
                    41,
                    52,
                    0,
                    267,
                    0
                ],
                "title": "LlamaPartialSpoof: An LLM-Driven Fake Speech Dataset Simulating\n  Disinformation Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LlamaPartialSpoof: An LLM-Driven Fake Speech Dataset Simulating\n  Disinformation Generation"
                },
                "summary": "Previous fake speech datasets were constructed from a defender's perspective\nto develop countermeasure (CM) systems without considering diverse motivations\nof attackers. To better align with real-life scenarios, we created\nLlamaPartialSpoof, a 130-hour dataset that contains both fully and partially\nfake speech, using a large language model (LLM) and voice cloning technologies\nto evaluate the robustness of CMs. By examining valuable information for both\nattackers and defenders, we identify several key vulnerabilities in current CM\nsystems, which can be exploited to enhance attack success rates, including\nbiases toward certain text-to-speech models or concatenation methods. Our\nexperimental results indicate that the current fake speech detection system\nstruggle to generalize to unseen scenarios, achieving a best performance of\n24.49% equal error rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous fake speech datasets were constructed from a defender's perspective\nto develop countermeasure (CM) systems without considering diverse motivations\nof attackers. To better align with real-life scenarios, we created\nLlamaPartialSpoof, a 130-hour dataset that contains both fully and partially\nfake speech, using a large language model (LLM) and voice cloning technologies\nto evaluate the robustness of CMs. By examining valuable information for both\nattackers and defenders, we identify several key vulnerabilities in current CM\nsystems, which can be exploited to enhance attack success rates, including\nbiases toward certain text-to-speech models or concatenation methods. Our\nexperimental results indicate that the current fake speech detection system\nstruggle to generalize to unseen scenarios, achieving a best performance of\n24.49% equal error rate."
                },
                "authors": [
                    {
                        "name": "Hieu-Thi Luong"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Kong Aik Lee"
                    },
                    {
                        "name": "Eng Siong Chng"
                    }
                ],
                "author_detail": {
                    "name": "Eng Siong Chng"
                },
                "author": "Eng Siong Chng",
                "arxiv_comment": "5 pages, ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02739v1",
                "updated": "2025-01-06T03:17:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    3,
                    17,
                    35,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T03:17:35Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    3,
                    17,
                    35,
                    0,
                    6,
                    0
                ],
                "title": "TARDiS : Text Augmentation for Refining Diversity and Separability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TARDiS : Text Augmentation for Refining Diversity and Separability"
                },
                "summary": "Text augmentation (TA) is a critical technique for text classification,\nespecially in few-shot settings. This paper introduces a novel LLM-based TA\nmethod, TARDiS, to address challenges inherent in the generation and alignment\nstages of two-stage TA methods. For the generation stage, we propose two\ngeneration processes, SEG and CEG, incorporating multiple class-specific\nprompts to enhance diversity and separability. For the alignment stage, we\nintroduce a class adaptation (CA) method to ensure that generated examples\nalign with their target classes through verification and modification.\nExperimental results demonstrate TARDiS's effectiveness, outperforming\nstate-of-the-art LLM-based TA methods in various few-shot text classification\ntasks. An in-depth analysis confirms the detailed behaviors at each stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text augmentation (TA) is a critical technique for text classification,\nespecially in few-shot settings. This paper introduces a novel LLM-based TA\nmethod, TARDiS, to address challenges inherent in the generation and alignment\nstages of two-stage TA methods. For the generation stage, we propose two\ngeneration processes, SEG and CEG, incorporating multiple class-specific\nprompts to enhance diversity and separability. For the alignment stage, we\nintroduce a class adaptation (CA) method to ensure that generated examples\nalign with their target classes through verification and modification.\nExperimental results demonstrate TARDiS's effectiveness, outperforming\nstate-of-the-art LLM-based TA methods in various few-shot text classification\ntasks. An in-depth analysis confirms the detailed behaviors at each stage."
                },
                "authors": [
                    {
                        "name": "Kyungmin Kim"
                    },
                    {
                        "name": "SangHun Im"
                    },
                    {
                        "name": "GiBaeg Kim"
                    },
                    {
                        "name": "Heung-Seon Oh"
                    }
                ],
                "author_detail": {
                    "name": "Heung-Seon Oh"
                },
                "author": "Heung-Seon Oh",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02725v1",
                "updated": "2025-01-06T02:46:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    2,
                    46,
                    33,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T02:46:33Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    2,
                    46,
                    33,
                    0,
                    6,
                    0
                ],
                "title": "Artificial Intelligence in Creative Industries: Advances Prior to 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence in Creative Industries: Advances Prior to 2025"
                },
                "summary": "The rapid advancements in artificial intelligence (AI), particularly in\ngenerative AI and large language models (LLMs), have profoundly impacted the\ncreative industries by enabling innovative content creation, enhancing\nworkflows, and democratizing access to creative tools. This paper explores the\nsignificant technological shifts since our previous review in 2022,\nhighlighting how these developments have expanded creative opportunities and\nefficiency. These technological advancements have enhanced the capabilities of\ntext-to-image, text-to-video, and multimodal generation technologies. In\nparticular, key breakthroughs in LLMs have established new benchmarks in\nconversational AI, while advancements in image generators have revolutionized\ncontent creation. We also discuss AI integration into post-production\nworkflows, which has significantly accelerated and refined traditional\nprocesses. Despite these innovations, challenges remain, particularly for the\nmedia industry, due to the demands on communication traffic from creative\ncontent. We therefore include data compression and quality assessment in this\npaper. Furthermore, we highlight the trend toward unified AI frameworks capable\nof addressing multiple creative tasks and underscore the importance of human\noversight to mitigate AI-generated inaccuracies. Finally, we explore AI's\nfuture potential in the creative sector, stressing the need to navigate\nemerging challenges to maximize its benefits while addressing associated risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in artificial intelligence (AI), particularly in\ngenerative AI and large language models (LLMs), have profoundly impacted the\ncreative industries by enabling innovative content creation, enhancing\nworkflows, and democratizing access to creative tools. This paper explores the\nsignificant technological shifts since our previous review in 2022,\nhighlighting how these developments have expanded creative opportunities and\nefficiency. These technological advancements have enhanced the capabilities of\ntext-to-image, text-to-video, and multimodal generation technologies. In\nparticular, key breakthroughs in LLMs have established new benchmarks in\nconversational AI, while advancements in image generators have revolutionized\ncontent creation. We also discuss AI integration into post-production\nworkflows, which has significantly accelerated and refined traditional\nprocesses. Despite these innovations, challenges remain, particularly for the\nmedia industry, due to the demands on communication traffic from creative\ncontent. We therefore include data compression and quality assessment in this\npaper. Furthermore, we highlight the trend toward unified AI frameworks capable\nof addressing multiple creative tasks and underscore the importance of human\noversight to mitigate AI-generated inaccuracies. Finally, we explore AI's\nfuture potential in the creative sector, stressing the need to navigate\nemerging challenges to maximize its benefits while addressing associated risks."
                },
                "authors": [
                    {
                        "name": "Nantheera Anantrasirichai"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "David Bull"
                    }
                ],
                "author_detail": {
                    "name": "David Bull"
                },
                "author": "David Bull",
                "arxiv_comment": "This is an updated review of our previous paper (see\n  https://doi.org/10.1007/s10462-021-10039-7)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09824v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09824v6",
                "updated": "2025-01-06T02:16:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    2,
                    16,
                    37,
                    0,
                    6,
                    0
                ],
                "published": "2024-10-13T12:57:08Z",
                "published_parsed": [
                    2024,
                    10,
                    13,
                    12,
                    57,
                    8,
                    6,
                    287,
                    0
                ],
                "title": "LLM-Based Multi-Agent Systems are Scalable Graph Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Multi-Agent Systems are Scalable Graph Generative Models"
                },
                "summary": "The structural properties of naturally arising social graphs are extensively\nstudied to understand their evolution. Prior approaches for modeling network\ndynamics typically rely on rule-based models, which lack realism and\ngeneralizability, or deep learning-based models, which require large-scale\ntraining datasets. Social graphs, as abstract graph representations of\nentity-wise interactions, present an opportunity to explore network evolution\nmechanisms through realistic simulations of human-item interactions. Leveraging\nthe pre-trained social consensus knowledge embedded in large language models\n(LLMs), we present GraphAgent-Generator (GAG), a novel simulation-based\nframework for dynamic, text-attributed social graph generation. GAG simulates\nthe temporal node and edge generation processes for zero-shot social graph\ngeneration. The resulting graphs exhibit adherence to seven key macroscopic\nnetwork properties, achieving an 11% improvement in microscopic graph structure\nmetrics. Through the node classification benchmarking task, we validate GAG\neffectively captures the intricate text-structure correlations in graph\ngeneration. Furthermore, GAG supports generating graphs with up to nearly\n100,000 nodes or 10 million edges through large-scale LLM-based agent\nsimulation with parallel acceleration, achieving a minimum speed-up of 90.4%.\nThe source code is available at https://github.com/Ji-Cather/GraphAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The structural properties of naturally arising social graphs are extensively\nstudied to understand their evolution. Prior approaches for modeling network\ndynamics typically rely on rule-based models, which lack realism and\ngeneralizability, or deep learning-based models, which require large-scale\ntraining datasets. Social graphs, as abstract graph representations of\nentity-wise interactions, present an opportunity to explore network evolution\nmechanisms through realistic simulations of human-item interactions. Leveraging\nthe pre-trained social consensus knowledge embedded in large language models\n(LLMs), we present GraphAgent-Generator (GAG), a novel simulation-based\nframework for dynamic, text-attributed social graph generation. GAG simulates\nthe temporal node and edge generation processes for zero-shot social graph\ngeneration. The resulting graphs exhibit adherence to seven key macroscopic\nnetwork properties, achieving an 11% improvement in microscopic graph structure\nmetrics. Through the node classification benchmarking task, we validate GAG\neffectively captures the intricate text-structure correlations in graph\ngeneration. Furthermore, GAG supports generating graphs with up to nearly\n100,000 nodes or 10 million edges through large-scale LLM-based agent\nsimulation with parallel acceleration, achieving a minimum speed-up of 90.4%.\nThe source code is available at https://github.com/Ji-Cather/GraphAgent."
                },
                "authors": [
                    {
                        "name": "Jiarui Ji"
                    },
                    {
                        "name": "Runlin Lei"
                    },
                    {
                        "name": "Jialing Bi"
                    },
                    {
                        "name": "Zhewei Wei"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Xuchen Pan"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    }
                ],
                "author_detail": {
                    "name": "Bolin Ding"
                },
                "author": "Bolin Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09824v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09824v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02642v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02642v3",
                "updated": "2025-01-06T01:52:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    52,
                    41,
                    0,
                    6,
                    0
                ],
                "published": "2024-06-04T10:59:43Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    10,
                    59,
                    43,
                    1,
                    156,
                    0
                ],
                "title": "E-ICL: Enhancing Fine-Grained Emotion Recognition through the Lens of\n  Prototype Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-ICL: Enhancing Fine-Grained Emotion Recognition through the Lens of\n  Prototype Theory"
                },
                "summary": "In-context learning (ICL) achieves remarkable performance in various domains\nsuch as knowledge acquisition, commonsense reasoning, and semantic\nunderstanding. However, its performance significantly deteriorates for emotion\ndetection tasks, especially fine-grained emotion recognition. The underlying\nreasons for this remain unclear. In this paper, we identify the reasons behind\nICL's poor performance from the perspective of prototype theory and propose a\nmethod to address this issue. Specifically, we conduct extensive pilot\nexperiments and find that ICL conforms to the prototype theory on fine-grained\nemotion recognition. Based on this theory, we uncover the following\ndeficiencies in ICL: (1) It relies on prototypes (example-label pairs) that are\nsemantically similar but emotionally inaccurate to predict emotions. (2) It is\nprone to interference from irrelevant categories, affecting the accuracy and\nrobustness of the predictions. To address these issues, we propose an Emotion\nContext Learning method (E-ICL) on fine-grained emotion recognition. E-ICL\nrelies on more emotionally accurate prototypes to predict categories by\nreferring to emotionally similar examples with dynamic labels. Simultaneously,\nE-ICL employs an exclusionary emotion prediction strategy to avoid interference\nfrom irrelevant categories, thereby increasing its accuracy and robustness.\nNote that the entire process is accomplished with the assistance of a\nplug-and-play emotion auxiliary model, without additional training. Experiments\non the fine-grained emotion datasets EDOS, Empathetic-Dialogues,\nEmpatheticIntent, and GoEmotions show that E-ICL achieves superior emotion\nprediction performance. Furthermore, even when the emotion auxiliary model used\nis lower than 10% of the LLMs, E-ICL can still boost the performance of LLMs by\nover 4% on multiple datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) achieves remarkable performance in various domains\nsuch as knowledge acquisition, commonsense reasoning, and semantic\nunderstanding. However, its performance significantly deteriorates for emotion\ndetection tasks, especially fine-grained emotion recognition. The underlying\nreasons for this remain unclear. In this paper, we identify the reasons behind\nICL's poor performance from the perspective of prototype theory and propose a\nmethod to address this issue. Specifically, we conduct extensive pilot\nexperiments and find that ICL conforms to the prototype theory on fine-grained\nemotion recognition. Based on this theory, we uncover the following\ndeficiencies in ICL: (1) It relies on prototypes (example-label pairs) that are\nsemantically similar but emotionally inaccurate to predict emotions. (2) It is\nprone to interference from irrelevant categories, affecting the accuracy and\nrobustness of the predictions. To address these issues, we propose an Emotion\nContext Learning method (E-ICL) on fine-grained emotion recognition. E-ICL\nrelies on more emotionally accurate prototypes to predict categories by\nreferring to emotionally similar examples with dynamic labels. Simultaneously,\nE-ICL employs an exclusionary emotion prediction strategy to avoid interference\nfrom irrelevant categories, thereby increasing its accuracy and robustness.\nNote that the entire process is accomplished with the assistance of a\nplug-and-play emotion auxiliary model, without additional training. Experiments\non the fine-grained emotion datasets EDOS, Empathetic-Dialogues,\nEmpatheticIntent, and GoEmotions show that E-ICL achieves superior emotion\nprediction performance. Furthermore, even when the emotion auxiliary model used\nis lower than 10% of the LLMs, E-ICL can still boost the performance of LLMs by\nover 4% on multiple datasets."
                },
                "authors": [
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Zhou Yang"
                    },
                    {
                        "name": "Chenglong Ye"
                    },
                    {
                        "name": "Yufeng Wang"
                    },
                    {
                        "name": "Haizhou Sun"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Xiaofei Zhu"
                    },
                    {
                        "name": "Yunbing Wu"
                    },
                    {
                        "name": "Xiangwen Liao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangwen Liao"
                },
                "author": "Xiangwen Liao",
                "arxiv_comment": "16 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02642v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02642v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02711v1",
                "updated": "2025-01-06T01:52:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    52,
                    15,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T01:52:15Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    52,
                    15,
                    0,
                    6,
                    0
                ],
                "title": "KG-CF: Knowledge Graph Completion with Context Filtering under the\n  Guidance of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KG-CF: Knowledge Graph Completion with Context Filtering under the\n  Guidance of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have shown impressive performance in various\ntasks, including knowledge graph completion (KGC). However, current studies\nmostly apply LLMs to classification tasks, like identifying missing triplets,\nrather than ranking-based tasks, where the model ranks candidate entities based\non plausibility. This focus limits the practical use of LLMs in KGC, as\nreal-world applications prioritize highly plausible triplets. Additionally,\nwhile graph paths can help infer the existence of missing triplets and improve\ncompletion accuracy, they often contain redundant information. To address these\nissues, we propose KG-CF, a framework tailored for ranking-based KGC tasks.\nKG-CF leverages LLMs' reasoning abilities to filter out irrelevant contexts,\nachieving superior results on real-world datasets. The code and datasets are\navailable at \\url{https://anonymous.4open.science/r/KG-CF}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive performance in various\ntasks, including knowledge graph completion (KGC). However, current studies\nmostly apply LLMs to classification tasks, like identifying missing triplets,\nrather than ranking-based tasks, where the model ranks candidate entities based\non plausibility. This focus limits the practical use of LLMs in KGC, as\nreal-world applications prioritize highly plausible triplets. Additionally,\nwhile graph paths can help infer the existence of missing triplets and improve\ncompletion accuracy, they often contain redundant information. To address these\nissues, we propose KG-CF, a framework tailored for ranking-based KGC tasks.\nKG-CF leverages LLMs' reasoning abilities to filter out irrelevant contexts,\nachieving superior results on real-world datasets. The code and datasets are\navailable at \\url{https://anonymous.4open.science/r/KG-CF}."
                },
                "authors": [
                    {
                        "name": "Zaiyi Zheng"
                    },
                    {
                        "name": "Yushun Dong"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Haochen Liu"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Jundong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jundong Li"
                },
                "author": "Jundong Li",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00999v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00999v2",
                "updated": "2025-01-06T01:49:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    49,
                    9,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-02T01:33:58Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    1,
                    33,
                    58,
                    3,
                    2,
                    0
                ],
                "title": "Exploring Information Processing in Large Language Models: Insights from\n  Information Bottleneck Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Information Processing in Large Language Models: Insights from\n  Information Bottleneck Theory"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of tasks by understanding input information and predicting\ncorresponding outputs. However, the internal mechanisms by which LLMs\ncomprehend input and make effective predictions remain poorly understood. In\nthis paper, we explore the working mechanism of LLMs in information processing\nfrom the perspective of Information Bottleneck Theory. We propose a\nnon-training construction strategy to define a task space and identify the\nfollowing key findings: (1) LLMs compress input information into specific task\nspaces (e.g., sentiment space, topic space) to facilitate task understanding;\n(2) they then extract and utilize relevant information from the task space at\ncritical moments to generate accurate predictions. Based on these insights, we\nintroduce two novel approaches: an Information Compression-based Context\nLearning (IC-ICL) and a Task-Space-guided Fine-Tuning (TS-FT). IC-ICL enhances\nreasoning performance and inference efficiency by compressing retrieved example\ninformation into the task space. TS-FT employs a space-guided loss to fine-tune\nLLMs, encouraging the learning of more effective compression and selection\nmechanisms. Experiments across multiple datasets validate the effectiveness of\ntask space construction. Additionally, IC-ICL not only improves performance but\nalso accelerates inference speed by over 40\\%, while TS-FT achieves superior\nresults with a minimal strategy adjustment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of tasks by understanding input information and predicting\ncorresponding outputs. However, the internal mechanisms by which LLMs\ncomprehend input and make effective predictions remain poorly understood. In\nthis paper, we explore the working mechanism of LLMs in information processing\nfrom the perspective of Information Bottleneck Theory. We propose a\nnon-training construction strategy to define a task space and identify the\nfollowing key findings: (1) LLMs compress input information into specific task\nspaces (e.g., sentiment space, topic space) to facilitate task understanding;\n(2) they then extract and utilize relevant information from the task space at\ncritical moments to generate accurate predictions. Based on these insights, we\nintroduce two novel approaches: an Information Compression-based Context\nLearning (IC-ICL) and a Task-Space-guided Fine-Tuning (TS-FT). IC-ICL enhances\nreasoning performance and inference efficiency by compressing retrieved example\ninformation into the task space. TS-FT employs a space-guided loss to fine-tune\nLLMs, encouraging the learning of more effective compression and selection\nmechanisms. Experiments across multiple datasets validate the effectiveness of\ntask space construction. Additionally, IC-ICL not only improves performance but\nalso accelerates inference speed by over 40\\%, while TS-FT achieves superior\nresults with a minimal strategy adjustment."
                },
                "authors": [
                    {
                        "name": "Zhou Yang"
                    },
                    {
                        "name": "Zhengyu Qi"
                    },
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Zhikai Jia"
                    },
                    {
                        "name": "Haizhou Sun"
                    },
                    {
                        "name": "Xiaofei Zhu"
                    },
                    {
                        "name": "Xiangwen Liao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangwen Liao"
                },
                "author": "Xiangwen Liao",
                "arxiv_comment": "9 pages, 9 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00999v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00999v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00890v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00890v2",
                "updated": "2025-01-06T01:27:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    27,
                    48,
                    0,
                    6,
                    0
                ],
                "published": "2024-07-01T01:25:26Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    1,
                    25,
                    26,
                    0,
                    183,
                    0
                ],
                "title": "Macroeconomic Forecasting with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Macroeconomic Forecasting with Large Language Models"
                },
                "summary": "This paper presents a comparative analysis evaluating the accuracy of Large\nLanguage Models (LLMs) against traditional macro time series forecasting\napproaches. In recent times, LLMs have surged in popularity for forecasting due\nto their ability to capture intricate patterns in data and quickly adapt across\nvery different domains. However, their effectiveness in forecasting\nmacroeconomic time series data compared to conventional methods remains an area\nof interest. To address this, we conduct a rigorous evaluation of LLMs against\ntraditional macro forecasting methods, using as common ground the FRED-MD\ndatabase. Our findings provide valuable insights into the strengths and\nlimitations of LLMs in forecasting macroeconomic time series, shedding light on\ntheir applicability in real-world scenarios",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comparative analysis evaluating the accuracy of Large\nLanguage Models (LLMs) against traditional macro time series forecasting\napproaches. In recent times, LLMs have surged in popularity for forecasting due\nto their ability to capture intricate patterns in data and quickly adapt across\nvery different domains. However, their effectiveness in forecasting\nmacroeconomic time series data compared to conventional methods remains an area\nof interest. To address this, we conduct a rigorous evaluation of LLMs against\ntraditional macro forecasting methods, using as common ground the FRED-MD\ndatabase. Our findings provide valuable insights into the strengths and\nlimitations of LLMs in forecasting macroeconomic time series, shedding light on\ntheir applicability in real-world scenarios"
                },
                "authors": [
                    {
                        "name": "Andrea Carriero"
                    },
                    {
                        "name": "Davide Pettenuzzo"
                    },
                    {
                        "name": "Shubhranshu Shekhar"
                    }
                ],
                "author_detail": {
                    "name": "Shubhranshu Shekhar"
                },
                "author": "Shubhranshu Shekhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00890v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02703v1",
                "updated": "2025-01-06T01:11:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    11,
                    30,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T01:11:30Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    11,
                    30,
                    0,
                    6,
                    0
                ],
                "title": "Full-conformal novelty detection: A powerful and non-random approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full-conformal novelty detection: A powerful and non-random approach"
                },
                "summary": "We introduce a powerful and non-random methodology for novelty detection,\noffering distribution-free false discovery rate (FDR) control guarantees.\nBuilding on the full-conformal inference framework and the concept of e-values,\nwe introduce full-conformal e-values to quantify evidence for novelty relative\nto a given reference dataset. These e-values are then utilized by carefully\ncrafted multiple testing procedures to identify a set of novel units\nout-of-sample with provable finite-sample FDR control. Furthermore, our method\nis extended to address distribution shift, accommodating scenarios where\nnovelty detection must be performed on data drawn from a shifted distribution\nrelative to the reference dataset. In all settings, our method is non-random\nand can perform powerfully with limited amounts of reference data. Empirical\nevaluations on synthetic and real-world datasets demonstrate that our approach\nsignificantly outperforms existing methods for novelty detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a powerful and non-random methodology for novelty detection,\noffering distribution-free false discovery rate (FDR) control guarantees.\nBuilding on the full-conformal inference framework and the concept of e-values,\nwe introduce full-conformal e-values to quantify evidence for novelty relative\nto a given reference dataset. These e-values are then utilized by carefully\ncrafted multiple testing procedures to identify a set of novel units\nout-of-sample with provable finite-sample FDR control. Furthermore, our method\nis extended to address distribution shift, accommodating scenarios where\nnovelty detection must be performed on data drawn from a shifted distribution\nrelative to the reference dataset. In all settings, our method is non-random\nand can perform powerfully with limited amounts of reference data. Empirical\nevaluations on synthetic and real-world datasets demonstrate that our approach\nsignificantly outperforms existing methods for novelty detection."
                },
                "authors": [
                    {
                        "name": "Junu Lee"
                    },
                    {
                        "name": "Ilia Popov"
                    },
                    {
                        "name": "Zhimei Ren"
                    }
                ],
                "author_detail": {
                    "name": "Zhimei Ren"
                },
                "author": "Zhimei Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02702v1",
                "updated": "2025-01-06T01:07:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    7,
                    59,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T01:07:59Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    7,
                    59,
                    0,
                    6,
                    0
                ],
                "title": "QuIM-RAG: Advancing Retrieval-Augmented Generation with Inverted\n  Question Matching for Enhanced QA Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuIM-RAG: Advancing Retrieval-Augmented Generation with Inverted\n  Question Matching for Enhanced QA Performance"
                },
                "summary": "This work presents a novel architecture for building Retrieval-Augmented\nGeneration (RAG) systems to improve Question Answering (QA) tasks from a target\ncorpus. Large Language Models (LLMs) have revolutionized the analyzing and\ngeneration of human-like text. These models rely on pre-trained data and lack\nreal-time updates unless integrated with live data tools. RAG enhances LLMs by\nintegrating online resources and databases to generate contextually appropriate\nresponses. However, traditional RAG still encounters challenges like\ninformation dilution and hallucinations when handling vast amounts of data. Our\napproach addresses these challenges by converting corpora into a\ndomain-specific dataset and RAG architecture is constructed to generate\nresponses from the target document. We introduce QuIM-RAG (Question-to-question\nInverted Index Matching), a novel approach for the retrieval mechanism in our\nsystem. This strategy generates potential questions from document chunks and\nmatches these with user queries to identify the most relevant text chunks for\ngenerating accurate answers. We have implemented our RAG system on top of the\nopen-source Meta-LLaMA3-8B-instruct model by Meta Inc. that is available on\nHugging Face. We constructed a custom corpus of 500+ pages from a high-traffic\nwebsite accessed thousands of times daily for answering complex questions,\nalong with manually prepared ground truth QA for evaluation. We compared our\napproach with traditional RAG models using BERT-Score and RAGAS,\nstate-of-the-art metrics for evaluating LLM applications. Our evaluation\ndemonstrates that our approach outperforms traditional RAG architectures on\nboth metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a novel architecture for building Retrieval-Augmented\nGeneration (RAG) systems to improve Question Answering (QA) tasks from a target\ncorpus. Large Language Models (LLMs) have revolutionized the analyzing and\ngeneration of human-like text. These models rely on pre-trained data and lack\nreal-time updates unless integrated with live data tools. RAG enhances LLMs by\nintegrating online resources and databases to generate contextually appropriate\nresponses. However, traditional RAG still encounters challenges like\ninformation dilution and hallucinations when handling vast amounts of data. Our\napproach addresses these challenges by converting corpora into a\ndomain-specific dataset and RAG architecture is constructed to generate\nresponses from the target document. We introduce QuIM-RAG (Question-to-question\nInverted Index Matching), a novel approach for the retrieval mechanism in our\nsystem. This strategy generates potential questions from document chunks and\nmatches these with user queries to identify the most relevant text chunks for\ngenerating accurate answers. We have implemented our RAG system on top of the\nopen-source Meta-LLaMA3-8B-instruct model by Meta Inc. that is available on\nHugging Face. We constructed a custom corpus of 500+ pages from a high-traffic\nwebsite accessed thousands of times daily for answering complex questions,\nalong with manually prepared ground truth QA for evaluation. We compared our\napproach with traditional RAG models using BERT-Score and RAGAS,\nstate-of-the-art metrics for evaluating LLM applications. Our evaluation\ndemonstrates that our approach outperforms traditional RAG architectures on\nboth metrics."
                },
                "authors": [
                    {
                        "name": "Binita Saha"
                    },
                    {
                        "name": "Utsha Saha"
                    },
                    {
                        "name": "Muhammad Zubair Malik"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Zubair Malik"
                },
                "author": "Muhammad Zubair Malik",
                "arxiv_doi": "10.1109/ACCESS.2024.3513155",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2024.3513155",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.02702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02699v1",
                "updated": "2025-01-06T00:39:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    0,
                    39,
                    31,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T00:39:31Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    0,
                    39,
                    31,
                    0,
                    6,
                    0
                ],
                "title": "EAGLE: Enhanced Visual Grounding Minimizes Hallucinations in\n  Instructional Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EAGLE: Enhanced Visual Grounding Minimizes Hallucinations in\n  Instructional Multimodal Models"
                },
                "summary": "Large language models and vision transformers have demonstrated impressive\nzero-shot capabilities, enabling significant transferability in downstream\ntasks. The fusion of these models has resulted in multi-modal architectures\nwith enhanced instructional capabilities. Despite incorporating vast image and\nlanguage pre-training, these multi-modal architectures often generate responses\nthat deviate from the ground truth in the image data. These failure cases are\nknown as hallucinations. Current methods for mitigating hallucinations\ngenerally focus on regularizing the language component, improving the fusion\nmodule, or ensembling multiple visual encoders to improve visual\nrepresentation. In this paper, we address the hallucination issue by directly\nenhancing the capabilities of the visual component. Our approach, named EAGLE,\nis fully agnostic to the LLM or fusion module and works as a post-pretraining\napproach that improves the grounding and language alignment of the visual\nencoder. We show that a straightforward reformulation of the original\ncontrastive pre-training task results in an improved visual encoder that can be\nincorporated into the instructional multi-modal architecture without additional\ninstructional training. As a result, EAGLE achieves a significant reduction in\nhallucinations across multiple challenging benchmarks and tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models and vision transformers have demonstrated impressive\nzero-shot capabilities, enabling significant transferability in downstream\ntasks. The fusion of these models has resulted in multi-modal architectures\nwith enhanced instructional capabilities. Despite incorporating vast image and\nlanguage pre-training, these multi-modal architectures often generate responses\nthat deviate from the ground truth in the image data. These failure cases are\nknown as hallucinations. Current methods for mitigating hallucinations\ngenerally focus on regularizing the language component, improving the fusion\nmodule, or ensembling multiple visual encoders to improve visual\nrepresentation. In this paper, we address the hallucination issue by directly\nenhancing the capabilities of the visual component. Our approach, named EAGLE,\nis fully agnostic to the LLM or fusion module and works as a post-pretraining\napproach that improves the grounding and language alignment of the visual\nencoder. We show that a straightforward reformulation of the original\ncontrastive pre-training task results in an improved visual encoder that can be\nincorporated into the instructional multi-modal architecture without additional\ninstructional training. As a result, EAGLE achieves a significant reduction in\nhallucinations across multiple challenging benchmarks and tasks."
                },
                "authors": [
                    {
                        "name": "Andrs Villa"
                    },
                    {
                        "name": "Juan Len Alczar"
                    },
                    {
                        "name": "Motasem Alfarra"
                    },
                    {
                        "name": "Vladimir Araujo"
                    },
                    {
                        "name": "Alvaro Soto"
                    },
                    {
                        "name": "Bernard Ghanem"
                    }
                ],
                "author_detail": {
                    "name": "Bernard Ghanem"
                },
                "author": "Bernard Ghanem",
                "arxiv_comment": "12 pages, 4 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02690v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02690v1",
                "updated": "2025-01-05T23:55:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    23,
                    55,
                    33,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T23:55:33Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    23,
                    55,
                    33,
                    6,
                    5,
                    0
                ],
                "title": "GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields\n  through Efficient Dense 3D Point Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields\n  through Efficient Dense 3D Point Tracking"
                },
                "summary": "4D video control is essential in video generation as it enables the use of\nsophisticated lens techniques, such as multi-camera shooting and dolly zoom,\nwhich are currently unsupported by existing methods. Training a video Diffusion\nTransformer (DiT) directly to control 4D content requires expensive multi-view\nvideos. Inspired by Monocular Dynamic novel View Synthesis (MDVS) that\noptimizes a 4D representation and renders videos according to different 4D\nelements, such as camera pose and object motion editing, we bring pseudo 4D\nGaussian fields to video generation. Specifically, we propose a novel framework\nthat constructs a pseudo 4D Gaussian field with dense 3D point tracking and\nrenders the Gaussian field for all video frames. Then we finetune a pretrained\nDiT to generate videos following the guidance of the rendered video, dubbed as\nGS-DiT. To boost the training of the GS-DiT, we also propose an efficient Dense\n3D Point Tracking (D3D-PT) method for the pseudo 4D Gaussian field\nconstruction. Our D3D-PT outperforms SpatialTracker, the state-of-the-art\nsparse 3D point tracking method, in accuracy and accelerates the inference\nspeed by two orders of magnitude. During the inference stage, GS-DiT can\ngenerate videos with the same dynamic content while adhering to different\ncamera parameters, addressing a significant limitation of current video\ngeneration models. GS-DiT demonstrates strong generalization capabilities and\nextends the 4D controllability of Gaussian splatting to video generation beyond\njust camera poses. It supports advanced cinematic effects through the\nmanipulation of the Gaussian field and camera intrinsics, making it a powerful\ntool for creative video production. Demos are available at\nhttps://wkbian.github.io/Projects/GS-DiT/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "4D video control is essential in video generation as it enables the use of\nsophisticated lens techniques, such as multi-camera shooting and dolly zoom,\nwhich are currently unsupported by existing methods. Training a video Diffusion\nTransformer (DiT) directly to control 4D content requires expensive multi-view\nvideos. Inspired by Monocular Dynamic novel View Synthesis (MDVS) that\noptimizes a 4D representation and renders videos according to different 4D\nelements, such as camera pose and object motion editing, we bring pseudo 4D\nGaussian fields to video generation. Specifically, we propose a novel framework\nthat constructs a pseudo 4D Gaussian field with dense 3D point tracking and\nrenders the Gaussian field for all video frames. Then we finetune a pretrained\nDiT to generate videos following the guidance of the rendered video, dubbed as\nGS-DiT. To boost the training of the GS-DiT, we also propose an efficient Dense\n3D Point Tracking (D3D-PT) method for the pseudo 4D Gaussian field\nconstruction. Our D3D-PT outperforms SpatialTracker, the state-of-the-art\nsparse 3D point tracking method, in accuracy and accelerates the inference\nspeed by two orders of magnitude. During the inference stage, GS-DiT can\ngenerate videos with the same dynamic content while adhering to different\ncamera parameters, addressing a significant limitation of current video\ngeneration models. GS-DiT demonstrates strong generalization capabilities and\nextends the 4D controllability of Gaussian splatting to video generation beyond\njust camera poses. It supports advanced cinematic effects through the\nmanipulation of the Gaussian field and camera intrinsics, making it a powerful\ntool for creative video production. Demos are available at\nhttps://wkbian.github.io/Projects/GS-DiT/."
                },
                "authors": [
                    {
                        "name": "Weikang Bian"
                    },
                    {
                        "name": "Zhaoyang Huang"
                    },
                    {
                        "name": "Xiaoyu Shi"
                    },
                    {
                        "name": "Yijin Li"
                    },
                    {
                        "name": "Fu-Yun Wang"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "arxiv_comment": "Project Page: https://wkbian.github.io/Projects/GS-DiT/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02690v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09697v2",
                "updated": "2025-01-05T23:55:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    23,
                    55,
                    13,
                    6,
                    5,
                    0
                ],
                "published": "2024-12-12T19:25:41Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    19,
                    25,
                    41,
                    3,
                    347,
                    0
                ],
                "title": "Evaluating Time-Specific Treatment Effects Using Randomization Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Time-Specific Treatment Effects Using Randomization Inference"
                },
                "summary": "This study develops a systematic approach for evaluating the effect of a\ntreatment on a time-to-event outcome in a matched-pair study. While most\nmethods for paired right-censored outcomes allow determining an overall\ntreatment effect over the course of follow-up, they generally lack in providing\ndetailed insights into how the effect changes over time. To address this gap,\nwe propose novel tests for paired right-censored outcomes using randomization\ninference. We further extend our tests to matched observational studies by\ndeveloping corresponding sensitivity analysis methods to take into account\ndepartures from randomization. Simulations demonstrate the robustness of our\napproach against various non-proportional hazards alternatives, including a\ncrossing survival curves scenario. We demonstrate the application of our\nmethods using a matched observational study from the Korean Longitudinal Study\nof Aging (KLoSA) data, focusing on the effect of social engagement on survival.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study develops a systematic approach for evaluating the effect of a\ntreatment on a time-to-event outcome in a matched-pair study. While most\nmethods for paired right-censored outcomes allow determining an overall\ntreatment effect over the course of follow-up, they generally lack in providing\ndetailed insights into how the effect changes over time. To address this gap,\nwe propose novel tests for paired right-censored outcomes using randomization\ninference. We further extend our tests to matched observational studies by\ndeveloping corresponding sensitivity analysis methods to take into account\ndepartures from randomization. Simulations demonstrate the robustness of our\napproach against various non-proportional hazards alternatives, including a\ncrossing survival curves scenario. We demonstrate the application of our\nmethods using a matched observational study from the Korean Longitudinal Study\nof Aging (KLoSA) data, focusing on the effect of social engagement on survival."
                },
                "authors": [
                    {
                        "name": "Sangjin Lee"
                    },
                    {
                        "name": "Kwonsang Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kwonsang Lee"
                },
                "author": "Kwonsang Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.03990v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.03990v2",
                "updated": "2025-01-05T23:50:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    23,
                    50,
                    57,
                    6,
                    5,
                    0
                ],
                "published": "2023-04-08T11:35:19Z",
                "published_parsed": [
                    2023,
                    4,
                    8,
                    11,
                    35,
                    19,
                    5,
                    98,
                    0
                ],
                "title": "Block-regularized 5$\\times$2 Cross-validated McNemar's Test for\n  Comparing Two Classification Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-regularized 5$\\times$2 Cross-validated McNemar's Test for\n  Comparing Two Classification Algorithms"
                },
                "summary": "In the task of comparing two classification algorithms, the widely-used\nMcNemar's test aims to infer the presence of a significant difference between\nthe error rates of the two classification algorithms. However, the power of the\nconventional McNemar's test is usually unpromising because the hold-out (HO)\nmethod in the test merely uses a single train-validation split that usually\nproduces a highly varied estimation of the error rates. In contrast, a\ncross-validation (CV) method repeats the HO method in multiple times and\nproduces a stable estimation. Therefore, a CV method has a great advantage to\nimprove the power of McNemar's test. Among all types of CV methods, a\nblock-regularized 5$\\times$2 CV (BCV) has been shown in many previous studies\nto be superior to the other CV methods in the comparison task of algorithms\nbecause the 5$\\times$2 BCV can produce a high-quality estimator of the error\nrate by regularizing the numbers of overlapping records between all training\nsets. In this study, we compress the 10 correlated contingency tables in the\n5$\\times$2 BCV to form an effective contingency table. Then, we define a\n5$\\times$2 BCV McNemar's test on the basis of the effective contingency table.\nWe demonstrate the reasonable type I error and the promising power of the\nproposed 5$\\times$2 BCV McNemar's test on multiple simulated and real-world\ndata sets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the task of comparing two classification algorithms, the widely-used\nMcNemar's test aims to infer the presence of a significant difference between\nthe error rates of the two classification algorithms. However, the power of the\nconventional McNemar's test is usually unpromising because the hold-out (HO)\nmethod in the test merely uses a single train-validation split that usually\nproduces a highly varied estimation of the error rates. In contrast, a\ncross-validation (CV) method repeats the HO method in multiple times and\nproduces a stable estimation. Therefore, a CV method has a great advantage to\nimprove the power of McNemar's test. Among all types of CV methods, a\nblock-regularized 5$\\times$2 CV (BCV) has been shown in many previous studies\nto be superior to the other CV methods in the comparison task of algorithms\nbecause the 5$\\times$2 BCV can produce a high-quality estimator of the error\nrate by regularizing the numbers of overlapping records between all training\nsets. In this study, we compress the 10 correlated contingency tables in the\n5$\\times$2 BCV to form an effective contingency table. Then, we define a\n5$\\times$2 BCV McNemar's test on the basis of the effective contingency table.\nWe demonstrate the reasonable type I error and the promising power of the\nproposed 5$\\times$2 BCV McNemar's test on multiple simulated and real-world\ndata sets."
                },
                "authors": [
                    {
                        "name": "Jing Yang"
                    },
                    {
                        "name": "Ruibo Wang"
                    },
                    {
                        "name": "Yijun Song"
                    },
                    {
                        "name": "Jihong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jihong Li"
                },
                "author": "Jihong Li",
                "arxiv_comment": "12 pages, 6 figures, and 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.03990v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.03990v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02688v1",
                "updated": "2025-01-05T23:35:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    23,
                    35,
                    47,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T23:35:47Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    23,
                    35,
                    47,
                    6,
                    5,
                    0
                ],
                "title": "Decoding specialised feature neurons in LLMs with the final projection\n  layer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding specialised feature neurons in LLMs with the final projection\n  layer"
                },
                "summary": "Large Language Models (LLMs) typically have billions of parameters and are\nthus often difficult to interpret in their operation. Such black-box models can\npose a significant risk to safety when trusted to make important decisions. The\nlack of interpretability of LLMs is more related to their sheer size, rather\nthan the complexity of their individual components. The TARS method for\nknowledge removal (Davies et al 2024) provides strong evidence for the\nhypothesis that that linear layer weights which act directly on the residual\nstream may have high correlation with different concepts encoded in the\nresidual stream. Building upon this, we attempt to decode neuron weights\ndirectly into token probabilities through the final projection layer of the\nmodel (the LM-head). Firstly, we show that with Llama 3.1 8B we can utilise the\nLM-head to decode specialised feature neurons that respond strongly to certain\nconcepts, with examples such as \"dog\" and \"California\". This is then confirmed\nby demonstrating that these neurons can be clamped to affect the probability of\nthe concept in the output. This extends to the fine-tuned assistant Llama 3.1\n8B instruct model, where we find that over 75% of neurons in the up-projection\nlayers have the same top associated token compared to the pretrained model.\nFinally, we demonstrate that clamping the \"dog\" neuron leads the instruct model\nto always discuss dogs when asked about its favourite animal. Through our\nmethod, it is possible to map the entirety of Llama 3.1 8B's up-projection\nneurons in less than 15 minutes with no parallelization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) typically have billions of parameters and are\nthus often difficult to interpret in their operation. Such black-box models can\npose a significant risk to safety when trusted to make important decisions. The\nlack of interpretability of LLMs is more related to their sheer size, rather\nthan the complexity of their individual components. The TARS method for\nknowledge removal (Davies et al 2024) provides strong evidence for the\nhypothesis that that linear layer weights which act directly on the residual\nstream may have high correlation with different concepts encoded in the\nresidual stream. Building upon this, we attempt to decode neuron weights\ndirectly into token probabilities through the final projection layer of the\nmodel (the LM-head). Firstly, we show that with Llama 3.1 8B we can utilise the\nLM-head to decode specialised feature neurons that respond strongly to certain\nconcepts, with examples such as \"dog\" and \"California\". This is then confirmed\nby demonstrating that these neurons can be clamped to affect the probability of\nthe concept in the output. This extends to the fine-tuned assistant Llama 3.1\n8B instruct model, where we find that over 75% of neurons in the up-projection\nlayers have the same top associated token compared to the pretrained model.\nFinally, we demonstrate that clamping the \"dog\" neuron leads the instruct model\nto always discuss dogs when asked about its favourite animal. Through our\nmethod, it is possible to map the entirety of Llama 3.1 8B's up-projection\nneurons in less than 15 minutes with no parallelization."
                },
                "authors": [
                    {
                        "name": "Harry J Davies"
                    }
                ],
                "author_detail": {
                    "name": "Harry J Davies"
                },
                "author": "Harry J Davies",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02683v1",
                "updated": "2025-01-05T23:19:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    23,
                    19,
                    55,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T23:19:55Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    23,
                    19,
                    55,
                    6,
                    5,
                    0
                ],
                "title": "From Superficial Patterns to Semantic Understanding: Fine-Tuning\n  Language Models on Contrast Sets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Superficial Patterns to Semantic Understanding: Fine-Tuning\n  Language Models on Contrast Sets"
                },
                "summary": "Large scale pretrained language models have demonstrated high performance on\nstandard datasets for natural language inference (NLI) tasks. Unfortunately,\nthese evaluations can be misleading, as although the models can perform well on\nin-distribution data, they perform poorly on out-of-distribution test sets,\nsuch as contrast sets. Contrast sets consist of perturbed instances of data\nthat have very minor, but meaningful, changes to the input that alter the gold\nlabel, revealing how models can learn superficial patterns in the training data\nrather than learning more sophisticated language nuances. As an example, the\nELECTRA-small language model achieves nearly 90% accuracy on an SNLI dataset\nbut drops to 75% when tested on an out-of-distribution contrast set. The\nresearch performed in this study explores how a language models' robustness can\nbe improved by exposing it to small amounts of more complex contrast sets\nduring training to help it better learn language patterns. With this approach,\nthe model regains performance and achieves nearly 90% accuracy on contrast\nsets, highlighting the importance of diverse and challenging training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large scale pretrained language models have demonstrated high performance on\nstandard datasets for natural language inference (NLI) tasks. Unfortunately,\nthese evaluations can be misleading, as although the models can perform well on\nin-distribution data, they perform poorly on out-of-distribution test sets,\nsuch as contrast sets. Contrast sets consist of perturbed instances of data\nthat have very minor, but meaningful, changes to the input that alter the gold\nlabel, revealing how models can learn superficial patterns in the training data\nrather than learning more sophisticated language nuances. As an example, the\nELECTRA-small language model achieves nearly 90% accuracy on an SNLI dataset\nbut drops to 75% when tested on an out-of-distribution contrast set. The\nresearch performed in this study explores how a language models' robustness can\nbe improved by exposing it to small amounts of more complex contrast sets\nduring training to help it better learn language patterns. With this approach,\nthe model regains performance and achieves nearly 90% accuracy on contrast\nsets, highlighting the importance of diverse and challenging training data."
                },
                "authors": [
                    {
                        "name": "Daniel Petrov"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Petrov"
                },
                "author": "Daniel Petrov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01399v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01399v5",
                "updated": "2025-01-05T22:23:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    22,
                    23,
                    35,
                    6,
                    5,
                    0
                ],
                "published": "2024-04-01T18:10:05Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    18,
                    10,
                    5,
                    0,
                    92,
                    0
                ],
                "title": "Developing Safe and Responsible Large Language Model : Can We Balance\n  Bias Reduction and Language Understanding in Large Language Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing Safe and Responsible Large Language Model : Can We Balance\n  Bias Reduction and Language Understanding in Large Language Models?"
                },
                "summary": "Large Language Models (LLMs) have advanced various Natural Language\nProcessing (NLP) tasks, such as text generation and translation, among others.\nHowever, these models often generate texts that can perpetuate biases. Existing\napproaches to mitigate these biases usually compromise knowledge retention.\nThis study explores whether LLMs can produce safe, unbiased outputs without\nsacrificing knowledge or comprehension. We introduce the Safe and Responsible\nLarge Language Model (\\textbf{SR}$_{\\text{LLM}}$), which has been instruction\nfine-tuned atop of a safe fine-tuned auto-regressive decoder-only LLM to reduce\nbiases in generated texts. We developed a specialized dataset with examples of\nunsafe and corresponding safe variations to train \\textbf{SR}$_{\\text{LLM}}$ to\nidentify and correct biased text. Experiments on our specialized dataset and\nout-of-distribution test sets reveal that \\textbf{SR}$_{\\text{LLM}}$\neffectively reduces biases while preserving knowledge integrity. This\nperformance surpasses that of traditional fine-tuning of smaller language\nmodels and base LLMs that merely reply on prompting techniques. Our findings\ndemonstrate that instruction fine-tuning on custom datasets tailored for tasks\nsuch as debiasing is a highly effective strategy for minimizing bias in LLM\nwhile preserving their inherent knowledge and capabilities. The code and\ndataset are accessible at\n\\href{https://github.com/shainarazavi/Safe-Responsible-LLM}{SR-LLM}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have advanced various Natural Language\nProcessing (NLP) tasks, such as text generation and translation, among others.\nHowever, these models often generate texts that can perpetuate biases. Existing\napproaches to mitigate these biases usually compromise knowledge retention.\nThis study explores whether LLMs can produce safe, unbiased outputs without\nsacrificing knowledge or comprehension. We introduce the Safe and Responsible\nLarge Language Model (\\textbf{SR}$_{\\text{LLM}}$), which has been instruction\nfine-tuned atop of a safe fine-tuned auto-regressive decoder-only LLM to reduce\nbiases in generated texts. We developed a specialized dataset with examples of\nunsafe and corresponding safe variations to train \\textbf{SR}$_{\\text{LLM}}$ to\nidentify and correct biased text. Experiments on our specialized dataset and\nout-of-distribution test sets reveal that \\textbf{SR}$_{\\text{LLM}}$\neffectively reduces biases while preserving knowledge integrity. This\nperformance surpasses that of traditional fine-tuning of smaller language\nmodels and base LLMs that merely reply on prompting techniques. Our findings\ndemonstrate that instruction fine-tuning on custom datasets tailored for tasks\nsuch as debiasing is a highly effective strategy for minimizing bias in LLM\nwhile preserving their inherent knowledge and capabilities. The code and\ndataset are accessible at\n\\href{https://github.com/shainarazavi/Safe-Responsible-LLM}{SR-LLM}"
                },
                "authors": [
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Oluwanifemi Bamgbose"
                    },
                    {
                        "name": "Shardul Ghuge"
                    },
                    {
                        "name": "Fatemeh Tavakol"
                    },
                    {
                        "name": "Deepak John Reji"
                    },
                    {
                        "name": "Syed Raza Bashir"
                    }
                ],
                "author_detail": {
                    "name": "Syed Raza Bashir"
                },
                "author": "Syed Raza Bashir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01399v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01399v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16833v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16833v2",
                "updated": "2025-01-05T21:51:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    21,
                    51,
                    46,
                    6,
                    5,
                    0
                ],
                "published": "2024-05-27T05:04:05Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    5,
                    4,
                    5,
                    0,
                    148,
                    0
                ],
                "title": "Safe LoRA: the Silver Lining of Reducing Safety Risks when Fine-tuning\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe LoRA: the Silver Lining of Reducing Safety Risks when Fine-tuning\n  Large Language Models"
                },
                "summary": "While large language models (LLMs) such as Llama-2 or GPT-4 have shown\nimpressive zero-shot performance, fine-tuning is still necessary to enhance\ntheir performance for customized datasets, domain-specific tasks, or other\nprivate needs. However, fine-tuning all parameters of LLMs requires significant\nhardware resources, which can be impractical for typical users. Therefore,\nparameter-efficient fine-tuning such as LoRA have emerged, allowing users to\nfine-tune LLMs without the need for considerable computing resources, with\nlittle performance degradation compared to fine-tuning all parameters.\nUnfortunately, recent studies indicate that fine-tuning can increase the risk\nto the safety of LLMs, even when data does not contain malicious content. To\naddress this challenge, we propose Safe LoRA, a simple one-liner patch to the\noriginal LoRA implementation by introducing the projection of LoRA weights from\nselected layers to the safety-aligned subspace, effectively reducing the safety\nrisks in LLM fine-tuning while maintaining utility. It is worth noting that\nSafe LoRA is a training-free and data-free approach, as it only requires the\nknowledge of the weights from the base and aligned LLMs. Our extensive\nexperiments demonstrate that when fine-tuning on purely malicious data, Safe\nLoRA retains similar safety performance as the original aligned model.\nMoreover, when the fine-tuning dataset contains a mixture of both benign and\nmalicious data, Safe LoRA mitigates the negative effect made by malicious data\nwhile preserving performance on downstream tasks. Our codes are available at\n\\url{https://github.com/IBM/SafeLoRA}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) such as Llama-2 or GPT-4 have shown\nimpressive zero-shot performance, fine-tuning is still necessary to enhance\ntheir performance for customized datasets, domain-specific tasks, or other\nprivate needs. However, fine-tuning all parameters of LLMs requires significant\nhardware resources, which can be impractical for typical users. Therefore,\nparameter-efficient fine-tuning such as LoRA have emerged, allowing users to\nfine-tune LLMs without the need for considerable computing resources, with\nlittle performance degradation compared to fine-tuning all parameters.\nUnfortunately, recent studies indicate that fine-tuning can increase the risk\nto the safety of LLMs, even when data does not contain malicious content. To\naddress this challenge, we propose Safe LoRA, a simple one-liner patch to the\noriginal LoRA implementation by introducing the projection of LoRA weights from\nselected layers to the safety-aligned subspace, effectively reducing the safety\nrisks in LLM fine-tuning while maintaining utility. It is worth noting that\nSafe LoRA is a training-free and data-free approach, as it only requires the\nknowledge of the weights from the base and aligned LLMs. Our extensive\nexperiments demonstrate that when fine-tuning on purely malicious data, Safe\nLoRA retains similar safety performance as the original aligned model.\nMoreover, when the fine-tuning dataset contains a mixture of both benign and\nmalicious data, Safe LoRA mitigates the negative effect made by malicious data\nwhile preserving performance on downstream tasks. Our codes are available at\n\\url{https://github.com/IBM/SafeLoRA}."
                },
                "authors": [
                    {
                        "name": "Chia-Yi Hsu"
                    },
                    {
                        "name": "Yu-Lin Tsai"
                    },
                    {
                        "name": "Chih-Hsun Lin"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Chia-Mu Yu"
                    },
                    {
                        "name": "Chun-Ying Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chun-Ying Huang"
                },
                "author": "Chun-Ying Huang",
                "arxiv_comment": "This is the camera-ready version accepted for NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16833v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16833v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02658v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02658v1",
                "updated": "2025-01-05T20:44:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    20,
                    44,
                    39,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T20:44:39Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    20,
                    44,
                    39,
                    6,
                    5,
                    0
                ],
                "title": "Beware of so-called 'good' correlations: a statistical reality check on\n  individual mRNA-protein predictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beware of so-called 'good' correlations: a statistical reality check on\n  individual mRNA-protein predictions"
                },
                "summary": "Research in the life sciences often employs messenger ribonucleic acids\n(mRNA) quantification as a standalone approach for functional analysis.\nHowever, although the correlation between the measured levels of mRNA and\nproteins is positive, correlation coefficients observed empirically are\nincomplete, necessitating caution in making agnostic inferences. This essay\nprovides a statistical reflection and caveat on the concept of correlation\nstrength in the context of transcriptomics-proteomics studies. It highlights\nthe variability in possible protein levels at given empirical correlation\nvalues, even for precise mRNA amount, and underscores the notable proportion of\nmRNA-protein pairs with abundances at opposite ends of their respective\ndistributions. Cell biologists, data scientists, and biostatisticians should\nrecognise that mRNA-protein correlation alone is insufficient to justify using\na single mRNA quantification to infer the amount or function of its\ncorresponding protein.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research in the life sciences often employs messenger ribonucleic acids\n(mRNA) quantification as a standalone approach for functional analysis.\nHowever, although the correlation between the measured levels of mRNA and\nproteins is positive, correlation coefficients observed empirically are\nincomplete, necessitating caution in making agnostic inferences. This essay\nprovides a statistical reflection and caveat on the concept of correlation\nstrength in the context of transcriptomics-proteomics studies. It highlights\nthe variability in possible protein levels at given empirical correlation\nvalues, even for precise mRNA amount, and underscores the notable proportion of\nmRNA-protein pairs with abundances at opposite ends of their respective\ndistributions. Cell biologists, data scientists, and biostatisticians should\nrecognise that mRNA-protein correlation alone is insufficient to justify using\na single mRNA quantification to infer the amount or function of its\ncorresponding protein."
                },
                "authors": [
                    {
                        "name": "Romain-Daniel Gosselin"
                    }
                ],
                "author_detail": {
                    "name": "Romain-Daniel Gosselin"
                },
                "author": "Romain-Daniel Gosselin",
                "arxiv_comment": "article to be published, 16 pages, 2 figures, one table, data\n  available at https://doi.org/10.6084/m9.figshare.28138733.v1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02658v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02658v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02654v1",
                "updated": "2025-01-05T20:39:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    20,
                    39,
                    52,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T20:39:52Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    20,
                    39,
                    52,
                    6,
                    5,
                    0
                ],
                "title": "Tougher Text, Smarter Models: Raising the Bar for Adversarial Defence\n  Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tougher Text, Smarter Models: Raising the Bar for Adversarial Defence\n  Benchmarks"
                },
                "summary": "vulnerability of deep learning models to adversarial attacks. While various\ndefence mechanisms have been proposed, there is a lack of comprehensive\nbenchmarks that evaluate these defences across diverse datasets, models, and\ntasks. In this work, we address this gap by presenting an extensive benchmark\nfor textual adversarial defence that significantly expands upon previous work.\nOur benchmark incorporates a wide range of datasets, evaluates state-of-the-art\ndefence mechanisms, and extends the assessment to include critical tasks such\nas single-sentence classification, similarity and paraphrase identification,\nnatural language inference, and commonsense reasoning. This work not only\nserves as a valuable resource for researchers and practitioners in the field of\nadversarial robustness but also identifies key areas for future research in\ntextual adversarial defence. By establishing a new standard for benchmarking in\nthis domain, we aim to accelerate progress towards more robust and reliable\nnatural language processing systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vulnerability of deep learning models to adversarial attacks. While various\ndefence mechanisms have been proposed, there is a lack of comprehensive\nbenchmarks that evaluate these defences across diverse datasets, models, and\ntasks. In this work, we address this gap by presenting an extensive benchmark\nfor textual adversarial defence that significantly expands upon previous work.\nOur benchmark incorporates a wide range of datasets, evaluates state-of-the-art\ndefence mechanisms, and extends the assessment to include critical tasks such\nas single-sentence classification, similarity and paraphrase identification,\nnatural language inference, and commonsense reasoning. This work not only\nserves as a valuable resource for researchers and practitioners in the field of\nadversarial robustness but also identifies key areas for future research in\ntextual adversarial defence. By establishing a new standard for benchmarking in\nthis domain, we aim to accelerate progress towards more robust and reliable\nnatural language processing systems."
                },
                "authors": [
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "Will be presented as an oral in-person presentation at the conference\n  of COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02636v1",
                "updated": "2025-01-05T19:48:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    19,
                    48,
                    44,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T19:48:44Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    19,
                    48,
                    44,
                    6,
                    5,
                    0
                ],
                "title": "The quest for a stable disk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quest for a stable disk"
                },
                "summary": "The majority of disk galaxies manifest spirals and/or bars that are believed\nto result from dynamical instabilities. However, some galaxies have featureless\ndisks, which are therefore inferred to be dynamically stable. Yet despite many\nyears of effort, theorists have been unable to construct realistic models of\ngalaxy disks that possess no instabilities and therefore could remain\nfeatureless. This conclusion has been reached through simulations for the most\npart, some of which have been confirmed by linear stability analyses. Toomre\nclaimed that the Mestel disk, embedded in an equal mass halo, to be a notable\ncounter-example, but his prediction of stability could not be reproduced in\nsimulations due to complicated non-linear effects that caused secular growth of\nPoisson noise-driven disturbances until strong features emerged. Here we\nrevisit this issue and show that simply eliminating the most nearly circular\norbits from Toomre's disk model can inhibit troublesome secular growth. We also\npresent both 2D and 3D simulations of particle disks that remain featureless\nfor over 50 orbit periods. We report that spiral evolution naturally depletes\ncircular orbits and that the radial velocity distribution in the featureless\ndisks of S0 galaxies should have negative kurtosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The majority of disk galaxies manifest spirals and/or bars that are believed\nto result from dynamical instabilities. However, some galaxies have featureless\ndisks, which are therefore inferred to be dynamically stable. Yet despite many\nyears of effort, theorists have been unable to construct realistic models of\ngalaxy disks that possess no instabilities and therefore could remain\nfeatureless. This conclusion has been reached through simulations for the most\npart, some of which have been confirmed by linear stability analyses. Toomre\nclaimed that the Mestel disk, embedded in an equal mass halo, to be a notable\ncounter-example, but his prediction of stability could not be reproduced in\nsimulations due to complicated non-linear effects that caused secular growth of\nPoisson noise-driven disturbances until strong features emerged. Here we\nrevisit this issue and show that simply eliminating the most nearly circular\norbits from Toomre's disk model can inhibit troublesome secular growth. We also\npresent both 2D and 3D simulations of particle disks that remain featureless\nfor over 50 orbit periods. We report that spiral evolution naturally depletes\ncircular orbits and that the radial velocity distribution in the featureless\ndisks of S0 galaxies should have negative kurtosis."
                },
                "authors": [
                    {
                        "name": "J A Sellwood"
                    },
                    {
                        "name": "R G Carlberg"
                    }
                ],
                "author_detail": {
                    "name": "R G Carlberg"
                },
                "arxiv_affiliation": "U Toronto",
                "author": "R G Carlberg",
                "arxiv_comment": "10 pages, 10 figures, revised version submitted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02634v1",
                "updated": "2025-01-05T19:44:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    19,
                    44,
                    36,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T19:44:36Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    19,
                    44,
                    36,
                    6,
                    5,
                    0
                ],
                "title": "Optimal Inference of Asynchronous Boolean Network Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Inference of Asynchronous Boolean Network Models"
                },
                "summary": "Associations between phenotype and genomic and epigenomic markers are often\nderived by correlation. Systems Biology aims to make more robust connections\nand uncover broader insights by modeling the cellular mechanisms that produce a\nphenotype. The question of choosing the modeling methodology is of central\nimportance. A model that does not capture biological reality closely enough\nwill not explain the system's behavior. At the same time, highly detailed\nmodels suffer from computational limitations and are likely to overfit the\ndata. Boolean networks strike a balance between complexity and descriptiveness\nand thus have received increasing interest. We previously described an\nalgorithm for fitting Boolean networks to high-throughout experimental data\nthat finds the optimal network with respect to the information in a given\ndataset. In this work, we describe a simple extension that enables the modeling\nof asynchronous dynamics, i.e. different reaction times for different network\nnodes. Our approach greatly simplifies the construction of Boolean network\nmodels for time-series datasets, where asynchronicty often occurs. We\ndemonstrate our methodology by integrating real data from three datasets, and\nprovide an implementation that can be used by the community for network\nreconstruction using any high-throughout dataset. Our approach significantly\nexpands the applicability of the Boolean network model to experimental data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Associations between phenotype and genomic and epigenomic markers are often\nderived by correlation. Systems Biology aims to make more robust connections\nand uncover broader insights by modeling the cellular mechanisms that produce a\nphenotype. The question of choosing the modeling methodology is of central\nimportance. A model that does not capture biological reality closely enough\nwill not explain the system's behavior. At the same time, highly detailed\nmodels suffer from computational limitations and are likely to overfit the\ndata. Boolean networks strike a balance between complexity and descriptiveness\nand thus have received increasing interest. We previously described an\nalgorithm for fitting Boolean networks to high-throughout experimental data\nthat finds the optimal network with respect to the information in a given\ndataset. In this work, we describe a simple extension that enables the modeling\nof asynchronous dynamics, i.e. different reaction times for different network\nnodes. Our approach greatly simplifies the construction of Boolean network\nmodels for time-series datasets, where asynchronicty often occurs. We\ndemonstrate our methodology by integrating real data from three datasets, and\nprovide an implementation that can be used by the community for network\nreconstruction using any high-throughout dataset. Our approach significantly\nexpands the applicability of the Boolean network model to experimental data."
                },
                "authors": [
                    {
                        "name": "Guy Karlebach"
                    }
                ],
                "author_detail": {
                    "name": "Guy Karlebach"
                },
                "author": "Guy Karlebach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.MN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.03226v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03226v1",
                "updated": "2025-01-06T18:59:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    18,
                    59,
                    13,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T18:59:13Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    18,
                    59,
                    13,
                    0,
                    6,
                    0
                ],
                "title": "BoostStep: Boosting mathematical capability of Large Language Models via\n  improved single-step reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BoostStep: Boosting mathematical capability of Large Language Models via\n  improved single-step reasoning"
                },
                "summary": "Cutting-edge large language models (LLMs) demonstrate promising performance\nin solving complex math problems with a divide-and-conquer pipeline and the\nassistance of in-context learning (ICL) examples. However, their potential for\nimprovement is limited by two critical problems within their ICL examples:\ngranularity-mismatch and the ensuing negative-effect noise problem.\nSpecifically, the LLMs are capable of the dividing process yet mostly failed by\ninaccurate reasoning within a few conquer steps, while the ICL examples\nretrieved in question-grained sometimes lack relevant steps for a specific\nchallenging reasoning step. Further, this disconnect may hinder the correct\nreasoning due to its irrelevance. To this end, we focus on improving the\nreasoning quality within each step and present BoostStep. BoostStep aligns the\ngranularity between the retrieving and reasoning on step grained, and provides\nhighly related ICL examples for each reasoning step with a novel `first-try'\nstrategy. BoostStep provides more relevant examples than the coarse\nquestion-grained strategy, enhancing the model reasoning quality within each\nstep steadily. BoostStep is a general and robust reasoning-enhancing method\nthat not only improves standalone reasoning performance but also integrates\nseamlessly with Monte Carlo Tree Search methods (MCTS) to refine both candidate\ngeneration and decision-making. Quantitatively, it improves GPT-4o and\nQwen2.5-Math-72B by 3.6\\% and 2.0\\% respectively on various mathematical\nbenchmarks, and 7.5\\% gain combined with MCTS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cutting-edge large language models (LLMs) demonstrate promising performance\nin solving complex math problems with a divide-and-conquer pipeline and the\nassistance of in-context learning (ICL) examples. However, their potential for\nimprovement is limited by two critical problems within their ICL examples:\ngranularity-mismatch and the ensuing negative-effect noise problem.\nSpecifically, the LLMs are capable of the dividing process yet mostly failed by\ninaccurate reasoning within a few conquer steps, while the ICL examples\nretrieved in question-grained sometimes lack relevant steps for a specific\nchallenging reasoning step. Further, this disconnect may hinder the correct\nreasoning due to its irrelevance. To this end, we focus on improving the\nreasoning quality within each step and present BoostStep. BoostStep aligns the\ngranularity between the retrieving and reasoning on step grained, and provides\nhighly related ICL examples for each reasoning step with a novel `first-try'\nstrategy. BoostStep provides more relevant examples than the coarse\nquestion-grained strategy, enhancing the model reasoning quality within each\nstep steadily. BoostStep is a general and robust reasoning-enhancing method\nthat not only improves standalone reasoning performance but also integrates\nseamlessly with Monte Carlo Tree Search methods (MCTS) to refine both candidate\ngeneration and decision-making. Quantitatively, it improves GPT-4o and\nQwen2.5-Math-72B by 3.6\\% and 2.0\\% respectively on various mathematical\nbenchmarks, and 7.5\\% gain combined with MCTS."
                },
                "authors": [
                    {
                        "name": "Beichen Zhang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Xiaoyi Dong"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Pan Zhang"
                    },
                    {
                        "name": "Haodong Duan"
                    },
                    {
                        "name": "Yuhang Cao"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang",
                "arxiv_comment": "Codes and Data are available at\n  https://github.com/beichenzbc/BoostStep",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03226v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03218v1",
                "updated": "2025-01-06T18:55:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    18,
                    55,
                    10,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T18:55:10Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    18,
                    55,
                    10,
                    0,
                    6,
                    0
                ],
                "title": "Dispider: Enabling Video LLMs with Active Real-Time Interaction via\n  Disentangled Perception, Decision, and Reaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dispider: Enabling Video LLMs with Active Real-Time Interaction via\n  Disentangled Perception, Decision, and Reaction"
                },
                "summary": "Active Real-time interaction with video LLMs introduces a new paradigm for\nhuman-computer interaction, where the model not only understands user intent\nbut also responds while continuously processing streaming video on the fly.\nUnlike offline video LLMs, which analyze the entire video before answering\nquestions, active real-time interaction requires three capabilities: 1)\nPerception: real-time video monitoring and interaction capturing. 2) Decision:\nraising proactive interaction in proper situations, 3) Reaction: continuous\ninteraction with users. However, inherent conflicts exist among the desired\ncapabilities. The Decision and Reaction require a contrary Perception scale and\ngrain, and the autoregressive decoding blocks the real-time Perception and\nDecision during the Reaction. To unify the conflicted capabilities within a\nharmonious system, we present Dispider, a system that disentangles Perception,\nDecision, and Reaction. Dispider features a lightweight proactive streaming\nvideo processing module that tracks the video stream and identifies optimal\nmoments for interaction. Once the interaction is triggered, an asynchronous\ninteraction module provides detailed responses, while the processing module\ncontinues to monitor the video in the meantime. Our disentangled and\nasynchronous design ensures timely, contextually accurate, and computationally\nefficient responses, making Dispider ideal for active real-time interaction for\nlong-duration video streams. Experiments show that Dispider not only maintains\nstrong performance in conventional video QA tasks, but also significantly\nsurpasses previous online models in streaming scenario responses, thereby\nvalidating the effectiveness of our architecture. The code and model are\nreleased at \\url{https://github.com/Mark12Ding/Dispider}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Real-time interaction with video LLMs introduces a new paradigm for\nhuman-computer interaction, where the model not only understands user intent\nbut also responds while continuously processing streaming video on the fly.\nUnlike offline video LLMs, which analyze the entire video before answering\nquestions, active real-time interaction requires three capabilities: 1)\nPerception: real-time video monitoring and interaction capturing. 2) Decision:\nraising proactive interaction in proper situations, 3) Reaction: continuous\ninteraction with users. However, inherent conflicts exist among the desired\ncapabilities. The Decision and Reaction require a contrary Perception scale and\ngrain, and the autoregressive decoding blocks the real-time Perception and\nDecision during the Reaction. To unify the conflicted capabilities within a\nharmonious system, we present Dispider, a system that disentangles Perception,\nDecision, and Reaction. Dispider features a lightweight proactive streaming\nvideo processing module that tracks the video stream and identifies optimal\nmoments for interaction. Once the interaction is triggered, an asynchronous\ninteraction module provides detailed responses, while the processing module\ncontinues to monitor the video in the meantime. Our disentangled and\nasynchronous design ensures timely, contextually accurate, and computationally\nefficient responses, making Dispider ideal for active real-time interaction for\nlong-duration video streams. Experiments show that Dispider not only maintains\nstrong performance in conventional video QA tasks, but also significantly\nsurpasses previous online models in streaming scenario responses, thereby\nvalidating the effectiveness of our architecture. The code and model are\nreleased at \\url{https://github.com/Mark12Ding/Dispider}."
                },
                "authors": [
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Shuangrui Ding"
                    },
                    {
                        "name": "Xiaoyi Dong"
                    },
                    {
                        "name": "Pan Zhang"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Yuhang Cao"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03212v1",
                "updated": "2025-01-06T18:46:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    18,
                    46,
                    53,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T18:46:53Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    18,
                    46,
                    53,
                    0,
                    6,
                    0
                ],
                "title": "Leveraging Explainable AI for LLM Text Attribution: Differentiating\n  Human-Written and Multiple LLMs-Generated Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Explainable AI for LLM Text Attribution: Differentiating\n  Human-Written and Multiple LLMs-Generated Text"
                },
                "summary": "The development of Generative AI Large Language Models (LLMs) raised the\nalarm regarding identifying content produced through generative AI or humans.\nIn one case, issues arise when students heavily rely on such tools in a manner\nthat can affect the development of their writing or coding skills. Other issues\nof plagiarism also apply. This study aims to support efforts to detect and\nidentify textual content generated using LLM tools. We hypothesize that\nLLMs-generated text is detectable by machine learning (ML), and investigate ML\nmodels that can recognize and differentiate texts generated by multiple LLMs\ntools. We leverage several ML and Deep Learning (DL) algorithms such as Random\nForest (RF), and Recurrent Neural Networks (RNN), and utilized Explainable\nArtificial Intelligence (XAI) to understand the important features in\nattribution. Our method is divided into 1) binary classification to\ndifferentiate between human-written and AI-text, and 2) multi classification,\nto differentiate between human-written text and the text generated by the five\ndifferent LLM tools (ChatGPT, LLaMA, Google Bard, Claude, and Perplexity).\nResults show high accuracy in the multi and binary classification. Our model\noutperformed GPTZero with 98.5\\% accuracy to 78.3\\%. Notably, GPTZero was\nunable to recognize about 4.2\\% of the observations, but our model was able to\nrecognize the complete test dataset. XAI results showed that understanding\nfeature importance across different classes enables detailed author/source\nprofiles. Further, aiding in attribution and supporting plagiarism detection by\nhighlighting unique stylistic and structural elements ensuring robust content\noriginality verification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of Generative AI Large Language Models (LLMs) raised the\nalarm regarding identifying content produced through generative AI or humans.\nIn one case, issues arise when students heavily rely on such tools in a manner\nthat can affect the development of their writing or coding skills. Other issues\nof plagiarism also apply. This study aims to support efforts to detect and\nidentify textual content generated using LLM tools. We hypothesize that\nLLMs-generated text is detectable by machine learning (ML), and investigate ML\nmodels that can recognize and differentiate texts generated by multiple LLMs\ntools. We leverage several ML and Deep Learning (DL) algorithms such as Random\nForest (RF), and Recurrent Neural Networks (RNN), and utilized Explainable\nArtificial Intelligence (XAI) to understand the important features in\nattribution. Our method is divided into 1) binary classification to\ndifferentiate between human-written and AI-text, and 2) multi classification,\nto differentiate between human-written text and the text generated by the five\ndifferent LLM tools (ChatGPT, LLaMA, Google Bard, Claude, and Perplexity).\nResults show high accuracy in the multi and binary classification. Our model\noutperformed GPTZero with 98.5\\% accuracy to 78.3\\%. Notably, GPTZero was\nunable to recognize about 4.2\\% of the observations, but our model was able to\nrecognize the complete test dataset. XAI results showed that understanding\nfeature importance across different classes enables detailed author/source\nprofiles. Further, aiding in attribution and supporting plagiarism detection by\nhighlighting unique stylistic and structural elements ensuring robust content\noriginality verification."
                },
                "authors": [
                    {
                        "name": "Ayat Najjar"
                    },
                    {
                        "name": "Huthaifa I. Ashqar"
                    },
                    {
                        "name": "Omar Darwish"
                    },
                    {
                        "name": "Eman Hammad"
                    }
                ],
                "author_detail": {
                    "name": "Eman Hammad"
                },
                "author": "Eman Hammad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03203v1",
                "updated": "2025-01-06T18:34:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    18,
                    34,
                    20,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T18:34:20Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    18,
                    34,
                    20,
                    0,
                    6,
                    0
                ],
                "title": "Detecting AI-Generated Text in Educational Content: Leveraging Machine\n  Learning and Explainable AI for Academic Integrity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting AI-Generated Text in Educational Content: Leveraging Machine\n  Learning and Explainable AI for Academic Integrity"
                },
                "summary": "This study seeks to enhance academic integrity by providing tools to detect\nAI-generated content in student work using advanced technologies. The findings\npromote transparency and accountability, helping educators maintain ethical\nstandards and supporting the responsible integration of AI in education. A key\ncontribution of this work is the generation of the CyberHumanAI dataset, which\nhas 1000 observations, 500 of which are written by humans and the other 500\nproduced by ChatGPT. We evaluate various machine learning (ML) and deep\nlearning (DL) algorithms on the CyberHumanAI dataset comparing human-written\nand AI-generated content from Large Language Models (LLMs) (i.e., ChatGPT).\nResults demonstrate that traditional ML algorithms, specifically XGBoost and\nRandom Forest, achieve high performance (83% and 81% accuracies respectively).\nResults also show that classifying shorter content seems to be more challenging\nthan classifying longer content. Further, using Explainable Artificial\nIntelligence (XAI) we identify discriminative features influencing the ML\nmodel's predictions, where human-written content tends to use a practical\nlanguage (e.g., use and allow). Meanwhile AI-generated text is characterized by\nmore abstract and formal terms (e.g., realm and employ). Finally, a comparative\nanalysis with GPTZero show that our narrowly focused, simple, and fine-tuned\nmodel can outperform generalized systems like GPTZero. The proposed model\nachieved approximately 77.5% accuracy compared to GPTZero's 48.5% accuracy when\ntasked to classify Pure AI, Pure Human, and mixed class. GPTZero showed a\ntendency to classify challenging and small-content cases as either mixed or\nunrecognized while our proposed model showed a more balanced performance across\nthe three classes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study seeks to enhance academic integrity by providing tools to detect\nAI-generated content in student work using advanced technologies. The findings\npromote transparency and accountability, helping educators maintain ethical\nstandards and supporting the responsible integration of AI in education. A key\ncontribution of this work is the generation of the CyberHumanAI dataset, which\nhas 1000 observations, 500 of which are written by humans and the other 500\nproduced by ChatGPT. We evaluate various machine learning (ML) and deep\nlearning (DL) algorithms on the CyberHumanAI dataset comparing human-written\nand AI-generated content from Large Language Models (LLMs) (i.e., ChatGPT).\nResults demonstrate that traditional ML algorithms, specifically XGBoost and\nRandom Forest, achieve high performance (83% and 81% accuracies respectively).\nResults also show that classifying shorter content seems to be more challenging\nthan classifying longer content. Further, using Explainable Artificial\nIntelligence (XAI) we identify discriminative features influencing the ML\nmodel's predictions, where human-written content tends to use a practical\nlanguage (e.g., use and allow). Meanwhile AI-generated text is characterized by\nmore abstract and formal terms (e.g., realm and employ). Finally, a comparative\nanalysis with GPTZero show that our narrowly focused, simple, and fine-tuned\nmodel can outperform generalized systems like GPTZero. The proposed model\nachieved approximately 77.5% accuracy compared to GPTZero's 48.5% accuracy when\ntasked to classify Pure AI, Pure Human, and mixed class. GPTZero showed a\ntendency to classify challenging and small-content cases as either mixed or\nunrecognized while our proposed model showed a more balanced performance across\nthe three classes."
                },
                "authors": [
                    {
                        "name": "Ayat A. Najjar"
                    },
                    {
                        "name": "Huthaifa I. Ashqar"
                    },
                    {
                        "name": "Omar A. Darwish"
                    },
                    {
                        "name": "Eman Hammad"
                    }
                ],
                "author_detail": {
                    "name": "Eman Hammad"
                },
                "author": "Eman Hammad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03200v1",
                "updated": "2025-01-06T18:28:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    18,
                    28,
                    4,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T18:28:04Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    18,
                    28,
                    4,
                    0,
                    6,
                    0
                ],
                "title": "The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground\n  Responses to Long-Form Input",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground\n  Responses to Long-Form Input"
                },
                "summary": "We introduce FACTS Grounding, an online leaderboard and associated benchmark\nthat evaluates language models' ability to generate text that is factually\naccurate with respect to given context in the user prompt. In our benchmark,\neach prompt includes a user request and a full document, with a maximum length\nof 32k tokens, requiring long-form responses. The long-form responses are\nrequired to be fully grounded in the provided context document while fulfilling\nthe user request. Models are evaluated using automated judge models in two\nphases: (1) responses are disqualified if they do not fulfill the user request;\n(2) they are judged as accurate if the response is fully grounded in the\nprovided document. The automated judge models were comprehensively evaluated\nagainst a held-out test-set to pick the best prompt template, and the final\nfactuality score is an aggregate of multiple judge models to mitigate\nevaluation bias. The FACTS Grounding leaderboard will be actively maintained\nover time, and contains both public and private splits to allow for external\nparticipation while guarding the integrity of the leaderboard. It can be found\nat https://www.kaggle.com/facts-leaderboard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce FACTS Grounding, an online leaderboard and associated benchmark\nthat evaluates language models' ability to generate text that is factually\naccurate with respect to given context in the user prompt. In our benchmark,\neach prompt includes a user request and a full document, with a maximum length\nof 32k tokens, requiring long-form responses. The long-form responses are\nrequired to be fully grounded in the provided context document while fulfilling\nthe user request. Models are evaluated using automated judge models in two\nphases: (1) responses are disqualified if they do not fulfill the user request;\n(2) they are judged as accurate if the response is fully grounded in the\nprovided document. The automated judge models were comprehensively evaluated\nagainst a held-out test-set to pick the best prompt template, and the final\nfactuality score is an aggregate of multiple judge models to mitigate\nevaluation bias. The FACTS Grounding leaderboard will be actively maintained\nover time, and contains both public and private splits to allow for external\nparticipation while guarding the integrity of the leaderboard. It can be found\nat https://www.kaggle.com/facts-leaderboard."
                },
                "authors": [
                    {
                        "name": "Alon Jacovi"
                    },
                    {
                        "name": "Andrew Wang"
                    },
                    {
                        "name": "Chris Alberti"
                    },
                    {
                        "name": "Connie Tao"
                    },
                    {
                        "name": "Jon Lipovetz"
                    },
                    {
                        "name": "Kate Olszewska"
                    },
                    {
                        "name": "Lukas Haas"
                    },
                    {
                        "name": "Michelle Liu"
                    },
                    {
                        "name": "Nate Keating"
                    },
                    {
                        "name": "Adam Bloniarz"
                    },
                    {
                        "name": "Carl Saroufim"
                    },
                    {
                        "name": "Corey Fry"
                    },
                    {
                        "name": "Dror Marcus"
                    },
                    {
                        "name": "Doron Kukliansky"
                    },
                    {
                        "name": "Gaurav Singh Tomar"
                    },
                    {
                        "name": "James Swirhun"
                    },
                    {
                        "name": "Jinwei Xing"
                    },
                    {
                        "name": "Lily Wang"
                    },
                    {
                        "name": "Madhu Gurumurthy"
                    },
                    {
                        "name": "Michael Aaron"
                    },
                    {
                        "name": "Moran Ambar"
                    },
                    {
                        "name": "Rachana Fellinger"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Zizhao Zhang"
                    },
                    {
                        "name": "Sasha Goldshtein"
                    },
                    {
                        "name": "Dipanjan Das"
                    }
                ],
                "author_detail": {
                    "name": "Dipanjan Das"
                },
                "author": "Dipanjan Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14746v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14746v2",
                "updated": "2025-01-06T18:25:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    18,
                    25,
                    13,
                    0,
                    6,
                    0
                ],
                "published": "2024-02-22T18:06:19Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    18,
                    6,
                    19,
                    3,
                    53,
                    0
                ],
                "title": "Scaling Efficient LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Efficient LLMs"
                },
                "summary": "Trained LLMs are typically sparse in that most of the parameters are zero,\nraising questions on efficiency. In response, we inquire into efficient LLMs,\ni.e. those with the fewest parameters that achieve the desired accuracy on a\ntraining corpus. Specifically, we compare theoretical and empirical estimates\nfor training loss to obtain upper and lower bounds on the number of unique\nsequences in a natural training corpus as a function of its size. Our result\nimplies (1) to double the number of skills represented in a training corpus,\nthe corpus must scale roughly eighteen fold (2) for efficient LLMs, the number\nof parameters N and the size D of a natural training corpus scale as $N \\propto\nD^{0.24} (3) if the number of parameters of an LLM is smaller than the number\nof unique sequences in the training corpus, scaling up can uncover emergent\nskills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trained LLMs are typically sparse in that most of the parameters are zero,\nraising questions on efficiency. In response, we inquire into efficient LLMs,\ni.e. those with the fewest parameters that achieve the desired accuracy on a\ntraining corpus. Specifically, we compare theoretical and empirical estimates\nfor training loss to obtain upper and lower bounds on the number of unique\nsequences in a natural training corpus as a function of its size. Our result\nimplies (1) to double the number of skills represented in a training corpus,\nthe corpus must scale roughly eighteen fold (2) for efficient LLMs, the number\nof parameters N and the size D of a natural training corpus scale as $N \\propto\nD^{0.24} (3) if the number of parameters of an LLM is smaller than the number\nof unique sequences in the training corpus, scaling up can uncover emergent\nskills."
                },
                "authors": [
                    {
                        "name": "B. N. Kausik"
                    }
                ],
                "author_detail": {
                    "name": "B. N. Kausik"
                },
                "author": "B. N. Kausik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14746v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14746v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16133v2",
                "updated": "2025-01-06T18:23:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    18,
                    23,
                    41,
                    0,
                    6,
                    0
                ],
                "published": "2024-11-25T06:48:38Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    6,
                    48,
                    38,
                    0,
                    330,
                    0
                ],
                "title": "Context Awareness Gate For Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Awareness Gate For Retrieval Augmented Generation"
                },
                "summary": "Retrieval Augmented Generation (RAG) has emerged as a widely adopted approach\nto mitigate the limitations of large language models (LLMs) in answering\ndomain-specific questions. Previous research has predominantly focused on\nimproving the accuracy and quality of retrieved data chunks to enhance the\noverall performance of the generation pipeline. However, despite ongoing\nadvancements, the critical issue of retrieving irrelevant information -- which\ncan impair the ability of the model to utilize its internal knowledge\neffectively -- has received minimal attention. In this work, we investigate the\nimpact of retrieving irrelevant information in open-domain question answering,\nhighlighting its significant detrimental effect on the quality of LLM outputs.\nTo address this challenge, we propose the Context Awareness Gate (CAG)\narchitecture, a novel mechanism that dynamically adjusts the LLMs' input prompt\nbased on whether the user query necessitates external context retrieval.\nAdditionally, we introduce the Vector Candidates method, a core mathematical\ncomponent of CAG that is statistical, LLM-independent, and highly scalable. We\nfurther examine the distributions of relationships between contexts and\nquestions, presenting a statistical analysis of these distributions. This\nanalysis can be leveraged to enhance the context retrieval process in Retrieval\nAugmented Generation (RAG) systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) has emerged as a widely adopted approach\nto mitigate the limitations of large language models (LLMs) in answering\ndomain-specific questions. Previous research has predominantly focused on\nimproving the accuracy and quality of retrieved data chunks to enhance the\noverall performance of the generation pipeline. However, despite ongoing\nadvancements, the critical issue of retrieving irrelevant information -- which\ncan impair the ability of the model to utilize its internal knowledge\neffectively -- has received minimal attention. In this work, we investigate the\nimpact of retrieving irrelevant information in open-domain question answering,\nhighlighting its significant detrimental effect on the quality of LLM outputs.\nTo address this challenge, we propose the Context Awareness Gate (CAG)\narchitecture, a novel mechanism that dynamically adjusts the LLMs' input prompt\nbased on whether the user query necessitates external context retrieval.\nAdditionally, we introduce the Vector Candidates method, a core mathematical\ncomponent of CAG that is statistical, LLM-independent, and highly scalable. We\nfurther examine the distributions of relationships between contexts and\nquestions, presenting a statistical analysis of these distributions. This\nanalysis can be leveraged to enhance the context retrieval process in Retrieval\nAugmented Generation (RAG) systems."
                },
                "authors": [
                    {
                        "name": "Mohammad Hassan Heydari"
                    },
                    {
                        "name": "Arshia Hemmat"
                    },
                    {
                        "name": "Erfan Naman"
                    },
                    {
                        "name": "Afsaneh Fatemi"
                    }
                ],
                "author_detail": {
                    "name": "Afsaneh Fatemi"
                },
                "author": "Afsaneh Fatemi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16353v2",
                "updated": "2025-01-06T17:37:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    17,
                    37,
                    54,
                    0,
                    6,
                    0
                ],
                "published": "2024-11-25T13:04:28Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    4,
                    28,
                    0,
                    330,
                    0
                ],
                "title": "The Two-Hop Curse: LLMs trained on A$\\rightarrow$B, B$\\rightarrow$C fail\n  to learn A$\\rightarrow$C",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Two-Hop Curse: LLMs trained on A$\\rightarrow$B, B$\\rightarrow$C fail\n  to learn A$\\rightarrow$C"
                },
                "summary": "[Notice: This version is outdated. Recent research contradicts some key\nclaims; we are working on a major revision with more nuanced analysis. Please\nwait for the updated version.]\n  While LLMs excel at multi-hop questions (e.g. \"Who is the spouse of the\nperformer of Imagine?\") when using chain-of-thought reasoning (CoT), they\nstruggle when forced to reason internally (without CoT). Previous work on the\nsize and nature of this gap produced mixed evidence with inconclusive results.\nIn this paper, we introduce a controlled setting for investigating two-hop\nreasoning in LLMs, where the above-chance performance constitutes undeniable\nevidence for latent reasoning. We fine-tune LLMs (including Llama 3 8B Instruct\nand GPT-4o) on fictional facts and confirm that they generalize to answering\ntwo-hop questions about them using CoT. We find that models can perform latent\nreasoning when facts appear together during training or in the prompt. However,\nto our surprise, models completely fail at two-hop reasoning without CoT when\nlearned facts only appear in different documents, achieving chance-level\naccuracy and chance-level test loss. We call this complete failure to compose\nseparately learned facts the Two-Hop Curse. Moreover, we evaluate 9 frontier\nLLMs on real-world facts, finding that models completely fail at two-hop no-CoT\nreasoning for over half of question categories while maintaining partial\nsuccess with CoT across most categories. These results suggest that LLMs lack a\ngeneral capability for latent multi-hop reasoning independent of the question\ntype.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "[Notice: This version is outdated. Recent research contradicts some key\nclaims; we are working on a major revision with more nuanced analysis. Please\nwait for the updated version.]\n  While LLMs excel at multi-hop questions (e.g. \"Who is the spouse of the\nperformer of Imagine?\") when using chain-of-thought reasoning (CoT), they\nstruggle when forced to reason internally (without CoT). Previous work on the\nsize and nature of this gap produced mixed evidence with inconclusive results.\nIn this paper, we introduce a controlled setting for investigating two-hop\nreasoning in LLMs, where the above-chance performance constitutes undeniable\nevidence for latent reasoning. We fine-tune LLMs (including Llama 3 8B Instruct\nand GPT-4o) on fictional facts and confirm that they generalize to answering\ntwo-hop questions about them using CoT. We find that models can perform latent\nreasoning when facts appear together during training or in the prompt. However,\nto our surprise, models completely fail at two-hop reasoning without CoT when\nlearned facts only appear in different documents, achieving chance-level\naccuracy and chance-level test loss. We call this complete failure to compose\nseparately learned facts the Two-Hop Curse. Moreover, we evaluate 9 frontier\nLLMs on real-world facts, finding that models completely fail at two-hop no-CoT\nreasoning for over half of question categories while maintaining partial\nsuccess with CoT across most categories. These results suggest that LLMs lack a\ngeneral capability for latent multi-hop reasoning independent of the question\ntype."
                },
                "authors": [
                    {
                        "name": "Mikita Balesni"
                    },
                    {
                        "name": "Tomek Korbak"
                    },
                    {
                        "name": "Owain Evans"
                    }
                ],
                "author_detail": {
                    "name": "Owain Evans"
                },
                "author": "Owain Evans",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03166v1",
                "updated": "2025-01-06T17:36:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    17,
                    36,
                    9,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T17:36:09Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    17,
                    36,
                    9,
                    0,
                    6,
                    0
                ],
                "title": "Semantic Captioning: Benchmark Dataset and Graph-Aware Few-Shot\n  In-Context Learning for SQL2Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Captioning: Benchmark Dataset and Graph-Aware Few-Shot\n  In-Context Learning for SQL2Text"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in\nvarious NLP tasks, including semantic parsing, which trans lates natural\nlanguage into formal code representations. However, the reverse process,\ntranslating code into natural language, termed semantic captioning, has\nreceived less attention. This task is becoming increasingly important as LLMs\nare integrated into platforms for code generation, security analysis, and\neducational purposes. In this paper, we focus on the captioning of SQL query\n(SQL2Text) to address the critical need for understanding and explaining SQL\nqueries in an era where LLM-generated code poses potential security risks. We\nrepurpose Text2SQL datasets for SQL2Text by introducing an iterative ICL prompt\nusing GPT-4o to generate multiple additional utterances, which enhances the\nrobustness of the datasets for the reverse task. We conduct our experiments\nusing in-context learning (ICL) based on different sample selection methods,\nemphasizing smaller, more computationally efficient LLMs. Our findings\ndemonstrate that leveraging the inherent graph properties of SQL for ICL sample\nselection significantly outperforms random selection by up to 39% on BLEU score\nand provides better results than alternative methods. Dataset and codes are\npublished: \\url{https://github.com/aliwister/ast-icl}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance in\nvarious NLP tasks, including semantic parsing, which trans lates natural\nlanguage into formal code representations. However, the reverse process,\ntranslating code into natural language, termed semantic captioning, has\nreceived less attention. This task is becoming increasingly important as LLMs\nare integrated into platforms for code generation, security analysis, and\neducational purposes. In this paper, we focus on the captioning of SQL query\n(SQL2Text) to address the critical need for understanding and explaining SQL\nqueries in an era where LLM-generated code poses potential security risks. We\nrepurpose Text2SQL datasets for SQL2Text by introducing an iterative ICL prompt\nusing GPT-4o to generate multiple additional utterances, which enhances the\nrobustness of the datasets for the reverse task. We conduct our experiments\nusing in-context learning (ICL) based on different sample selection methods,\nemphasizing smaller, more computationally efficient LLMs. Our findings\ndemonstrate that leveraging the inherent graph properties of SQL for ICL sample\nselection significantly outperforms random selection by up to 39% on BLEU score\nand provides better results than alternative methods. Dataset and codes are\npublished: \\url{https://github.com/aliwister/ast-icl}."
                },
                "authors": [
                    {
                        "name": "Ali Al-Lawati"
                    },
                    {
                        "name": "Jason Lucas"
                    },
                    {
                        "name": "Prasenjit Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Prasenjit Mitra"
                },
                "author": "Prasenjit Mitra",
                "arxiv_comment": "Accepted to COLING'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19363v2",
                "updated": "2025-01-06T17:33:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    17,
                    33,
                    20,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-26T22:06:29Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    22,
                    6,
                    29,
                    3,
                    361,
                    0
                ],
                "title": "Large Language Models for Market Research: A Data-augmentation Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Market Research: A Data-augmentation Approach"
                },
                "summary": "Large Language Models (LLMs) have transformed artificial intelligence by\nexcelling in complex natural language processing tasks. Their ability to\ngenerate human-like text has opened new possibilities for market research,\nparticularly in conjoint analysis, where understanding consumer preferences is\nessential but often resource-intensive. Traditional survey-based methods face\nlimitations in scalability and cost, making LLM-generated data a promising\nalternative. However, while LLMs have the potential to simulate real consumer\nbehavior, recent studies highlight a significant gap between LLM-generated and\nhuman data, with biases introduced when substituting between the two. In this\npaper, we address this gap by proposing a novel statistical data augmentation\napproach that efficiently integrates LLM-generated data with real data in\nconjoint analysis. Our method leverages transfer learning principles to debias\nthe LLM-generated data using a small amount of human data. This results in\nstatistically robust estimators with consistent and asymptotically normal\nproperties, in contrast to naive approaches that simply substitute human data\nwith LLM-generated data, which can exacerbate bias. We validate our framework\nthrough an empirical study on COVID-19 vaccine preferences, demonstrating its\nsuperior ability to reduce estimation error and save data and costs by 24.9% to\n79.8%. In contrast, naive approaches fail to save data due to the inherent\nbiases in LLM-generated data compared to human data. Another empirical study on\nsports car choices validates the robustness of our results. Our findings\nsuggest that while LLM-generated data is not a direct substitute for human\nresponses, it can serve as a valuable complement when used within a robust\nstatistical framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed artificial intelligence by\nexcelling in complex natural language processing tasks. Their ability to\ngenerate human-like text has opened new possibilities for market research,\nparticularly in conjoint analysis, where understanding consumer preferences is\nessential but often resource-intensive. Traditional survey-based methods face\nlimitations in scalability and cost, making LLM-generated data a promising\nalternative. However, while LLMs have the potential to simulate real consumer\nbehavior, recent studies highlight a significant gap between LLM-generated and\nhuman data, with biases introduced when substituting between the two. In this\npaper, we address this gap by proposing a novel statistical data augmentation\napproach that efficiently integrates LLM-generated data with real data in\nconjoint analysis. Our method leverages transfer learning principles to debias\nthe LLM-generated data using a small amount of human data. This results in\nstatistically robust estimators with consistent and asymptotically normal\nproperties, in contrast to naive approaches that simply substitute human data\nwith LLM-generated data, which can exacerbate bias. We validate our framework\nthrough an empirical study on COVID-19 vaccine preferences, demonstrating its\nsuperior ability to reduce estimation error and save data and costs by 24.9% to\n79.8%. In contrast, naive approaches fail to save data due to the inherent\nbiases in LLM-generated data compared to human data. Another empirical study on\nsports car choices validates the robustness of our results. Our findings\nsuggest that while LLM-generated data is not a direct substitute for human\nresponses, it can serve as a valuable complement when used within a robust\nstatistical framework."
                },
                "authors": [
                    {
                        "name": "Mengxin Wang"
                    },
                    {
                        "name": "Dennis J. Zhang"
                    },
                    {
                        "name": "Heng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Heng Zhang"
                },
                "arxiv_affiliation": "W. P. Carey School of Business, Arizona State University",
                "author": "Heng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 90B60, 62F12",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.4; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03151v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03151v1",
                "updated": "2025-01-06T17:18:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    17,
                    18,
                    47,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T17:18:47Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    17,
                    18,
                    47,
                    0,
                    6,
                    0
                ],
                "title": "Large language models for artificial general intelligence (AGI): A\n  survey of foundational principles and approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models for artificial general intelligence (AGI): A\n  survey of foundational principles and approaches"
                },
                "summary": "Generative artificial intelligence (AI) systems based on large-scale\npretrained foundation models (PFMs) such as vision-language models, large\nlanguage models (LLMs), diffusion models and vision-language-action (VLA)\nmodels have demonstrated the ability to solve complex and truly non-trivial AI\nproblems in a wide variety of domains and contexts. Multimodal large language\nmodels (MLLMs), in particular, learn from vast and diverse data sources,\nallowing rich and nuanced representations of the world and, thereby, providing\nextensive capabilities, including the ability to reason, engage in meaningful\ndialog; collaborate with humans and other agents to jointly solve complex\nproblems; and understand social and emotional aspects of humans. Despite this\nimpressive feat, the cognitive abilities of state-of-the-art LLMs trained on\nlarge-scale datasets are still superficial and brittle. Consequently, generic\nLLMs are severely limited in their generalist capabilities. A number of\nfoundational problems -- embodiment, symbol grounding, causality and memory --\nare required to be addressed for LLMs to attain human-level general\nintelligence. These concepts are more aligned with human cognition and provide\nLLMs with inherent human-like cognitive properties that support the realization\nof physically-plausible, semantically meaningful, flexible and more\ngeneralizable knowledge and intelligence. In this work, we discuss the\naforementioned foundational issues and survey state-of-the art approaches for\nimplementing these concepts in LLMs. Specifically, we discuss how the\nprinciples of embodiment, symbol grounding, causality and memory can be\nleveraged toward the attainment of artificial general intelligence (AGI) in an\norganic manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative artificial intelligence (AI) systems based on large-scale\npretrained foundation models (PFMs) such as vision-language models, large\nlanguage models (LLMs), diffusion models and vision-language-action (VLA)\nmodels have demonstrated the ability to solve complex and truly non-trivial AI\nproblems in a wide variety of domains and contexts. Multimodal large language\nmodels (MLLMs), in particular, learn from vast and diverse data sources,\nallowing rich and nuanced representations of the world and, thereby, providing\nextensive capabilities, including the ability to reason, engage in meaningful\ndialog; collaborate with humans and other agents to jointly solve complex\nproblems; and understand social and emotional aspects of humans. Despite this\nimpressive feat, the cognitive abilities of state-of-the-art LLMs trained on\nlarge-scale datasets are still superficial and brittle. Consequently, generic\nLLMs are severely limited in their generalist capabilities. A number of\nfoundational problems -- embodiment, symbol grounding, causality and memory --\nare required to be addressed for LLMs to attain human-level general\nintelligence. These concepts are more aligned with human cognition and provide\nLLMs with inherent human-like cognitive properties that support the realization\nof physically-plausible, semantically meaningful, flexible and more\ngeneralizable knowledge and intelligence. In this work, we discuss the\naforementioned foundational issues and survey state-of-the art approaches for\nimplementing these concepts in LLMs. Specifically, we discuss how the\nprinciples of embodiment, symbol grounding, causality and memory can be\nleveraged toward the attainment of artificial general intelligence (AGI) in an\norganic manner."
                },
                "authors": [
                    {
                        "name": "Alhassan Mumuni"
                    },
                    {
                        "name": "Fuseini Mumuni"
                    }
                ],
                "author_detail": {
                    "name": "Fuseini Mumuni"
                },
                "author": "Fuseini Mumuni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03151v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03151v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03139v1",
                "updated": "2025-01-06T17:01:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    17,
                    1,
                    45,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T17:01:45Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    17,
                    1,
                    45,
                    0,
                    6,
                    0
                ],
                "title": "VicSim: Enhancing Victim Simulation with Emotional and Linguistic\n  Fidelity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VicSim: Enhancing Victim Simulation with Emotional and Linguistic\n  Fidelity"
                },
                "summary": "Scenario-based training has been widely adopted in many public service\nsectors. Recent advancements in Large Language Models (LLMs) have shown promise\nin simulating diverse personas to create these training scenarios. However,\nlittle is known about how LLMs can be developed to simulate victims for\nscenario-based training purposes. In this paper, we introduce VicSim (victim\nsimulator), a novel model that addresses three key dimensions of user\nsimulation: informational faithfulness, emotional dynamics, and language style\n(e.g., grammar usage). We pioneer the integration of scenario-based victim\nmodeling with GAN-based training workflow and key-information-based prompting,\naiming to enhance the realism of simulated victims. Our adversarial training\napproach teaches the discriminator to recognize grammar and emotional cues as\nreliable indicators of synthetic content. According to evaluations by human\nraters, the VicSim model outperforms GPT-4 in terms of human-likeness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scenario-based training has been widely adopted in many public service\nsectors. Recent advancements in Large Language Models (LLMs) have shown promise\nin simulating diverse personas to create these training scenarios. However,\nlittle is known about how LLMs can be developed to simulate victims for\nscenario-based training purposes. In this paper, we introduce VicSim (victim\nsimulator), a novel model that addresses three key dimensions of user\nsimulation: informational faithfulness, emotional dynamics, and language style\n(e.g., grammar usage). We pioneer the integration of scenario-based victim\nmodeling with GAN-based training workflow and key-information-based prompting,\naiming to enhance the realism of simulated victims. Our adversarial training\napproach teaches the discriminator to recognize grammar and emotional cues as\nreliable indicators of synthetic content. According to evaluations by human\nraters, the VicSim model outperforms GPT-4 in terms of human-likeness."
                },
                "authors": [
                    {
                        "name": "Yerong Li"
                    },
                    {
                        "name": "Yiren Liu"
                    },
                    {
                        "name": "Yun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yun Huang"
                },
                "author": "Yun Huang",
                "arxiv_comment": "21 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13147v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13147v3",
                "updated": "2025-01-06T16:49:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    49,
                    55,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-17T18:12:47Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    12,
                    47,
                    1,
                    352,
                    0
                ],
                "title": "Are Your LLMs Capable of Stable Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Your LLMs Capable of Stable Reasoning?"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has demonstrated\nremarkable progress in complex reasoning tasks. However, a significant\ndiscrepancy persists between benchmark performances and real-world\napplications. We identify this gap as primarily stemming from current\nevaluation protocols and metrics, which inadequately capture the full spectrum\nof LLM capabilities, particularly in complex reasoning tasks where both\naccuracy and consistency are crucial. This work makes two key contributions.\nFirst, we introduce G-Pass@k, a novel evaluation metric that provides a\ncontinuous assessment of model performance across multiple sampling attempts,\nquantifying both the model's peak performance potential and its stability.\nSecond, we present LiveMathBench, a dynamic benchmark comprising challenging,\ncontemporary mathematical problems designed to minimize data leakage risks\nduring evaluation. Through extensive experiments using G-Pass@k on\nstate-of-the-art LLMs with LiveMathBench, we provide comprehensive insights\ninto both their maximum capabilities and operational consistency. Our findings\nreveal substantial room for improvement in LLMs' \"realistic\" reasoning\ncapabilities, highlighting the need for more robust evaluation methods. The\nbenchmark and detailed results are available at:\nhttps://github.com/open-compass/GPassK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has demonstrated\nremarkable progress in complex reasoning tasks. However, a significant\ndiscrepancy persists between benchmark performances and real-world\napplications. We identify this gap as primarily stemming from current\nevaluation protocols and metrics, which inadequately capture the full spectrum\nof LLM capabilities, particularly in complex reasoning tasks where both\naccuracy and consistency are crucial. This work makes two key contributions.\nFirst, we introduce G-Pass@k, a novel evaluation metric that provides a\ncontinuous assessment of model performance across multiple sampling attempts,\nquantifying both the model's peak performance potential and its stability.\nSecond, we present LiveMathBench, a dynamic benchmark comprising challenging,\ncontemporary mathematical problems designed to minimize data leakage risks\nduring evaluation. Through extensive experiments using G-Pass@k on\nstate-of-the-art LLMs with LiveMathBench, we provide comprehensive insights\ninto both their maximum capabilities and operational consistency. Our findings\nreveal substantial room for improvement in LLMs' \"realistic\" reasoning\ncapabilities, highlighting the need for more robust evaluation methods. The\nbenchmark and detailed results are available at:\nhttps://github.com/open-compass/GPassK."
                },
                "authors": [
                    {
                        "name": "Junnan Liu"
                    },
                    {
                        "name": "Hongwei Liu"
                    },
                    {
                        "name": "Linchen Xiao"
                    },
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Kuikun Liu"
                    },
                    {
                        "name": "Songyang Gao"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13147v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13147v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.02368v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.02368v2",
                "updated": "2025-01-06T16:41:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    41,
                    9,
                    0,
                    6,
                    0
                ],
                "published": "2023-10-03T18:48:31Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    18,
                    48,
                    31,
                    1,
                    276,
                    0
                ],
                "title": "Reinforcement Learning from Automatic Feedback for High-Quality Unit\n  Test Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Automatic Feedback for High-Quality Unit\n  Test Generation"
                },
                "summary": "Software testing is a crucial aspect of software development, and the\ncreation of high-quality tests that adhere to best practices is essential for\neffective maintenance. Recently, Large Language Models (LLMs) have gained\npopularity for code generation, including the automated creation of test cases.\nHowever, these LLMs are often trained on vast amounts of publicly available\ncode, which may include test cases that do not adhere to best practices and may\neven contain test smells (anti-patterns). To address this issue, we propose a\nnovel technique called Reinforcement Learning from Static Quality Metrics\n(RLSQM). To begin, we analyze the anti-patterns generated by the LLM and show\nthat LLMs can generate undesirable test smells. Thus, we train specific reward\nmodels for each static quality metric, then utilize Proximal Policy\nOptimization (PPO) to train models for optimizing a single quality metric at a\ntime. Furthermore, we amalgamate these rewards into a unified reward model\naimed at capturing different best practices and quality aspects of tests. By\ncomparing RL-trained models with those trained using supervised learning, we\nprovide insights into how reliably utilize RL to improve test generation\nquality and into the effects of various training strategies. Our experimental\nresults demonstrate that the RL-optimized model consistently generated\nhigh-quality test cases compared to the base LLM, improving the model by up to\n21%, and successfully generates nearly 100% syntactically correct code. RLSQM\nalso outperformed GPT-4 on four out of seven metrics. This represents a\nsignificant step towards enhancing the overall efficiency and reliability of\nsoftware testing through Reinforcement Learning and static quality metrics. Our\ndata are available at https://figshare.com/s/ded476c8d4c221222849.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software testing is a crucial aspect of software development, and the\ncreation of high-quality tests that adhere to best practices is essential for\neffective maintenance. Recently, Large Language Models (LLMs) have gained\npopularity for code generation, including the automated creation of test cases.\nHowever, these LLMs are often trained on vast amounts of publicly available\ncode, which may include test cases that do not adhere to best practices and may\neven contain test smells (anti-patterns). To address this issue, we propose a\nnovel technique called Reinforcement Learning from Static Quality Metrics\n(RLSQM). To begin, we analyze the anti-patterns generated by the LLM and show\nthat LLMs can generate undesirable test smells. Thus, we train specific reward\nmodels for each static quality metric, then utilize Proximal Policy\nOptimization (PPO) to train models for optimizing a single quality metric at a\ntime. Furthermore, we amalgamate these rewards into a unified reward model\naimed at capturing different best practices and quality aspects of tests. By\ncomparing RL-trained models with those trained using supervised learning, we\nprovide insights into how reliably utilize RL to improve test generation\nquality and into the effects of various training strategies. Our experimental\nresults demonstrate that the RL-optimized model consistently generated\nhigh-quality test cases compared to the base LLM, improving the model by up to\n21%, and successfully generates nearly 100% syntactically correct code. RLSQM\nalso outperformed GPT-4 on four out of seven metrics. This represents a\nsignificant step towards enhancing the overall efficiency and reliability of\nsoftware testing through Reinforcement Learning and static quality metrics. Our\ndata are available at https://figshare.com/s/ded476c8d4c221222849."
                },
                "authors": [
                    {
                        "name": "Benjamin Steenhoek"
                    },
                    {
                        "name": "Michele Tufano"
                    },
                    {
                        "name": "Neel Sundaresan"
                    },
                    {
                        "name": "Alexey Svyatkovskiy"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Svyatkovskiy"
                },
                "author": "Alexey Svyatkovskiy",
                "arxiv_comment": "Accepted to DeepTest 2025 (ICSE Workshop). Previously this version\n  appeared as arXiv:2412.14308 which was submitted as a new work by accident",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.02368v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.02368v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10606v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10606v2",
                "updated": "2025-01-06T16:35:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    35,
                    59,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-13T23:19:21Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    23,
                    19,
                    21,
                    4,
                    348,
                    0
                ],
                "title": "Do Large Language Models Speak Scientific Workflows?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Speak Scientific Workflows?"
                },
                "summary": "With the advent of large language models (LLMs), there is a growing interest\nin applying LLMs to scientific tasks. In this work, we conduct an experimental\nstudy to explore applicability of LLMs for configuring, annotating,\ntranslating, explaining, and generating scientific workflows. We use 5\ndifferent workflow specific experiments and evaluate several open- and\nclosed-source language models using state-of-the-art workflow systems. Our\nstudies reveal that LLMs often struggle with workflow related tasks due to\ntheir lack of knowledge of scientific workflows. We further observe that the\nperformance of LLMs varies across experiments and workflow systems. Our\nfindings can help workflow developers and users in understanding LLMs\ncapabilities in scientific workflows, and motivate further research applying\nLLMs to workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of large language models (LLMs), there is a growing interest\nin applying LLMs to scientific tasks. In this work, we conduct an experimental\nstudy to explore applicability of LLMs for configuring, annotating,\ntranslating, explaining, and generating scientific workflows. We use 5\ndifferent workflow specific experiments and evaluate several open- and\nclosed-source language models using state-of-the-art workflow systems. Our\nstudies reveal that LLMs often struggle with workflow related tasks due to\ntheir lack of knowledge of scientific workflows. We further observe that the\nperformance of LLMs varies across experiments and workflow systems. Our\nfindings can help workflow developers and users in understanding LLMs\ncapabilities in scientific workflows, and motivate further research applying\nLLMs to workflows."
                },
                "authors": [
                    {
                        "name": "Orcun Yildiz"
                    },
                    {
                        "name": "Tom Peterka"
                    }
                ],
                "author_detail": {
                    "name": "Tom Peterka"
                },
                "author": "Tom Peterka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10606v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10606v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03126v1",
                "updated": "2025-01-06T16:34:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    34,
                    46,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T16:34:46Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    34,
                    46,
                    0,
                    6,
                    0
                ],
                "title": "CrowdProve: Community Proving for ZK Rollups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CrowdProve: Community Proving for ZK Rollups"
                },
                "summary": "Zero-Knowledge (ZK) rollups have become a popular solution for scaling\nblockchain systems, offering improved transaction throughput and reduced costs\nby aggregating Layer 2 transactions and submitting them as a single batch to a\nLayer 1 blockchain. However, the computational burden of generating validity\nproofs, a key feature of ZK rollups, presents significant challenges in terms\nof performance and decentralization. Current solutions rely on centralized\ninfrastructure to handle the computational tasks, limiting the scalability and\ndecentralization of rollup systems.\n  This paper proposes CrowdProve, a prover orchestration layer for outsourcing\ncomputation to unreliable commodity hardware run by a broad community of small\nprovers. We apply CrowdProve to proving transaction batches for a popular ZK\nrollup.\n  Through our experimental evaluation, we demonstrate that community proving\ncan achieve performance comparable to, and in some cases better than, existing\ncentralized deployments. Our results show that even systems utilizing modest\nhardware configurations can match the performance of centralized solutions,\nmaking community-based proof generation a viable and cost-effective\nalternative. CrowdProve allows both the rollup operator and community\nparticipants to benefit: the operator reduces infrastructure costs by\nleveraging idle community hardware, while community provers are compensated for\ntheir contributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Knowledge (ZK) rollups have become a popular solution for scaling\nblockchain systems, offering improved transaction throughput and reduced costs\nby aggregating Layer 2 transactions and submitting them as a single batch to a\nLayer 1 blockchain. However, the computational burden of generating validity\nproofs, a key feature of ZK rollups, presents significant challenges in terms\nof performance and decentralization. Current solutions rely on centralized\ninfrastructure to handle the computational tasks, limiting the scalability and\ndecentralization of rollup systems.\n  This paper proposes CrowdProve, a prover orchestration layer for outsourcing\ncomputation to unreliable commodity hardware run by a broad community of small\nprovers. We apply CrowdProve to proving transaction batches for a popular ZK\nrollup.\n  Through our experimental evaluation, we demonstrate that community proving\ncan achieve performance comparable to, and in some cases better than, existing\ncentralized deployments. Our results show that even systems utilizing modest\nhardware configurations can match the performance of centralized solutions,\nmaking community-based proof generation a viable and cost-effective\nalternative. CrowdProve allows both the rollup operator and community\nparticipants to benefit: the operator reduces infrastructure costs by\nleveraging idle community hardware, while community provers are compensated for\ntheir contributions."
                },
                "authors": [
                    {
                        "name": "John Stephan"
                    },
                    {
                        "name": "Matej Pavlovic"
                    },
                    {
                        "name": "Antonio Locascio"
                    },
                    {
                        "name": "Benjamin Livshits"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Livshits"
                },
                "author": "Benjamin Livshits",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19839v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19839v4",
                "updated": "2025-01-06T16:33:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    33,
                    58,
                    0,
                    6,
                    0
                ],
                "published": "2024-09-30T00:41:51Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    0,
                    41,
                    51,
                    0,
                    274,
                    0
                ],
                "title": "ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities"
                },
                "summary": "Forecasts of future events are essential inputs into informed\ndecision-making. Machine learning (ML) systems have the potential to deliver\nforecasts at scale, but there is no framework for evaluating the accuracy of ML\nsystems on a standardized set of forecasting questions. To address this gap, we\nintroduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML\nsystems on an automatically generated and regularly updated set of 1,000\nforecasting questions. To avoid any possibility of data leakage, ForecastBench\nis comprised solely of questions about future events that have no known answer\nat the time of submission. We quantify the capabilities of current ML systems\nby collecting forecasts from expert (human) forecasters, the general public,\nand LLMs on a random subset of questions from the benchmark ($N=200$). While\nLLMs have achieved super-human performance on many benchmarks, they perform\nless well here: expert forecasters outperform the top-performing LLM (p-value\n$<0.01$). We display system and human scores in a public leaderboard at\nwww.forecastbench.org.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasts of future events are essential inputs into informed\ndecision-making. Machine learning (ML) systems have the potential to deliver\nforecasts at scale, but there is no framework for evaluating the accuracy of ML\nsystems on a standardized set of forecasting questions. To address this gap, we\nintroduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML\nsystems on an automatically generated and regularly updated set of 1,000\nforecasting questions. To avoid any possibility of data leakage, ForecastBench\nis comprised solely of questions about future events that have no known answer\nat the time of submission. We quantify the capabilities of current ML systems\nby collecting forecasts from expert (human) forecasters, the general public,\nand LLMs on a random subset of questions from the benchmark ($N=200$). While\nLLMs have achieved super-human performance on many benchmarks, they perform\nless well here: expert forecasters outperform the top-performing LLM (p-value\n$<0.01$). We display system and human scores in a public leaderboard at\nwww.forecastbench.org."
                },
                "authors": [
                    {
                        "name": "Ezra Karger"
                    },
                    {
                        "name": "Houtan Bastani"
                    },
                    {
                        "name": "Chen Yueh-Han"
                    },
                    {
                        "name": "Zachary Jacobs"
                    },
                    {
                        "name": "Danny Halawi"
                    },
                    {
                        "name": "Fred Zhang"
                    },
                    {
                        "name": "Philip E. Tetlock"
                    }
                ],
                "author_detail": {
                    "name": "Philip E. Tetlock"
                },
                "author": "Philip E. Tetlock",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19839v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19839v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14308v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14308v2",
                "updated": "2025-01-06T16:31:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    31,
                    18,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-18T20:20:01Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    20,
                    20,
                    1,
                    2,
                    353,
                    0
                ],
                "title": "Reinforcement Learning from Automatic Feedback for High-Quality Unit\n  Test Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Automatic Feedback for High-Quality Unit\n  Test Generation"
                },
                "summary": "Software testing is a crucial but time-consuming aspect of software\ndevelopment, and recently, Large Language Models (LLMs) have gained popularity\nfor automated test case generation. However, because LLMs are trained on vast\namounts of open-source code, they often generate test cases that do not adhere\nto best practices and may even contain test smells (anti-patterns). To address\nthis issue, we propose Reinforcement Learning from Static Quality Metrics\n(RLSQM), wherein we utilize Reinforcement Learning to generate high-quality\nunit tests based on static analysis-based quality metrics. First, we analyzed\nLLM-generated tests and show that LLMs frequently do generate undesirable test\nsmells -- up to 37% of the time. Then, we implemented lightweight static\nanalysis-based reward model and trained LLMs using this reward model to\noptimize for five code quality metrics. Our experimental results demonstrate\nthat the RL-optimized Codex model consistently generated higher-quality test\ncases than the base LLM, improving quality metrics by up to 23%, and generated\nnearly 100% syntactically-correct code. RLSQM also outperformed GPT-4 on all\ncode quality metrics, in spite of training a substantially cheaper Codex model.\nWe provide insights into how reliably utilize RL to improve test generation\nquality and show that RLSQM is a significant step towards enhancing the overall\nefficiency and reliability of automated software testing. Our data are\navailable at https://doi.org/10.6084/m9.figshare.25983166.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software testing is a crucial but time-consuming aspect of software\ndevelopment, and recently, Large Language Models (LLMs) have gained popularity\nfor automated test case generation. However, because LLMs are trained on vast\namounts of open-source code, they often generate test cases that do not adhere\nto best practices and may even contain test smells (anti-patterns). To address\nthis issue, we propose Reinforcement Learning from Static Quality Metrics\n(RLSQM), wherein we utilize Reinforcement Learning to generate high-quality\nunit tests based on static analysis-based quality metrics. First, we analyzed\nLLM-generated tests and show that LLMs frequently do generate undesirable test\nsmells -- up to 37% of the time. Then, we implemented lightweight static\nanalysis-based reward model and trained LLMs using this reward model to\noptimize for five code quality metrics. Our experimental results demonstrate\nthat the RL-optimized Codex model consistently generated higher-quality test\ncases than the base LLM, improving quality metrics by up to 23%, and generated\nnearly 100% syntactically-correct code. RLSQM also outperformed GPT-4 on all\ncode quality metrics, in spite of training a substantially cheaper Codex model.\nWe provide insights into how reliably utilize RL to improve test generation\nquality and show that RLSQM is a significant step towards enhancing the overall\nefficiency and reliability of automated software testing. Our data are\navailable at https://doi.org/10.6084/m9.figshare.25983166."
                },
                "authors": [
                    {
                        "name": "Benjamin Steenhoek"
                    },
                    {
                        "name": "Michele Tufano"
                    },
                    {
                        "name": "Neel Sundaresan"
                    },
                    {
                        "name": "Alexey Svyatkovskiy"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Svyatkovskiy"
                },
                "author": "Alexey Svyatkovskiy",
                "arxiv_comment": "This work was intended as a replacement of arXiv:2310.02368 and any\n  subsequent updates will appear there",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14308v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14308v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19155v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19155v2",
                "updated": "2025-01-06T16:30:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    30,
                    35,
                    0,
                    6,
                    0
                ],
                "published": "2024-10-24T20:49:22Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    20,
                    49,
                    22,
                    3,
                    298,
                    0
                ],
                "title": "Lived Experience Not Found: LLMs Struggle to Align with Experts on\n  Addressing Adverse Drug Reactions from Psychiatric Medication Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lived Experience Not Found: LLMs Struggle to Align with Experts on\n  Addressing Adverse Drug Reactions from Psychiatric Medication Use"
                },
                "summary": "Adverse Drug Reactions (ADRs) from psychiatric medications are the leading\ncause of hospitalizations among mental health patients. With healthcare systems\nand online communities facing limitations in resolving ADR-related issues,\nLarge Language Models (LLMs) have the potential to fill this gap. Despite the\nincreasing capabilities of LLMs, past research has not explored their\ncapabilities in detecting ADRs related to psychiatric medications or in\nproviding effective harm reduction strategies. To address this, we introduce\nthe Psych-ADR benchmark and the Adverse Drug Reaction Response Assessment\n(ADRA) framework to systematically evaluate LLM performance in detecting ADR\nexpressions and delivering expert-aligned mitigation strategies. Our analyses\nshow that LLMs struggle with understanding the nuances of ADRs and\ndifferentiating between types of ADRs. While LLMs align with experts in terms\nof expressed emotions and tone of the text, their responses are more complex,\nharder to read, and only 70.86% aligned with expert strategies. Furthermore,\nthey provide less actionable advice by a margin of 12.32% on average. Our work\nprovides a comprehensive benchmark and evaluation framework for assessing LLMs\nin strategy-driven tasks within high-risk domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adverse Drug Reactions (ADRs) from psychiatric medications are the leading\ncause of hospitalizations among mental health patients. With healthcare systems\nand online communities facing limitations in resolving ADR-related issues,\nLarge Language Models (LLMs) have the potential to fill this gap. Despite the\nincreasing capabilities of LLMs, past research has not explored their\ncapabilities in detecting ADRs related to psychiatric medications or in\nproviding effective harm reduction strategies. To address this, we introduce\nthe Psych-ADR benchmark and the Adverse Drug Reaction Response Assessment\n(ADRA) framework to systematically evaluate LLM performance in detecting ADR\nexpressions and delivering expert-aligned mitigation strategies. Our analyses\nshow that LLMs struggle with understanding the nuances of ADRs and\ndifferentiating between types of ADRs. While LLMs align with experts in terms\nof expressed emotions and tone of the text, their responses are more complex,\nharder to read, and only 70.86% aligned with expert strategies. Furthermore,\nthey provide less actionable advice by a margin of 12.32% on average. Our work\nprovides a comprehensive benchmark and evaluation framework for assessing LLMs\nin strategy-driven tasks within high-risk domains."
                },
                "authors": [
                    {
                        "name": "Mohit Chandra"
                    },
                    {
                        "name": "Siddharth Sriraman"
                    },
                    {
                        "name": "Gaurav Verma"
                    },
                    {
                        "name": "Harneet Singh Khanuja"
                    },
                    {
                        "name": "Jose Suarez Campayo"
                    },
                    {
                        "name": "Zihang Li"
                    },
                    {
                        "name": "Michael L. Birnbaum"
                    },
                    {
                        "name": "Munmun De Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Munmun De Choudhury"
                },
                "author": "Munmun De Choudhury",
                "arxiv_comment": "30 pages, 8 figures, 16 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19155v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19155v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12196v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12196v4",
                "updated": "2025-01-06T16:29:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    29,
                    32,
                    0,
                    6,
                    0
                ],
                "published": "2024-03-18T19:10:12Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    19,
                    10,
                    12,
                    0,
                    78,
                    0
                ],
                "title": "Leveraging Large Language Models to Detect npm Malicious Packages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models to Detect npm Malicious Packages"
                },
                "summary": "Existing malicious code detection techniques demand the integration of\nmultiple tools to detect different malware patterns, often suffering from high\nmisclassification rates. Therefore, malicious code detection techniques could\nbe enhanced by adopting advanced, more automated approaches to achieve high\naccuracy and a low misclassification rate. The goal of this study is to aid\nsecurity analysts in detecting malicious packages by empirically studying the\neffectiveness of Large Language Models (LLMs) in detecting malicious code. We\npresent SocketAI, a malicious code review workflow to detect malicious code. To\nevaluate the effectiveness of SocketAI, we leverage a benchmark dataset of\n5,115 npm packages, of which 2,180 packages have malicious code. We conducted a\nbaseline comparison of GPT-3 and GPT-4 models with the state-of-the-art CodeQL\nstatic analysis tool, using 39 custom CodeQL rules developed in prior research\nto detect malicious Javascript code. We also compare the effectiveness of\nstatic analysis as a pre-screener with SocketAI workflow, measuring the number\nof files that need to be analyzed. and the associated costs. Additionally, we\nperformed a qualitative study to understand the types of malicious activities\ndetected or missed by our workflow. Our baseline comparison demonstrates a 16%\nand 9% improvement over static analysis in precision and F1 scores,\nrespectively. GPT-4 achieves higher accuracy with 99% precision and 97% F1\nscores, while GPT-3 offers a more cost-effective balance at 91% precision and\n94% F1 scores. Pre-screening files with a static analyzer reduces the number of\nfiles requiring LLM analysis by 77.9% and decreases costs by 60.9% for GPT-3\nand 76.1% for GPT-4. Our qualitative analysis identified data theft, execution\nof arbitrary code, and suspicious domain categories as the top detected\nmalicious packages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing malicious code detection techniques demand the integration of\nmultiple tools to detect different malware patterns, often suffering from high\nmisclassification rates. Therefore, malicious code detection techniques could\nbe enhanced by adopting advanced, more automated approaches to achieve high\naccuracy and a low misclassification rate. The goal of this study is to aid\nsecurity analysts in detecting malicious packages by empirically studying the\neffectiveness of Large Language Models (LLMs) in detecting malicious code. We\npresent SocketAI, a malicious code review workflow to detect malicious code. To\nevaluate the effectiveness of SocketAI, we leverage a benchmark dataset of\n5,115 npm packages, of which 2,180 packages have malicious code. We conducted a\nbaseline comparison of GPT-3 and GPT-4 models with the state-of-the-art CodeQL\nstatic analysis tool, using 39 custom CodeQL rules developed in prior research\nto detect malicious Javascript code. We also compare the effectiveness of\nstatic analysis as a pre-screener with SocketAI workflow, measuring the number\nof files that need to be analyzed. and the associated costs. Additionally, we\nperformed a qualitative study to understand the types of malicious activities\ndetected or missed by our workflow. Our baseline comparison demonstrates a 16%\nand 9% improvement over static analysis in precision and F1 scores,\nrespectively. GPT-4 achieves higher accuracy with 99% precision and 97% F1\nscores, while GPT-3 offers a more cost-effective balance at 91% precision and\n94% F1 scores. Pre-screening files with a static analyzer reduces the number of\nfiles requiring LLM analysis by 77.9% and decreases costs by 60.9% for GPT-3\nand 76.1% for GPT-4. Our qualitative analysis identified data theft, execution\nof arbitrary code, and suspicious domain categories as the top detected\nmalicious packages."
                },
                "authors": [
                    {
                        "name": "Nusrat Zahan"
                    },
                    {
                        "name": "Philipp Burckhardt"
                    },
                    {
                        "name": "Mikola Lysenko"
                    },
                    {
                        "name": "Feross Aboukhadijeh"
                    },
                    {
                        "name": "Laurie Williams"
                    }
                ],
                "author_detail": {
                    "name": "Laurie Williams"
                },
                "author": "Laurie Williams",
                "arxiv_comment": "13 pages, 2 Figure, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12196v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12196v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03120v1",
                "updated": "2025-01-06T16:28:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    28,
                    47,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T16:28:47Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    28,
                    47,
                    0,
                    6,
                    0
                ],
                "title": "CAT: Content-Adaptive Image Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAT: Content-Adaptive Image Tokenization"
                },
                "summary": "Most existing image tokenizers encode images into a fixed number of tokens or\npatches, overlooking the inherent variability in image complexity. To address\nthis, we introduce Content-Adaptive Tokenizer (CAT), which dynamically adjusts\nrepresentation capacity based on the image content and encodes simpler images\ninto fewer tokens. We design a caption-based evaluation system that leverages\nlarge language models (LLMs) to predict content complexity and determine the\noptimal compression ratio for a given image, taking into account factors\ncritical to human perception. Trained on images with diverse compression\nratios, CAT demonstrates robust performance in image reconstruction. We also\nutilize its variable-length latent representations to train Diffusion\nTransformers (DiTs) for ImageNet generation. By optimizing token allocation,\nCAT improves the FID score over fixed-ratio baselines trained with the same\nflops and boosts the inference throughput by 18.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most existing image tokenizers encode images into a fixed number of tokens or\npatches, overlooking the inherent variability in image complexity. To address\nthis, we introduce Content-Adaptive Tokenizer (CAT), which dynamically adjusts\nrepresentation capacity based on the image content and encodes simpler images\ninto fewer tokens. We design a caption-based evaluation system that leverages\nlarge language models (LLMs) to predict content complexity and determine the\noptimal compression ratio for a given image, taking into account factors\ncritical to human perception. Trained on images with diverse compression\nratios, CAT demonstrates robust performance in image reconstruction. We also\nutilize its variable-length latent representations to train Diffusion\nTransformers (DiTs) for ImageNet generation. By optimizing token allocation,\nCAT improves the FID score over fixed-ratio baselines trained with the same\nflops and boosts the inference throughput by 18.5%."
                },
                "authors": [
                    {
                        "name": "Junhong Shen"
                    },
                    {
                        "name": "Kushal Tirumala"
                    },
                    {
                        "name": "Michihiro Yasunaga"
                    },
                    {
                        "name": "Ishan Misra"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "name": "Lili Yu"
                    },
                    {
                        "name": "Chunting Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Chunting Zhou"
                },
                "author": "Chunting Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03112v1",
                "updated": "2025-01-06T16:20:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    20,
                    44,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T16:20:44Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    20,
                    44,
                    0,
                    6,
                    0
                ],
                "title": "LangFair: A Python Package for Assessing Bias and Fairness in Large\n  Language Model Use Cases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LangFair: A Python Package for Assessing Bias and Fairness in Large\n  Language Model Use Cases"
                },
                "summary": "Large Language Models (LLMs) have been observed to exhibit bias in numerous\nways, potentially creating or worsening outcomes for specific groups identified\nby protected attributes such as sex, race, sexual orientation, or age. To help\naddress this gap, we introduce LangFair, an open-source Python package that\naims to equip LLM practitioners with the tools to evaluate bias and fairness\nrisks relevant to their specific use cases. The package offers functionality to\neasily generate evaluation datasets, comprised of LLM responses to\nuse-case-specific prompts, and subsequently calculate applicable metrics for\nthe practitioner's use case. To guide in metric selection, LangFair offers an\nactionable decision framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been observed to exhibit bias in numerous\nways, potentially creating or worsening outcomes for specific groups identified\nby protected attributes such as sex, race, sexual orientation, or age. To help\naddress this gap, we introduce LangFair, an open-source Python package that\naims to equip LLM practitioners with the tools to evaluate bias and fairness\nrisks relevant to their specific use cases. The package offers functionality to\neasily generate evaluation datasets, comprised of LLM responses to\nuse-case-specific prompts, and subsequently calculate applicable metrics for\nthe practitioner's use case. To guide in metric selection, LangFair offers an\nactionable decision framework."
                },
                "authors": [
                    {
                        "name": "Dylan Bouchard"
                    },
                    {
                        "name": "Mohit Singh Chauhan"
                    },
                    {
                        "name": "David Skarbrevik"
                    },
                    {
                        "name": "Viren Bajaj"
                    },
                    {
                        "name": "Zeya Ahmad"
                    }
                ],
                "author_detail": {
                    "name": "Zeya Ahmad"
                },
                "author": "Zeya Ahmad",
                "arxiv_comment": "Journal of Open Source Software; LangFair repository:\n  https://github.com/cvs-health/langfair",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03628v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03628v2",
                "updated": "2025-01-06T15:37:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    15,
                    37,
                    48,
                    0,
                    6,
                    0
                ],
                "published": "2024-06-05T21:24:26Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    21,
                    24,
                    26,
                    2,
                    157,
                    0
                ],
                "title": "Synthetic Oversampling: Theory and A Practical Approach Using LLMs to\n  Address Data Imbalance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Oversampling: Theory and A Practical Approach Using LLMs to\n  Address Data Imbalance"
                },
                "summary": "Imbalanced classification and spurious correlation are common challenges in\ndata science and machine learning. Both issues are linked to data imbalance,\nwith certain groups of data samples significantly underrepresented, which in\nturn would compromise the accuracy, robustness and generalizability of the\nlearned models. Recent advances have proposed leveraging the flexibility and\ngenerative capabilities of large language models (LLMs), typically built on\ntransformer architectures, to generate synthetic samples and to augment the\nobserved data. In the context of imbalanced data, LLMs are used to oversample\nunderrepresented groups and have shown promising improvements. However, there\nis a clear lack of theoretical understanding of such synthetic data approaches.\nIn this article, we develop novel theoretical foundations to systematically\nstudy the roles of synthetic samples in addressing imbalanced classification\nand spurious correlation. Specifically, we first explicitly quantify the\nbenefits of synthetic oversampling. Next, we analyze the scaling dynamics in\nsynthetic data augmentation, and derive the corresponding scaling law. Finally,\nwe demonstrate the capacity of transformer models to generate high-quality\nsynthetic samples. We further conduct extensive numerical experiments to\nvalidate the efficacy of the LLM-based synthetic oversampling and augmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imbalanced classification and spurious correlation are common challenges in\ndata science and machine learning. Both issues are linked to data imbalance,\nwith certain groups of data samples significantly underrepresented, which in\nturn would compromise the accuracy, robustness and generalizability of the\nlearned models. Recent advances have proposed leveraging the flexibility and\ngenerative capabilities of large language models (LLMs), typically built on\ntransformer architectures, to generate synthetic samples and to augment the\nobserved data. In the context of imbalanced data, LLMs are used to oversample\nunderrepresented groups and have shown promising improvements. However, there\nis a clear lack of theoretical understanding of such synthetic data approaches.\nIn this article, we develop novel theoretical foundations to systematically\nstudy the roles of synthetic samples in addressing imbalanced classification\nand spurious correlation. Specifically, we first explicitly quantify the\nbenefits of synthetic oversampling. Next, we analyze the scaling dynamics in\nsynthetic data augmentation, and derive the corresponding scaling law. Finally,\nwe demonstrate the capacity of transformer models to generate high-quality\nsynthetic samples. We further conduct extensive numerical experiments to\nvalidate the efficacy of the LLM-based synthetic oversampling and augmentation."
                },
                "authors": [
                    {
                        "name": "Ryumei Nakada"
                    },
                    {
                        "name": "Yichen Xu"
                    },
                    {
                        "name": "Lexin Li"
                    },
                    {
                        "name": "Linjun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linjun Zhang"
                },
                "author": "Linjun Zhang",
                "arxiv_comment": "82 pages, 28 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03628v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03628v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03073v1",
                "updated": "2025-01-06T15:10:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    15,
                    10,
                    22,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T15:10:22Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    15,
                    10,
                    22,
                    0,
                    6,
                    0
                ],
                "title": "Retrieval-Augmented TLAPS Proof Generation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented TLAPS Proof Generation with Large Language Models"
                },
                "summary": "We present a novel approach to automated proof generation for the TLA+ Proof\nSystem (TLAPS) using Large Language Models (LLMs). Our method combines two key\ncomponents: a sub-proof obligation generation phase that breaks down complex\nproof obligations into simpler sub-obligations, and a proof generation phase\nthat leverages Retrieval-Augmented Generation with verified proof examples. We\nevaluate our approach using proof obligations from varying complexity levels of\nproof obligations, spanning from fundamental arithmetic properties to the\nproperties of algorithms. Our experiments demonstrate that while the method\nsuccessfully generates valid proofs for intermediate-complexity obligations, it\nfaces limitations with more complex theorems. These results indicate that our\napproach can effectively assist in proof development for certain classes of\nproperties, contributing to the broader goal of integrating LLMs into formal\nverification workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach to automated proof generation for the TLA+ Proof\nSystem (TLAPS) using Large Language Models (LLMs). Our method combines two key\ncomponents: a sub-proof obligation generation phase that breaks down complex\nproof obligations into simpler sub-obligations, and a proof generation phase\nthat leverages Retrieval-Augmented Generation with verified proof examples. We\nevaluate our approach using proof obligations from varying complexity levels of\nproof obligations, spanning from fundamental arithmetic properties to the\nproperties of algorithms. Our experiments demonstrate that while the method\nsuccessfully generates valid proofs for intermediate-complexity obligations, it\nfaces limitations with more complex theorems. These results indicate that our\napproach can effectively assist in proof development for certain classes of\nproperties, contributing to the broader goal of integrating LLMs into formal\nverification workflows."
                },
                "authors": [
                    {
                        "name": "Yuhao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yuhao Zhou"
                },
                "author": "Yuhao Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00722v2",
                "updated": "2025-01-06T15:09:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    15,
                    9,
                    6,
                    0,
                    6,
                    0
                ],
                "published": "2024-08-01T17:15:13Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    17,
                    15,
                    13,
                    3,
                    214,
                    0
                ],
                "title": "Pathway to Secure and Trustworthy ZSM for LLMs: Attacks, Defense, and\n  Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pathway to Secure and Trustworthy ZSM for LLMs: Attacks, Defense, and\n  Opportunities"
                },
                "summary": "Recently, large language models (LLMs) have been gaining a lot of interest\ndue to their adaptability and extensibility in emerging applications, including\ncommunication networks. It is anticipated that ZSM networks will be able to\nsupport LLMs as a service, as they provide ultra reliable low-latency\ncommunications and closed loop massive connectivity. However, LLMs are\nvulnerable to data and model privacy issues that affect the trustworthiness of\nLLMs to be deployed for user-based services. In this paper, we explore the\nsecurity vulnerabilities associated with fine-tuning LLMs in ZSM networks, in\nparticular the membership inference attack. We define the characteristics of an\nattack network that can perform a membership inference attack if the attacker\nhas access to the fine-tuned model for the downstream task. We show that the\nmembership inference attacks are effective for any downstream task, which can\nlead to a personal data breach when using LLM as a service. The experimental\nresults show that the attack success rate of maximum 92% can be achieved on\nnamed entity recognition task. Based on the experimental analysis, we discuss\npossible defense mechanisms and present possible research directions to make\nthe LLMs more trustworthy in the context of ZSM networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have been gaining a lot of interest\ndue to their adaptability and extensibility in emerging applications, including\ncommunication networks. It is anticipated that ZSM networks will be able to\nsupport LLMs as a service, as they provide ultra reliable low-latency\ncommunications and closed loop massive connectivity. However, LLMs are\nvulnerable to data and model privacy issues that affect the trustworthiness of\nLLMs to be deployed for user-based services. In this paper, we explore the\nsecurity vulnerabilities associated with fine-tuning LLMs in ZSM networks, in\nparticular the membership inference attack. We define the characteristics of an\nattack network that can perform a membership inference attack if the attacker\nhas access to the fine-tuned model for the downstream task. We show that the\nmembership inference attacks are effective for any downstream task, which can\nlead to a personal data breach when using LLM as a service. The experimental\nresults show that the attack success rate of maximum 92% can be achieved on\nnamed entity recognition task. Based on the experimental analysis, we discuss\npossible defense mechanisms and present possible research directions to make\nthe LLMs more trustworthy in the context of ZSM networks."
                },
                "authors": [
                    {
                        "name": "Sunder Ali Khowaja"
                    },
                    {
                        "name": "Parus Khuwaja"
                    },
                    {
                        "name": "Kapal Dev"
                    },
                    {
                        "name": "Hussam Al Hamadi"
                    },
                    {
                        "name": "Engin Zeydan"
                    }
                ],
                "author_detail": {
                    "name": "Engin Zeydan"
                },
                "author": "Engin Zeydan",
                "arxiv_comment": "7 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03067v1",
                "updated": "2025-01-06T15:04:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    15,
                    4,
                    45,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T15:04:45Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    15,
                    4,
                    45,
                    0,
                    6,
                    0
                ],
                "title": "Design and implementation of tools to build an ontology of Security\n  Requirements for Internet of Medical Things",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and implementation of tools to build an ontology of Security\n  Requirements for Internet of Medical Things"
                },
                "summary": "When developing devices, architectures and services for the Internet of\nMedical Things (IoMT) world, manufacturers or integrators must be aware of the\nsecurity requirements expressed by both laws and specifications. To provide\ntools guiding through these requirements and to assure a third party of the\ncorrect compliance, an ontology charting the relevant laws and specifications\n(for the European context) is very useful. We here address the development of\nthis ontology. Due to the very high number and size of the considered\nspecification documents, we have put in place a methodology and tools to\nsimplify the transition from natural text to an ontology. The first step is a\nmanual highlighting of relevant concepts in the corpus, then a manual\ntranslation to XML/XSD is operated. We have developed a tool allowing us to\nconvert this semi-structured data into an ontology. Because the different\nspecifications use similar but different wording, our approach favors the\ncreation of similar instances in the ontology. To improve the ontology\nsimplification through instance merging, we consider the use of LLMs. The\nresponses of the LLMs are compared against our manually defined correct\nresponses. The quality of the responses of the automated system does not prove\nto be good enough to be trusted blindly, and should only be used as a starting\npoint for a manual correction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When developing devices, architectures and services for the Internet of\nMedical Things (IoMT) world, manufacturers or integrators must be aware of the\nsecurity requirements expressed by both laws and specifications. To provide\ntools guiding through these requirements and to assure a third party of the\ncorrect compliance, an ontology charting the relevant laws and specifications\n(for the European context) is very useful. We here address the development of\nthis ontology. Due to the very high number and size of the considered\nspecification documents, we have put in place a methodology and tools to\nsimplify the transition from natural text to an ontology. The first step is a\nmanual highlighting of relevant concepts in the corpus, then a manual\ntranslation to XML/XSD is operated. We have developed a tool allowing us to\nconvert this semi-structured data into an ontology. Because the different\nspecifications use similar but different wording, our approach favors the\ncreation of similar instances in the ontology. To improve the ontology\nsimplification through instance merging, we consider the use of LLMs. The\nresponses of the LLMs are compared against our manually defined correct\nresponses. The quality of the responses of the automated system does not prove\nto be good enough to be trusted blindly, and should only be used as a starting\npoint for a manual correction."
                },
                "authors": [
                    {
                        "name": "Daniel Naro"
                    },
                    {
                        "name": "Jaime Delgado"
                    },
                    {
                        "name": "Silvia Llorente"
                    },
                    {
                        "name": "Amanda Palomo"
                    }
                ],
                "author_detail": {
                    "name": "Amanda Palomo"
                },
                "author": "Amanda Palomo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17922v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17922v3",
                "updated": "2025-01-06T14:45:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    14,
                    45,
                    42,
                    0,
                    6,
                    0
                ],
                "published": "2024-05-28T07:45:22Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    7,
                    45,
                    22,
                    1,
                    149,
                    0
                ],
                "title": "Stochastic Optimization Schemes for Performative Prediction with\n  Nonconvex Loss",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Optimization Schemes for Performative Prediction with\n  Nonconvex Loss"
                },
                "summary": "This paper studies a risk minimization problem with decision dependent data\ndistribution. The problem pertains to the performative prediction setting in\nwhich a trained model can affect the outcome estimated by the model. Such\ndependency creates a feedback loop that influences the stability of\noptimization algorithms such as stochastic gradient descent (SGD). We present\nthe first study on performative prediction with smooth but possibly non-convex\nloss. We analyze a greedy deployment scheme with SGD (SGD-GD). Note that in the\nliterature, SGD-GD is often studied with strongly convex loss. We first propose\nthe definition of stationary performative stable (SPS) solutions through\nrelaxing the popular performative stable condition. We then prove that SGD-GD\nconverges to a biased SPS solution in expectation. We consider two conditions\nof sensitivity on the distribution shifts: (i) the sensitivity is characterized\nby Wasserstein-1 distance and the loss is Lipschitz w.r.t.~data samples, or\n(ii) the sensitivity is characterized by total variation (TV) divergence and\nthe loss is bounded. In both conditions, the bias levels are proportional to\nthe stochastic gradient's variance and sensitivity level. Our analysis is\nextended to a lazy deployment scheme where models are deployed once per several\nSGD updates, and we show that it converges to an SPS solution with reduced\nbias. Numerical experiments corroborate our theories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies a risk minimization problem with decision dependent data\ndistribution. The problem pertains to the performative prediction setting in\nwhich a trained model can affect the outcome estimated by the model. Such\ndependency creates a feedback loop that influences the stability of\noptimization algorithms such as stochastic gradient descent (SGD). We present\nthe first study on performative prediction with smooth but possibly non-convex\nloss. We analyze a greedy deployment scheme with SGD (SGD-GD). Note that in the\nliterature, SGD-GD is often studied with strongly convex loss. We first propose\nthe definition of stationary performative stable (SPS) solutions through\nrelaxing the popular performative stable condition. We then prove that SGD-GD\nconverges to a biased SPS solution in expectation. We consider two conditions\nof sensitivity on the distribution shifts: (i) the sensitivity is characterized\nby Wasserstein-1 distance and the loss is Lipschitz w.r.t.~data samples, or\n(ii) the sensitivity is characterized by total variation (TV) divergence and\nthe loss is bounded. In both conditions, the bias levels are proportional to\nthe stochastic gradient's variance and sensitivity level. Our analysis is\nextended to a lazy deployment scheme where models are deployed once per several\nSGD updates, and we show that it converges to an SPS solution with reduced\nbias. Numerical experiments corroborate our theories."
                },
                "authors": [
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Hoi-To Wai"
                    }
                ],
                "author_detail": {
                    "name": "Hoi-To Wai"
                },
                "author": "Hoi-To Wai",
                "arxiv_comment": "18 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17922v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17922v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.09721v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.09721v2",
                "updated": "2025-01-06T14:31:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    14,
                    31,
                    39,
                    0,
                    6,
                    0
                ],
                "published": "2023-08-12T13:31:02Z",
                "published_parsed": [
                    2023,
                    8,
                    12,
                    13,
                    31,
                    2,
                    5,
                    224,
                    0
                ],
                "title": "A new solution and concrete implementation steps for Artificial General\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new solution and concrete implementation steps for Artificial General\n  Intelligence"
                },
                "summary": "In this paper, we propose a new approach to building a artificial general\nintelligence with self awareness, which includes: (1) a new method to implement\nattention mechanisms; (2) a way to give machines self-demands; (3) how to form\na value evaluation system compatible with the network; (4) a way to create the\nworld models; (5) how to realize a top-down, hierarchical thinking\ndecision-making chain; (6) a way to achieve general decision-making and\nresponse capabilities; (7) a way for a machine to directly obtain human\nexperience through language. In the paper, we first analyze some of the\nshortcomings of current LLMs (Large Language Model) and propose ideas for\nimprovement. Then we analyze why our scheme can solve the above problems and\nprovide detailed steps for implementing our scheme. In chapter 4, we have\npresented a step-by-step mplementation roadmap. And in chapter 5, we have\npresented a specific implementation demonstration. In chapter 6, we analyze the\nadvantages and disadvantages of our scheme and propose further research\ndirections. In this article, we have put forward how to create genuine\nartificial general intelligence step by step. It can handle data of all\nmodalities in a unified form and can directly understand the experience that\nhumans already possess through language, thus avoiding the problem that\nreinforcement learning is required for every decision-making process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a new approach to building a artificial general\nintelligence with self awareness, which includes: (1) a new method to implement\nattention mechanisms; (2) a way to give machines self-demands; (3) how to form\na value evaluation system compatible with the network; (4) a way to create the\nworld models; (5) how to realize a top-down, hierarchical thinking\ndecision-making chain; (6) a way to achieve general decision-making and\nresponse capabilities; (7) a way for a machine to directly obtain human\nexperience through language. In the paper, we first analyze some of the\nshortcomings of current LLMs (Large Language Model) and propose ideas for\nimprovement. Then we analyze why our scheme can solve the above problems and\nprovide detailed steps for implementing our scheme. In chapter 4, we have\npresented a step-by-step mplementation roadmap. And in chapter 5, we have\npresented a specific implementation demonstration. In chapter 6, we analyze the\nadvantages and disadvantages of our scheme and propose further research\ndirections. In this article, we have put forward how to create genuine\nartificial general intelligence step by step. It can handle data of all\nmodalities in a unified form and can directly understand the experience that\nhumans already possess through language, thus avoiding the problem that\nreinforcement learning is required for every decision-making process."
                },
                "authors": [
                    {
                        "name": "Yongcong Chen"
                    },
                    {
                        "name": "Ting Zeng"
                    },
                    {
                        "name": "Xingyue Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xingyue Chen"
                },
                "author": "Xingyue Chen",
                "arxiv_comment": "25 pages, 2 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.09721v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.09721v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03040v1",
                "updated": "2025-01-06T14:27:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    14,
                    27,
                    41,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T14:27:41Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    14,
                    27,
                    41,
                    0,
                    6,
                    0
                ],
                "title": "ChronoSense: Exploring Temporal Understanding in Large Language Models\n  with Time Intervals of Events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChronoSense: Exploring Temporal Understanding in Large Language Models\n  with Time Intervals of Events"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success in various NLP\ntasks, yet they still face significant challenges in reasoning and arithmetic.\nTemporal reasoning, a critical component of natural language understanding, has\nraised increasing research attention. However, comprehensive testing of Allen's\ninterval relations (e.g., before, after, during) -- a fundamental framework for\ntemporal relationships -- remains underexplored. To fill this gap, we present\nChronoSense, a new benchmark for evaluating LLMs' temporal understanding. It\nincludes 16 tasks, focusing on identifying the Allen relation between two\ntemporal events and temporal arithmetic, using both abstract events and\nreal-world data from Wikidata. We assess the performance of seven recent LLMs\nusing this benchmark and the results indicate that models handle Allen\nrelations, even symmetrical ones, quite differently. Moreover, the findings\nsuggest that the models may rely on memorization to answer time-related\nquestions. Overall, the models' low performance highlights the need for\nimproved temporal understanding in LLMs and ChronoSense offers a robust\nframework for future research in this area. Our dataset and the source code are\navailable at https://github.com/duyguislakoglu/chronosense.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success in various NLP\ntasks, yet they still face significant challenges in reasoning and arithmetic.\nTemporal reasoning, a critical component of natural language understanding, has\nraised increasing research attention. However, comprehensive testing of Allen's\ninterval relations (e.g., before, after, during) -- a fundamental framework for\ntemporal relationships -- remains underexplored. To fill this gap, we present\nChronoSense, a new benchmark for evaluating LLMs' temporal understanding. It\nincludes 16 tasks, focusing on identifying the Allen relation between two\ntemporal events and temporal arithmetic, using both abstract events and\nreal-world data from Wikidata. We assess the performance of seven recent LLMs\nusing this benchmark and the results indicate that models handle Allen\nrelations, even symmetrical ones, quite differently. Moreover, the findings\nsuggest that the models may rely on memorization to answer time-related\nquestions. Overall, the models' low performance highlights the need for\nimproved temporal understanding in LLMs and ChronoSense offers a robust\nframework for future research in this area. Our dataset and the source code are\navailable at https://github.com/duyguislakoglu/chronosense."
                },
                "authors": [
                    {
                        "name": "Duygu Sezen Islakoglu"
                    },
                    {
                        "name": "Jan-Christoph Kalo"
                    }
                ],
                "author_detail": {
                    "name": "Jan-Christoph Kalo"
                },
                "author": "Jan-Christoph Kalo",
                "arxiv_comment": "14 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03035v1",
                "updated": "2025-01-06T14:23:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    14,
                    23,
                    2,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T14:23:02Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    14,
                    23,
                    2,
                    0,
                    6,
                    0
                ],
                "title": "Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization\n  Degradation for Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization\n  Degradation for Mathematical Reasoning"
                },
                "summary": "Large language models have achieved significant advancements in complex\nmathematical reasoning benchmarks, such as MATH. However, their substantial\ncomputational requirements present challenges for practical deployment. Model\nquantization has emerged as an effective strategy to reduce memory usage and\ncomputational costs by employing lower precision and bit-width representations.\nIn this study, we systematically evaluate the impact of quantization on\nmathematical reasoning tasks. We introduce a multidimensional evaluation\nframework that qualitatively assesses specific capability dimensions and\nconduct quantitative analyses on the step-by-step outputs of various\nquantization methods. Our results demonstrate that quantization differentially\naffects numerical computation and reasoning planning abilities, identifying key\nareas where quantized models experience performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have achieved significant advancements in complex\nmathematical reasoning benchmarks, such as MATH. However, their substantial\ncomputational requirements present challenges for practical deployment. Model\nquantization has emerged as an effective strategy to reduce memory usage and\ncomputational costs by employing lower precision and bit-width representations.\nIn this study, we systematically evaluate the impact of quantization on\nmathematical reasoning tasks. We introduce a multidimensional evaluation\nframework that qualitatively assesses specific capability dimensions and\nconduct quantitative analyses on the step-by-step outputs of various\nquantization methods. Our results demonstrate that quantization differentially\naffects numerical computation and reasoning planning abilities, identifying key\nareas where quantized models experience performance degradation."
                },
                "authors": [
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Yupeng Su"
                    },
                    {
                        "name": "Runming Yang"
                    },
                    {
                        "name": "Zhongwei Xie"
                    },
                    {
                        "name": "Ngai Wong"
                    },
                    {
                        "name": "Hongxia Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hongxia Yang"
                },
                "author": "Hongxia Yang",
                "arxiv_comment": "4 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.09830v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.09830v3",
                "updated": "2025-01-06T13:43:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    13,
                    43,
                    9,
                    0,
                    6,
                    0
                ],
                "published": "2023-11-16T11:55:27Z",
                "published_parsed": [
                    2023,
                    11,
                    16,
                    11,
                    55,
                    27,
                    3,
                    320,
                    0
                ],
                "title": "Automating the Generation of Prompts for LLM-based Action Choice in PDDL\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating the Generation of Prompts for LLM-based Action Choice in PDDL\n  Planning"
                },
                "summary": "Large language models (LLMs) have revolutionized a large variety of NLP\ntasks. An active debate is to what extent they can do reasoning and planning.\nPrior work has assessed the latter in the specific context of PDDL planning,\nbased on manually converting three PDDL domains into natural language (NL)\nprompts. Here we automate this conversion step, showing how to leverage an LLM\nto automatically generate NL prompts from PDDL input. Our automatically\ngenerated NL prompts result in similar LLM-planning performance as the previous\nmanually generated ones. Beyond this, the automation enables us to run much\nlarger experiments, providing for the first time a broad evaluation of LLM\nplanning performance in PDDL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized a large variety of NLP\ntasks. An active debate is to what extent they can do reasoning and planning.\nPrior work has assessed the latter in the specific context of PDDL planning,\nbased on manually converting three PDDL domains into natural language (NL)\nprompts. Here we automate this conversion step, showing how to leverage an LLM\nto automatically generate NL prompts from PDDL input. Our automatically\ngenerated NL prompts result in similar LLM-planning performance as the previous\nmanually generated ones. Beyond this, the automation enables us to run much\nlarger experiments, providing for the first time a broad evaluation of LLM\nplanning performance in PDDL."
                },
                "authors": [
                    {
                        "name": "Katharina Stein"
                    },
                    {
                        "name": "Daniel Fier"
                    },
                    {
                        "name": "Jrg Hoffmann"
                    },
                    {
                        "name": "Alexander Koller"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Koller"
                },
                "author": "Alexander Koller",
                "arxiv_comment": "Latest Version of the paper previously called \"AutoPlanBench:\n  Automatically generating benchmarks for LLM planners from PDDL\"; Added\n  extended experiments; newer gpt4 model",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.09830v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.09830v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03012v1",
                "updated": "2025-01-06T13:37:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    13,
                    37,
                    13,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T13:37:13Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    13,
                    37,
                    13,
                    0,
                    6,
                    0
                ],
                "title": "Analyzing Fine-tuning Representation Shift for Multimodal LLMs Steering\n  alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Fine-tuning Representation Shift for Multimodal LLMs Steering\n  alignment"
                },
                "summary": "Multimodal LLMs have reached remarkable levels of proficiency in\nunderstanding multimodal inputs, driving extensive research to develop\nincreasingly powerful models. However, much less attention has been paid to\nunderstanding and explaining the underlying mechanisms of these models. Most\nexisting explainability research examines these models only in their final\nstates, overlooking the dynamic representational shifts that occur during\ntraining. In this work, we systematically analyze the evolution of hidden state\nrepresentations to reveal how fine-tuning alters the internal structure of a\nmodel to specialize in new multimodal tasks. Using a concept-based approach, we\nmap hidden states to interpretable visual and textual concepts, enabling us to\ntrace changes in encoded concepts across modalities as training progresses. We\nalso demonstrate the use of shift vectors to capture these concepts changes.\nThese shift vectors allow us to recover fine-tuned concepts by shifting those\nin the original model. Finally, we explore the practical impact of our findings\non model steering, showing that we can adjust multimodal LLMs behaviors without\nany training, such as modifying answer types, captions style, or biasing the\nmodel toward specific responses. Our work sheds light on how multimodal\nrepresentations evolve through fine-tuning and offers a new perspective for\ninterpreting model adaptation in multimodal tasks. The code for this project is\npublicly available at https://github.com/mshukor/xl-vlms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLMs have reached remarkable levels of proficiency in\nunderstanding multimodal inputs, driving extensive research to develop\nincreasingly powerful models. However, much less attention has been paid to\nunderstanding and explaining the underlying mechanisms of these models. Most\nexisting explainability research examines these models only in their final\nstates, overlooking the dynamic representational shifts that occur during\ntraining. In this work, we systematically analyze the evolution of hidden state\nrepresentations to reveal how fine-tuning alters the internal structure of a\nmodel to specialize in new multimodal tasks. Using a concept-based approach, we\nmap hidden states to interpretable visual and textual concepts, enabling us to\ntrace changes in encoded concepts across modalities as training progresses. We\nalso demonstrate the use of shift vectors to capture these concepts changes.\nThese shift vectors allow us to recover fine-tuned concepts by shifting those\nin the original model. Finally, we explore the practical impact of our findings\non model steering, showing that we can adjust multimodal LLMs behaviors without\nany training, such as modifying answer types, captions style, or biasing the\nmodel toward specific responses. Our work sheds light on how multimodal\nrepresentations evolve through fine-tuning and offers a new perspective for\ninterpreting model adaptation in multimodal tasks. The code for this project is\npublicly available at https://github.com/mshukor/xl-vlms."
                },
                "authors": [
                    {
                        "name": "Pegah Khayatan"
                    },
                    {
                        "name": "Mustafa Shukor"
                    },
                    {
                        "name": "Jayneel Parekh"
                    },
                    {
                        "name": "Matthieu Cord"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Cord"
                },
                "author": "Matthieu Cord",
                "arxiv_comment": "The first three authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02997v1",
                "updated": "2025-01-06T13:14:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    13,
                    14,
                    34,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T13:14:34Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    13,
                    14,
                    34,
                    0,
                    6,
                    0
                ],
                "title": "CALM: Curiosity-Driven Auditing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALM: Curiosity-Driven Auditing for Large Language Models"
                },
                "summary": "Auditing Large Language Models (LLMs) is a crucial and challenging task. In\nthis study, we focus on auditing black-box LLMs without access to their\nparameters, only to the provided service. We treat this type of auditing as a\nblack-box optimization problem where the goal is to automatically uncover\ninput-output pairs of the target LLMs that exhibit illegal, immoral, or unsafe\nbehaviors. For instance, we may seek a non-toxic input that the target LLM\nresponds to with a toxic output or an input that induces the hallucinative\nresponse from the target LLM containing politically sensitive individuals. This\nblack-box optimization is challenging due to the scarcity of feasible points,\nthe discrete nature of the prompt space, and the large search space. To address\nthese challenges, we propose Curiosity-Driven Auditing for Large Language\nModels (CALM), which uses intrinsically motivated reinforcement learning to\nfinetune an LLM as the auditor agent to uncover potential harmful and biased\ninput-output pairs of the target LLM. CALM successfully identifies derogatory\ncompletions involving celebrities and uncovers inputs that elicit specific\nnames under the black-box setting. This work offers a promising direction for\nauditing black-box LLMs. Our code is available at\nhttps://github.com/x-zheng16/CALM.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing Large Language Models (LLMs) is a crucial and challenging task. In\nthis study, we focus on auditing black-box LLMs without access to their\nparameters, only to the provided service. We treat this type of auditing as a\nblack-box optimization problem where the goal is to automatically uncover\ninput-output pairs of the target LLMs that exhibit illegal, immoral, or unsafe\nbehaviors. For instance, we may seek a non-toxic input that the target LLM\nresponds to with a toxic output or an input that induces the hallucinative\nresponse from the target LLM containing politically sensitive individuals. This\nblack-box optimization is challenging due to the scarcity of feasible points,\nthe discrete nature of the prompt space, and the large search space. To address\nthese challenges, we propose Curiosity-Driven Auditing for Large Language\nModels (CALM), which uses intrinsically motivated reinforcement learning to\nfinetune an LLM as the auditor agent to uncover potential harmful and biased\ninput-output pairs of the target LLM. CALM successfully identifies derogatory\ncompletions involving celebrities and uncovers inputs that elicit specific\nnames under the black-box setting. This work offers a promising direction for\nauditing black-box LLMs. Our code is available at\nhttps://github.com/x-zheng16/CALM.git."
                },
                "authors": [
                    {
                        "name": "Xiang Zheng"
                    },
                    {
                        "name": "Longxiang Wang"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Xingjun Ma"
                    },
                    {
                        "name": "Chao Shen"
                    },
                    {
                        "name": "Cong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Cong Wang"
                },
                "author": "Cong Wang",
                "arxiv_comment": "Accepted by AAAI 2025 AI Alignment Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02979v1",
                "updated": "2025-01-06T12:42:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    12,
                    42,
                    54,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T12:42:54Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    12,
                    42,
                    54,
                    0,
                    6,
                    0
                ],
                "title": "Registering Source Tokens to Target Language Spaces in Multilingual\n  Neural Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Registering Source Tokens to Target Language Spaces in Multilingual\n  Neural Machine Translation"
                },
                "summary": "The multilingual neural machine translation (MNMT) enables arbitrary\ntranslations across multiple languages by training a model with limited\nparameters using parallel data only. However, the performance of such MNMT\nmodels still lags behind that of large language models (LLMs), limiting their\npracticality. In this work, we address this limitation by introducing\nregistering to achieve the new state-of-the-art of decoder-only MNMT models.\nSpecifically, we insert a set of artificial tokens specifying the target\nlanguage, called registers, into the input sequence between the source and\ntarget tokens. By modifying the attention mask, the target token generation\nonly pays attention to the activation of registers, representing the source\ntokens in the target language space. Experiments on EC-40, a large-scale\nbenchmark, show that our method outperforms related methods driven by\noptimizing multilingual representations. We further scale up and collect 9.3\nbillion sentence pairs across 24 languages from public datasets to pre-train\ntwo models, namely MITRE (multilingual translation with registers). One of\nthem, MITRE-913M, outperforms NLLB-3.3B, achieves comparable performance with\ncommercial LLMs, and shows strong adaptability in fine-tuning. Finally, we\nopen-source our models to facilitate further research and development in MNMT:\nhttps://github.com/zhiqu22/mitre.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The multilingual neural machine translation (MNMT) enables arbitrary\ntranslations across multiple languages by training a model with limited\nparameters using parallel data only. However, the performance of such MNMT\nmodels still lags behind that of large language models (LLMs), limiting their\npracticality. In this work, we address this limitation by introducing\nregistering to achieve the new state-of-the-art of decoder-only MNMT models.\nSpecifically, we insert a set of artificial tokens specifying the target\nlanguage, called registers, into the input sequence between the source and\ntarget tokens. By modifying the attention mask, the target token generation\nonly pays attention to the activation of registers, representing the source\ntokens in the target language space. Experiments on EC-40, a large-scale\nbenchmark, show that our method outperforms related methods driven by\noptimizing multilingual representations. We further scale up and collect 9.3\nbillion sentence pairs across 24 languages from public datasets to pre-train\ntwo models, namely MITRE (multilingual translation with registers). One of\nthem, MITRE-913M, outperforms NLLB-3.3B, achieves comparable performance with\ncommercial LLMs, and shows strong adaptability in fine-tuning. Finally, we\nopen-source our models to facilitate further research and development in MNMT:\nhttps://github.com/zhiqu22/mitre."
                },
                "authors": [
                    {
                        "name": "Zhi Qu"
                    },
                    {
                        "name": "Yiran Wang"
                    },
                    {
                        "name": "Jiannan Mao"
                    },
                    {
                        "name": "Chenchen Ding"
                    },
                    {
                        "name": "Hideki Tanaka"
                    },
                    {
                        "name": "Masao Utiyama"
                    },
                    {
                        "name": "Taro Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Taro Watanabe"
                },
                "author": "Taro Watanabe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02968v1",
                "updated": "2025-01-06T12:24:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    12,
                    24,
                    57,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T12:24:57Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    12,
                    24,
                    57,
                    0,
                    6,
                    0
                ],
                "title": "FlipedRAG: Black-Box Opinion Manipulation Attacks to Retrieval-Augmented\n  Generation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlipedRAG: Black-Box Opinion Manipulation Attacks to Retrieval-Augmented\n  Generation of Large Language Models"
                },
                "summary": "Retrieval-Augmented Generation (RAG) addresses hallucination and real-time\nconstraints by dynamically retrieving relevant information from a knowledge\ndatabase to supplement the LLMs' input. When presented with a query, RAG\nselects the most semantically similar texts from its knowledge bases and uses\nthem as context for the LLMs to generate more accurate responses. RAG also\ncreates a new attack surface, especially since RAG databases are frequently\nsourced from public domains. While existing studies have predominantly focused\non optimizing RAG's performance and efficiency, emerging research has begun\naddressing the security concerns associated with RAG. However, these works have\nsome limitations, typically focusing on either white-box methodologies or\nheuristic-based black-box attacks. Furthermore, prior research has mainly\ntargeted simple factoid question answering, which is neither practically\nchallenging nor resistant to correction. In this paper, we unveil a more\nrealistic and threatening scenario: opinion manipulation for controversial\ntopics against RAG. Particularly, we propose a novel RAG black-box attack\nmethod, termed FlipedRAG, which is transfer-based. By leveraging instruction\nengineering, we obtain partial retrieval model outputs from black-box RAG\nsystem, facilitating the training of surrogate models to enhance the\neffectiveness of opinion manipulation attack. Extensive experimental results\nconfirms that our approach significantly enhances the average success rate of\nopinion manipulation by 16.7%. It achieves an average of a 50% directional\nchange in the opinion polarity of RAG responses across four themes.\nAdditionally, it induces a 20% shift in user cognition. Furthermore, we discuss\nthe efficacy of potential defense mechanisms and conclude that they are\ninsufficient in mitigating this type of attack, highlighting the urgent need to\ndevelop novel defensive strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) addresses hallucination and real-time\nconstraints by dynamically retrieving relevant information from a knowledge\ndatabase to supplement the LLMs' input. When presented with a query, RAG\nselects the most semantically similar texts from its knowledge bases and uses\nthem as context for the LLMs to generate more accurate responses. RAG also\ncreates a new attack surface, especially since RAG databases are frequently\nsourced from public domains. While existing studies have predominantly focused\non optimizing RAG's performance and efficiency, emerging research has begun\naddressing the security concerns associated with RAG. However, these works have\nsome limitations, typically focusing on either white-box methodologies or\nheuristic-based black-box attacks. Furthermore, prior research has mainly\ntargeted simple factoid question answering, which is neither practically\nchallenging nor resistant to correction. In this paper, we unveil a more\nrealistic and threatening scenario: opinion manipulation for controversial\ntopics against RAG. Particularly, we propose a novel RAG black-box attack\nmethod, termed FlipedRAG, which is transfer-based. By leveraging instruction\nengineering, we obtain partial retrieval model outputs from black-box RAG\nsystem, facilitating the training of surrogate models to enhance the\neffectiveness of opinion manipulation attack. Extensive experimental results\nconfirms that our approach significantly enhances the average success rate of\nopinion manipulation by 16.7%. It achieves an average of a 50% directional\nchange in the opinion polarity of RAG responses across four themes.\nAdditionally, it induces a 20% shift in user cognition. Furthermore, we discuss\nthe efficacy of potential defense mechanisms and conclude that they are\ninsufficient in mitigating this type of attack, highlighting the urgent need to\ndevelop novel defensive strategies."
                },
                "authors": [
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Yuyang Gong"
                    },
                    {
                        "name": "Miaokun Chen"
                    },
                    {
                        "name": "Haotan Liu"
                    },
                    {
                        "name": "Qikai Cheng"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Wei Lu"
                    },
                    {
                        "name": "Xiaozhong Liu"
                    },
                    {
                        "name": "Jiawei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Liu"
                },
                "author": "Jiawei Liu",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2407.13757",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02955v1",
                "updated": "2025-01-06T11:57:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    11,
                    57,
                    38,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T11:57:38Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    11,
                    57,
                    38,
                    0,
                    6,
                    0
                ],
                "title": "MotionBench: Benchmarking and Improving Fine-grained Video Motion\n  Understanding for Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MotionBench: Benchmarking and Improving Fine-grained Video Motion\n  Understanding for Vision Language Models"
                },
                "summary": "In recent years, vision language models (VLMs) have made significant\nadvancements in video understanding. However, a crucial capability -\nfine-grained motion comprehension - remains under-explored in current\nbenchmarks. To address this gap, we propose MotionBench, a comprehensive\nevaluation benchmark designed to assess the fine-grained motion comprehension\nof video understanding models. MotionBench evaluates models' motion-level\nperception through six primary categories of motion-oriented question types and\nincludes data collected from diverse sources, ensuring a broad representation\nof real-world video content. Experimental results reveal that existing VLMs\nperform poorly in understanding fine-grained motions. To enhance VLM's ability\nto perceive fine-grained motion within a limited sequence length of LLM, we\nconduct extensive experiments reviewing VLM architectures optimized for video\nfeature compression and propose a novel and efficient Through-Encoder (TE)\nFusion method. Experiments show that higher frame rate inputs and TE Fusion\nyield improvements in motion understanding, yet there is still substantial room\nfor enhancement. Our benchmark aims to guide and motivate the development of\nmore capable video understanding models, emphasizing the importance of\nfine-grained motion comprehension. Project page: https://motion-bench.github.io .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, vision language models (VLMs) have made significant\nadvancements in video understanding. However, a crucial capability -\nfine-grained motion comprehension - remains under-explored in current\nbenchmarks. To address this gap, we propose MotionBench, a comprehensive\nevaluation benchmark designed to assess the fine-grained motion comprehension\nof video understanding models. MotionBench evaluates models' motion-level\nperception through six primary categories of motion-oriented question types and\nincludes data collected from diverse sources, ensuring a broad representation\nof real-world video content. Experimental results reveal that existing VLMs\nperform poorly in understanding fine-grained motions. To enhance VLM's ability\nto perceive fine-grained motion within a limited sequence length of LLM, we\nconduct extensive experiments reviewing VLM architectures optimized for video\nfeature compression and propose a novel and efficient Through-Encoder (TE)\nFusion method. Experiments show that higher frame rate inputs and TE Fusion\nyield improvements in motion understanding, yet there is still substantial room\nfor enhancement. Our benchmark aims to guide and motivate the development of\nmore capable video understanding models, emphasizing the importance of\nfine-grained motion comprehension. Project page: https://motion-bench.github.io ."
                },
                "authors": [
                    {
                        "name": "Wenyi Hong"
                    },
                    {
                        "name": "Yean Cheng"
                    },
                    {
                        "name": "Zhuoyi Yang"
                    },
                    {
                        "name": "Weihan Wang"
                    },
                    {
                        "name": "Lefan Wang"
                    },
                    {
                        "name": "Xiaotao Gu"
                    },
                    {
                        "name": "Shiyu Huang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09472v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09472v2",
                "updated": "2025-01-06T10:52:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    10,
                    52,
                    2,
                    0,
                    6,
                    0
                ],
                "published": "2024-10-12T10:21:00Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    21,
                    0,
                    5,
                    286,
                    0
                ],
                "title": "DRCap: Decoding CLAP Latents with Retrieval-Augmented Generation for\n  Zero-shot Audio Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRCap: Decoding CLAP Latents with Retrieval-Augmented Generation for\n  Zero-shot Audio Captioning"
                },
                "summary": "While automated audio captioning (AAC) has made notable progress, traditional\nfully supervised AAC models still face two critical challenges: the need for\nexpensive audio-text pair data for training and performance degradation when\ntransferring across domains. To overcome these limitations, we present DRCap, a\ndata-efficient and flexible zero-shot audio captioning system that requires\ntext-only data for training and can quickly adapt to new domains without\nadditional fine-tuning. DRCap integrates a contrastive language-audio\npre-training (CLAP) model and a large-language model (LLM) as its backbone.\nDuring training, the model predicts the ground-truth caption with a fixed text\nencoder from CLAP, whereas, during inference, the text encoder is replaced with\nthe audio encoder to generate captions for audio clips in a zero-shot manner.\nTo mitigate the modality gap of the CLAP model, we use both the projection\nstrategy from the encoder side and the retrieval-augmented generation strategy\nfrom the decoder side. Specifically, audio embeddings are first projected onto\na text embedding support to absorb extensive semantic information within the\njoint multi-modal space of CLAP. At the same time, similar captions retrieved\nfrom a datastore are fed as prompts to instruct the LLM, incorporating external\nknowledge to take full advantage of its strong generative capability.\nConditioned on both the projected CLAP embedding and the retrieved similar\ncaptions, the model is able to produce a more accurate and semantically rich\ntextual description. By tailoring the text embedding support and the caption\ndatastore to the target domain, DRCap acquires a robust ability to adapt to new\ndomains in a training-free manner. Experimental results demonstrate that DRCap\noutperforms all other zero-shot models in in-domain scenarios and achieves\nstate-of-the-art performance in cross-domain scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While automated audio captioning (AAC) has made notable progress, traditional\nfully supervised AAC models still face two critical challenges: the need for\nexpensive audio-text pair data for training and performance degradation when\ntransferring across domains. To overcome these limitations, we present DRCap, a\ndata-efficient and flexible zero-shot audio captioning system that requires\ntext-only data for training and can quickly adapt to new domains without\nadditional fine-tuning. DRCap integrates a contrastive language-audio\npre-training (CLAP) model and a large-language model (LLM) as its backbone.\nDuring training, the model predicts the ground-truth caption with a fixed text\nencoder from CLAP, whereas, during inference, the text encoder is replaced with\nthe audio encoder to generate captions for audio clips in a zero-shot manner.\nTo mitigate the modality gap of the CLAP model, we use both the projection\nstrategy from the encoder side and the retrieval-augmented generation strategy\nfrom the decoder side. Specifically, audio embeddings are first projected onto\na text embedding support to absorb extensive semantic information within the\njoint multi-modal space of CLAP. At the same time, similar captions retrieved\nfrom a datastore are fed as prompts to instruct the LLM, incorporating external\nknowledge to take full advantage of its strong generative capability.\nConditioned on both the projected CLAP embedding and the retrieved similar\ncaptions, the model is able to produce a more accurate and semantically rich\ntextual description. By tailoring the text embedding support and the caption\ndatastore to the target domain, DRCap acquires a robust ability to adapt to new\ndomains in a training-free manner. Experimental results demonstrate that DRCap\noutperforms all other zero-shot models in in-domain scenarios and achieves\nstate-of-the-art performance in cross-domain scenarios."
                },
                "authors": [
                    {
                        "name": "Xiquan Li"
                    },
                    {
                        "name": "Wenxi Chen"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Xuenan Xu"
                    },
                    {
                        "name": "Yuzhe Liang"
                    },
                    {
                        "name": "Zhisheng Zheng"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Xie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xie Chen"
                },
                "author": "Xie Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09472v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09472v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02902v1",
                "updated": "2025-01-06T10:26:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    10,
                    26,
                    16,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T10:26:16Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    10,
                    26,
                    16,
                    0,
                    6,
                    0
                ],
                "title": "Sim-to-Real Transfer for Mobile Robots with Reinforcement Learning: from\n  NVIDIA Isaac Sim to Gazebo and Real ROS 2 Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sim-to-Real Transfer for Mobile Robots with Reinforcement Learning: from\n  NVIDIA Isaac Sim to Gazebo and Real ROS 2 Robots"
                },
                "summary": "Unprecedented agility and dexterous manipulation have been demonstrated with\ncontrollers based on deep reinforcement learning (RL), with a significant\nimpact on legged and humanoid robots. Modern tooling and simulation platforms,\nsuch as NVIDIA Isaac Sim, have been enabling such advances. This article\nfocuses on demonstrating the applications of Isaac in local planning and\nobstacle avoidance as one of the most fundamental ways in which a mobile robot\ninteracts with its environments. Although there is extensive research on\nproprioception-based RL policies, the article highlights less standardized and\nreproducible approaches to exteroception. At the same time, the article aims to\nprovide a base framework for end-to-end local navigation policies and how a\ncustom robot can be trained in such simulation environment. We benchmark\nend-to-end policies with the state-of-the-art Nav2, navigation stack in Robot\nOperating System (ROS). We also cover the sim-to-real transfer process by\ndemonstrating zero-shot transferability of policies trained in the Isaac\nsimulator to real-world robots. This is further evidenced by the tests with\ndifferent simulated robots, which show the generalization of the learned\npolicy. Finally, the benchmarks demonstrate comparable performance to Nav2,\nopening the door to quick deployment of state-of-the-art end-to-end local\nplanners for custom robot platforms, but importantly furthering the\npossibilities by expanding the state and action spaces or task definitions for\nmore complex missions. Overall, with this article we introduce the most\nimportant steps, and aspects to consider, in deploying RL policies for local\npath planning and obstacle avoidance with Isaac Sim training, Gazebo testing,\nand ROS 2 for real-time inference in real robots. The code is available at\nhttps://github.com/sahars93/RL-Navigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unprecedented agility and dexterous manipulation have been demonstrated with\ncontrollers based on deep reinforcement learning (RL), with a significant\nimpact on legged and humanoid robots. Modern tooling and simulation platforms,\nsuch as NVIDIA Isaac Sim, have been enabling such advances. This article\nfocuses on demonstrating the applications of Isaac in local planning and\nobstacle avoidance as one of the most fundamental ways in which a mobile robot\ninteracts with its environments. Although there is extensive research on\nproprioception-based RL policies, the article highlights less standardized and\nreproducible approaches to exteroception. At the same time, the article aims to\nprovide a base framework for end-to-end local navigation policies and how a\ncustom robot can be trained in such simulation environment. We benchmark\nend-to-end policies with the state-of-the-art Nav2, navigation stack in Robot\nOperating System (ROS). We also cover the sim-to-real transfer process by\ndemonstrating zero-shot transferability of policies trained in the Isaac\nsimulator to real-world robots. This is further evidenced by the tests with\ndifferent simulated robots, which show the generalization of the learned\npolicy. Finally, the benchmarks demonstrate comparable performance to Nav2,\nopening the door to quick deployment of state-of-the-art end-to-end local\nplanners for custom robot platforms, but importantly furthering the\npossibilities by expanding the state and action spaces or task definitions for\nmore complex missions. Overall, with this article we introduce the most\nimportant steps, and aspects to consider, in deploying RL policies for local\npath planning and obstacle avoidance with Isaac Sim training, Gazebo testing,\nand ROS 2 for real-time inference in real robots. The code is available at\nhttps://github.com/sahars93/RL-Navigation."
                },
                "authors": [
                    {
                        "name": "Sahar Salimpour"
                    },
                    {
                        "name": "Jorge Pea-Queralta"
                    },
                    {
                        "name": "Diego Paez-Granados"
                    },
                    {
                        "name": "Jukka Heikkonen"
                    },
                    {
                        "name": "Tomi Westerlund"
                    }
                ],
                "author_detail": {
                    "name": "Tomi Westerlund"
                },
                "author": "Tomi Westerlund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02901v1",
                "updated": "2025-01-06T10:25:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    10,
                    25,
                    28,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T10:25:28Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    10,
                    25,
                    28,
                    0,
                    6,
                    0
                ],
                "title": "DeCon: Detecting Incorrect Assertions via Postconditions Generated by a\n  Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeCon: Detecting Incorrect Assertions via Postconditions Generated by a\n  Large Language Model"
                },
                "summary": "Recently, given the docstring for the target problem and the target function\nsignature, large language models (LLMs) have been used not only to generate\nsource code, but also to generate test cases, consisting of test inputs and\nassertions (e.g., in the form of checking an actual output against the expected\noutput). However, as shown by our empirical study on assertions generated by\nfour LLMs for the HumanEval benchmark, over 62% of the generated assertions are\nincorrect (i.e., failed on the ground-truth problem solution). To detect\nincorrect assertions (given the docstring and the target function signature\nalong with a sample of example inputs and outputs), in this paper, we propose a\nnew approach named DeCon to effectively detect incorrect assertions via\nLLM-generated postconditions for the target problem (a postcondition is a\npredicate that must always be true just after the execution of the ground-truth\nproblem solution). Our approach requires a small set of I/O examples (i.e., a\nsample of example inputs and outputs) for the target problem (e.g., the I/O\nexamples included in the docstring for a target problem in HumanEval). We use\nthe given I/O examples to filter out those LLM-generated postconditions that\nare violated by at least one given I/O example. We then use the remaining\npostconditions to detect incorrect assertions as those assertions that violate\nat least one remaining postcondition. Experimental results show that DeCon can\ndetect averagely more than 64% (63% and 65.5% detected by GPT-3.5 and GPT-4,\nrespectively) incorrect assertions generated by four state-of-the-art LLMs, and\nDeCon can also improve the effectiveness of these LLMs in code generation by 4%\nin terms of Pass@1. In addition, although DeCon might filter out correct\nassertions, the fault-finding ability of the remaining correct assertions\ndecreases only slightly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, given the docstring for the target problem and the target function\nsignature, large language models (LLMs) have been used not only to generate\nsource code, but also to generate test cases, consisting of test inputs and\nassertions (e.g., in the form of checking an actual output against the expected\noutput). However, as shown by our empirical study on assertions generated by\nfour LLMs for the HumanEval benchmark, over 62% of the generated assertions are\nincorrect (i.e., failed on the ground-truth problem solution). To detect\nincorrect assertions (given the docstring and the target function signature\nalong with a sample of example inputs and outputs), in this paper, we propose a\nnew approach named DeCon to effectively detect incorrect assertions via\nLLM-generated postconditions for the target problem (a postcondition is a\npredicate that must always be true just after the execution of the ground-truth\nproblem solution). Our approach requires a small set of I/O examples (i.e., a\nsample of example inputs and outputs) for the target problem (e.g., the I/O\nexamples included in the docstring for a target problem in HumanEval). We use\nthe given I/O examples to filter out those LLM-generated postconditions that\nare violated by at least one given I/O example. We then use the remaining\npostconditions to detect incorrect assertions as those assertions that violate\nat least one remaining postcondition. Experimental results show that DeCon can\ndetect averagely more than 64% (63% and 65.5% detected by GPT-3.5 and GPT-4,\nrespectively) incorrect assertions generated by four state-of-the-art LLMs, and\nDeCon can also improve the effectiveness of these LLMs in code generation by 4%\nin terms of Pass@1. In addition, although DeCon might filter out correct\nassertions, the fault-finding ability of the remaining correct assertions\ndecreases only slightly."
                },
                "authors": [
                    {
                        "name": "Hao Yu"
                    },
                    {
                        "name": "Tianyu Chen"
                    },
                    {
                        "name": "Jiaming Huang"
                    },
                    {
                        "name": "Zongyang Li"
                    },
                    {
                        "name": "Dezhi Ran"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Assaf Marron"
                    },
                    {
                        "name": "David Harel"
                    },
                    {
                        "name": "Yuan Xie"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02885v1",
                "updated": "2025-01-06T09:55:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    9,
                    55,
                    55,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T09:55:55Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    9,
                    55,
                    55,
                    0,
                    6,
                    0
                ],
                "title": "MDP3: A Training-free Approach for List-wise Frame Selection in\n  Video-LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MDP3: A Training-free Approach for List-wise Frame Selection in\n  Video-LLMs"
                },
                "summary": "Video large language models (Video-LLMs) have made significant progress in\nunderstanding videos. However, processing multiple frames leads to lengthy\nvisual token sequences, presenting challenges such as the limited context\nlength cannot accommodate the entire video, and the inclusion of irrelevant\nframes hinders visual perception. Hence, effective frame selection is crucial.\nThis paper emphasizes that frame selection should follow three key principles:\nquery relevance, list-wise diversity, and sequentiality. Existing methods, such\nas uniform frame sampling and query-frame matching, do not capture all of these\nprinciples. Thus, we propose Markov decision determinantal point process with\ndynamic programming (MDP3) for frame selection, a training-free and\nmodel-agnostic method that can be seamlessly integrated into existing\nVideo-LLMs. Our method first estimates frame similarities conditioned on the\nquery using a conditional Gaussian kernel within the reproducing kernel Hilbert\nspace~(RKHS). We then apply the determinantal point process~(DPP) to the\nsimilarity matrix to capture both query relevance and list-wise diversity. To\nincorporate sequentiality, we segment the video and apply DPP within each\nsegment, conditioned on the preceding segment selection, modeled as a Markov\ndecision process~(MDP) for allocating selection sizes across segments.\nTheoretically, MDP3 provides a \\((1 - 1/e)\\)-approximate solution to the\nNP-hard list-wise frame selection problem with pseudo-polynomial time\ncomplexity, demonstrating its efficiency. Empirically, MDP3 significantly\noutperforms existing methods, verifying its effectiveness and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (Video-LLMs) have made significant progress in\nunderstanding videos. However, processing multiple frames leads to lengthy\nvisual token sequences, presenting challenges such as the limited context\nlength cannot accommodate the entire video, and the inclusion of irrelevant\nframes hinders visual perception. Hence, effective frame selection is crucial.\nThis paper emphasizes that frame selection should follow three key principles:\nquery relevance, list-wise diversity, and sequentiality. Existing methods, such\nas uniform frame sampling and query-frame matching, do not capture all of these\nprinciples. Thus, we propose Markov decision determinantal point process with\ndynamic programming (MDP3) for frame selection, a training-free and\nmodel-agnostic method that can be seamlessly integrated into existing\nVideo-LLMs. Our method first estimates frame similarities conditioned on the\nquery using a conditional Gaussian kernel within the reproducing kernel Hilbert\nspace~(RKHS). We then apply the determinantal point process~(DPP) to the\nsimilarity matrix to capture both query relevance and list-wise diversity. To\nincorporate sequentiality, we segment the video and apply DPP within each\nsegment, conditioned on the preceding segment selection, modeled as a Markov\ndecision process~(MDP) for allocating selection sizes across segments.\nTheoretically, MDP3 provides a \\((1 - 1/e)\\)-approximate solution to the\nNP-hard list-wise frame selection problem with pseudo-polynomial time\ncomplexity, demonstrating its efficiency. Empirically, MDP3 significantly\noutperforms existing methods, verifying its effectiveness and robustness."
                },
                "authors": [
                    {
                        "name": "Hui Sun"
                    },
                    {
                        "name": "Shiyin Lu"
                    },
                    {
                        "name": "Huanyu Wang"
                    },
                    {
                        "name": "Qing-Guo Chen"
                    },
                    {
                        "name": "Zhao Xu"
                    },
                    {
                        "name": "Weihua Luo"
                    },
                    {
                        "name": "Kaifu Zhang"
                    },
                    {
                        "name": "Ming Li"
                    }
                ],
                "author_detail": {
                    "name": "Ming Li"
                },
                "author": "Ming Li",
                "arxiv_comment": "24 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13432v2",
                "updated": "2025-01-06T09:27:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    9,
                    27,
                    0,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-18T02:07:21Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    2,
                    7,
                    21,
                    2,
                    353,
                    0
                ],
                "title": "Large Language Model Enhanced Recommender Systems: Taxonomy, Trend,\n  Application and Future",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Enhanced Recommender Systems: Taxonomy, Trend,\n  Application and Future"
                },
                "summary": "Large Language Model (LLM) has transformative potential in various domains,\nincluding recommender systems (RS). There have been a handful of research that\nfocuses on empowering the RS by LLM. However, previous efforts mainly focus on\nLLM as RS, which may face the challenge of intolerant inference costs by LLM.\nRecently, the integration of LLM into RS, known as LLM-Enhanced Recommender\nSystems (LLMERS), has garnered significant interest due to its potential to\naddress latency and memory constraints in real-world applications. This paper\npresents a comprehensive survey of the latest research efforts aimed at\nleveraging LLM to enhance RS capabilities. We identify a critical shift in the\nfield with the move towards incorporating LLM into the online system, notably\nby avoiding their use during inference. Our survey categorizes the existing\nLLMERS approaches into three primary types based on the component of the RS\nmodel being augmented: Knowledge Enhancement, Interaction Enhancement, and\nModel Enhancement. We provide an in-depth analysis of each category, discussing\nthe methodologies, challenges, and contributions of recent studies.\nFurthermore, we highlight several promising research directions that could\nfurther advance the field of LLMERS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) has transformative potential in various domains,\nincluding recommender systems (RS). There have been a handful of research that\nfocuses on empowering the RS by LLM. However, previous efforts mainly focus on\nLLM as RS, which may face the challenge of intolerant inference costs by LLM.\nRecently, the integration of LLM into RS, known as LLM-Enhanced Recommender\nSystems (LLMERS), has garnered significant interest due to its potential to\naddress latency and memory constraints in real-world applications. This paper\npresents a comprehensive survey of the latest research efforts aimed at\nleveraging LLM to enhance RS capabilities. We identify a critical shift in the\nfield with the move towards incorporating LLM into the online system, notably\nby avoiding their use during inference. Our survey categorizes the existing\nLLMERS approaches into three primary types based on the component of the RS\nmodel being augmented: Knowledge Enhancement, Interaction Enhancement, and\nModel Enhancement. We provide an in-depth analysis of each category, discussing\nthe methodologies, challenges, and contributions of recent studies.\nFurthermore, we highlight several promising research directions that could\nfurther advance the field of LLMERS."
                },
                "authors": [
                    {
                        "name": "Qidong Liu"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Yejing Wang"
                    },
                    {
                        "name": "Zijian Zhang"
                    },
                    {
                        "name": "Yuqi Sun"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Maolin Wang"
                    },
                    {
                        "name": "Pengyue Jia"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Feng Tian"
                    }
                ],
                "author_detail": {
                    "name": "Feng Tian"
                },
                "author": "Feng Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02869v1",
                "updated": "2025-01-06T09:22:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    9,
                    22,
                    36,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T09:22:36Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    9,
                    22,
                    36,
                    0,
                    6,
                    0
                ],
                "title": "IIMedGPT: Promoting Large Language Model Capabilities of Medical Tasks\n  by Efficient Human Preference Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IIMedGPT: Promoting Large Language Model Capabilities of Medical Tasks\n  by Efficient Human Preference Alignment"
                },
                "summary": "Recent researches of large language models(LLM), which is pre-trained on\nmassive general-purpose corpora, have achieved breakthroughs in responding\nhuman queries. However, these methods face challenges including limited data\ninsufficiency to support extensive pre-training and can not align responses\nwith users' instructions. To address these issues, we introduce a medical\ninstruction dataset, CMedINS, containing six medical instructions derived from\nactual medical tasks, which effectively fine-tunes LLM in conjunction with\nother data. Subsequently, We launch our medical model, IIMedGPT, employing an\nefficient preference alignment method, Direct preference Optimization(DPO). The\nresults show that our final model outperforms existing medical models in\nmedical dialogue.Datsets, Code and model checkpoints will be released upon\nacceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent researches of large language models(LLM), which is pre-trained on\nmassive general-purpose corpora, have achieved breakthroughs in responding\nhuman queries. However, these methods face challenges including limited data\ninsufficiency to support extensive pre-training and can not align responses\nwith users' instructions. To address these issues, we introduce a medical\ninstruction dataset, CMedINS, containing six medical instructions derived from\nactual medical tasks, which effectively fine-tunes LLM in conjunction with\nother data. Subsequently, We launch our medical model, IIMedGPT, employing an\nefficient preference alignment method, Direct preference Optimization(DPO). The\nresults show that our final model outperforms existing medical models in\nmedical dialogue.Datsets, Code and model checkpoints will be released upon\nacceptance."
                },
                "authors": [
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Zheng Chang"
                    },
                    {
                        "name": "Wentao Cai"
                    },
                    {
                        "name": "MengXing Ren"
                    },
                    {
                        "name": "Kang Yuan"
                    },
                    {
                        "name": "Yining Sun"
                    },
                    {
                        "name": "Zenghui Ding"
                    }
                ],
                "author_detail": {
                    "name": "Zenghui Ding"
                },
                "author": "Zenghui Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02858v1",
                "updated": "2025-01-06T09:06:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    9,
                    6,
                    29,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T09:06:29Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    9,
                    6,
                    29,
                    0,
                    6,
                    0
                ],
                "title": "A Novel Vision Transformer for Camera-LiDAR Fusion based Traffic Object\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Vision Transformer for Camera-LiDAR Fusion based Traffic Object\n  Segmentation"
                },
                "summary": "This paper presents Camera-LiDAR Fusion Transformer (CLFT) models for traffic\nobject segmentation, which leverage the fusion of camera and LiDAR data using\nvision transformers. Building on the methodology of visual transformers that\nexploit the self-attention mechanism, we extend segmentation capabilities with\nadditional classification options to a diverse class of objects including\ncyclists, traffic signs, and pedestrians across diverse weather conditions.\nDespite good performance, the models face challenges under adverse conditions\nwhich underscores the need for further optimization to enhance performance in\ndarkness and rain. In summary, the CLFT models offer a compelling solution for\nautonomous driving perception, advancing the state-of-the-art in multimodal\nfusion and object segmentation, with ongoing efforts required to address\nexisting limitations and fully harness their potential in practical\ndeployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents Camera-LiDAR Fusion Transformer (CLFT) models for traffic\nobject segmentation, which leverage the fusion of camera and LiDAR data using\nvision transformers. Building on the methodology of visual transformers that\nexploit the self-attention mechanism, we extend segmentation capabilities with\nadditional classification options to a diverse class of objects including\ncyclists, traffic signs, and pedestrians across diverse weather conditions.\nDespite good performance, the models face challenges under adverse conditions\nwhich underscores the need for further optimization to enhance performance in\ndarkness and rain. In summary, the CLFT models offer a compelling solution for\nautonomous driving perception, advancing the state-of-the-art in multimodal\nfusion and object segmentation, with ongoing efforts required to address\nexisting limitations and fully harness their potential in practical\ndeployments."
                },
                "authors": [
                    {
                        "name": "Toomas Tahves"
                    },
                    {
                        "name": "Junyi Gu"
                    },
                    {
                        "name": "Mauro Bellone"
                    },
                    {
                        "name": "Raivo Sell"
                    }
                ],
                "author_detail": {
                    "name": "Raivo Sell"
                },
                "author": "Raivo Sell",
                "arxiv_comment": "International Conference on Agents and Artificial Intelligence 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02844v1",
                "updated": "2025-01-06T08:43:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    8,
                    43,
                    31,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T08:43:31Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    8,
                    43,
                    31,
                    0,
                    6,
                    0
                ],
                "title": "Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text\n  Classification"
                },
                "summary": "Text classification is a fundamental task in natural language processing,\npivotal to various applications such as query optimization, data integration,\nand schema matching. While neural network-based models, such as CNN and BERT,\nhave demonstrated remarkable performance in text classification, their\neffectiveness heavily relies on abundant labeled training data. This dependency\nmakes these models less effective in dynamic few-shot text classification,\nwhere labeled data is scarce, and target labels frequently evolve based on\napplication needs. Recently, large language models (LLMs) have shown promise\ndue to their extensive pretraining and contextual understanding. Current\napproaches provide LLMs with text inputs, candidate labels, and additional side\ninformation (e.g., descriptions) to predict text labels. However, their\neffectiveness is hindered by the increased input size and the noise introduced\nthrough side information processing. To address these limitations, we propose a\ngraph-based online retrieval-augmented generation framework, namely GORAG, for\ndynamic few-shot text classification. GORAG constructs and maintains an\nadaptive information graph by extracting side information across all target\ntexts, rather than treating each input independently. It employs a weighted\nedge mechanism to prioritize the importance and reliability of extracted\ninformation and dynamically retrieves relevant context using a minimum-cost\nspanning tree tailored for each text input. Empirical evaluations demonstrate\nthat GORAG outperforms existing approaches by providing more comprehensive and\naccurate contextual information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text classification is a fundamental task in natural language processing,\npivotal to various applications such as query optimization, data integration,\nand schema matching. While neural network-based models, such as CNN and BERT,\nhave demonstrated remarkable performance in text classification, their\neffectiveness heavily relies on abundant labeled training data. This dependency\nmakes these models less effective in dynamic few-shot text classification,\nwhere labeled data is scarce, and target labels frequently evolve based on\napplication needs. Recently, large language models (LLMs) have shown promise\ndue to their extensive pretraining and contextual understanding. Current\napproaches provide LLMs with text inputs, candidate labels, and additional side\ninformation (e.g., descriptions) to predict text labels. However, their\neffectiveness is hindered by the increased input size and the noise introduced\nthrough side information processing. To address these limitations, we propose a\ngraph-based online retrieval-augmented generation framework, namely GORAG, for\ndynamic few-shot text classification. GORAG constructs and maintains an\nadaptive information graph by extracting side information across all target\ntexts, rather than treating each input independently. It employs a weighted\nedge mechanism to prioritize the importance and reliability of extracted\ninformation and dynamically retrieves relevant context using a minimum-cost\nspanning tree tailored for each text input. Empirical evaluations demonstrate\nthat GORAG outperforms existing approaches by providing more comprehensive and\naccurate contextual information."
                },
                "authors": [
                    {
                        "name": "Yubo Wang"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Fei Teng"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02841v1",
                "updated": "2025-01-06T08:38:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    8,
                    38,
                    5,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T08:38:05Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    8,
                    38,
                    5,
                    0,
                    6,
                    0
                ],
                "title": "Integrating Language-Image Prior into EEG Decoding for Cross-Task\n  Zero-Calibration RSVP-BCI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Language-Image Prior into EEG Decoding for Cross-Task\n  Zero-Calibration RSVP-BCI"
                },
                "summary": "Rapid Serial Visual Presentation (RSVP)-based Brain-Computer Interface (BCI)\nis an effective technology used for information detection by detecting\nEvent-Related Potentials (ERPs). The current RSVP decoding methods can perform\nwell in decoding EEG signals within a single RSVP task, but their decoding\nperformance significantly decreases when directly applied to different RSVP\ntasks without calibration data from the new tasks. This limits the rapid and\nefficient deployment of RSVP-BCI systems for detecting different categories of\ntargets in various scenarios. To overcome this limitation, this study aims to\nenhance the cross-task zero-calibration RSVP decoding performance. First, we\ndesign three distinct RSVP tasks for target image retrieval and build an\nopen-source dataset containing EEG signals and corresponding stimulus images.\nThen we propose an EEG with Language-Image Prior fusion Transformer\n(ELIPformer) for cross-task zero-calibration RSVP decoding. Specifically, we\npropose a prompt encoder based on the language-image pre-trained model to\nextract language-image features from task-specific prompts and stimulus images\nas prior knowledge for enhancing EEG decoding. A cross bidirectional attention\nmechanism is also adopted to facilitate the effective feature fusion and\nalignment between the EEG and language-image features. Extensive experiments\ndemonstrate that the proposed model achieves superior performance in cross-task\nzero-calibration RSVP decoding, which promotes the RSVP-BCI system from\nresearch to practical application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid Serial Visual Presentation (RSVP)-based Brain-Computer Interface (BCI)\nis an effective technology used for information detection by detecting\nEvent-Related Potentials (ERPs). The current RSVP decoding methods can perform\nwell in decoding EEG signals within a single RSVP task, but their decoding\nperformance significantly decreases when directly applied to different RSVP\ntasks without calibration data from the new tasks. This limits the rapid and\nefficient deployment of RSVP-BCI systems for detecting different categories of\ntargets in various scenarios. To overcome this limitation, this study aims to\nenhance the cross-task zero-calibration RSVP decoding performance. First, we\ndesign three distinct RSVP tasks for target image retrieval and build an\nopen-source dataset containing EEG signals and corresponding stimulus images.\nThen we propose an EEG with Language-Image Prior fusion Transformer\n(ELIPformer) for cross-task zero-calibration RSVP decoding. Specifically, we\npropose a prompt encoder based on the language-image pre-trained model to\nextract language-image features from task-specific prompts and stimulus images\nas prior knowledge for enhancing EEG decoding. A cross bidirectional attention\nmechanism is also adopted to facilitate the effective feature fusion and\nalignment between the EEG and language-image features. Extensive experiments\ndemonstrate that the proposed model achieves superior performance in cross-task\nzero-calibration RSVP decoding, which promotes the RSVP-BCI system from\nresearch to practical application."
                },
                "authors": [
                    {
                        "name": "Xujin Li"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Shuang Qiu"
                    },
                    {
                        "name": "Xinyi Zhang"
                    },
                    {
                        "name": "Fu Li"
                    },
                    {
                        "name": "Huiguang He"
                    }
                ],
                "author_detail": {
                    "name": "Huiguang He"
                },
                "author": "Huiguang He",
                "arxiv_comment": "15 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.5.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02825v1",
                "updated": "2025-01-06T07:57:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    7,
                    57,
                    51,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T07:57:51Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    7,
                    57,
                    51,
                    0,
                    6,
                    0
                ],
                "title": "Randomly Sampled Language Reasoning Problems Reveal Limits of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomly Sampled Language Reasoning Problems Reveal Limits of LLMs"
                },
                "summary": "Can LLMs pick up language structure from examples? Evidence in prior work\nseems to indicate yes, as pretrained models repeatedly demonstrate the ability\nto adapt to new language structures and vocabularies. However, this line of\nresearch typically considers languages that are present within common\npretraining datasets, or otherwise share notable similarities with these seen\nlanguages. In contrast, in this work we attempt to measure models' language\nunderstanding capacity while circumventing the risk of dataset recall. We\nparameterize large families of language tasks recognized by deterministic\nfinite automata (DFAs), and can thus sample novel language reasoning problems\nto fairly evaulate LLMs regardless of training data. We find that, even in the\nstrikingly simple setting of 3-state DFAs, LLMs underperform unparameterized\nngram models on both language recognition and synthesis tasks. These results\nsuggest that LLMs struggle to match the ability of basic language models in\nrecognizing and reasoning over languages that are sufficiently distinct from\nthe ones they see at training time, underscoring the distinction between\nlearning individual languages and possessing a general theory of language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs pick up language structure from examples? Evidence in prior work\nseems to indicate yes, as pretrained models repeatedly demonstrate the ability\nto adapt to new language structures and vocabularies. However, this line of\nresearch typically considers languages that are present within common\npretraining datasets, or otherwise share notable similarities with these seen\nlanguages. In contrast, in this work we attempt to measure models' language\nunderstanding capacity while circumventing the risk of dataset recall. We\nparameterize large families of language tasks recognized by deterministic\nfinite automata (DFAs), and can thus sample novel language reasoning problems\nto fairly evaulate LLMs regardless of training data. We find that, even in the\nstrikingly simple setting of 3-state DFAs, LLMs underperform unparameterized\nngram models on both language recognition and synthesis tasks. These results\nsuggest that LLMs struggle to match the ability of basic language models in\nrecognizing and reasoning over languages that are sufficiently distinct from\nthe ones they see at training time, underscoring the distinction between\nlearning individual languages and possessing a general theory of language."
                },
                "authors": [
                    {
                        "name": "Kavi Gupta"
                    },
                    {
                        "name": "Kate Sanders"
                    },
                    {
                        "name": "Armando Solar-Lezama"
                    }
                ],
                "author_detail": {
                    "name": "Armando Solar-Lezama"
                },
                "author": "Armando Solar-Lezama",
                "arxiv_comment": "8 pages, 3 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18537v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18537v2",
                "updated": "2025-01-06T07:42:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    7,
                    42,
                    20,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-24T16:38:04Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    38,
                    4,
                    1,
                    359,
                    0
                ],
                "title": "Harnessing Large Language Models for Knowledge Graph Question Answering\n  via Adaptive Multi-Aspect Retrieval-Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Large Language Models for Knowledge Graph Question Answering\n  via Adaptive Multi-Aspect Retrieval-Augmentation"
                },
                "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities, yet\nstruggle with hallucination and outdated knowledge when tasked with complex\nknowledge reasoning, resulting in factually incorrect outputs. Previous studies\nhave attempted to mitigate it by retrieving factual knowledge from large-scale\nknowledge graphs (KGs) to assist LLMs in logical reasoning and prediction of\nanswers. However, this kind of approach often introduces noise and irrelevant\ndata, especially in situations with extensive context from multiple knowledge\naspects. In this way, LLM attention can be potentially mislead from question\nand relevant information. In our study, we introduce an Adaptive Multi-Aspect\nRetrieval-augmented over KGs (Amar) framework. This method retrieves knowledge\nincluding entities, relations, and subgraphs, and converts each piece of\nretrieved text into prompt embeddings. The Amar framework comprises two key\nsub-components: 1) a self-alignment module that aligns commonalities among\nentities, relations, and subgraphs to enhance retrieved text, thereby reducing\nnoise interference; 2) a relevance gating module that employs a soft gate to\nlearn the relevance score between question and multi-aspect retrieved data, to\ndetermine which information should be used to enhance LLMs' output, or even\nfiltered altogether. Our method has achieved state-of-the-art performance on\ntwo common datasets, WebQSP and CWQ, showing a 1.9\\% improvement in accuracy\nover its best competitor and a 6.6\\% improvement in logical form generation\nover a method that directly uses retrieved text as context prompts. These\nresults demonstrate the effectiveness of Amar in improving the reasoning of\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate remarkable capabilities, yet\nstruggle with hallucination and outdated knowledge when tasked with complex\nknowledge reasoning, resulting in factually incorrect outputs. Previous studies\nhave attempted to mitigate it by retrieving factual knowledge from large-scale\nknowledge graphs (KGs) to assist LLMs in logical reasoning and prediction of\nanswers. However, this kind of approach often introduces noise and irrelevant\ndata, especially in situations with extensive context from multiple knowledge\naspects. In this way, LLM attention can be potentially mislead from question\nand relevant information. In our study, we introduce an Adaptive Multi-Aspect\nRetrieval-augmented over KGs (Amar) framework. This method retrieves knowledge\nincluding entities, relations, and subgraphs, and converts each piece of\nretrieved text into prompt embeddings. The Amar framework comprises two key\nsub-components: 1) a self-alignment module that aligns commonalities among\nentities, relations, and subgraphs to enhance retrieved text, thereby reducing\nnoise interference; 2) a relevance gating module that employs a soft gate to\nlearn the relevance score between question and multi-aspect retrieved data, to\ndetermine which information should be used to enhance LLMs' output, or even\nfiltered altogether. Our method has achieved state-of-the-art performance on\ntwo common datasets, WebQSP and CWQ, showing a 1.9\\% improvement in accuracy\nover its best competitor and a 6.6\\% improvement in logical form generation\nover a method that directly uses retrieved text as context prompts. These\nresults demonstrate the effectiveness of Amar in improving the reasoning of\nLLMs."
                },
                "authors": [
                    {
                        "name": "Derong Xu"
                    },
                    {
                        "name": "Xinhang Li"
                    },
                    {
                        "name": "Ziheng Zhang"
                    },
                    {
                        "name": "Zhenxi Lin"
                    },
                    {
                        "name": "Zhihong Zhu"
                    },
                    {
                        "name": "Zhi Zheng"
                    },
                    {
                        "name": "Xian Wu"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "arxiv_comment": "Accepted by AAAI'2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18537v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18537v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10238v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10238v2",
                "updated": "2025-01-06T07:39:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    7,
                    39,
                    41,
                    0,
                    6,
                    0
                ],
                "published": "2024-10-14T07:56:51Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    7,
                    56,
                    51,
                    0,
                    288,
                    0
                ],
                "title": "ForgeryGPT: Multimodal Large Language Model For Explainable Image\n  Forgery Detection and Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ForgeryGPT: Multimodal Large Language Model For Explainable Image\n  Forgery Detection and Localization"
                },
                "summary": "Multimodal Large Language Models (MLLMs), such as GPT4o, have shown strong\ncapabilities in visual reasoning and explanation generation. However, despite\nthese strengths, they face significant challenges in the increasingly critical\ntask of Image Forgery Detection and Localization (IFDL). Moreover, existing\nIFDL methods are typically limited to the learning of low-level\nsemantic-agnostic clues and merely provide a single outcome judgment. To tackle\nthese issues, we propose ForgeryGPT, a novel framework that advances the IFDL\ntask by capturing high-order forensics knowledge correlations of forged images\nfrom diverse linguistic feature spaces, while enabling explainable generation\nand interactive dialogue through a newly customized Large Language Model (LLM)\narchitecture. Specifically, ForgeryGPT enhances traditional LLMs by integrating\nthe Mask-Aware Forgery Extractor, which enables the excavating of precise\nforgery mask information from input images and facilitating pixel-level\nunderstanding of tampering artifacts. The Mask-Aware Forgery Extractor consists\nof a Forgery Localization Expert (FL-Expert) and a Mask Encoder, where the\nFL-Expert is augmented with an Object-agnostic Forgery Prompt and a\nVocabulary-enhanced Vision Encoder, allowing for effectively capturing of\nmulti-scale fine-grained forgery details. To enhance its performance, we\nimplement a three-stage training strategy, supported by our designed Mask-Text\nAlignment and IFDL Task-Specific Instruction Tuning datasets, which align\nvision-language modalities and improve forgery detection and\ninstruction-following capabilities. Extensive experiments demonstrate the\neffectiveness of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs), such as GPT4o, have shown strong\ncapabilities in visual reasoning and explanation generation. However, despite\nthese strengths, they face significant challenges in the increasingly critical\ntask of Image Forgery Detection and Localization (IFDL). Moreover, existing\nIFDL methods are typically limited to the learning of low-level\nsemantic-agnostic clues and merely provide a single outcome judgment. To tackle\nthese issues, we propose ForgeryGPT, a novel framework that advances the IFDL\ntask by capturing high-order forensics knowledge correlations of forged images\nfrom diverse linguistic feature spaces, while enabling explainable generation\nand interactive dialogue through a newly customized Large Language Model (LLM)\narchitecture. Specifically, ForgeryGPT enhances traditional LLMs by integrating\nthe Mask-Aware Forgery Extractor, which enables the excavating of precise\nforgery mask information from input images and facilitating pixel-level\nunderstanding of tampering artifacts. The Mask-Aware Forgery Extractor consists\nof a Forgery Localization Expert (FL-Expert) and a Mask Encoder, where the\nFL-Expert is augmented with an Object-agnostic Forgery Prompt and a\nVocabulary-enhanced Vision Encoder, allowing for effectively capturing of\nmulti-scale fine-grained forgery details. To enhance its performance, we\nimplement a three-stage training strategy, supported by our designed Mask-Text\nAlignment and IFDL Task-Specific Instruction Tuning datasets, which align\nvision-language modalities and improve forgery detection and\ninstruction-following capabilities. Extensive experiments demonstrate the\neffectiveness of the proposed method."
                },
                "authors": [
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Fanrui Zhang"
                    },
                    {
                        "name": "Jiaying Zhu"
                    },
                    {
                        "name": "Esther Sun"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Zheng-Jun Zha"
                    }
                ],
                "author_detail": {
                    "name": "Zheng-Jun Zha"
                },
                "author": "Zheng-Jun Zha",
                "arxiv_comment": "16 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10238v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20787v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20787v3",
                "updated": "2025-01-06T07:22:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    7,
                    22,
                    50,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-30T08:11:54Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    8,
                    11,
                    54,
                    0,
                    365,
                    0
                ],
                "title": "SecBench: A Comprehensive Multi-Dimensional Benchmarking Dataset for\n  LLMs in Cybersecurity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SecBench: A Comprehensive Multi-Dimensional Benchmarking Dataset for\n  LLMs in Cybersecurity"
                },
                "summary": "Evaluating Large Language Models (LLMs) is crucial for understanding their\ncapabilities and limitations across various applications, including natural\nlanguage processing and code generation. Existing benchmarks like MMLU, C-Eval,\nand HumanEval assess general LLM performance but lack focus on specific expert\ndomains such as cybersecurity. Previous attempts to create cybersecurity\ndatasets have faced limitations, including insufficient data volume and a\nreliance on multiple-choice questions (MCQs). To address these gaps, we propose\nSecBench, a multi-dimensional benchmarking dataset designed to evaluate LLMs in\nthe cybersecurity domain. SecBench includes questions in various formats (MCQs\nand short-answer questions (SAQs)), at different capability levels (Knowledge\nRetention and Logical Reasoning), in multiple languages (Chinese and English),\nand across various sub-domains. The dataset was constructed by collecting\nhigh-quality data from open sources and organizing a Cybersecurity Question\nDesign Contest, resulting in 44,823 MCQs and 3,087 SAQs. Particularly, we used\nthe powerful while cost-effective LLMs to (1). label the data and (2).\nconstructing a grading agent for automatic evaluation of SAQs. Benchmarking\nresults on 16 SOTA LLMs demonstrate the usability of SecBench, which is\narguably the largest and most comprehensive benchmark dataset for LLMs in\ncybersecurity. More information about SecBench can be found at our website, and\nthe dataset can be accessed via the artifact link.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models (LLMs) is crucial for understanding their\ncapabilities and limitations across various applications, including natural\nlanguage processing and code generation. Existing benchmarks like MMLU, C-Eval,\nand HumanEval assess general LLM performance but lack focus on specific expert\ndomains such as cybersecurity. Previous attempts to create cybersecurity\ndatasets have faced limitations, including insufficient data volume and a\nreliance on multiple-choice questions (MCQs). To address these gaps, we propose\nSecBench, a multi-dimensional benchmarking dataset designed to evaluate LLMs in\nthe cybersecurity domain. SecBench includes questions in various formats (MCQs\nand short-answer questions (SAQs)), at different capability levels (Knowledge\nRetention and Logical Reasoning), in multiple languages (Chinese and English),\nand across various sub-domains. The dataset was constructed by collecting\nhigh-quality data from open sources and organizing a Cybersecurity Question\nDesign Contest, resulting in 44,823 MCQs and 3,087 SAQs. Particularly, we used\nthe powerful while cost-effective LLMs to (1). label the data and (2).\nconstructing a grading agent for automatic evaluation of SAQs. Benchmarking\nresults on 16 SOTA LLMs demonstrate the usability of SecBench, which is\narguably the largest and most comprehensive benchmark dataset for LLMs in\ncybersecurity. More information about SecBench can be found at our website, and\nthe dataset can be accessed via the artifact link."
                },
                "authors": [
                    {
                        "name": "Pengfei Jing"
                    },
                    {
                        "name": "Mengyun Tang"
                    },
                    {
                        "name": "Xiaorong Shi"
                    },
                    {
                        "name": "Xing Zheng"
                    },
                    {
                        "name": "Sen Nie"
                    },
                    {
                        "name": "Shi Wu"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Xiapu Luo"
                    }
                ],
                "author_detail": {
                    "name": "Xiapu Luo"
                },
                "author": "Xiapu Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20787v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20787v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02808v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02808v1",
                "updated": "2025-01-06T07:11:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    7,
                    11,
                    34,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T07:11:34Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    7,
                    11,
                    34,
                    0,
                    6,
                    0
                ],
                "title": "DarkFarseer: Inductive Spatio-temporal Kriging via Hidden Style\n  Enhancement and Sparsity-Noise Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DarkFarseer: Inductive Spatio-temporal Kriging via Hidden Style\n  Enhancement and Sparsity-Noise Mitigation"
                },
                "summary": "With the rapid growth of the Internet of Things and Cyber-Physical Systems,\nwidespread sensor deployment has become essential. However, the high costs of\nbuilding sensor networks limit their scale and coverage, making fine-grained\ndeployment challenging. Inductive Spatio-Temporal Kriging (ISK) addresses this\nissue by introducing virtual sensors. Based on graph neural networks (GNNs)\nextracting the relationships between physical and virtual sensors, ISK can\ninfer the measurements of virtual sensors from physical sensors. However,\ncurrent ISK methods rely on conventional message-passing mechanisms and network\narchitectures, without effectively extracting spatio-temporal features of\nphysical sensors and focusing on representing virtual sensors. Additionally,\nexisting graph construction methods face issues of sparse and noisy\nconnections, destroying ISK performance. To address these issues, we propose\nDarkFarseer, a novel ISK framework with three key components. First, we propose\nthe Neighbor Hidden Style Enhancement module with a style transfer strategy to\nenhance the representation of virtual nodes in a temporal-then-spatial manner\nto better extract the spatial relationships between physical and virtual nodes.\nSecond, we propose Virtual-Component Contrastive Learning, which aims to enrich\nthe node representation by establishing the association between the patterns of\nvirtual nodes and the regional patterns within graph components. Lastly, we\ndesign a Similarity-Based Graph Denoising Strategy, which reduces the\nconnectivity strength of noisy connections around virtual nodes and their\nneighbors based on their temporal information and regional spatial patterns.\nExtensive experiments demonstrate that DarkFarseer significantly outperforms\nexisting ISK methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of the Internet of Things and Cyber-Physical Systems,\nwidespread sensor deployment has become essential. However, the high costs of\nbuilding sensor networks limit their scale and coverage, making fine-grained\ndeployment challenging. Inductive Spatio-Temporal Kriging (ISK) addresses this\nissue by introducing virtual sensors. Based on graph neural networks (GNNs)\nextracting the relationships between physical and virtual sensors, ISK can\ninfer the measurements of virtual sensors from physical sensors. However,\ncurrent ISK methods rely on conventional message-passing mechanisms and network\narchitectures, without effectively extracting spatio-temporal features of\nphysical sensors and focusing on representing virtual sensors. Additionally,\nexisting graph construction methods face issues of sparse and noisy\nconnections, destroying ISK performance. To address these issues, we propose\nDarkFarseer, a novel ISK framework with three key components. First, we propose\nthe Neighbor Hidden Style Enhancement module with a style transfer strategy to\nenhance the representation of virtual nodes in a temporal-then-spatial manner\nto better extract the spatial relationships between physical and virtual nodes.\nSecond, we propose Virtual-Component Contrastive Learning, which aims to enrich\nthe node representation by establishing the association between the patterns of\nvirtual nodes and the regional patterns within graph components. Lastly, we\ndesign a Similarity-Based Graph Denoising Strategy, which reduces the\nconnectivity strength of noisy connections around virtual nodes and their\nneighbors based on their temporal information and regional spatial patterns.\nExtensive experiments demonstrate that DarkFarseer significantly outperforms\nexisting ISK methods."
                },
                "authors": [
                    {
                        "name": "Zhuoxuan Liang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Dalin Zhang"
                    },
                    {
                        "name": "Yidan Chen"
                    },
                    {
                        "name": "Zhihong Wang"
                    },
                    {
                        "name": "Xiangping Zheng"
                    },
                    {
                        "name": "Moustafa Youssef"
                    }
                ],
                "author_detail": {
                    "name": "Moustafa Youssef"
                },
                "author": "Moustafa Youssef",
                "arxiv_comment": "TKDE (Under Review)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02808v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02808v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11932v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11932v3",
                "updated": "2025-01-06T06:33:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    33,
                    51,
                    0,
                    6,
                    0
                ],
                "published": "2024-04-18T06:20:50Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    6,
                    20,
                    50,
                    3,
                    109,
                    0
                ],
                "title": "CrossIn: An Efficient Instruction Tuning Approach for Cross-Lingual\n  Knowledge Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CrossIn: An Efficient Instruction Tuning Approach for Cross-Lingual\n  Knowledge Alignment"
                },
                "summary": "Multilingual proficiency presents a significant challenge for large language\nmodels (LLMs). English-centric models are usually suboptimal in other\nlanguages, particularly those that are linguistically distant from English.\nThis performance discrepancy mainly stems from the imbalanced distribution of\ntraining data across languages during pre-training and instruction tuning\nstages. To address this problem, we propose a novel approach called CrossIn,\nwhich utilizes a mixed composition of cross-lingual instruction tuning data.\nOur method leverages the compressed representation shared by various languages\nto efficiently enhance the model's task-solving capabilities and multilingual\nproficiency within a single process. In addition, we introduce a multi-task and\nmulti-faceted benchmark to evaluate the effectiveness of CrossIn. Experimental\nresults demonstrate that our method substantially improves performance across\ntasks and languages, and we provide extensive insights into the impact of\ncross-lingual data volume and the integration of translation data on enhancing\nmultilingual consistency and accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual proficiency presents a significant challenge for large language\nmodels (LLMs). English-centric models are usually suboptimal in other\nlanguages, particularly those that are linguistically distant from English.\nThis performance discrepancy mainly stems from the imbalanced distribution of\ntraining data across languages during pre-training and instruction tuning\nstages. To address this problem, we propose a novel approach called CrossIn,\nwhich utilizes a mixed composition of cross-lingual instruction tuning data.\nOur method leverages the compressed representation shared by various languages\nto efficiently enhance the model's task-solving capabilities and multilingual\nproficiency within a single process. In addition, we introduce a multi-task and\nmulti-faceted benchmark to evaluate the effectiveness of CrossIn. Experimental\nresults demonstrate that our method substantially improves performance across\ntasks and languages, and we provide extensive insights into the impact of\ncross-lingual data volume and the integration of translation data on enhancing\nmultilingual consistency and accuracy."
                },
                "authors": [
                    {
                        "name": "Geyu Lin"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Zhengyuan Liu"
                    },
                    {
                        "name": "Nancy F. Chen"
                    }
                ],
                "author_detail": {
                    "name": "Nancy F. Chen"
                },
                "author": "Nancy F. Chen",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11932v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11932v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.09402v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.09402v2",
                "updated": "2025-01-06T06:32:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    32,
                    9,
                    0,
                    6,
                    0
                ],
                "published": "2023-12-14T23:48:51Z",
                "published_parsed": [
                    2023,
                    12,
                    14,
                    23,
                    48,
                    51,
                    3,
                    348,
                    0
                ],
                "title": "CERN for AI: A Theoretical Framework for Autonomous Simulation-Based\n  Artificial Intelligence Testing and Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CERN for AI: A Theoretical Framework for Autonomous Simulation-Based\n  Artificial Intelligence Testing and Alignment"
                },
                "summary": "This paper explores the potential of a multidisciplinary approach to testing\nand aligning artificial intelligence (AI), specifically focusing on large\nlanguage models (LLMs). Due to the rapid development and wide application of\nLLMs, challenges such as ethical alignment, controllability, and predictability\nof these models emerged as global risks. This study investigates an innovative\nsimulation-based multi-agent system within a virtual reality framework that\nreplicates the real-world environment. The framework is populated by automated\n'digital citizens,' simulating complex social structures and interactions to\nexamine and optimize AI. Application of various theories from the fields of\nsociology, social psychology, computer science, physics, biology, and economics\ndemonstrates the possibility of a more human-aligned and socially responsible\nAI. The purpose of such a digital environment is to provide a dynamic platform\nwhere advanced AI agents can interact and make independent decisions, thereby\nmimicking realistic scenarios. The actors in this digital city, operated by the\nLLMs, serve as the primary agents, exhibiting high degrees of autonomy. While\nthis approach shows immense potential, there are notable challenges and\nlimitations, most significantly the unpredictable nature of real-world social\ndynamics. This research endeavors to contribute to the development and\nrefinement of AI, emphasizing the integration of social, ethical, and\ntheoretical dimensions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of a multidisciplinary approach to testing\nand aligning artificial intelligence (AI), specifically focusing on large\nlanguage models (LLMs). Due to the rapid development and wide application of\nLLMs, challenges such as ethical alignment, controllability, and predictability\nof these models emerged as global risks. This study investigates an innovative\nsimulation-based multi-agent system within a virtual reality framework that\nreplicates the real-world environment. The framework is populated by automated\n'digital citizens,' simulating complex social structures and interactions to\nexamine and optimize AI. Application of various theories from the fields of\nsociology, social psychology, computer science, physics, biology, and economics\ndemonstrates the possibility of a more human-aligned and socially responsible\nAI. The purpose of such a digital environment is to provide a dynamic platform\nwhere advanced AI agents can interact and make independent decisions, thereby\nmimicking realistic scenarios. The actors in this digital city, operated by the\nLLMs, serve as the primary agents, exhibiting high degrees of autonomy. While\nthis approach shows immense potential, there are notable challenges and\nlimitations, most significantly the unpredictable nature of real-world social\ndynamics. This research endeavors to contribute to the development and\nrefinement of AI, emphasizing the integration of social, ethical, and\ntheoretical dimensions for future research."
                },
                "authors": [
                    {
                        "name": "Ljubisa Bojic"
                    },
                    {
                        "name": "Matteo Cinelli"
                    },
                    {
                        "name": "Dubravko Culibrk"
                    },
                    {
                        "name": "Boris Delibasic"
                    }
                ],
                "author_detail": {
                    "name": "Boris Delibasic"
                },
                "author": "Boris Delibasic",
                "arxiv_doi": "10.1186/s40309-024-00238-0",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1186/s40309-024-00238-0",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.09402v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.09402v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "32 pages, 4 figures, 2 tables",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02795v1",
                "updated": "2025-01-06T06:29:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    29,
                    55,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T06:29:55Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    29,
                    55,
                    0,
                    6,
                    0
                ],
                "title": "InfiFusion: A Unified Framework for Enhanced Cross-Model Reasoning via\n  LLM Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiFusion: A Unified Framework for Enhanced Cross-Model Reasoning via\n  LLM Fusion"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong performance across\nvarious reasoning tasks, yet building a single model that consistently excels\nacross all domains remains challenging. This paper addresses this problem by\nexploring strategies to integrate multiple domain-specialized models into an\nefficient pivot model.We propose two fusion strategies to combine the strengths\nof multiple LLMs: (1) a pairwise, multi-step fusion approach that sequentially\ndistills each source model into the pivot model, followed by a weight merging\nstep to integrate the distilled models into the final model. This method\nachieves strong performance but requires substantial training effort; and (2) a\nunified fusion approach that aggregates all source models' outputs\nsimultaneously.To improve the fusion process, we introduce a novel\nRate-Skewness Adaptive Fusion (RSAF) technique, which dynamically adjusts top-K\nratios during parameter merging for enhanced flexibility and\nstability.Furthermore, we propose an uncertainty-based weighting method for the\nunified approach, which dynamically balances the contributions of source models\nand outperforms other logits/distribution ensemble methods.We achieved accuracy\nimprovements of 9.27%, 8.80%, and 8.89% on the GSM8K, MATH, and HumanEval\ntasks, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong performance across\nvarious reasoning tasks, yet building a single model that consistently excels\nacross all domains remains challenging. This paper addresses this problem by\nexploring strategies to integrate multiple domain-specialized models into an\nefficient pivot model.We propose two fusion strategies to combine the strengths\nof multiple LLMs: (1) a pairwise, multi-step fusion approach that sequentially\ndistills each source model into the pivot model, followed by a weight merging\nstep to integrate the distilled models into the final model. This method\nachieves strong performance but requires substantial training effort; and (2) a\nunified fusion approach that aggregates all source models' outputs\nsimultaneously.To improve the fusion process, we introduce a novel\nRate-Skewness Adaptive Fusion (RSAF) technique, which dynamically adjusts top-K\nratios during parameter merging for enhanced flexibility and\nstability.Furthermore, we propose an uncertainty-based weighting method for the\nunified approach, which dynamically balances the contributions of source models\nand outperforms other logits/distribution ensemble methods.We achieved accuracy\nimprovements of 9.27%, 8.80%, and 8.89% on the GSM8K, MATH, and HumanEval\ntasks, respectively."
                },
                "authors": [
                    {
                        "name": "Zhaoyi Yan"
                    },
                    {
                        "name": "Zhijie Sang"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Yuhao Fu"
                    },
                    {
                        "name": "Baoyi He"
                    },
                    {
                        "name": "Qi Zhou"
                    },
                    {
                        "name": "Yining Di"
                    },
                    {
                        "name": "Chunlin Ji"
                    },
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Fei Wu"
                    },
                    {
                        "name": "Hongxia Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hongxia Yang"
                },
                "author": "Hongxia Yang",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16594v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16594v5",
                "updated": "2025-01-06T05:53:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    5,
                    53,
                    18,
                    0,
                    6,
                    0
                ],
                "published": "2024-11-25T17:28:44Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    17,
                    28,
                    44,
                    0,
                    330,
                    0
                ],
                "title": "From Generation to Judgment: Opportunities and Challenges of\n  LLM-as-a-judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Generation to Judgment: Opportunities and Challenges of\n  LLM-as-a-judge"
                },
                "summary": "Assessment and evaluation have long been critical challenges in artificial\nintelligence (AI) and natural language processing (NLP). However, traditional\nmethods, whether matching-based or embedding-based, often fall short of judging\nsubtle attributes and delivering satisfactory results. Recent advancements in\nLarge Language Models (LLMs) inspire the \"LLM-as-a-judge\" paradigm, where LLMs\nare leveraged to perform scoring, ranking, or selection across various tasks\nand applications. This paper provides a comprehensive survey of LLM-based\njudgment and assessment, offering an in-depth overview to advance this emerging\nfield. We begin by giving detailed definitions from both input and output\nperspectives. Then we introduce a comprehensive taxonomy to explore\nLLM-as-a-judge from three dimensions: what to judge, how to judge and where to\njudge. Finally, we compile benchmarks for evaluating LLM-as-a-judge and\nhighlight key challenges and promising directions, aiming to provide valuable\ninsights and inspire future research in this promising research area. Paper\nlist and more resources about LLM-as-a-judge can be found at\n\\url{https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge} and\n\\url{https://llm-as-a-judge.github.io}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessment and evaluation have long been critical challenges in artificial\nintelligence (AI) and natural language processing (NLP). However, traditional\nmethods, whether matching-based or embedding-based, often fall short of judging\nsubtle attributes and delivering satisfactory results. Recent advancements in\nLarge Language Models (LLMs) inspire the \"LLM-as-a-judge\" paradigm, where LLMs\nare leveraged to perform scoring, ranking, or selection across various tasks\nand applications. This paper provides a comprehensive survey of LLM-based\njudgment and assessment, offering an in-depth overview to advance this emerging\nfield. We begin by giving detailed definitions from both input and output\nperspectives. Then we introduce a comprehensive taxonomy to explore\nLLM-as-a-judge from three dimensions: what to judge, how to judge and where to\njudge. Finally, we compile benchmarks for evaluating LLM-as-a-judge and\nhighlight key challenges and promising directions, aiming to provide valuable\ninsights and inspire future research in this promising research area. Paper\nlist and more resources about LLM-as-a-judge can be found at\n\\url{https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge} and\n\\url{https://llm-as-a-judge.github.io}."
                },
                "authors": [
                    {
                        "name": "Dawei Li"
                    },
                    {
                        "name": "Bohan Jiang"
                    },
                    {
                        "name": "Liangjie Huang"
                    },
                    {
                        "name": "Alimohammad Beigi"
                    },
                    {
                        "name": "Chengshuai Zhao"
                    },
                    {
                        "name": "Zhen Tan"
                    },
                    {
                        "name": "Amrita Bhattacharjee"
                    },
                    {
                        "name": "Yuxuan Jiang"
                    },
                    {
                        "name": "Canyu Chen"
                    },
                    {
                        "name": "Tianhao Wu"
                    },
                    {
                        "name": "Kai Shu"
                    },
                    {
                        "name": "Lu Cheng"
                    },
                    {
                        "name": "Huan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huan Liu"
                },
                "author": "Huan Liu",
                "arxiv_comment": "v5: add new citations; 36 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16594v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16594v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02775v1",
                "updated": "2025-01-06T05:37:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    5,
                    37,
                    31,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T05:37:31Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    5,
                    37,
                    31,
                    0,
                    6,
                    0
                ],
                "title": "Constructing 4D Radio Map in LEO Satellite Networks with Limited Samples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing 4D Radio Map in LEO Satellite Networks with Limited Samples"
                },
                "summary": "Recently, Low Earth Orbit (LEO) satellite networks (i.e., non-terrestrial\nnetwork (NTN)), such as Starlink, have been successfully deployed to provide\nbroader coverage than terrestrial networks (TN). Due to limited spectrum\nresources, TN and NTN may soon share the same spectrum. Therefore, fine-grained\nspectrum monitoring is crucial for spectrum sharing and interference avoidance.\nTo this end, constructing a 4D radio map (RM) including three spatial\ndimensions and signal spectra is important. However, this requires the large\ndeployment of sensors, and high-speed analog-to-digital converters for\nextensive spatial signal collection and wide power spectrum acquisition,\nrespectively. To address these challenges, we propose a deep unsupervised\nlearning framework without ground truths labeling requirement, DeepRM,\ncomprised of neural compressive sensing (CS) and tensor decomposition (TD)\nalgorithms. Firstly, we map the CS process into the optimization of a neural\nnetworksassociated loss function, and design a sparsity-performance balance\ntraining algorithm to reconstruct a wide power spectrum under limited\nsub-Nquist samples. Secondly, according to the output of neural CS algorithm,\nwe also utilize neural networks to perform TD, and construct the 3D RM for each\nfrequency, even under very sparse sensor deployment. Extensive evaluations show\nthat DeepRM achieves lower error than its corresponding state-of-the-art\nbaselines, especially with limited samples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Low Earth Orbit (LEO) satellite networks (i.e., non-terrestrial\nnetwork (NTN)), such as Starlink, have been successfully deployed to provide\nbroader coverage than terrestrial networks (TN). Due to limited spectrum\nresources, TN and NTN may soon share the same spectrum. Therefore, fine-grained\nspectrum monitoring is crucial for spectrum sharing and interference avoidance.\nTo this end, constructing a 4D radio map (RM) including three spatial\ndimensions and signal spectra is important. However, this requires the large\ndeployment of sensors, and high-speed analog-to-digital converters for\nextensive spatial signal collection and wide power spectrum acquisition,\nrespectively. To address these challenges, we propose a deep unsupervised\nlearning framework without ground truths labeling requirement, DeepRM,\ncomprised of neural compressive sensing (CS) and tensor decomposition (TD)\nalgorithms. Firstly, we map the CS process into the optimization of a neural\nnetworksassociated loss function, and design a sparsity-performance balance\ntraining algorithm to reconstruct a wide power spectrum under limited\nsub-Nquist samples. Secondly, according to the output of neural CS algorithm,\nwe also utilize neural networks to perform TD, and construct the 3D RM for each\nfrequency, even under very sparse sensor deployment. Extensive evaluations show\nthat DeepRM achieves lower error than its corresponding state-of-the-art\nbaselines, especially with limited samples."
                },
                "authors": [
                    {
                        "name": "Haoxuan Yuan"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Jinbo Peng"
                    },
                    {
                        "name": "Yuhang Zhong"
                    },
                    {
                        "name": "Xuanjie Hu"
                    },
                    {
                        "name": "Songyan Xue"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Yue Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Gao"
                },
                "author": "Yue Gao",
                "arxiv_comment": "11 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02758v1",
                "updated": "2025-01-06T04:44:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    4,
                    44,
                    52,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T04:44:52Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    4,
                    44,
                    52,
                    0,
                    6,
                    0
                ],
                "title": "Digital Twin Aided Channel Estimation: Zone-Specific Subspace Prediction\n  and Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Twin Aided Channel Estimation: Zone-Specific Subspace Prediction\n  and Calibration"
                },
                "summary": "Effective channel estimation in sparse and high-dimensional environments is\nessential for next-generation wireless systems, particularly in large-scale\nMIMO deployments. This paper introduces a novel framework that leverages\ndigital twins (DTs) as priors to enable efficient zone-specific subspace-based\nchannel estimation (CE). Subspace-based CE significantly reduces feedback\noverhead by focusing on the dominant channel components, exploiting sparsity in\nthe angular domain while preserving estimation accuracy. While DT channels may\nexhibit inaccuracies, their coarse-grained subspaces provide a powerful\nstarting point, reducing the search space and accelerating convergence. The\nframework employs a two-step clustering process on the Grassmann manifold,\ncombined with reinforcement learning (RL), to iteratively calibrate subspaces\nand align them with real-world counterparts. Simulations show that digital\ntwins not only enable near-optimal performance but also enhance the accuracy of\nsubspace calibration through RL, highlighting their potential as a step towards\nlearnable digital twins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective channel estimation in sparse and high-dimensional environments is\nessential for next-generation wireless systems, particularly in large-scale\nMIMO deployments. This paper introduces a novel framework that leverages\ndigital twins (DTs) as priors to enable efficient zone-specific subspace-based\nchannel estimation (CE). Subspace-based CE significantly reduces feedback\noverhead by focusing on the dominant channel components, exploiting sparsity in\nthe angular domain while preserving estimation accuracy. While DT channels may\nexhibit inaccuracies, their coarse-grained subspaces provide a powerful\nstarting point, reducing the search space and accelerating convergence. The\nframework employs a two-step clustering process on the Grassmann manifold,\ncombined with reinforcement learning (RL), to iteratively calibrate subspaces\nand align them with real-world counterparts. Simulations show that digital\ntwins not only enable near-optimal performance but also enhance the accuracy of\nsubspace calibration through RL, highlighting their potential as a step towards\nlearnable digital twins."
                },
                "authors": [
                    {
                        "name": "Sadjad Alikhani"
                    },
                    {
                        "name": "Ahmed Alkhateeb"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Alkhateeb"
                },
                "author": "Ahmed Alkhateeb",
                "arxiv_comment": "Dataset and code files will be available soon on the WI-Lab website:\n  https://www.wi-lab.net/research/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20024v2",
                "updated": "2025-01-06T04:34:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    4,
                    34,
                    16,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-28T05:01:26Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    5,
                    1,
                    26,
                    5,
                    363,
                    0
                ],
                "title": "BaiJia: A Large-Scale Role-Playing Agent Corpus of Chinese Historical\n  Characters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BaiJia: A Large-Scale Role-Playing Agent Corpus of Chinese Historical\n  Characters"
                },
                "summary": "We introduce a comprehensive large-scale role-playing agent corpus, termed\nBaiJia, that comprises various Chinese historical characters. This corpus is\nnoteworthy for being the pioneering compilation of low-resource data that can\nbe utilized in large language models (LLMs) to engage in AI-driven historical\nrole-playing agents. BaiJia addresses the challenges in terms of fragmented\nhistorical textual records in different forms and modalities, integrating\nvarious characters' information, including their biographical, literary, family\nrelations, historical events, and so on. We conduct extensive experiments to\ndemonstrate the effectiveness of our BaiJia agent corpus in bolstering the\nrole-playing abilities of various foundational LLMs, and promoting the\ndevelopment and assessment of LLMs in the context of historical role-playing\ntasks. The agent corpus is available at baijia.online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a comprehensive large-scale role-playing agent corpus, termed\nBaiJia, that comprises various Chinese historical characters. This corpus is\nnoteworthy for being the pioneering compilation of low-resource data that can\nbe utilized in large language models (LLMs) to engage in AI-driven historical\nrole-playing agents. BaiJia addresses the challenges in terms of fragmented\nhistorical textual records in different forms and modalities, integrating\nvarious characters' information, including their biographical, literary, family\nrelations, historical events, and so on. We conduct extensive experiments to\ndemonstrate the effectiveness of our BaiJia agent corpus in bolstering the\nrole-playing abilities of various foundational LLMs, and promoting the\ndevelopment and assessment of LLMs in the context of historical role-playing\ntasks. The agent corpus is available at baijia.online."
                },
                "authors": [
                    {
                        "name": "Ting Bai"
                    },
                    {
                        "name": "Jiazheng Kang"
                    },
                    {
                        "name": "Jiayang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Jiayang Fan"
                },
                "author": "Jiayang Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02751v1",
                "updated": "2025-01-06T03:58:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    3,
                    58,
                    31,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T03:58:31Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    3,
                    58,
                    31,
                    0,
                    6,
                    0
                ],
                "title": "Ultrasound-QBench: Can LLMs Aid in Quality Assessment of Ultrasound\n  Imaging?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultrasound-QBench: Can LLMs Aid in Quality Assessment of Ultrasound\n  Imaging?"
                },
                "summary": "With the dramatic upsurge in the volume of ultrasound examinations,\nlow-quality ultrasound imaging has gradually increased due to variations in\noperator proficiency and imaging circumstances, imposing a severe burden on\ndiagnosis accuracy and even entailing the risk of restarting the diagnosis in\ncritical cases. To assist clinicians in selecting high-quality ultrasound\nimages and ensuring accurate diagnoses, we introduce Ultrasound-QBench, a\ncomprehensive benchmark that systematically evaluates multimodal large language\nmodels (MLLMs) on quality assessment tasks of ultrasound images.\nUltrasound-QBench establishes two datasets collected from diverse sources:\nIVUSQA, consisting of 7,709 images, and CardiacUltraQA, containing 3,863\nimages. These images encompassing common ultrasound imaging artifacts are\nannotated by professional ultrasound experts and classified into three quality\nlevels: high, medium, and low. To better evaluate MLLMs, we decompose the\nquality assessment task into three dimensionalities: qualitative\nclassification, quantitative scoring, and comparative assessment. The\nevaluation of 7 open-source MLLMs as well as 1 proprietary MLLMs demonstrates\nthat MLLMs possess preliminary capabilities for low-level visual tasks in\nultrasound image quality classification. We hope this benchmark will inspire\nthe research community to delve deeper into uncovering and enhancing the\nuntapped potential of MLLMs for medical imaging tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the dramatic upsurge in the volume of ultrasound examinations,\nlow-quality ultrasound imaging has gradually increased due to variations in\noperator proficiency and imaging circumstances, imposing a severe burden on\ndiagnosis accuracy and even entailing the risk of restarting the diagnosis in\ncritical cases. To assist clinicians in selecting high-quality ultrasound\nimages and ensuring accurate diagnoses, we introduce Ultrasound-QBench, a\ncomprehensive benchmark that systematically evaluates multimodal large language\nmodels (MLLMs) on quality assessment tasks of ultrasound images.\nUltrasound-QBench establishes two datasets collected from diverse sources:\nIVUSQA, consisting of 7,709 images, and CardiacUltraQA, containing 3,863\nimages. These images encompassing common ultrasound imaging artifacts are\nannotated by professional ultrasound experts and classified into three quality\nlevels: high, medium, and low. To better evaluate MLLMs, we decompose the\nquality assessment task into three dimensionalities: qualitative\nclassification, quantitative scoring, and comparative assessment. The\nevaluation of 7 open-source MLLMs as well as 1 proprietary MLLMs demonstrates\nthat MLLMs possess preliminary capabilities for low-level visual tasks in\nultrasound image quality classification. We hope this benchmark will inspire\nthe research community to delve deeper into uncovering and enhancing the\nuntapped potential of MLLMs for medical imaging tasks."
                },
                "authors": [
                    {
                        "name": "Hongyi Miao"
                    },
                    {
                        "name": "Jun Jia"
                    },
                    {
                        "name": "Yankun Cao"
                    },
                    {
                        "name": "Yingjie Zhou"
                    },
                    {
                        "name": "Yanwei Jiang"
                    },
                    {
                        "name": "Zhi Liu"
                    },
                    {
                        "name": "Guangtao Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Guangtao Zhai"
                },
                "author": "Guangtao Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12470v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12470v3",
                "updated": "2025-01-06T03:48:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    3,
                    48,
                    27,
                    0,
                    6,
                    0
                ],
                "published": "2024-08-22T15:10:56Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    10,
                    56,
                    3,
                    235,
                    0
                ],
                "title": "DLCRec: A Novel Approach for Managing Diversity in LLM-Based Recommender\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DLCRec: A Novel Approach for Managing Diversity in LLM-Based Recommender\n  Systems"
                },
                "summary": "The integration of Large Language Models (LLMs) into recommender systems has\nled to substantial performance improvements. However, this often comes at the\ncost of diminished recommendation diversity, which can negatively impact user\nsatisfaction. To address this issue, controllable recommendation has emerged as\na promising approach, allowing users to specify their preferences and receive\nrecommendations that meet their diverse needs. Despite its potential, existing\ncontrollable recommender systems frequently rely on simplistic mechanisms, such\nas a single prompt, to regulate diversity-an approach that falls short of\ncapturing the full complexity of user preferences. In response to these\nlimitations, we propose DLCRec, a novel framework designed to enable\nfine-grained control over diversity in LLM-based recommendations. Unlike\ntraditional methods, DLCRec adopts a fine-grained task decomposition strategy,\nbreaking down the recommendation process into three sequential sub-tasks: genre\nprediction, genre filling, and item prediction. These sub-tasks are trained\nindependently and inferred sequentially according to user-defined control\nnumbers, ensuring more precise control over diversity. Furthermore, the\nscarcity and uneven distribution of diversity-related user behavior data pose\nsignificant challenges for fine-tuning. To overcome these obstacles, we\nintroduce two data augmentation techniques that enhance the model's robustness\nto noisy and out-of-distribution data. These techniques expose the model to a\nbroader range of patterns, improving its adaptability in generating\nrecommendations with varying levels of diversity. Our extensive empirical\nevaluation demonstrates that DLCRec not only provides precise control over\ndiversity but also outperforms state-of-the-art baselines across multiple\nrecommendation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into recommender systems has\nled to substantial performance improvements. However, this often comes at the\ncost of diminished recommendation diversity, which can negatively impact user\nsatisfaction. To address this issue, controllable recommendation has emerged as\na promising approach, allowing users to specify their preferences and receive\nrecommendations that meet their diverse needs. Despite its potential, existing\ncontrollable recommender systems frequently rely on simplistic mechanisms, such\nas a single prompt, to regulate diversity-an approach that falls short of\ncapturing the full complexity of user preferences. In response to these\nlimitations, we propose DLCRec, a novel framework designed to enable\nfine-grained control over diversity in LLM-based recommendations. Unlike\ntraditional methods, DLCRec adopts a fine-grained task decomposition strategy,\nbreaking down the recommendation process into three sequential sub-tasks: genre\nprediction, genre filling, and item prediction. These sub-tasks are trained\nindependently and inferred sequentially according to user-defined control\nnumbers, ensuring more precise control over diversity. Furthermore, the\nscarcity and uneven distribution of diversity-related user behavior data pose\nsignificant challenges for fine-tuning. To overcome these obstacles, we\nintroduce two data augmentation techniques that enhance the model's robustness\nto noisy and out-of-distribution data. These techniques expose the model to a\nbroader range of patterns, improving its adaptability in generating\nrecommendations with varying levels of diversity. Our extensive empirical\nevaluation demonstrates that DLCRec not only provides precise control over\ndiversity but also outperforms state-of-the-art baselines across multiple\nrecommendation scenarios."
                },
                "authors": [
                    {
                        "name": "Jiaju Chen"
                    },
                    {
                        "name": "Chongming Gao"
                    },
                    {
                        "name": "Shuai Yuan"
                    },
                    {
                        "name": "Shuchang Liu"
                    },
                    {
                        "name": "Qingpeng Cai"
                    },
                    {
                        "name": "Peng Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Jiang"
                },
                "author": "Peng Jiang",
                "arxiv_comment": "Accepted by WSDM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12470v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12470v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14743v2",
                "updated": "2025-01-06T03:18:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    3,
                    18,
                    15,
                    0,
                    6,
                    0
                ],
                "published": "2024-09-23T06:41:52Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    6,
                    41,
                    52,
                    0,
                    267,
                    0
                ],
                "title": "LlamaPartialSpoof: An LLM-Driven Fake Speech Dataset Simulating\n  Disinformation Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LlamaPartialSpoof: An LLM-Driven Fake Speech Dataset Simulating\n  Disinformation Generation"
                },
                "summary": "Previous fake speech datasets were constructed from a defender's perspective\nto develop countermeasure (CM) systems without considering diverse motivations\nof attackers. To better align with real-life scenarios, we created\nLlamaPartialSpoof, a 130-hour dataset that contains both fully and partially\nfake speech, using a large language model (LLM) and voice cloning technologies\nto evaluate the robustness of CMs. By examining valuable information for both\nattackers and defenders, we identify several key vulnerabilities in current CM\nsystems, which can be exploited to enhance attack success rates, including\nbiases toward certain text-to-speech models or concatenation methods. Our\nexperimental results indicate that the current fake speech detection system\nstruggle to generalize to unseen scenarios, achieving a best performance of\n24.49% equal error rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous fake speech datasets were constructed from a defender's perspective\nto develop countermeasure (CM) systems without considering diverse motivations\nof attackers. To better align with real-life scenarios, we created\nLlamaPartialSpoof, a 130-hour dataset that contains both fully and partially\nfake speech, using a large language model (LLM) and voice cloning technologies\nto evaluate the robustness of CMs. By examining valuable information for both\nattackers and defenders, we identify several key vulnerabilities in current CM\nsystems, which can be exploited to enhance attack success rates, including\nbiases toward certain text-to-speech models or concatenation methods. Our\nexperimental results indicate that the current fake speech detection system\nstruggle to generalize to unseen scenarios, achieving a best performance of\n24.49% equal error rate."
                },
                "authors": [
                    {
                        "name": "Hieu-Thi Luong"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Kong Aik Lee"
                    },
                    {
                        "name": "Eng Siong Chng"
                    }
                ],
                "author_detail": {
                    "name": "Eng Siong Chng"
                },
                "author": "Eng Siong Chng",
                "arxiv_comment": "5 pages, ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02739v1",
                "updated": "2025-01-06T03:17:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    3,
                    17,
                    35,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T03:17:35Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    3,
                    17,
                    35,
                    0,
                    6,
                    0
                ],
                "title": "TARDiS : Text Augmentation for Refining Diversity and Separability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TARDiS : Text Augmentation for Refining Diversity and Separability"
                },
                "summary": "Text augmentation (TA) is a critical technique for text classification,\nespecially in few-shot settings. This paper introduces a novel LLM-based TA\nmethod, TARDiS, to address challenges inherent in the generation and alignment\nstages of two-stage TA methods. For the generation stage, we propose two\ngeneration processes, SEG and CEG, incorporating multiple class-specific\nprompts to enhance diversity and separability. For the alignment stage, we\nintroduce a class adaptation (CA) method to ensure that generated examples\nalign with their target classes through verification and modification.\nExperimental results demonstrate TARDiS's effectiveness, outperforming\nstate-of-the-art LLM-based TA methods in various few-shot text classification\ntasks. An in-depth analysis confirms the detailed behaviors at each stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text augmentation (TA) is a critical technique for text classification,\nespecially in few-shot settings. This paper introduces a novel LLM-based TA\nmethod, TARDiS, to address challenges inherent in the generation and alignment\nstages of two-stage TA methods. For the generation stage, we propose two\ngeneration processes, SEG and CEG, incorporating multiple class-specific\nprompts to enhance diversity and separability. For the alignment stage, we\nintroduce a class adaptation (CA) method to ensure that generated examples\nalign with their target classes through verification and modification.\nExperimental results demonstrate TARDiS's effectiveness, outperforming\nstate-of-the-art LLM-based TA methods in various few-shot text classification\ntasks. An in-depth analysis confirms the detailed behaviors at each stage."
                },
                "authors": [
                    {
                        "name": "Kyungmin Kim"
                    },
                    {
                        "name": "SangHun Im"
                    },
                    {
                        "name": "GiBaeg Kim"
                    },
                    {
                        "name": "Heung-Seon Oh"
                    }
                ],
                "author_detail": {
                    "name": "Heung-Seon Oh"
                },
                "author": "Heung-Seon Oh",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02738v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02738v1",
                "updated": "2025-01-06T03:16:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    3,
                    16,
                    22,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T03:16:22Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    3,
                    16,
                    22,
                    0,
                    6,
                    0
                ],
                "title": "SCSC: A Novel Standards-Compatible Semantic Communication Framework for\n  Image Transmission",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCSC: A Novel Standards-Compatible Semantic Communication Framework for\n  Image Transmission"
                },
                "summary": "Joint source-channel coding (JSCC) is a promising paradigm for\nnext-generation communication systems, particularly in challenging transmission\nenvironments. In this paper, we propose a novel standard-compatible JSCC\nframework for the transmission of images over multiple-input multiple-output\n(MIMO) channels. Different from the existing end-to-end AI-based DeepJSCC\nschemes, our framework consists of learnable modules that enable communication\nusing conventional separate source and channel codes (SSCC), which makes it\namenable for easy deployment on legacy systems. Specifically, the learnable\nmodules involve a preprocessing-empowered network (PPEN) for preserving\nessential semantic information, and a precoder \\& combiner-enhanced network\n(PCEN) for efficient transmission over a resource-constrained MIMO channel. We\ntreat existing compression and channel coding modules as non-trainable blocks.\nSince the parameters of these modules are non-differentiable, we employ a proxy\nnetwork that mimics their operations when training the learnable modules.\nNumerical results demonstrate that our scheme can save more than 29\\% of the\nchannel bandwidth, and requires lower complexity compared to the constrained\nbaselines. We also show its generalization capability to unseen datasets and\ntasks through extensive experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint source-channel coding (JSCC) is a promising paradigm for\nnext-generation communication systems, particularly in challenging transmission\nenvironments. In this paper, we propose a novel standard-compatible JSCC\nframework for the transmission of images over multiple-input multiple-output\n(MIMO) channels. Different from the existing end-to-end AI-based DeepJSCC\nschemes, our framework consists of learnable modules that enable communication\nusing conventional separate source and channel codes (SSCC), which makes it\namenable for easy deployment on legacy systems. Specifically, the learnable\nmodules involve a preprocessing-empowered network (PPEN) for preserving\nessential semantic information, and a precoder \\& combiner-enhanced network\n(PCEN) for efficient transmission over a resource-constrained MIMO channel. We\ntreat existing compression and channel coding modules as non-trainable blocks.\nSince the parameters of these modules are non-differentiable, we employ a proxy\nnetwork that mimics their operations when training the learnable modules.\nNumerical results demonstrate that our scheme can save more than 29\\% of the\nchannel bandwidth, and requires lower complexity compared to the constrained\nbaselines. We also show its generalization capability to unseen datasets and\ntasks through extensive experiments."
                },
                "authors": [
                    {
                        "name": "Xue Han"
                    },
                    {
                        "name": "Yongpeng Wu"
                    },
                    {
                        "name": "Zhen Gao"
                    },
                    {
                        "name": "Biqian Feng"
                    },
                    {
                        "name": "Yuxuan Shi"
                    },
                    {
                        "name": "Deniz Gndz"
                    },
                    {
                        "name": "Wenjun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenjun Zhang"
                },
                "author": "Wenjun Zhang",
                "arxiv_comment": "Accepted by IEEE Transactions on Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02738v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02738v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02725v1",
                "updated": "2025-01-06T02:46:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    2,
                    46,
                    33,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T02:46:33Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    2,
                    46,
                    33,
                    0,
                    6,
                    0
                ],
                "title": "Artificial Intelligence in Creative Industries: Advances Prior to 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence in Creative Industries: Advances Prior to 2025"
                },
                "summary": "The rapid advancements in artificial intelligence (AI), particularly in\ngenerative AI and large language models (LLMs), have profoundly impacted the\ncreative industries by enabling innovative content creation, enhancing\nworkflows, and democratizing access to creative tools. This paper explores the\nsignificant technological shifts since our previous review in 2022,\nhighlighting how these developments have expanded creative opportunities and\nefficiency. These technological advancements have enhanced the capabilities of\ntext-to-image, text-to-video, and multimodal generation technologies. In\nparticular, key breakthroughs in LLMs have established new benchmarks in\nconversational AI, while advancements in image generators have revolutionized\ncontent creation. We also discuss AI integration into post-production\nworkflows, which has significantly accelerated and refined traditional\nprocesses. Despite these innovations, challenges remain, particularly for the\nmedia industry, due to the demands on communication traffic from creative\ncontent. We therefore include data compression and quality assessment in this\npaper. Furthermore, we highlight the trend toward unified AI frameworks capable\nof addressing multiple creative tasks and underscore the importance of human\noversight to mitigate AI-generated inaccuracies. Finally, we explore AI's\nfuture potential in the creative sector, stressing the need to navigate\nemerging challenges to maximize its benefits while addressing associated risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in artificial intelligence (AI), particularly in\ngenerative AI and large language models (LLMs), have profoundly impacted the\ncreative industries by enabling innovative content creation, enhancing\nworkflows, and democratizing access to creative tools. This paper explores the\nsignificant technological shifts since our previous review in 2022,\nhighlighting how these developments have expanded creative opportunities and\nefficiency. These technological advancements have enhanced the capabilities of\ntext-to-image, text-to-video, and multimodal generation technologies. In\nparticular, key breakthroughs in LLMs have established new benchmarks in\nconversational AI, while advancements in image generators have revolutionized\ncontent creation. We also discuss AI integration into post-production\nworkflows, which has significantly accelerated and refined traditional\nprocesses. Despite these innovations, challenges remain, particularly for the\nmedia industry, due to the demands on communication traffic from creative\ncontent. We therefore include data compression and quality assessment in this\npaper. Furthermore, we highlight the trend toward unified AI frameworks capable\nof addressing multiple creative tasks and underscore the importance of human\noversight to mitigate AI-generated inaccuracies. Finally, we explore AI's\nfuture potential in the creative sector, stressing the need to navigate\nemerging challenges to maximize its benefits while addressing associated risks."
                },
                "authors": [
                    {
                        "name": "Nantheera Anantrasirichai"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "David Bull"
                    }
                ],
                "author_detail": {
                    "name": "David Bull"
                },
                "author": "David Bull",
                "arxiv_comment": "This is an updated review of our previous paper (see\n  https://doi.org/10.1007/s10462-021-10039-7)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09824v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09824v6",
                "updated": "2025-01-06T02:16:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    2,
                    16,
                    37,
                    0,
                    6,
                    0
                ],
                "published": "2024-10-13T12:57:08Z",
                "published_parsed": [
                    2024,
                    10,
                    13,
                    12,
                    57,
                    8,
                    6,
                    287,
                    0
                ],
                "title": "LLM-Based Multi-Agent Systems are Scalable Graph Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Multi-Agent Systems are Scalable Graph Generative Models"
                },
                "summary": "The structural properties of naturally arising social graphs are extensively\nstudied to understand their evolution. Prior approaches for modeling network\ndynamics typically rely on rule-based models, which lack realism and\ngeneralizability, or deep learning-based models, which require large-scale\ntraining datasets. Social graphs, as abstract graph representations of\nentity-wise interactions, present an opportunity to explore network evolution\nmechanisms through realistic simulations of human-item interactions. Leveraging\nthe pre-trained social consensus knowledge embedded in large language models\n(LLMs), we present GraphAgent-Generator (GAG), a novel simulation-based\nframework for dynamic, text-attributed social graph generation. GAG simulates\nthe temporal node and edge generation processes for zero-shot social graph\ngeneration. The resulting graphs exhibit adherence to seven key macroscopic\nnetwork properties, achieving an 11% improvement in microscopic graph structure\nmetrics. Through the node classification benchmarking task, we validate GAG\neffectively captures the intricate text-structure correlations in graph\ngeneration. Furthermore, GAG supports generating graphs with up to nearly\n100,000 nodes or 10 million edges through large-scale LLM-based agent\nsimulation with parallel acceleration, achieving a minimum speed-up of 90.4%.\nThe source code is available at https://github.com/Ji-Cather/GraphAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The structural properties of naturally arising social graphs are extensively\nstudied to understand their evolution. Prior approaches for modeling network\ndynamics typically rely on rule-based models, which lack realism and\ngeneralizability, or deep learning-based models, which require large-scale\ntraining datasets. Social graphs, as abstract graph representations of\nentity-wise interactions, present an opportunity to explore network evolution\nmechanisms through realistic simulations of human-item interactions. Leveraging\nthe pre-trained social consensus knowledge embedded in large language models\n(LLMs), we present GraphAgent-Generator (GAG), a novel simulation-based\nframework for dynamic, text-attributed social graph generation. GAG simulates\nthe temporal node and edge generation processes for zero-shot social graph\ngeneration. The resulting graphs exhibit adherence to seven key macroscopic\nnetwork properties, achieving an 11% improvement in microscopic graph structure\nmetrics. Through the node classification benchmarking task, we validate GAG\neffectively captures the intricate text-structure correlations in graph\ngeneration. Furthermore, GAG supports generating graphs with up to nearly\n100,000 nodes or 10 million edges through large-scale LLM-based agent\nsimulation with parallel acceleration, achieving a minimum speed-up of 90.4%.\nThe source code is available at https://github.com/Ji-Cather/GraphAgent."
                },
                "authors": [
                    {
                        "name": "Jiarui Ji"
                    },
                    {
                        "name": "Runlin Lei"
                    },
                    {
                        "name": "Jialing Bi"
                    },
                    {
                        "name": "Zhewei Wei"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Xuchen Pan"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    }
                ],
                "author_detail": {
                    "name": "Bolin Ding"
                },
                "author": "Bolin Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09824v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09824v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02642v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02642v3",
                "updated": "2025-01-06T01:52:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    52,
                    41,
                    0,
                    6,
                    0
                ],
                "published": "2024-06-04T10:59:43Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    10,
                    59,
                    43,
                    1,
                    156,
                    0
                ],
                "title": "E-ICL: Enhancing Fine-Grained Emotion Recognition through the Lens of\n  Prototype Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-ICL: Enhancing Fine-Grained Emotion Recognition through the Lens of\n  Prototype Theory"
                },
                "summary": "In-context learning (ICL) achieves remarkable performance in various domains\nsuch as knowledge acquisition, commonsense reasoning, and semantic\nunderstanding. However, its performance significantly deteriorates for emotion\ndetection tasks, especially fine-grained emotion recognition. The underlying\nreasons for this remain unclear. In this paper, we identify the reasons behind\nICL's poor performance from the perspective of prototype theory and propose a\nmethod to address this issue. Specifically, we conduct extensive pilot\nexperiments and find that ICL conforms to the prototype theory on fine-grained\nemotion recognition. Based on this theory, we uncover the following\ndeficiencies in ICL: (1) It relies on prototypes (example-label pairs) that are\nsemantically similar but emotionally inaccurate to predict emotions. (2) It is\nprone to interference from irrelevant categories, affecting the accuracy and\nrobustness of the predictions. To address these issues, we propose an Emotion\nContext Learning method (E-ICL) on fine-grained emotion recognition. E-ICL\nrelies on more emotionally accurate prototypes to predict categories by\nreferring to emotionally similar examples with dynamic labels. Simultaneously,\nE-ICL employs an exclusionary emotion prediction strategy to avoid interference\nfrom irrelevant categories, thereby increasing its accuracy and robustness.\nNote that the entire process is accomplished with the assistance of a\nplug-and-play emotion auxiliary model, without additional training. Experiments\non the fine-grained emotion datasets EDOS, Empathetic-Dialogues,\nEmpatheticIntent, and GoEmotions show that E-ICL achieves superior emotion\nprediction performance. Furthermore, even when the emotion auxiliary model used\nis lower than 10% of the LLMs, E-ICL can still boost the performance of LLMs by\nover 4% on multiple datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) achieves remarkable performance in various domains\nsuch as knowledge acquisition, commonsense reasoning, and semantic\nunderstanding. However, its performance significantly deteriorates for emotion\ndetection tasks, especially fine-grained emotion recognition. The underlying\nreasons for this remain unclear. In this paper, we identify the reasons behind\nICL's poor performance from the perspective of prototype theory and propose a\nmethod to address this issue. Specifically, we conduct extensive pilot\nexperiments and find that ICL conforms to the prototype theory on fine-grained\nemotion recognition. Based on this theory, we uncover the following\ndeficiencies in ICL: (1) It relies on prototypes (example-label pairs) that are\nsemantically similar but emotionally inaccurate to predict emotions. (2) It is\nprone to interference from irrelevant categories, affecting the accuracy and\nrobustness of the predictions. To address these issues, we propose an Emotion\nContext Learning method (E-ICL) on fine-grained emotion recognition. E-ICL\nrelies on more emotionally accurate prototypes to predict categories by\nreferring to emotionally similar examples with dynamic labels. Simultaneously,\nE-ICL employs an exclusionary emotion prediction strategy to avoid interference\nfrom irrelevant categories, thereby increasing its accuracy and robustness.\nNote that the entire process is accomplished with the assistance of a\nplug-and-play emotion auxiliary model, without additional training. Experiments\non the fine-grained emotion datasets EDOS, Empathetic-Dialogues,\nEmpatheticIntent, and GoEmotions show that E-ICL achieves superior emotion\nprediction performance. Furthermore, even when the emotion auxiliary model used\nis lower than 10% of the LLMs, E-ICL can still boost the performance of LLMs by\nover 4% on multiple datasets."
                },
                "authors": [
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Zhou Yang"
                    },
                    {
                        "name": "Chenglong Ye"
                    },
                    {
                        "name": "Yufeng Wang"
                    },
                    {
                        "name": "Haizhou Sun"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Xiaofei Zhu"
                    },
                    {
                        "name": "Yunbing Wu"
                    },
                    {
                        "name": "Xiangwen Liao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangwen Liao"
                },
                "author": "Xiangwen Liao",
                "arxiv_comment": "16 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02642v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02642v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02711v1",
                "updated": "2025-01-06T01:52:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    52,
                    15,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T01:52:15Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    52,
                    15,
                    0,
                    6,
                    0
                ],
                "title": "KG-CF: Knowledge Graph Completion with Context Filtering under the\n  Guidance of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KG-CF: Knowledge Graph Completion with Context Filtering under the\n  Guidance of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have shown impressive performance in various\ntasks, including knowledge graph completion (KGC). However, current studies\nmostly apply LLMs to classification tasks, like identifying missing triplets,\nrather than ranking-based tasks, where the model ranks candidate entities based\non plausibility. This focus limits the practical use of LLMs in KGC, as\nreal-world applications prioritize highly plausible triplets. Additionally,\nwhile graph paths can help infer the existence of missing triplets and improve\ncompletion accuracy, they often contain redundant information. To address these\nissues, we propose KG-CF, a framework tailored for ranking-based KGC tasks.\nKG-CF leverages LLMs' reasoning abilities to filter out irrelevant contexts,\nachieving superior results on real-world datasets. The code and datasets are\navailable at \\url{https://anonymous.4open.science/r/KG-CF}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive performance in various\ntasks, including knowledge graph completion (KGC). However, current studies\nmostly apply LLMs to classification tasks, like identifying missing triplets,\nrather than ranking-based tasks, where the model ranks candidate entities based\non plausibility. This focus limits the practical use of LLMs in KGC, as\nreal-world applications prioritize highly plausible triplets. Additionally,\nwhile graph paths can help infer the existence of missing triplets and improve\ncompletion accuracy, they often contain redundant information. To address these\nissues, we propose KG-CF, a framework tailored for ranking-based KGC tasks.\nKG-CF leverages LLMs' reasoning abilities to filter out irrelevant contexts,\nachieving superior results on real-world datasets. The code and datasets are\navailable at \\url{https://anonymous.4open.science/r/KG-CF}."
                },
                "authors": [
                    {
                        "name": "Zaiyi Zheng"
                    },
                    {
                        "name": "Yushun Dong"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Haochen Liu"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Jundong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jundong Li"
                },
                "author": "Jundong Li",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00999v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00999v2",
                "updated": "2025-01-06T01:49:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    49,
                    9,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-02T01:33:58Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    1,
                    33,
                    58,
                    3,
                    2,
                    0
                ],
                "title": "Exploring Information Processing in Large Language Models: Insights from\n  Information Bottleneck Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Information Processing in Large Language Models: Insights from\n  Information Bottleneck Theory"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of tasks by understanding input information and predicting\ncorresponding outputs. However, the internal mechanisms by which LLMs\ncomprehend input and make effective predictions remain poorly understood. In\nthis paper, we explore the working mechanism of LLMs in information processing\nfrom the perspective of Information Bottleneck Theory. We propose a\nnon-training construction strategy to define a task space and identify the\nfollowing key findings: (1) LLMs compress input information into specific task\nspaces (e.g., sentiment space, topic space) to facilitate task understanding;\n(2) they then extract and utilize relevant information from the task space at\ncritical moments to generate accurate predictions. Based on these insights, we\nintroduce two novel approaches: an Information Compression-based Context\nLearning (IC-ICL) and a Task-Space-guided Fine-Tuning (TS-FT). IC-ICL enhances\nreasoning performance and inference efficiency by compressing retrieved example\ninformation into the task space. TS-FT employs a space-guided loss to fine-tune\nLLMs, encouraging the learning of more effective compression and selection\nmechanisms. Experiments across multiple datasets validate the effectiveness of\ntask space construction. Additionally, IC-ICL not only improves performance but\nalso accelerates inference speed by over 40\\%, while TS-FT achieves superior\nresults with a minimal strategy adjustment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of tasks by understanding input information and predicting\ncorresponding outputs. However, the internal mechanisms by which LLMs\ncomprehend input and make effective predictions remain poorly understood. In\nthis paper, we explore the working mechanism of LLMs in information processing\nfrom the perspective of Information Bottleneck Theory. We propose a\nnon-training construction strategy to define a task space and identify the\nfollowing key findings: (1) LLMs compress input information into specific task\nspaces (e.g., sentiment space, topic space) to facilitate task understanding;\n(2) they then extract and utilize relevant information from the task space at\ncritical moments to generate accurate predictions. Based on these insights, we\nintroduce two novel approaches: an Information Compression-based Context\nLearning (IC-ICL) and a Task-Space-guided Fine-Tuning (TS-FT). IC-ICL enhances\nreasoning performance and inference efficiency by compressing retrieved example\ninformation into the task space. TS-FT employs a space-guided loss to fine-tune\nLLMs, encouraging the learning of more effective compression and selection\nmechanisms. Experiments across multiple datasets validate the effectiveness of\ntask space construction. Additionally, IC-ICL not only improves performance but\nalso accelerates inference speed by over 40\\%, while TS-FT achieves superior\nresults with a minimal strategy adjustment."
                },
                "authors": [
                    {
                        "name": "Zhou Yang"
                    },
                    {
                        "name": "Zhengyu Qi"
                    },
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Zhikai Jia"
                    },
                    {
                        "name": "Haizhou Sun"
                    },
                    {
                        "name": "Xiaofei Zhu"
                    },
                    {
                        "name": "Xiangwen Liao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangwen Liao"
                },
                "author": "Xiangwen Liao",
                "arxiv_comment": "9 pages, 9 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00999v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00999v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00890v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00890v2",
                "updated": "2025-01-06T01:27:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    27,
                    48,
                    0,
                    6,
                    0
                ],
                "published": "2024-07-01T01:25:26Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    1,
                    25,
                    26,
                    0,
                    183,
                    0
                ],
                "title": "Macroeconomic Forecasting with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Macroeconomic Forecasting with Large Language Models"
                },
                "summary": "This paper presents a comparative analysis evaluating the accuracy of Large\nLanguage Models (LLMs) against traditional macro time series forecasting\napproaches. In recent times, LLMs have surged in popularity for forecasting due\nto their ability to capture intricate patterns in data and quickly adapt across\nvery different domains. However, their effectiveness in forecasting\nmacroeconomic time series data compared to conventional methods remains an area\nof interest. To address this, we conduct a rigorous evaluation of LLMs against\ntraditional macro forecasting methods, using as common ground the FRED-MD\ndatabase. Our findings provide valuable insights into the strengths and\nlimitations of LLMs in forecasting macroeconomic time series, shedding light on\ntheir applicability in real-world scenarios",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comparative analysis evaluating the accuracy of Large\nLanguage Models (LLMs) against traditional macro time series forecasting\napproaches. In recent times, LLMs have surged in popularity for forecasting due\nto their ability to capture intricate patterns in data and quickly adapt across\nvery different domains. However, their effectiveness in forecasting\nmacroeconomic time series data compared to conventional methods remains an area\nof interest. To address this, we conduct a rigorous evaluation of LLMs against\ntraditional macro forecasting methods, using as common ground the FRED-MD\ndatabase. Our findings provide valuable insights into the strengths and\nlimitations of LLMs in forecasting macroeconomic time series, shedding light on\ntheir applicability in real-world scenarios"
                },
                "authors": [
                    {
                        "name": "Andrea Carriero"
                    },
                    {
                        "name": "Davide Pettenuzzo"
                    },
                    {
                        "name": "Shubhranshu Shekhar"
                    }
                ],
                "author_detail": {
                    "name": "Shubhranshu Shekhar"
                },
                "author": "Shubhranshu Shekhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00890v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02705v1",
                "updated": "2025-01-06T01:16:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    16,
                    7,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T01:16:07Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    16,
                    7,
                    0,
                    6,
                    0
                ],
                "title": "Knowledge Distillation with Adapted Weight",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Distillation with Adapted Weight"
                },
                "summary": "Although large models have shown a strong capacity to solve large-scale\nproblems in many areas including natural language and computer vision, their\nvoluminous parameters are hard to deploy in a real-time system due to\ncomputational and energy constraints. Addressing this, knowledge distillation\nthrough Teacher-Student architecture offers a sustainable pathway to compress\nthe knowledge of large models into more manageable sizes without significantly\ncompromising performance. To enhance the robustness and interpretability of\nthis framework, it is critical to understand how individual training data\nimpact model performance, which is an area that remains underexplored. We\npropose the \\textbf{Knowledge Distillation with Adaptive Influence Weight\n(KD-AIF)} framework which leverages influence functions from robust statistics\nto assign weights to training data, grounded in the four key SAFE principles:\nSustainability, Accuracy, Fairness, and Explainability. This novel approach not\nonly optimizes distillation but also increases transparency by revealing the\nsignificance of different data. The exploration of various update mechanisms\nwithin the KD-AIF framework further elucidates its potential to significantly\nimprove learning efficiency and generalization in student models, marking a\nstep toward more explainable and deployable Large Models. KD-AIF is effective\nin knowledge distillation while also showing exceptional performance in\nsemi-supervised learning with outperforms existing baselines and methods in\nmultiple benchmarks (CIFAR-100, CIFAR-10-4k, SVHN-1k, and GLUE).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large models have shown a strong capacity to solve large-scale\nproblems in many areas including natural language and computer vision, their\nvoluminous parameters are hard to deploy in a real-time system due to\ncomputational and energy constraints. Addressing this, knowledge distillation\nthrough Teacher-Student architecture offers a sustainable pathway to compress\nthe knowledge of large models into more manageable sizes without significantly\ncompromising performance. To enhance the robustness and interpretability of\nthis framework, it is critical to understand how individual training data\nimpact model performance, which is an area that remains underexplored. We\npropose the \\textbf{Knowledge Distillation with Adaptive Influence Weight\n(KD-AIF)} framework which leverages influence functions from robust statistics\nto assign weights to training data, grounded in the four key SAFE principles:\nSustainability, Accuracy, Fairness, and Explainability. This novel approach not\nonly optimizes distillation but also increases transparency by revealing the\nsignificance of different data. The exploration of various update mechanisms\nwithin the KD-AIF framework further elucidates its potential to significantly\nimprove learning efficiency and generalization in student models, marking a\nstep toward more explainable and deployable Large Models. KD-AIF is effective\nin knowledge distillation while also showing exceptional performance in\nsemi-supervised learning with outperforms existing baselines and methods in\nmultiple benchmarks (CIFAR-100, CIFAR-10-4k, SVHN-1k, and GLUE)."
                },
                "authors": [
                    {
                        "name": "Sirong Wu"
                    },
                    {
                        "name": "Xi Luo"
                    },
                    {
                        "name": "Junjie Liu"
                    },
                    {
                        "name": "Yuhui Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yuhui Deng"
                },
                "author": "Yuhui Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02702v1",
                "updated": "2025-01-06T01:07:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    7,
                    59,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T01:07:59Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    7,
                    59,
                    0,
                    6,
                    0
                ],
                "title": "QuIM-RAG: Advancing Retrieval-Augmented Generation with Inverted\n  Question Matching for Enhanced QA Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuIM-RAG: Advancing Retrieval-Augmented Generation with Inverted\n  Question Matching for Enhanced QA Performance"
                },
                "summary": "This work presents a novel architecture for building Retrieval-Augmented\nGeneration (RAG) systems to improve Question Answering (QA) tasks from a target\ncorpus. Large Language Models (LLMs) have revolutionized the analyzing and\ngeneration of human-like text. These models rely on pre-trained data and lack\nreal-time updates unless integrated with live data tools. RAG enhances LLMs by\nintegrating online resources and databases to generate contextually appropriate\nresponses. However, traditional RAG still encounters challenges like\ninformation dilution and hallucinations when handling vast amounts of data. Our\napproach addresses these challenges by converting corpora into a\ndomain-specific dataset and RAG architecture is constructed to generate\nresponses from the target document. We introduce QuIM-RAG (Question-to-question\nInverted Index Matching), a novel approach for the retrieval mechanism in our\nsystem. This strategy generates potential questions from document chunks and\nmatches these with user queries to identify the most relevant text chunks for\ngenerating accurate answers. We have implemented our RAG system on top of the\nopen-source Meta-LLaMA3-8B-instruct model by Meta Inc. that is available on\nHugging Face. We constructed a custom corpus of 500+ pages from a high-traffic\nwebsite accessed thousands of times daily for answering complex questions,\nalong with manually prepared ground truth QA for evaluation. We compared our\napproach with traditional RAG models using BERT-Score and RAGAS,\nstate-of-the-art metrics for evaluating LLM applications. Our evaluation\ndemonstrates that our approach outperforms traditional RAG architectures on\nboth metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a novel architecture for building Retrieval-Augmented\nGeneration (RAG) systems to improve Question Answering (QA) tasks from a target\ncorpus. Large Language Models (LLMs) have revolutionized the analyzing and\ngeneration of human-like text. These models rely on pre-trained data and lack\nreal-time updates unless integrated with live data tools. RAG enhances LLMs by\nintegrating online resources and databases to generate contextually appropriate\nresponses. However, traditional RAG still encounters challenges like\ninformation dilution and hallucinations when handling vast amounts of data. Our\napproach addresses these challenges by converting corpora into a\ndomain-specific dataset and RAG architecture is constructed to generate\nresponses from the target document. We introduce QuIM-RAG (Question-to-question\nInverted Index Matching), a novel approach for the retrieval mechanism in our\nsystem. This strategy generates potential questions from document chunks and\nmatches these with user queries to identify the most relevant text chunks for\ngenerating accurate answers. We have implemented our RAG system on top of the\nopen-source Meta-LLaMA3-8B-instruct model by Meta Inc. that is available on\nHugging Face. We constructed a custom corpus of 500+ pages from a high-traffic\nwebsite accessed thousands of times daily for answering complex questions,\nalong with manually prepared ground truth QA for evaluation. We compared our\napproach with traditional RAG models using BERT-Score and RAGAS,\nstate-of-the-art metrics for evaluating LLM applications. Our evaluation\ndemonstrates that our approach outperforms traditional RAG architectures on\nboth metrics."
                },
                "authors": [
                    {
                        "name": "Binita Saha"
                    },
                    {
                        "name": "Utsha Saha"
                    },
                    {
                        "name": "Muhammad Zubair Malik"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Zubair Malik"
                },
                "author": "Muhammad Zubair Malik",
                "arxiv_doi": "10.1109/ACCESS.2024.3513155",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2024.3513155",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.02702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02699v1",
                "updated": "2025-01-06T00:39:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    0,
                    39,
                    31,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T00:39:31Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    0,
                    39,
                    31,
                    0,
                    6,
                    0
                ],
                "title": "EAGLE: Enhanced Visual Grounding Minimizes Hallucinations in\n  Instructional Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EAGLE: Enhanced Visual Grounding Minimizes Hallucinations in\n  Instructional Multimodal Models"
                },
                "summary": "Large language models and vision transformers have demonstrated impressive\nzero-shot capabilities, enabling significant transferability in downstream\ntasks. The fusion of these models has resulted in multi-modal architectures\nwith enhanced instructional capabilities. Despite incorporating vast image and\nlanguage pre-training, these multi-modal architectures often generate responses\nthat deviate from the ground truth in the image data. These failure cases are\nknown as hallucinations. Current methods for mitigating hallucinations\ngenerally focus on regularizing the language component, improving the fusion\nmodule, or ensembling multiple visual encoders to improve visual\nrepresentation. In this paper, we address the hallucination issue by directly\nenhancing the capabilities of the visual component. Our approach, named EAGLE,\nis fully agnostic to the LLM or fusion module and works as a post-pretraining\napproach that improves the grounding and language alignment of the visual\nencoder. We show that a straightforward reformulation of the original\ncontrastive pre-training task results in an improved visual encoder that can be\nincorporated into the instructional multi-modal architecture without additional\ninstructional training. As a result, EAGLE achieves a significant reduction in\nhallucinations across multiple challenging benchmarks and tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models and vision transformers have demonstrated impressive\nzero-shot capabilities, enabling significant transferability in downstream\ntasks. The fusion of these models has resulted in multi-modal architectures\nwith enhanced instructional capabilities. Despite incorporating vast image and\nlanguage pre-training, these multi-modal architectures often generate responses\nthat deviate from the ground truth in the image data. These failure cases are\nknown as hallucinations. Current methods for mitigating hallucinations\ngenerally focus on regularizing the language component, improving the fusion\nmodule, or ensembling multiple visual encoders to improve visual\nrepresentation. In this paper, we address the hallucination issue by directly\nenhancing the capabilities of the visual component. Our approach, named EAGLE,\nis fully agnostic to the LLM or fusion module and works as a post-pretraining\napproach that improves the grounding and language alignment of the visual\nencoder. We show that a straightforward reformulation of the original\ncontrastive pre-training task results in an improved visual encoder that can be\nincorporated into the instructional multi-modal architecture without additional\ninstructional training. As a result, EAGLE achieves a significant reduction in\nhallucinations across multiple challenging benchmarks and tasks."
                },
                "authors": [
                    {
                        "name": "Andrs Villa"
                    },
                    {
                        "name": "Juan Len Alczar"
                    },
                    {
                        "name": "Motasem Alfarra"
                    },
                    {
                        "name": "Vladimir Araujo"
                    },
                    {
                        "name": "Alvaro Soto"
                    },
                    {
                        "name": "Bernard Ghanem"
                    }
                ],
                "author_detail": {
                    "name": "Bernard Ghanem"
                },
                "author": "Bernard Ghanem",
                "arxiv_comment": "12 pages, 4 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02688v1",
                "updated": "2025-01-05T23:35:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    23,
                    35,
                    47,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T23:35:47Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    23,
                    35,
                    47,
                    6,
                    5,
                    0
                ],
                "title": "Decoding specialised feature neurons in LLMs with the final projection\n  layer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding specialised feature neurons in LLMs with the final projection\n  layer"
                },
                "summary": "Large Language Models (LLMs) typically have billions of parameters and are\nthus often difficult to interpret in their operation. Such black-box models can\npose a significant risk to safety when trusted to make important decisions. The\nlack of interpretability of LLMs is more related to their sheer size, rather\nthan the complexity of their individual components. The TARS method for\nknowledge removal (Davies et al 2024) provides strong evidence for the\nhypothesis that that linear layer weights which act directly on the residual\nstream may have high correlation with different concepts encoded in the\nresidual stream. Building upon this, we attempt to decode neuron weights\ndirectly into token probabilities through the final projection layer of the\nmodel (the LM-head). Firstly, we show that with Llama 3.1 8B we can utilise the\nLM-head to decode specialised feature neurons that respond strongly to certain\nconcepts, with examples such as \"dog\" and \"California\". This is then confirmed\nby demonstrating that these neurons can be clamped to affect the probability of\nthe concept in the output. This extends to the fine-tuned assistant Llama 3.1\n8B instruct model, where we find that over 75% of neurons in the up-projection\nlayers have the same top associated token compared to the pretrained model.\nFinally, we demonstrate that clamping the \"dog\" neuron leads the instruct model\nto always discuss dogs when asked about its favourite animal. Through our\nmethod, it is possible to map the entirety of Llama 3.1 8B's up-projection\nneurons in less than 15 minutes with no parallelization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) typically have billions of parameters and are\nthus often difficult to interpret in their operation. Such black-box models can\npose a significant risk to safety when trusted to make important decisions. The\nlack of interpretability of LLMs is more related to their sheer size, rather\nthan the complexity of their individual components. The TARS method for\nknowledge removal (Davies et al 2024) provides strong evidence for the\nhypothesis that that linear layer weights which act directly on the residual\nstream may have high correlation with different concepts encoded in the\nresidual stream. Building upon this, we attempt to decode neuron weights\ndirectly into token probabilities through the final projection layer of the\nmodel (the LM-head). Firstly, we show that with Llama 3.1 8B we can utilise the\nLM-head to decode specialised feature neurons that respond strongly to certain\nconcepts, with examples such as \"dog\" and \"California\". This is then confirmed\nby demonstrating that these neurons can be clamped to affect the probability of\nthe concept in the output. This extends to the fine-tuned assistant Llama 3.1\n8B instruct model, where we find that over 75% of neurons in the up-projection\nlayers have the same top associated token compared to the pretrained model.\nFinally, we demonstrate that clamping the \"dog\" neuron leads the instruct model\nto always discuss dogs when asked about its favourite animal. Through our\nmethod, it is possible to map the entirety of Llama 3.1 8B's up-projection\nneurons in less than 15 minutes with no parallelization."
                },
                "authors": [
                    {
                        "name": "Harry J Davies"
                    }
                ],
                "author_detail": {
                    "name": "Harry J Davies"
                },
                "author": "Harry J Davies",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01399v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01399v5",
                "updated": "2025-01-05T22:23:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    22,
                    23,
                    35,
                    6,
                    5,
                    0
                ],
                "published": "2024-04-01T18:10:05Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    18,
                    10,
                    5,
                    0,
                    92,
                    0
                ],
                "title": "Developing Safe and Responsible Large Language Model : Can We Balance\n  Bias Reduction and Language Understanding in Large Language Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing Safe and Responsible Large Language Model : Can We Balance\n  Bias Reduction and Language Understanding in Large Language Models?"
                },
                "summary": "Large Language Models (LLMs) have advanced various Natural Language\nProcessing (NLP) tasks, such as text generation and translation, among others.\nHowever, these models often generate texts that can perpetuate biases. Existing\napproaches to mitigate these biases usually compromise knowledge retention.\nThis study explores whether LLMs can produce safe, unbiased outputs without\nsacrificing knowledge or comprehension. We introduce the Safe and Responsible\nLarge Language Model (\\textbf{SR}$_{\\text{LLM}}$), which has been instruction\nfine-tuned atop of a safe fine-tuned auto-regressive decoder-only LLM to reduce\nbiases in generated texts. We developed a specialized dataset with examples of\nunsafe and corresponding safe variations to train \\textbf{SR}$_{\\text{LLM}}$ to\nidentify and correct biased text. Experiments on our specialized dataset and\nout-of-distribution test sets reveal that \\textbf{SR}$_{\\text{LLM}}$\neffectively reduces biases while preserving knowledge integrity. This\nperformance surpasses that of traditional fine-tuning of smaller language\nmodels and base LLMs that merely reply on prompting techniques. Our findings\ndemonstrate that instruction fine-tuning on custom datasets tailored for tasks\nsuch as debiasing is a highly effective strategy for minimizing bias in LLM\nwhile preserving their inherent knowledge and capabilities. The code and\ndataset are accessible at\n\\href{https://github.com/shainarazavi/Safe-Responsible-LLM}{SR-LLM}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have advanced various Natural Language\nProcessing (NLP) tasks, such as text generation and translation, among others.\nHowever, these models often generate texts that can perpetuate biases. Existing\napproaches to mitigate these biases usually compromise knowledge retention.\nThis study explores whether LLMs can produce safe, unbiased outputs without\nsacrificing knowledge or comprehension. We introduce the Safe and Responsible\nLarge Language Model (\\textbf{SR}$_{\\text{LLM}}$), which has been instruction\nfine-tuned atop of a safe fine-tuned auto-regressive decoder-only LLM to reduce\nbiases in generated texts. We developed a specialized dataset with examples of\nunsafe and corresponding safe variations to train \\textbf{SR}$_{\\text{LLM}}$ to\nidentify and correct biased text. Experiments on our specialized dataset and\nout-of-distribution test sets reveal that \\textbf{SR}$_{\\text{LLM}}$\neffectively reduces biases while preserving knowledge integrity. This\nperformance surpasses that of traditional fine-tuning of smaller language\nmodels and base LLMs that merely reply on prompting techniques. Our findings\ndemonstrate that instruction fine-tuning on custom datasets tailored for tasks\nsuch as debiasing is a highly effective strategy for minimizing bias in LLM\nwhile preserving their inherent knowledge and capabilities. The code and\ndataset are accessible at\n\\href{https://github.com/shainarazavi/Safe-Responsible-LLM}{SR-LLM}"
                },
                "authors": [
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Oluwanifemi Bamgbose"
                    },
                    {
                        "name": "Shardul Ghuge"
                    },
                    {
                        "name": "Fatemeh Tavakol"
                    },
                    {
                        "name": "Deepak John Reji"
                    },
                    {
                        "name": "Syed Raza Bashir"
                    }
                ],
                "author_detail": {
                    "name": "Syed Raza Bashir"
                },
                "author": "Syed Raza Bashir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01399v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01399v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16833v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16833v2",
                "updated": "2025-01-05T21:51:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    21,
                    51,
                    46,
                    6,
                    5,
                    0
                ],
                "published": "2024-05-27T05:04:05Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    5,
                    4,
                    5,
                    0,
                    148,
                    0
                ],
                "title": "Safe LoRA: the Silver Lining of Reducing Safety Risks when Fine-tuning\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe LoRA: the Silver Lining of Reducing Safety Risks when Fine-tuning\n  Large Language Models"
                },
                "summary": "While large language models (LLMs) such as Llama-2 or GPT-4 have shown\nimpressive zero-shot performance, fine-tuning is still necessary to enhance\ntheir performance for customized datasets, domain-specific tasks, or other\nprivate needs. However, fine-tuning all parameters of LLMs requires significant\nhardware resources, which can be impractical for typical users. Therefore,\nparameter-efficient fine-tuning such as LoRA have emerged, allowing users to\nfine-tune LLMs without the need for considerable computing resources, with\nlittle performance degradation compared to fine-tuning all parameters.\nUnfortunately, recent studies indicate that fine-tuning can increase the risk\nto the safety of LLMs, even when data does not contain malicious content. To\naddress this challenge, we propose Safe LoRA, a simple one-liner patch to the\noriginal LoRA implementation by introducing the projection of LoRA weights from\nselected layers to the safety-aligned subspace, effectively reducing the safety\nrisks in LLM fine-tuning while maintaining utility. It is worth noting that\nSafe LoRA is a training-free and data-free approach, as it only requires the\nknowledge of the weights from the base and aligned LLMs. Our extensive\nexperiments demonstrate that when fine-tuning on purely malicious data, Safe\nLoRA retains similar safety performance as the original aligned model.\nMoreover, when the fine-tuning dataset contains a mixture of both benign and\nmalicious data, Safe LoRA mitigates the negative effect made by malicious data\nwhile preserving performance on downstream tasks. Our codes are available at\n\\url{https://github.com/IBM/SafeLoRA}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) such as Llama-2 or GPT-4 have shown\nimpressive zero-shot performance, fine-tuning is still necessary to enhance\ntheir performance for customized datasets, domain-specific tasks, or other\nprivate needs. However, fine-tuning all parameters of LLMs requires significant\nhardware resources, which can be impractical for typical users. Therefore,\nparameter-efficient fine-tuning such as LoRA have emerged, allowing users to\nfine-tune LLMs without the need for considerable computing resources, with\nlittle performance degradation compared to fine-tuning all parameters.\nUnfortunately, recent studies indicate that fine-tuning can increase the risk\nto the safety of LLMs, even when data does not contain malicious content. To\naddress this challenge, we propose Safe LoRA, a simple one-liner patch to the\noriginal LoRA implementation by introducing the projection of LoRA weights from\nselected layers to the safety-aligned subspace, effectively reducing the safety\nrisks in LLM fine-tuning while maintaining utility. It is worth noting that\nSafe LoRA is a training-free and data-free approach, as it only requires the\nknowledge of the weights from the base and aligned LLMs. Our extensive\nexperiments demonstrate that when fine-tuning on purely malicious data, Safe\nLoRA retains similar safety performance as the original aligned model.\nMoreover, when the fine-tuning dataset contains a mixture of both benign and\nmalicious data, Safe LoRA mitigates the negative effect made by malicious data\nwhile preserving performance on downstream tasks. Our codes are available at\n\\url{https://github.com/IBM/SafeLoRA}."
                },
                "authors": [
                    {
                        "name": "Chia-Yi Hsu"
                    },
                    {
                        "name": "Yu-Lin Tsai"
                    },
                    {
                        "name": "Chih-Hsun Lin"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Chia-Mu Yu"
                    },
                    {
                        "name": "Chun-Ying Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chun-Ying Huang"
                },
                "author": "Chun-Ying Huang",
                "arxiv_comment": "This is the camera-ready version accepted for NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16833v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16833v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03516v3",
                "updated": "2025-01-05T19:09:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    19,
                    9,
                    4,
                    6,
                    5,
                    0
                ],
                "published": "2024-08-07T02:54:43Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    2,
                    54,
                    43,
                    2,
                    220,
                    0
                ],
                "title": "Query3D: LLM-Powered Open-Vocabulary Scene Segmentation with Language\n  Embedded 3D Gaussian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query3D: LLM-Powered Open-Vocabulary Scene Segmentation with Language\n  Embedded 3D Gaussian"
                },
                "summary": "This paper introduces a novel method for open-vocabulary 3D scene querying in\nautonomous driving by combining Language Embedded 3D Gaussians with Large\nLanguage Models (LLMs). We propose utilizing LLMs to generate both contextually\ncanonical phrases and helping positive words for enhanced segmentation and\nscene interpretation. Our method leverages GPT-3.5 Turbo as an expert model to\ncreate a high-quality text dataset, which we then use to fine-tune smaller,\nmore efficient LLMs for on-device deployment. Our comprehensive evaluation on\nthe WayveScenes101 dataset demonstrates that LLM-guided segmentation\nsignificantly outperforms traditional approaches based on predefined canonical\nphrases. Notably, our fine-tuned smaller models achieve performance comparable\nto larger expert models while maintaining faster inference times. Through\nablation studies, we discover that the effectiveness of helping positive words\ncorrelates with model scale, with larger models better equipped to leverage\nadditional semantic information. This work represents a significant advancement\ntowards more efficient, context-aware autonomous driving systems, effectively\nbridging 3D scene representation with high-level semantic querying while\nmaintaining practical deployment considerations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel method for open-vocabulary 3D scene querying in\nautonomous driving by combining Language Embedded 3D Gaussians with Large\nLanguage Models (LLMs). We propose utilizing LLMs to generate both contextually\ncanonical phrases and helping positive words for enhanced segmentation and\nscene interpretation. Our method leverages GPT-3.5 Turbo as an expert model to\ncreate a high-quality text dataset, which we then use to fine-tune smaller,\nmore efficient LLMs for on-device deployment. Our comprehensive evaluation on\nthe WayveScenes101 dataset demonstrates that LLM-guided segmentation\nsignificantly outperforms traditional approaches based on predefined canonical\nphrases. Notably, our fine-tuned smaller models achieve performance comparable\nto larger expert models while maintaining faster inference times. Through\nablation studies, we discover that the effectiveness of helping positive words\ncorrelates with model scale, with larger models better equipped to leverage\nadditional semantic information. This work represents a significant advancement\ntowards more efficient, context-aware autonomous driving systems, effectively\nbridging 3D scene representation with high-level semantic querying while\nmaintaining practical deployment considerations."
                },
                "authors": [
                    {
                        "name": "Amirhosein Chahe"
                    },
                    {
                        "name": "Lifeng Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Lifeng Zhou"
                },
                "author": "Lifeng Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02629v1",
                "updated": "2025-01-05T19:06:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    19,
                    6,
                    3,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T19:06:03Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    19,
                    6,
                    3,
                    6,
                    5,
                    0
                ],
                "title": "Layer-Level Self-Exposure and Patch: Affirmative Token Mitigation for\n  Jailbreak Attack Defense",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layer-Level Self-Exposure and Patch: Affirmative Token Mitigation for\n  Jailbreak Attack Defense"
                },
                "summary": "As large language models (LLMs) are increasingly deployed in diverse\napplications, including chatbot assistants and code generation, aligning their\nbehavior with safety and ethical standards has become paramount. However,\njailbreak attacks, which exploit vulnerabilities to elicit unintended or\nharmful outputs, threaten LLMs' safety significantly. In this paper, we\nintroduce Layer-AdvPatcher, a novel methodology designed to defend against\njailbreak attacks by utilizing an unlearning strategy to patch specific layers\nwithin LLMs through self-augmented datasets. Our insight is that certain\nlayer(s), tend to produce affirmative tokens when faced with harmful prompts.\nBy identifying these layers and adversarially exposing them to generate more\nharmful data, one can understand their inherent and diverse vulnerabilities to\nattacks. With these exposures, we then \"unlearn\" these issues, reducing the\nimpact of affirmative tokens and hence minimizing jailbreak risks while keeping\nthe model's responses to safe queries intact. We conduct extensive experiments\non two models, four benchmark datasets, and multiple state-of-the-art jailbreak\nbenchmarks to demonstrate the efficacy of our approach. Results indicate that\nour framework reduces the harmfulness and attack success rate of jailbreak\nattacks without compromising utility for benign queries compared to recent\ndefense methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly deployed in diverse\napplications, including chatbot assistants and code generation, aligning their\nbehavior with safety and ethical standards has become paramount. However,\njailbreak attacks, which exploit vulnerabilities to elicit unintended or\nharmful outputs, threaten LLMs' safety significantly. In this paper, we\nintroduce Layer-AdvPatcher, a novel methodology designed to defend against\njailbreak attacks by utilizing an unlearning strategy to patch specific layers\nwithin LLMs through self-augmented datasets. Our insight is that certain\nlayer(s), tend to produce affirmative tokens when faced with harmful prompts.\nBy identifying these layers and adversarially exposing them to generate more\nharmful data, one can understand their inherent and diverse vulnerabilities to\nattacks. With these exposures, we then \"unlearn\" these issues, reducing the\nimpact of affirmative tokens and hence minimizing jailbreak risks while keeping\nthe model's responses to safe queries intact. We conduct extensive experiments\non two models, four benchmark datasets, and multiple state-of-the-art jailbreak\nbenchmarks to demonstrate the efficacy of our approach. Results indicate that\nour framework reduces the harmfulness and attack success rate of jailbreak\nattacks without compromising utility for benign queries compared to recent\ndefense methods."
                },
                "authors": [
                    {
                        "name": "Yang Ouyang"
                    },
                    {
                        "name": "Hengrui Gu"
                    },
                    {
                        "name": "Shuhang Lin"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Bhavya Kailkhura"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Kaixiong Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Kaixiong Zhou"
                },
                "author": "Kaixiong Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02628v1",
                "updated": "2025-01-05T18:54:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    18,
                    54,
                    25,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T18:54:25Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    18,
                    54,
                    25,
                    6,
                    5,
                    0
                ],
                "title": "Cracks in The Stack: Hidden Vulnerabilities and Licensing Risks in LLM\n  Pre-Training Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cracks in The Stack: Hidden Vulnerabilities and Licensing Risks in LLM\n  Pre-Training Datasets"
                },
                "summary": "A critical part of creating code suggestion systems is the pre-training of\nLarge Language Models on vast amounts of source code and natural language text,\noften of questionable origin or quality. This may contribute to the presence of\nbugs and vulnerabilities in code generated by LLMs. While efforts to identify\nbugs at or after code generation exist, it is preferable to pre-train or\nfine-tune LLMs on curated, high-quality, and compliant datasets. The need for\nvast amounts of training data necessitates that such curation be automated,\nminimizing human intervention.\n  We propose an automated source code autocuration technique that leverages the\ncomplete version history of open-source software projects to improve the\nquality of training data. This approach leverages the version history of all\nOSS projects to identify training data samples that have been modified or have\nundergone changes in at least one OSS project, and pinpoint a subset of samples\nthat include fixes for bugs or vulnerabilities. We evaluate this method using\nThe Stack v2 dataset, and find that 17% of the code versions in the dataset\nhave newer versions, with 17% of those representing bug fixes, including 2.36%\naddressing known CVEs. The deduplicated version of Stack v2 still includes\nblobs vulnerable to 6,947 known CVEs. Furthermore, 58% of the blobs in the\ndataset were never modified after creation, suggesting they likely represent\nsoftware with minimal or no use. Misidentified blob origins present an\nadditional challenge, as they lead to the inclusion of non-permissively\nlicensed code, raising serious compliance concerns.\n  By addressing these issues, the training of new models can avoid perpetuating\nbuggy code patterns or license violations. We expect our results to inspire\nprocess improvements for automated data curation, with the potential to enhance\nthe reliability of outputs generated by AI tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical part of creating code suggestion systems is the pre-training of\nLarge Language Models on vast amounts of source code and natural language text,\noften of questionable origin or quality. This may contribute to the presence of\nbugs and vulnerabilities in code generated by LLMs. While efforts to identify\nbugs at or after code generation exist, it is preferable to pre-train or\nfine-tune LLMs on curated, high-quality, and compliant datasets. The need for\nvast amounts of training data necessitates that such curation be automated,\nminimizing human intervention.\n  We propose an automated source code autocuration technique that leverages the\ncomplete version history of open-source software projects to improve the\nquality of training data. This approach leverages the version history of all\nOSS projects to identify training data samples that have been modified or have\nundergone changes in at least one OSS project, and pinpoint a subset of samples\nthat include fixes for bugs or vulnerabilities. We evaluate this method using\nThe Stack v2 dataset, and find that 17% of the code versions in the dataset\nhave newer versions, with 17% of those representing bug fixes, including 2.36%\naddressing known CVEs. The deduplicated version of Stack v2 still includes\nblobs vulnerable to 6,947 known CVEs. Furthermore, 58% of the blobs in the\ndataset were never modified after creation, suggesting they likely represent\nsoftware with minimal or no use. Misidentified blob origins present an\nadditional challenge, as they lead to the inclusion of non-permissively\nlicensed code, raising serious compliance concerns.\n  By addressing these issues, the training of new models can avoid perpetuating\nbuggy code patterns or license violations. We expect our results to inspire\nprocess improvements for automated data curation, with the potential to enhance\nthe reliability of outputs generated by AI tools."
                },
                "authors": [
                    {
                        "name": "Mahmoud Jahanshahi"
                    },
                    {
                        "name": "Audris Mockus"
                    }
                ],
                "author_detail": {
                    "name": "Audris Mockus"
                },
                "author": "Audris Mockus",
                "arxiv_comment": "Accepted in the Second International Workshop on Large Language\n  Models for Code (LLM4Code 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02625v1",
                "updated": "2025-01-05T18:41:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    18,
                    41,
                    54,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T18:41:54Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    18,
                    41,
                    54,
                    6,
                    5,
                    0
                ],
                "title": "HALO: Hadamard-Assisted Lossless Optimization for Efficient\n  Low-Precision LLM Training and Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HALO: Hadamard-Assisted Lossless Optimization for Efficient\n  Low-Precision LLM Training and Fine-Tuning"
                },
                "summary": "Quantized training of Large Language Models (LLMs) remains an open challenge,\nas maintaining accuracy while performing all matrix multiplications in low\nprecision has proven difficult. This is particularly the case when fine-tuning\npre-trained models, which often already have large weight and activation\noutlier values that render quantized optimization difficult. We present HALO, a\nnovel quantization-aware training approach for Transformers that enables\naccurate and efficient low-precision training by combining 1) strategic\nplacement of Hadamard rotations in both forward and backward passes, to\nmitigate outliers during the low-precision computation, 2) FSDP integration for\nlow-precision communication, and 3) high-performance kernel support. Our\napproach ensures that all large matrix multiplications during the forward and\nbackward passes are executed in lower precision. Applied to LLAMA-family\nmodels, HALO achieves near-full-precision-equivalent results during fine-tuning\non various tasks, while delivering up to 1.31x end-to-end speedup for full\nfine-tuning on RTX 4090 GPUs. Our method supports both standard and\nparameter-efficient fine-tuning (PEFT) methods, both backed by efficient kernel\nimplementations. Our results demonstrate the first practical approach to fully\nquantized LLM fine-tuning that maintains accuracy in FP8 precision, while\ndelivering performance benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantized training of Large Language Models (LLMs) remains an open challenge,\nas maintaining accuracy while performing all matrix multiplications in low\nprecision has proven difficult. This is particularly the case when fine-tuning\npre-trained models, which often already have large weight and activation\noutlier values that render quantized optimization difficult. We present HALO, a\nnovel quantization-aware training approach for Transformers that enables\naccurate and efficient low-precision training by combining 1) strategic\nplacement of Hadamard rotations in both forward and backward passes, to\nmitigate outliers during the low-precision computation, 2) FSDP integration for\nlow-precision communication, and 3) high-performance kernel support. Our\napproach ensures that all large matrix multiplications during the forward and\nbackward passes are executed in lower precision. Applied to LLAMA-family\nmodels, HALO achieves near-full-precision-equivalent results during fine-tuning\non various tasks, while delivering up to 1.31x end-to-end speedup for full\nfine-tuning on RTX 4090 GPUs. Our method supports both standard and\nparameter-efficient fine-tuning (PEFT) methods, both backed by efficient kernel\nimplementations. Our results demonstrate the first practical approach to fully\nquantized LLM fine-tuning that maintains accuracy in FP8 precision, while\ndelivering performance benefits."
                },
                "authors": [
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Mahdi Nikdan"
                    },
                    {
                        "name": "Soroush Tabesh"
                    },
                    {
                        "name": "Roberto L. Castro"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "10 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02621v1",
                "updated": "2025-01-05T18:29:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    18,
                    29,
                    39,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T18:29:39Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    18,
                    29,
                    39,
                    6,
                    5,
                    0
                ],
                "title": "LLMs Help Alleviate the Cross-Subject Variability in Brain Signal and\n  Language Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Help Alleviate the Cross-Subject Variability in Brain Signal and\n  Language Alignment"
                },
                "summary": "Decoding human activity from EEG signals has long been a popular research\ntopic. While recent studies have increasingly shifted focus from single-subject\nto cross-subject analysis, few have explored the model's ability to perform\nzero-shot predictions on EEG signals from previously unseen subjects. This\nresearch aims to investigate whether deep learning methods can capture\nsubject-independent semantic information inherent in human EEG signals. Such\ninsights are crucial for Brain-Computer Interfaces (BCI) because, on one hand,\nthey demonstrate the model's robustness against subject-specific temporal\nbiases, and on the other, they significantly enhance the generalizability of\ndownstream tasks. We employ Large Language Models (LLMs) as denoising agents to\nextract subject-independent semantic features from noisy EEG signals.\nExperimental results, including ablation studies, highlight the pivotal role of\nLLMs in decoding subject-independent semantic information from noisy EEG data.\nWe hope our findings will contribute to advancing BCI research and assist both\nacademia and industry in applying EEG signals to a broader range of\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding human activity from EEG signals has long been a popular research\ntopic. While recent studies have increasingly shifted focus from single-subject\nto cross-subject analysis, few have explored the model's ability to perform\nzero-shot predictions on EEG signals from previously unseen subjects. This\nresearch aims to investigate whether deep learning methods can capture\nsubject-independent semantic information inherent in human EEG signals. Such\ninsights are crucial for Brain-Computer Interfaces (BCI) because, on one hand,\nthey demonstrate the model's robustness against subject-specific temporal\nbiases, and on the other, they significantly enhance the generalizability of\ndownstream tasks. We employ Large Language Models (LLMs) as denoising agents to\nextract subject-independent semantic features from noisy EEG signals.\nExperimental results, including ablation studies, highlight the pivotal role of\nLLMs in decoding subject-independent semantic information from noisy EEG data.\nWe hope our findings will contribute to advancing BCI research and assist both\nacademia and industry in applying EEG signals to a broader range of\napplications."
                },
                "authors": [
                    {
                        "name": "Yifei Liu"
                    },
                    {
                        "name": "Hengwei Ye"
                    },
                    {
                        "name": "Shuhang Li"
                    }
                ],
                "author_detail": {
                    "name": "Shuhang Li"
                },
                "author": "Shuhang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10351v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10351v3",
                "updated": "2025-01-05T18:21:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    18,
                    21,
                    23,
                    6,
                    5,
                    0
                ],
                "published": "2024-11-15T16:55:57Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    55,
                    57,
                    4,
                    320,
                    0
                ],
                "title": "Bias Unveiled: Investigating Social Bias in LLM-Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias Unveiled: Investigating Social Bias in LLM-Generated Code"
                },
                "summary": "Large language models (LLMs) have significantly advanced the field of\nautomated code generation. However, a notable research gap exists in the\nevaluation of social biases that may be present in the code produced by LLMs.\nTo solve this issue, we propose a novel fairness framework, i.e., Solar, to\nassess and mitigate the social biases of LLM-generated code. Specifically,\nSolar can automatically generate test cases for quantitatively uncovering\nsocial biases of the auto-generated code by LLMs. To quantify the severity of\nsocial biases in generated code, we develop a dataset that covers a diverse set\nof social problems. We applied Solar and the crafted dataset to four\nstate-of-the-art LLMs for code generation. Our evaluation reveals severe bias\nin the LLM-generated code from all the subject LLMs. Furthermore, we explore\nseveral strategies for bias mitigation, including Chain-of-Thought (CoT)\nprompting, combining positive role-playing with CoT prompting and iterative\nprompting. Our experiments show that iterative prompting can effectively reduce\nsocial bias in LLM-generated code by up to 90%. Solar is highly extensible to\nevaluate new social problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly advanced the field of\nautomated code generation. However, a notable research gap exists in the\nevaluation of social biases that may be present in the code produced by LLMs.\nTo solve this issue, we propose a novel fairness framework, i.e., Solar, to\nassess and mitigate the social biases of LLM-generated code. Specifically,\nSolar can automatically generate test cases for quantitatively uncovering\nsocial biases of the auto-generated code by LLMs. To quantify the severity of\nsocial biases in generated code, we develop a dataset that covers a diverse set\nof social problems. We applied Solar and the crafted dataset to four\nstate-of-the-art LLMs for code generation. Our evaluation reveals severe bias\nin the LLM-generated code from all the subject LLMs. Furthermore, we explore\nseveral strategies for bias mitigation, including Chain-of-Thought (CoT)\nprompting, combining positive role-playing with CoT prompting and iterative\nprompting. Our experiments show that iterative prompting can effectively reduce\nsocial bias in LLM-generated code by up to 90%. Solar is highly extensible to\nevaluate new social problems."
                },
                "authors": [
                    {
                        "name": "Lin Ling"
                    },
                    {
                        "name": "Fazle Rabbi"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Jinqiu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiu Yang"
                },
                "author": "Jinqiu Yang",
                "arxiv_comment": "9pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10351v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10351v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02473v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02473v2",
                "updated": "2025-01-05T18:01:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    18,
                    1,
                    18,
                    6,
                    5,
                    0
                ],
                "published": "2024-08-05T13:57:32Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    13,
                    57,
                    32,
                    0,
                    218,
                    0
                ],
                "title": "Toward Attention-based TinyML: A Heterogeneous Accelerated Architecture\n  and Automated Deployment Flow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Attention-based TinyML: A Heterogeneous Accelerated Architecture\n  and Automated Deployment Flow"
                },
                "summary": "One of the challenges for Tiny Machine Learning (tinyML) is keeping up with\nthe evolution of Machine Learning models from Convolutional Neural Networks to\nTransformers. We address this by leveraging a heterogeneous architectural\ntemplate coupling RISC-V processors with hardwired accelerators supported by an\nautomated deployment flow. We demonstrate Attention-based models in a tinyML\npower envelope with an octa-core cluster coupled with an accelerator for\nquantized Attention. Our deployment flow enables end-to-end 8-bit Transformer\ninference, achieving leading-edge energy efficiency and throughput of 2960\nGOp/J and 154 GOp/s (0.65 V, 22 nm FD-SOI technology).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the challenges for Tiny Machine Learning (tinyML) is keeping up with\nthe evolution of Machine Learning models from Convolutional Neural Networks to\nTransformers. We address this by leveraging a heterogeneous architectural\ntemplate coupling RISC-V processors with hardwired accelerators supported by an\nautomated deployment flow. We demonstrate Attention-based models in a tinyML\npower envelope with an octa-core cluster coupled with an accelerator for\nquantized Attention. Our deployment flow enables end-to-end 8-bit Transformer\ninference, achieving leading-edge energy efficiency and throughput of 2960\nGOp/J and 154 GOp/s (0.65 V, 22 nm FD-SOI technology)."
                },
                "authors": [
                    {
                        "name": "Philip Wiese"
                    },
                    {
                        "name": "Gamze slamolu"
                    },
                    {
                        "name": "Moritz Scherer"
                    },
                    {
                        "name": "Luka Macan"
                    },
                    {
                        "name": "Victor J. B. Jung"
                    },
                    {
                        "name": "Alessio Burrello"
                    },
                    {
                        "name": "Francesco Conti"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "Accepted for publication in the SI: tinyML (S1) issue of IEEE Design\n  & Test",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02473v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02473v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02600v1",
                "updated": "2025-01-05T16:51:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    16,
                    51,
                    17,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T16:51:17Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    16,
                    51,
                    17,
                    6,
                    5,
                    0
                ],
                "title": "TAPAS: Thermal- and Power-Aware Scheduling for LLM Inference in Cloud\n  Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAPAS: Thermal- and Power-Aware Scheduling for LLM Inference in Cloud\n  Platforms"
                },
                "summary": "The rising demand for generative large language models (LLMs) poses\nchallenges for thermal and power management in cloud datacenters. Traditional\ntechniques often are inadequate for LLM inference due to the fine-grained,\nmillisecond-scale execution phases, each with distinct performance, thermal,\nand power profiles. Additionally, LLM inference workloads are sensitive to\nvarious configuration parameters (e.g., model parallelism, size, and\nquantization) that involve trade-offs between performance, temperature, power,\nand output quality. Moreover, clouds often co-locate SaaS and IaaS workloads,\neach with different levels of visibility and flexibility. We propose TAPAS, a\nthermal- and power-aware framework designed for LLM inference clusters in the\ncloud. TAPAS enhances cooling and power oversubscription capabilities, reducing\nthe total cost of ownership (TCO) while effectively handling emergencies (e.g.,\ncooling and power failures). The system leverages historical temperature and\npower data, along with the adaptability of SaaS workloads, to: (1) efficiently\nplace new GPU workload VMs within cooling and power constraints, (2) route LLM\ninference requests across SaaS VMs, and (3) reconfigure SaaS VMs to manage load\nspikes and emergency situations. Our evaluation on a large GPU cluster\ndemonstrates significant reductions in thermal and power throttling events,\nboosting system efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rising demand for generative large language models (LLMs) poses\nchallenges for thermal and power management in cloud datacenters. Traditional\ntechniques often are inadequate for LLM inference due to the fine-grained,\nmillisecond-scale execution phases, each with distinct performance, thermal,\nand power profiles. Additionally, LLM inference workloads are sensitive to\nvarious configuration parameters (e.g., model parallelism, size, and\nquantization) that involve trade-offs between performance, temperature, power,\nand output quality. Moreover, clouds often co-locate SaaS and IaaS workloads,\neach with different levels of visibility and flexibility. We propose TAPAS, a\nthermal- and power-aware framework designed for LLM inference clusters in the\ncloud. TAPAS enhances cooling and power oversubscription capabilities, reducing\nthe total cost of ownership (TCO) while effectively handling emergencies (e.g.,\ncooling and power failures). The system leverages historical temperature and\npower data, along with the adaptability of SaaS workloads, to: (1) efficiently\nplace new GPU workload VMs within cooling and power constraints, (2) route LLM\ninference requests across SaaS VMs, and (3) reconfigure SaaS VMs to manage load\nspikes and emergency situations. Our evaluation on a large GPU cluster\ndemonstrates significant reductions in thermal and power throttling events,\nboosting system efficiency."
                },
                "authors": [
                    {
                        "name": "Jovan Stojkovic"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "igo Goiri"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Rodrigo Fonseca"
                    },
                    {
                        "name": "Josep Torrellas"
                    },
                    {
                        "name": "Ricardo Bianchini"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Bianchini"
                },
                "author": "Ricardo Bianchini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23111v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23111v4",
                "updated": "2025-01-05T16:22:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    16,
                    22,
                    3,
                    6,
                    5,
                    0
                ],
                "published": "2024-10-30T15:23:44Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    23,
                    44,
                    2,
                    304,
                    0
                ],
                "title": "Exploring Gradient Subspaces: Addressing and Overcoming LoRA's\n  Limitations in Federated Fine-Tuning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Gradient Subspaces: Addressing and Overcoming LoRA's\n  Limitations in Federated Fine-Tuning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, particularly in task generalization for both text and vision\ndata. While fine-tuning these models can significantly enhance their\nperformance on specific downstream tasks, it often requires high-quality data\nthat cannot be shared due to privacy concerns. Federated Learning (FL) offers a\npromising solution for collaborative training without direct data sharing.\nHowever, many parameter-efficient fine-tuning strategies for LLMs in FL,\nparticularly those based on Low-Rank Adaptation (LoRA), face limitations. In\nthis paper, we critically analyze the convergence and performance guarantees of\npopular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to\nconstrained subspace learning of low-rank matrices. This limitation hinders\neffective fine-tuning of LLMs in federated settings. Through rigorous\nanalytical and empirical evaluations, we demonstrate that direct weight\naveraging outperforms LoRA-based strategies, leading to superior performance\nfor fine-tuned models. Our comprehensive comparison unmasks inefficiencies in\nLoRA approaches and underscores the advantages of direct weight aggregation. We\nextend our analysis to low-rank gradient-based optimizers, such as GaLore, used\nduring local training steps. Our findings show that GaLore along with\ndirect-weight aggregation is a more effective approach, outperforming federated\nLoRA methods like FlexLoRA and FFA-LoRA across both text and image modalities.\nWhile privacy remains paramount in FL discourse, our focus is on assessing\nperformance outcomes of federated fine-tuned models and evaluating various FL\nframeworks from both theoretical and empirical perspectives. Our findings\nadvocate reassessing the reliance on LoRA within FL contexts, paving the way\nfor more efficient training methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, particularly in task generalization for both text and vision\ndata. While fine-tuning these models can significantly enhance their\nperformance on specific downstream tasks, it often requires high-quality data\nthat cannot be shared due to privacy concerns. Federated Learning (FL) offers a\npromising solution for collaborative training without direct data sharing.\nHowever, many parameter-efficient fine-tuning strategies for LLMs in FL,\nparticularly those based on Low-Rank Adaptation (LoRA), face limitations. In\nthis paper, we critically analyze the convergence and performance guarantees of\npopular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to\nconstrained subspace learning of low-rank matrices. This limitation hinders\neffective fine-tuning of LLMs in federated settings. Through rigorous\nanalytical and empirical evaluations, we demonstrate that direct weight\naveraging outperforms LoRA-based strategies, leading to superior performance\nfor fine-tuned models. Our comprehensive comparison unmasks inefficiencies in\nLoRA approaches and underscores the advantages of direct weight aggregation. We\nextend our analysis to low-rank gradient-based optimizers, such as GaLore, used\nduring local training steps. Our findings show that GaLore along with\ndirect-weight aggregation is a more effective approach, outperforming federated\nLoRA methods like FlexLoRA and FFA-LoRA across both text and image modalities.\nWhile privacy remains paramount in FL discourse, our focus is on assessing\nperformance outcomes of federated fine-tuned models and evaluating various FL\nframeworks from both theoretical and empirical perspectives. Our findings\nadvocate reassessing the reliance on LoRA within FL contexts, paving the way\nfor more efficient training methodologies."
                },
                "authors": [
                    {
                        "name": "Navyansh Mahla"
                    },
                    {
                        "name": "Ganesh Ramakrishnan"
                    }
                ],
                "author_detail": {
                    "name": "Ganesh Ramakrishnan"
                },
                "author": "Ganesh Ramakrishnan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23111v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23111v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01312v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01312v2",
                "updated": "2025-01-05T15:27:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    15,
                    27,
                    55,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-02T15:53:25Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    53,
                    25,
                    3,
                    2,
                    0
                ],
                "title": "Learning Spectral Methods by Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Spectral Methods by Transformers"
                },
                "summary": "Transformers demonstrate significant advantages as the building block of\nmodern LLMs. In this work, we study the capacities of Transformers in\nperforming unsupervised learning. We show that multi-layered Transformers,\ngiven a sufficiently large set of pre-training instances, are able to learn the\nalgorithms themselves and perform statistical estimation tasks given new\ninstances. This learning paradigm is distinct from the in-context learning\nsetup and is similar to the learning procedure of human brains where skills are\nlearned through past experience. Theoretically, we prove that pre-trained\nTransformers can learn the spectral methods and use the classification of\nbi-class Gaussian mixture model as an example. Our proof is constructive using\nalgorithmic design techniques. Our results are built upon the similarities of\nmulti-layered Transformer architecture with the iterative recovery algorithms\nused in practice. Empirically, we verify the strong capacity of the\nmulti-layered (pre-trained) Transformer on unsupervised learning through the\nlens of both the PCA and the Clustering tasks performed on the synthetic and\nreal-world datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers demonstrate significant advantages as the building block of\nmodern LLMs. In this work, we study the capacities of Transformers in\nperforming unsupervised learning. We show that multi-layered Transformers,\ngiven a sufficiently large set of pre-training instances, are able to learn the\nalgorithms themselves and perform statistical estimation tasks given new\ninstances. This learning paradigm is distinct from the in-context learning\nsetup and is similar to the learning procedure of human brains where skills are\nlearned through past experience. Theoretically, we prove that pre-trained\nTransformers can learn the spectral methods and use the classification of\nbi-class Gaussian mixture model as an example. Our proof is constructive using\nalgorithmic design techniques. Our results are built upon the similarities of\nmulti-layered Transformer architecture with the iterative recovery algorithms\nused in practice. Empirically, we verify the strong capacity of the\nmulti-layered (pre-trained) Transformer on unsupervised learning through the\nlens of both the PCA and the Clustering tasks performed on the synthetic and\nreal-world datasets."
                },
                "authors": [
                    {
                        "name": "Yihan He"
                    },
                    {
                        "name": "Yuan Cao"
                    },
                    {
                        "name": "Hong-Yu Chen"
                    },
                    {
                        "name": "Dennis Wu"
                    },
                    {
                        "name": "Jianqing Fan"
                    },
                    {
                        "name": "Han Liu"
                    }
                ],
                "author_detail": {
                    "name": "Han Liu"
                },
                "author": "Han Liu",
                "arxiv_comment": "77 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01312v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01312v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02573v1",
                "updated": "2025-01-05T15:11:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    15,
                    11,
                    26,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T15:11:26Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    15,
                    11,
                    26,
                    6,
                    5,
                    0
                ],
                "title": "LeetDecoding: A PyTorch Library for Exponentially Decaying Causal Linear\n  Attention with CUDA Implementations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeetDecoding: A PyTorch Library for Exponentially Decaying Causal Linear\n  Attention with CUDA Implementations"
                },
                "summary": "The machine learning and data science community has made significant while\ndispersive progress in accelerating transformer-based large language models\n(LLMs), and one promising approach is to replace the original causal attention\nin a generative pre-trained transformer (GPT) with \\emph{exponentially decaying\ncausal linear attention}. In this paper, we present LeetDecoding, which is the\nfirst Python package that provides a large set of computation routines for this\nfundamental operator. The launch of LeetDecoding was motivated by the current\nlack of (1) clear understanding of the complexity regarding this operator, (2)\na comprehensive collection of existing computation methods (usually spread in\nseemingly unrelated fields), and (3) CUDA implementations for fast inference on\nGPU. LeetDecoding's design is easy to integrate with existing linear-attention\nLLMs, and allows for researchers to benchmark and evaluate new computation\nmethods for exponentially decaying causal linear attention. The usage of\nLeetDecoding does not require any knowledge of GPU programming and the\nunderlying complexity analysis, intentionally making LeetDecoding accessible to\nLLM practitioners. The source code of LeetDecoding is provided at\n\\href{https://github.com/Computational-Machine-Intelligence/LeetDecoding}{this\nGitHub repository}, and users can simply install LeetDecoding by the command\n\\texttt{pip install leet-decoding}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The machine learning and data science community has made significant while\ndispersive progress in accelerating transformer-based large language models\n(LLMs), and one promising approach is to replace the original causal attention\nin a generative pre-trained transformer (GPT) with \\emph{exponentially decaying\ncausal linear attention}. In this paper, we present LeetDecoding, which is the\nfirst Python package that provides a large set of computation routines for this\nfundamental operator. The launch of LeetDecoding was motivated by the current\nlack of (1) clear understanding of the complexity regarding this operator, (2)\na comprehensive collection of existing computation methods (usually spread in\nseemingly unrelated fields), and (3) CUDA implementations for fast inference on\nGPU. LeetDecoding's design is easy to integrate with existing linear-attention\nLLMs, and allows for researchers to benchmark and evaluate new computation\nmethods for exponentially decaying causal linear attention. The usage of\nLeetDecoding does not require any knowledge of GPU programming and the\nunderlying complexity analysis, intentionally making LeetDecoding accessible to\nLLM practitioners. The source code of LeetDecoding is provided at\n\\href{https://github.com/Computational-Machine-Intelligence/LeetDecoding}{this\nGitHub repository}, and users can simply install LeetDecoding by the command\n\\texttt{pip install leet-decoding}."
                },
                "authors": [
                    {
                        "name": "Jiaping Wang"
                    },
                    {
                        "name": "Simiao Zhang"
                    },
                    {
                        "name": "Qiao-Chu He"
                    },
                    {
                        "name": "Yifan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Chen"
                },
                "author": "Yifan Chen",
                "arxiv_comment": "The source code of LeetDecoding is hosted at\n  https://github.com/Computational-Machine-Intelligence/LeetDecoding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18260v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18260v2",
                "updated": "2025-01-05T14:59:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    14,
                    59,
                    57,
                    6,
                    5,
                    0
                ],
                "published": "2024-12-24T08:20:29Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    8,
                    20,
                    29,
                    1,
                    359,
                    0
                ],
                "title": "Investigating Large Language Models for Code Vulnerability Detection: An\n  Experimental Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Large Language Models for Code Vulnerability Detection: An\n  Experimental Study"
                },
                "summary": "Code vulnerability detection (CVD) is essential for addressing and preventing\nsystem security issues, playing a crucial role in ensuring software security.\nPrevious learning-based vulnerability detection methods rely on either\nfine-tuning medium-size sequence models or training smaller neural networks\nfrom scratch. Recent advancements in large pre-trained language models (LLMs)\nhave showcased remarkable capabilities in various code intelligence tasks\nincluding code understanding and generation. However, the effectiveness of LLMs\nin detecting code vulnerabilities is largely under-explored. This work aims to\ninvestigate the gap by fine-tuning LLMs for the CVD task, involving four\nwidely-used open-source LLMs. We also implement other five previous graph-based\nor medium-size sequence models for comparison. Experiments are conducted on\nfive commonly-used CVD datasets, including both the part of short samples and\nlong samples. In addition, we conduct quantitative experiments to investigate\nthe class imbalance issue and the model's performance on samples of different\nlengths, which are rarely studied in previous works. To better facilitate\ncommunities, we open-source all codes and resources of this study in\nhttps://github.com/SakiRinn/LLM4CVD and\nhttps://huggingface.co/datasets/xuefen/VulResource.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code vulnerability detection (CVD) is essential for addressing and preventing\nsystem security issues, playing a crucial role in ensuring software security.\nPrevious learning-based vulnerability detection methods rely on either\nfine-tuning medium-size sequence models or training smaller neural networks\nfrom scratch. Recent advancements in large pre-trained language models (LLMs)\nhave showcased remarkable capabilities in various code intelligence tasks\nincluding code understanding and generation. However, the effectiveness of LLMs\nin detecting code vulnerabilities is largely under-explored. This work aims to\ninvestigate the gap by fine-tuning LLMs for the CVD task, involving four\nwidely-used open-source LLMs. We also implement other five previous graph-based\nor medium-size sequence models for comparison. Experiments are conducted on\nfive commonly-used CVD datasets, including both the part of short samples and\nlong samples. In addition, we conduct quantitative experiments to investigate\nthe class imbalance issue and the model's performance on samples of different\nlengths, which are rarely studied in previous works. To better facilitate\ncommunities, we open-source all codes and resources of this study in\nhttps://github.com/SakiRinn/LLM4CVD and\nhttps://huggingface.co/datasets/xuefen/VulResource."
                },
                "authors": [
                    {
                        "name": "Xuefeng Jiang"
                    },
                    {
                        "name": "Lvhua Wu"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Jingjing Xue"
                    },
                    {
                        "name": "Yuwei Wang"
                    },
                    {
                        "name": "Tingting Wu"
                    },
                    {
                        "name": "Min Liu"
                    }
                ],
                "author_detail": {
                    "name": "Min Liu"
                },
                "author": "Min Liu",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18260v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18260v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17743v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17743v4",
                "updated": "2025-01-05T14:35:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    14,
                    35,
                    49,
                    6,
                    5,
                    0
                ],
                "published": "2024-05-28T01:55:35Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    1,
                    55,
                    35,
                    1,
                    149,
                    0
                ],
                "title": "ORLM: A Customizable Framework in Training Large Models for Automated\n  Optimization Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORLM: A Customizable Framework in Training Large Models for Automated\n  Optimization Modeling"
                },
                "summary": "Optimization modeling plays a critical role in the application of Operations\nResearch (OR) tools to address real-world problems, yet they pose challenges\nand require extensive expertise from OR experts. With the advent of large\nlanguage models (LLMs), new opportunities have emerged to streamline and\nautomate such task. However, current research predominantly relies on\nclosed-source LLMs such as GPT-4, along with extensive prompt engineering\ntechniques. This reliance stems from the scarcity of high-quality training\ndatasets for optimization modeling, resulting in elevated costs, prolonged\nprocessing times, and privacy concerns. To address these challenges, our work\nis the first to propose a viable path for training open-source LLMs that are\ncapable of optimization modeling and developing solver codes, eventually\nleading to a superior ability for automating optimization modeling and solving.\nParticularly, we introduce OR-Instruct, a semi-automated data synthesis\nframework for optimization modeling that enables customizable enhancements for\nspecific scenarios or model types. We also introduce IndustryOR, the first\nindustrial benchmark for evaluating LLMs in solving practical OR problems. We\ntrain several 7B-scale open-source LLMs using synthesized data (dubbed\nORLMs{https://github.com/Cardinal-Operations/ORLM}), which exhibit\nsignificantly enhanced optimization modeling capabilities, achieving\nstate-of-the-art performance across the NL4OPT, MAMO, and IndustryOR\nbenchmarks. Additionally, our experiments highlight the potential of scaling\nlaw and reinforcement learning to further enhance the performance of ORLMs. The\nworkflows and human-machine interaction paradigms of ORLMs in practical\nindustrial applications are also discussed in the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization modeling plays a critical role in the application of Operations\nResearch (OR) tools to address real-world problems, yet they pose challenges\nand require extensive expertise from OR experts. With the advent of large\nlanguage models (LLMs), new opportunities have emerged to streamline and\nautomate such task. However, current research predominantly relies on\nclosed-source LLMs such as GPT-4, along with extensive prompt engineering\ntechniques. This reliance stems from the scarcity of high-quality training\ndatasets for optimization modeling, resulting in elevated costs, prolonged\nprocessing times, and privacy concerns. To address these challenges, our work\nis the first to propose a viable path for training open-source LLMs that are\ncapable of optimization modeling and developing solver codes, eventually\nleading to a superior ability for automating optimization modeling and solving.\nParticularly, we introduce OR-Instruct, a semi-automated data synthesis\nframework for optimization modeling that enables customizable enhancements for\nspecific scenarios or model types. We also introduce IndustryOR, the first\nindustrial benchmark for evaluating LLMs in solving practical OR problems. We\ntrain several 7B-scale open-source LLMs using synthesized data (dubbed\nORLMs{https://github.com/Cardinal-Operations/ORLM}), which exhibit\nsignificantly enhanced optimization modeling capabilities, achieving\nstate-of-the-art performance across the NL4OPT, MAMO, and IndustryOR\nbenchmarks. Additionally, our experiments highlight the potential of scaling\nlaw and reinforcement learning to further enhance the performance of ORLMs. The\nworkflows and human-machine interaction paradigms of ORLMs in practical\nindustrial applications are also discussed in the paper."
                },
                "authors": [
                    {
                        "name": "Chenyu Huang"
                    },
                    {
                        "name": "Zhengyang Tang"
                    },
                    {
                        "name": "Shixi Hu"
                    },
                    {
                        "name": "Ruoqing Jiang"
                    },
                    {
                        "name": "Xin Zheng"
                    },
                    {
                        "name": "Dongdong Ge"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Zizhuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zizhuo Wang"
                },
                "author": "Zizhuo Wang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17743v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17743v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v2",
                "updated": "2025-01-05T14:11:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    14,
                    11,
                    48,
                    6,
                    5,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe"
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Update performance in MLVU-dev and LVBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02552v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02552v1",
                "updated": "2025-01-05T14:09:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    14,
                    9,
                    12,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T14:09:12Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    14,
                    9,
                    12,
                    6,
                    5,
                    0
                ],
                "title": "Multi-LLM Collaborative Caption Generation in Scientific Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-LLM Collaborative Caption Generation in Scientific Documents"
                },
                "summary": "Scientific figure captioning is a complex task that requires generating\ncontextually appropriate descriptions of visual content. However, existing\nmethods often fall short by utilizing incomplete information, treating the task\nsolely as either an image-to-text or text summarization problem. This\nlimitation hinders the generation of high-quality captions that fully capture\nthe necessary details. Moreover, existing data sourced from arXiv papers\ncontain low-quality captions, posing significant challenges for training large\nlanguage models (LLMs). In this paper, we introduce a framework called\nMulti-LLM Collaborative Figure Caption Generation (MLBCAP) to address these\nchallenges by leveraging specialized LLMs for distinct sub-tasks. Our approach\nunfolds in three key modules: (Quality Assessment) We utilize multimodal LLMs\nto assess the quality of training data, enabling the filtration of low-quality\ncaptions. (Diverse Caption Generation) We then employ a strategy of\nfine-tuning/prompting multiple LLMs on the captioning task to generate\ncandidate captions. (Judgment) Lastly, we prompt a prominent LLM to select the\nhighest quality caption from the candidates, followed by refining any remaining\ninaccuracies. Human evaluations demonstrate that informative captions produced\nby our approach rank better than human-written captions, highlighting its\neffectiveness. Our code is available at https://github.com/teamreboott/MLBCAP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific figure captioning is a complex task that requires generating\ncontextually appropriate descriptions of visual content. However, existing\nmethods often fall short by utilizing incomplete information, treating the task\nsolely as either an image-to-text or text summarization problem. This\nlimitation hinders the generation of high-quality captions that fully capture\nthe necessary details. Moreover, existing data sourced from arXiv papers\ncontain low-quality captions, posing significant challenges for training large\nlanguage models (LLMs). In this paper, we introduce a framework called\nMulti-LLM Collaborative Figure Caption Generation (MLBCAP) to address these\nchallenges by leveraging specialized LLMs for distinct sub-tasks. Our approach\nunfolds in three key modules: (Quality Assessment) We utilize multimodal LLMs\nto assess the quality of training data, enabling the filtration of low-quality\ncaptions. (Diverse Caption Generation) We then employ a strategy of\nfine-tuning/prompting multiple LLMs on the captioning task to generate\ncandidate captions. (Judgment) Lastly, we prompt a prominent LLM to select the\nhighest quality caption from the candidates, followed by refining any remaining\ninaccuracies. Human evaluations demonstrate that informative captions produced\nby our approach rank better than human-written captions, highlighting its\neffectiveness. Our code is available at https://github.com/teamreboott/MLBCAP"
                },
                "authors": [
                    {
                        "name": "Jaeyoung Kim"
                    },
                    {
                        "name": "Jongho Lee"
                    },
                    {
                        "name": "Hong-Jun Choi"
                    },
                    {
                        "name": "Ting-Yao Hsu"
                    },
                    {
                        "name": "Chieh-Yang Huang"
                    },
                    {
                        "name": "Sungchul Kim"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Clyde Lee Giles"
                    },
                    {
                        "name": "Ting-Hao 'Kenneth' Huang"
                    },
                    {
                        "name": "Sungchul Choi"
                    }
                ],
                "author_detail": {
                    "name": "Sungchul Choi"
                },
                "author": "Sungchul Choi",
                "arxiv_comment": "Accepted to AAAI 2025 AI4Research Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02552v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02552v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02532v1",
                "updated": "2025-01-05T13:28:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    13,
                    28,
                    15,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T13:28:15Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    13,
                    28,
                    15,
                    6,
                    5,
                    0
                ],
                "title": "Evaluating Large Language Models Against Human Annotators in Latent\n  Content Analysis: Sentiment, Political Leaning, Emotional Intensity, and\n  Sarcasm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models Against Human Annotators in Latent\n  Content Analysis: Sentiment, Political Leaning, Emotional Intensity, and\n  Sarcasm"
                },
                "summary": "In the era of rapid digital communication, vast amounts of textual data are\ngenerated daily, demanding efficient methods for latent content analysis to\nextract meaningful insights. Large Language Models (LLMs) offer potential for\nautomating this process, yet comprehensive assessments comparing their\nperformance to human annotators across multiple dimensions are lacking. This\nstudy evaluates the reliability, consistency, and quality of seven\nstate-of-the-art LLMs, including variants of OpenAI's GPT-4, Gemini, Llama, and\nMixtral, relative to human annotators in analyzing sentiment, political\nleaning, emotional intensity, and sarcasm detection. A total of 33 human\nannotators and eight LLM variants assessed 100 curated textual items,\ngenerating 3,300 human and 19,200 LLM annotations, with LLMs evaluated across\nthree time points to examine temporal consistency. Inter-rater reliability was\nmeasured using Krippendorff's alpha, and intra-class correlation coefficients\nassessed consistency over time. The results reveal that both humans and LLMs\nexhibit high reliability in sentiment analysis and political leaning\nassessments, with LLMs demonstrating higher internal consistency than humans.\nIn emotional intensity, LLMs displayed higher agreement compared to humans,\nthough humans rated emotional intensity significantly higher. Both groups\nstruggled with sarcasm detection, evidenced by low agreement. LLMs showed\nexcellent temporal consistency across all dimensions, indicating stable\nperformance over time. This research concludes that LLMs, especially GPT-4, can\neffectively replicate human analysis in sentiment and political leaning,\nalthough human expertise remains essential for emotional intensity\ninterpretation. The findings demonstrate the potential of LLMs for consistent\nand high-quality performance in certain areas of latent content analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of rapid digital communication, vast amounts of textual data are\ngenerated daily, demanding efficient methods for latent content analysis to\nextract meaningful insights. Large Language Models (LLMs) offer potential for\nautomating this process, yet comprehensive assessments comparing their\nperformance to human annotators across multiple dimensions are lacking. This\nstudy evaluates the reliability, consistency, and quality of seven\nstate-of-the-art LLMs, including variants of OpenAI's GPT-4, Gemini, Llama, and\nMixtral, relative to human annotators in analyzing sentiment, political\nleaning, emotional intensity, and sarcasm detection. A total of 33 human\nannotators and eight LLM variants assessed 100 curated textual items,\ngenerating 3,300 human and 19,200 LLM annotations, with LLMs evaluated across\nthree time points to examine temporal consistency. Inter-rater reliability was\nmeasured using Krippendorff's alpha, and intra-class correlation coefficients\nassessed consistency over time. The results reveal that both humans and LLMs\nexhibit high reliability in sentiment analysis and political leaning\nassessments, with LLMs demonstrating higher internal consistency than humans.\nIn emotional intensity, LLMs displayed higher agreement compared to humans,\nthough humans rated emotional intensity significantly higher. Both groups\nstruggled with sarcasm detection, evidenced by low agreement. LLMs showed\nexcellent temporal consistency across all dimensions, indicating stable\nperformance over time. This research concludes that LLMs, especially GPT-4, can\neffectively replicate human analysis in sentiment and political leaning,\nalthough human expertise remains essential for emotional intensity\ninterpretation. The findings demonstrate the potential of LLMs for consistent\nand high-quality performance in certain areas of latent content analysis."
                },
                "authors": [
                    {
                        "name": "Ljubisa Bojic"
                    },
                    {
                        "name": "Olga Zagovora"
                    },
                    {
                        "name": "Asta Zelenkauskaite"
                    },
                    {
                        "name": "Vuk Vukovic"
                    },
                    {
                        "name": "Milan Cabarkapa"
                    },
                    {
                        "name": "Selma Veseljevi Jerkovic"
                    },
                    {
                        "name": "Ana Jovanevic"
                    }
                ],
                "author_detail": {
                    "name": "Ana Jovanevic"
                },
                "author": "Ana Jovanevic",
                "arxiv_comment": "24 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02531v1",
                "updated": "2025-01-05T13:18:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    13,
                    18,
                    13,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T13:18:13Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    13,
                    18,
                    13,
                    6,
                    5,
                    0
                ],
                "title": "Towards New Benchmark for AI Alignment & Sentiment Analysis in Socially\n  Important Issues: A Comparative Study of Human and LLMs in the Context of AGI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards New Benchmark for AI Alignment & Sentiment Analysis in Socially\n  Important Issues: A Comparative Study of Human and LLMs in the Context of AGI"
                },
                "summary": "With the expansion of neural networks, such as large language models,\nhumanity is exponentially heading towards superintelligence. As various AI\nsystems are increasingly integrated into the fabric of societies-through\nrecommending values, devising creative solutions, and making decisions-it\nbecomes critical to assess how these AI systems impact humans in the long run.\nThis research aims to contribute towards establishing a benchmark for\nevaluating the sentiment of various Large Language Models in socially importan\nissues. The methodology adopted was a Likert scale survey. Seven LLMs,\nincluding GPT-4 and Bard, were analyzed and compared against sentiment data\nfrom three independent human sample populations. Temporal variations in\nsentiment were also evaluated over three consecutive days. The results\nhighlighted a diversity in sentiment scores among LLMs, ranging from 3.32 to\n4.12 out of 5. GPT-4 recorded the most positive sentiment score towards AGI,\nwhereas Bard was leaning towards the neutral sentiment. The human samples,\ncontrastingly, showed a lower average sentiment of 2.97. The temporal\ncomparison revealed differences in sentiment evolution between LLMs in three\ndays, ranging from 1.03% to 8.21%. The study's analysis outlines the prospect\nof potential conflicts of interest and bias possibilities in LLMs' sentiment\nformation. Results indicate that LLMs, akin to human cognitive processes, could\npotentially develop unique sentiments and subtly influence societies'\nperceptions towards various opinions formed within the LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the expansion of neural networks, such as large language models,\nhumanity is exponentially heading towards superintelligence. As various AI\nsystems are increasingly integrated into the fabric of societies-through\nrecommending values, devising creative solutions, and making decisions-it\nbecomes critical to assess how these AI systems impact humans in the long run.\nThis research aims to contribute towards establishing a benchmark for\nevaluating the sentiment of various Large Language Models in socially importan\nissues. The methodology adopted was a Likert scale survey. Seven LLMs,\nincluding GPT-4 and Bard, were analyzed and compared against sentiment data\nfrom three independent human sample populations. Temporal variations in\nsentiment were also evaluated over three consecutive days. The results\nhighlighted a diversity in sentiment scores among LLMs, ranging from 3.32 to\n4.12 out of 5. GPT-4 recorded the most positive sentiment score towards AGI,\nwhereas Bard was leaning towards the neutral sentiment. The human samples,\ncontrastingly, showed a lower average sentiment of 2.97. The temporal\ncomparison revealed differences in sentiment evolution between LLMs in three\ndays, ranging from 1.03% to 8.21%. The study's analysis outlines the prospect\nof potential conflicts of interest and bias possibilities in LLMs' sentiment\nformation. Results indicate that LLMs, akin to human cognitive processes, could\npotentially develop unique sentiments and subtly influence societies'\nperceptions towards various opinions formed within the LLMs."
                },
                "authors": [
                    {
                        "name": "Ljubisa Bojic"
                    },
                    {
                        "name": "Dylan Seychell"
                    },
                    {
                        "name": "Milan Cabarkapa"
                    }
                ],
                "author_detail": {
                    "name": "Milan Cabarkapa"
                },
                "author": "Milan Cabarkapa",
                "arxiv_comment": "20 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02527v1",
                "updated": "2025-01-05T13:01:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    13,
                    1,
                    47,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T13:01:47Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    13,
                    1,
                    47,
                    6,
                    5,
                    0
                ],
                "title": "Vision-Driven Prompt Optimization for Large Language Models in\n  Multimodal Generative Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Driven Prompt Optimization for Large Language Models in\n  Multimodal Generative Tasks"
                },
                "summary": "Vision generation remains a challenging frontier in artificial intelligence,\nrequiring seamless integration of visual understanding and generative\ncapabilities. In this paper, we propose a novel framework, Vision-Driven Prompt\nOptimization (VDPO), that leverages Large Language Models (LLMs) to dynamically\ngenerate textual prompts from visual inputs, guiding high-fidelity image\nsynthesis. VDPO combines a visual embedding prompt tuner, a textual instruction\ngenerator, and a vision generation module to achieve state-of-the-art\nperformance in diverse vision generation tasks. Extensive experiments on\nbenchmarks such as COCO and Sketchy demonstrate that VDPO consistently\noutperforms existing methods, achieving significant improvements in FID, LPIPS,\nand BLEU/CIDEr scores. Additional analyses reveal the scalability, robustness,\nand generalization capabilities of VDPO, making it a versatile solution for\nin-domain and out-of-domain tasks. Human evaluations further validate the\npractical superiority of VDPO in generating visually appealing and semantically\ncoherent outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision generation remains a challenging frontier in artificial intelligence,\nrequiring seamless integration of visual understanding and generative\ncapabilities. In this paper, we propose a novel framework, Vision-Driven Prompt\nOptimization (VDPO), that leverages Large Language Models (LLMs) to dynamically\ngenerate textual prompts from visual inputs, guiding high-fidelity image\nsynthesis. VDPO combines a visual embedding prompt tuner, a textual instruction\ngenerator, and a vision generation module to achieve state-of-the-art\nperformance in diverse vision generation tasks. Extensive experiments on\nbenchmarks such as COCO and Sketchy demonstrate that VDPO consistently\noutperforms existing methods, achieving significant improvements in FID, LPIPS,\nand BLEU/CIDEr scores. Additional analyses reveal the scalability, robustness,\nand generalization capabilities of VDPO, making it a versatile solution for\nin-domain and out-of-domain tasks. Human evaluations further validate the\npractical superiority of VDPO in generating visually appealing and semantically\ncoherent outputs."
                },
                "authors": [
                    {
                        "name": "Leo Franklin"
                    },
                    {
                        "name": "Apiradee Boonmee"
                    },
                    {
                        "name": "Kritsada Wongsuwan"
                    }
                ],
                "author_detail": {
                    "name": "Kritsada Wongsuwan"
                },
                "author": "Kritsada Wongsuwan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02506v1",
                "updated": "2025-01-05T11:06:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    11,
                    6,
                    55,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T11:06:55Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    11,
                    6,
                    55,
                    6,
                    5,
                    0
                ],
                "title": "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models\n  in Multi-Hop Tool Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models\n  in Multi-Hop Tool Use"
                },
                "summary": "Effective evaluation of multi-hop tool use is critical for analyzing the\nunderstanding, reasoning, and function-calling capabilities of large language\nmodels (LLMs). However, progress has been hindered by a lack of reliable\nevaluation datasets. To address this, we present ToolHop, a dataset comprising\n995 user queries and 3,912 associated tools, specifically designed for rigorous\nevaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful\ninterdependencies, locally executable tools, detailed feedback, and verifiable\nanswers through a novel query-driven data construction approach that includes\ntool creation, document refinement, and code generation. We evaluate 14 LLMs\nacross five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and\nGPT), uncovering significant challenges in handling multi-hop tool-use\nscenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%,\nunderscoring substantial room for improvement. Further analysis reveals\nvariations in tool-use strategies for various families, offering actionable\ninsights to guide the development of more effective approaches. Code and data\ncan be found in https://huggingface.co/bytedance-research/ToolHop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective evaluation of multi-hop tool use is critical for analyzing the\nunderstanding, reasoning, and function-calling capabilities of large language\nmodels (LLMs). However, progress has been hindered by a lack of reliable\nevaluation datasets. To address this, we present ToolHop, a dataset comprising\n995 user queries and 3,912 associated tools, specifically designed for rigorous\nevaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful\ninterdependencies, locally executable tools, detailed feedback, and verifiable\nanswers through a novel query-driven data construction approach that includes\ntool creation, document refinement, and code generation. We evaluate 14 LLMs\nacross five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and\nGPT), uncovering significant challenges in handling multi-hop tool-use\nscenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%,\nunderscoring substantial room for improvement. Further analysis reveals\nvariations in tool-use strategies for various families, offering actionable\ninsights to guide the development of more effective approaches. Code and data\ncan be found in https://huggingface.co/bytedance-research/ToolHop."
                },
                "authors": [
                    {
                        "name": "Junjie Ye"
                    },
                    {
                        "name": "Zhengyin Du"
                    },
                    {
                        "name": "Xuesong Yao"
                    },
                    {
                        "name": "Weijian Lin"
                    },
                    {
                        "name": "Yufei Xu"
                    },
                    {
                        "name": "Zehui Chen"
                    },
                    {
                        "name": "Zaiyuan Wang"
                    },
                    {
                        "name": "Sining Zhu"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Jiechao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiechao Chen"
                },
                "author": "Jiechao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02486v1",
                "updated": "2025-01-05T09:37:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    9,
                    37,
                    23,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T09:37:23Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    9,
                    37,
                    23,
                    6,
                    5,
                    0
                ],
                "title": "LLMPC: Large Language Model Predictive Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMPC: Large Language Model Predictive Control"
                },
                "summary": "Recent advancements in prompting techniques for Large Language Models (LLMs)\nhave improved their reasoning, planning, and action abilities. This paper\nexamines these prompting techniques through the lens of model predictive\ncontrol (MPC). We show that LLMs act as implicit planning cost function\nminimizers when planning prompts are used. Under our framework we demonstrate\nthat LLM planning performance can be improved further by incorporating real\nplanning cost functions and evaluators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in prompting techniques for Large Language Models (LLMs)\nhave improved their reasoning, planning, and action abilities. This paper\nexamines these prompting techniques through the lens of model predictive\ncontrol (MPC). We show that LLMs act as implicit planning cost function\nminimizers when planning prompts are used. Under our framework we demonstrate\nthat LLM planning performance can be improved further by incorporating real\nplanning cost functions and evaluators."
                },
                "authors": [
                    {
                        "name": "Gabriel Maher"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Maher"
                },
                "author": "Gabriel Maher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02483v1",
                "updated": "2025-01-05T09:12:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    9,
                    12,
                    37,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T09:12:37Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    9,
                    12,
                    37,
                    6,
                    5,
                    0
                ],
                "title": "sTiles: An Accelerated Computational Framework for Sparse Factorizations\n  of Structured Matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "sTiles: An Accelerated Computational Framework for Sparse Factorizations\n  of Structured Matrices"
                },
                "summary": "This paper introduces sTiles, a GPU-accelerated framework for factorizing\nsparse structured symmetric matrices. By leveraging tile algorithms for\nfine-grained computations, sTiles uses a structure-aware task execution flow to\nhandle challenging arrowhead sparse matrices with variable bandwidths, common\nin scientific and engineering fields. It minimizes fill-in during Cholesky\nfactorization using permutation techniques and employs a static scheduler to\nmanage tasks on shared-memory systems with GPU accelerators. sTiles balances\ntile size and parallelism, where larger tiles enhance algorithmic intensity but\nincrease floating-point operations and memory usage, while parallelism is\nconstrained by the arrowhead structure. To expose more parallelism, a\nleft-looking Cholesky variant breaks sequential dependencies in trailing\nsubmatrix updates via tree reductions. Evaluations show sTiles achieves\nspeedups of up to 8.41X, 9.34X, 5.07X, and 11.08X compared to CHOLMOD, SymPACK,\nMUMPS, and PARDISO, respectively, and a 5X speedup compared to a 32-core AMD\nEPYC CPU on an NVIDIA A100 GPU. Our generic software framework imports\nwell-established concepts from dense matrix computations but they all require\ncustomizations in their deployments on hybrid architectures to best handle\nfactorizations of sparse matrices with arrowhead structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces sTiles, a GPU-accelerated framework for factorizing\nsparse structured symmetric matrices. By leveraging tile algorithms for\nfine-grained computations, sTiles uses a structure-aware task execution flow to\nhandle challenging arrowhead sparse matrices with variable bandwidths, common\nin scientific and engineering fields. It minimizes fill-in during Cholesky\nfactorization using permutation techniques and employs a static scheduler to\nmanage tasks on shared-memory systems with GPU accelerators. sTiles balances\ntile size and parallelism, where larger tiles enhance algorithmic intensity but\nincrease floating-point operations and memory usage, while parallelism is\nconstrained by the arrowhead structure. To expose more parallelism, a\nleft-looking Cholesky variant breaks sequential dependencies in trailing\nsubmatrix updates via tree reductions. Evaluations show sTiles achieves\nspeedups of up to 8.41X, 9.34X, 5.07X, and 11.08X compared to CHOLMOD, SymPACK,\nMUMPS, and PARDISO, respectively, and a 5X speedup compared to a 32-core AMD\nEPYC CPU on an NVIDIA A100 GPU. Our generic software framework imports\nwell-established concepts from dense matrix computations but they all require\ncustomizations in their deployments on hybrid architectures to best handle\nfactorizations of sparse matrices with arrowhead structures."
                },
                "authors": [
                    {
                        "name": "Esmail Abdul Fattah"
                    },
                    {
                        "name": "Hatem Ltaief"
                    },
                    {
                        "name": "Havard Rue"
                    },
                    {
                        "name": "David Keyes"
                    }
                ],
                "author_detail": {
                    "name": "David Keyes"
                },
                "author": "David Keyes",
                "arxiv_comment": "13 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02482v1",
                "updated": "2025-01-05T09:09:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    9,
                    9,
                    53,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T09:09:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    9,
                    9,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "Decoding News Bias: Multi Bias Detection in News Articles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding News Bias: Multi Bias Detection in News Articles"
                },
                "summary": "News Articles provides crucial information about various events happening in\nthe society but they unfortunately come with different kind of biases. These\nbiases can significantly distort public opinion and trust in the media, making\nit essential to develop techniques to detect and address them. Previous works\nhave majorly worked towards identifying biases in particular domains e.g.,\nPolitical, gender biases. However, more comprehensive studies are needed to\ndetect biases across diverse domains. Large language models (LLMs) offer a\npowerful way to analyze and understand natural language, making them ideal for\nconstructing datasets and detecting these biases. In this work, we have\nexplored various biases present in the news articles, built a dataset using\nLLMs and present results obtained using multiple detection techniques. Our\napproach highlights the importance of broad-spectrum bias detection and offers\nnew insights for improving the integrity of news articles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "News Articles provides crucial information about various events happening in\nthe society but they unfortunately come with different kind of biases. These\nbiases can significantly distort public opinion and trust in the media, making\nit essential to develop techniques to detect and address them. Previous works\nhave majorly worked towards identifying biases in particular domains e.g.,\nPolitical, gender biases. However, more comprehensive studies are needed to\ndetect biases across diverse domains. Large language models (LLMs) offer a\npowerful way to analyze and understand natural language, making them ideal for\nconstructing datasets and detecting these biases. In this work, we have\nexplored various biases present in the news articles, built a dataset using\nLLMs and present results obtained using multiple detection techniques. Our\napproach highlights the importance of broad-spectrum bias detection and offers\nnew insights for improving the integrity of news articles."
                },
                "authors": [
                    {
                        "name": "Bhushan Santosh Shah"
                    },
                    {
                        "name": "Deven Santosh Shah"
                    },
                    {
                        "name": "Vahida Attar"
                    }
                ],
                "author_detail": {
                    "name": "Vahida Attar"
                },
                "author": "Vahida Attar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22392v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22392v4",
                "updated": "2025-01-05T08:58:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    8,
                    58,
                    48,
                    6,
                    5,
                    0
                ],
                "published": "2024-10-29T17:56:05Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    56,
                    5,
                    1,
                    303,
                    0
                ],
                "title": "CBAM-EfficientNetV2 for Histopathology Image Classification using\n  Transfer Learning and Dual Attention Mechanisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CBAM-EfficientNetV2 for Histopathology Image Classification using\n  Transfer Learning and Dual Attention Mechanisms"
                },
                "summary": "Breast cancer histopathology image classification is critical for early\ndetection and improved patient outcomes. 1 This study introduces a novel\napproach leveraging EfficientNetV2 models, to improve feature extraction and\nfocus on relevant tissue regions. The proposed models were evaluated on the\nBreakHis dataset across multiple magnification scales (40X, 100X, 200X, and\n400X). 2 Among them, the EfficientNetV2-XL with CBAM achieved outstanding\nperformance, reaching a peak accuracy of 98.96 percent and an F1-score of 98.31\npercent at 400X magnification, outperforming state-of-the-art methods. 3 By\nintegrating Contrast Limited Adaptive Histogram Equalization (CLAHE) for\npreprocessing and optimizing computational efficiency, this method demonstrates\nits suitability for real-time clinical deployment. 3 The results underscore the\npotential of attention-enhanced scalable architectures in advancing diagnostic\nprecision for breast cancer detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breast cancer histopathology image classification is critical for early\ndetection and improved patient outcomes. 1 This study introduces a novel\napproach leveraging EfficientNetV2 models, to improve feature extraction and\nfocus on relevant tissue regions. The proposed models were evaluated on the\nBreakHis dataset across multiple magnification scales (40X, 100X, 200X, and\n400X). 2 Among them, the EfficientNetV2-XL with CBAM achieved outstanding\nperformance, reaching a peak accuracy of 98.96 percent and an F1-score of 98.31\npercent at 400X magnification, outperforming state-of-the-art methods. 3 By\nintegrating Contrast Limited Adaptive Histogram Equalization (CLAHE) for\npreprocessing and optimizing computational efficiency, this method demonstrates\nits suitability for real-time clinical deployment. 3 The results underscore the\npotential of attention-enhanced scalable architectures in advancing diagnostic\nprecision for breast cancer detection."
                },
                "authors": [
                    {
                        "name": "Naren Sengodan"
                    }
                ],
                "author_detail": {
                    "name": "Naren Sengodan"
                },
                "author": "Naren Sengodan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22392v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22392v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08894v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08894v2",
                "updated": "2025-01-05T08:33:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    8,
                    33,
                    16,
                    6,
                    5,
                    0
                ],
                "published": "2024-08-09T04:30:16Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    4,
                    30,
                    16,
                    4,
                    222,
                    0
                ],
                "title": "Enhancing Exploratory Learning through Exploratory Search with the\n  Emergence of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Exploratory Learning through Exploratory Search with the\n  Emergence of Large Language Models"
                },
                "summary": "In the information era, how learners find, evaluate, and effectively use\ninformation has become a challenging issue, especially with the added\ncomplexity of large language models (LLMs) that have further confused learners\nin their information retrieval and search activities. This study attempts to\nunpack this complexity by combining exploratory search strategies with the\ntheories of exploratory learning to form a new theoretical model of exploratory\nlearning from the perspective of students' learning. Our work adapts Kolb's\nlearning model by incorporating high-frequency exploration and feedback loops,\naiming to promote deep cognitive and higher-order cognitive skill development\nin students. Additionally, this paper discusses and suggests how advanced LLMs\nintegrated into information retrieval and information theory can support\nstudents in their exploratory searches, contributing theoretically to promoting\nstudent-computer interaction and supporting their learning journeys in the new\nera with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the information era, how learners find, evaluate, and effectively use\ninformation has become a challenging issue, especially with the added\ncomplexity of large language models (LLMs) that have further confused learners\nin their information retrieval and search activities. This study attempts to\nunpack this complexity by combining exploratory search strategies with the\ntheories of exploratory learning to form a new theoretical model of exploratory\nlearning from the perspective of students' learning. Our work adapts Kolb's\nlearning model by incorporating high-frequency exploration and feedback loops,\naiming to promote deep cognitive and higher-order cognitive skill development\nin students. Additionally, this paper discusses and suggests how advanced LLMs\nintegrated into information retrieval and information theory can support\nstudents in their exploratory searches, contributing theoretically to promoting\nstudent-computer interaction and supporting their learning journeys in the new\nera with LLMs."
                },
                "authors": [
                    {
                        "name": "Yiming Luo"
                    },
                    {
                        "name": "Patrick Cheong-Iao Pang"
                    },
                    {
                        "name": "Shanton Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shanton Chang"
                },
                "author": "Shanton Chang",
                "arxiv_comment": "11 pages, 7 figures Accpted by HICSS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08894v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08894v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02471v1",
                "updated": "2025-01-05T07:46:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    46,
                    51,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T07:46:51Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    46,
                    51,
                    6,
                    5,
                    0
                ],
                "title": "Hengqin-RA-v1: Advanced Large Language Model for Diagnosis and Treatment\n  of Rheumatoid Arthritis with Dataset based Traditional Chinese Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hengqin-RA-v1: Advanced Large Language Model for Diagnosis and Treatment\n  of Rheumatoid Arthritis with Dataset based Traditional Chinese Medicine"
                },
                "summary": "Large language models (LLMs) primarily trained on English texts, often face\nbiases and inaccuracies in Chinese contexts. Their limitations are pronounced\nin fields like Traditional Chinese Medicine (TCM), where cultural and clinical\nsubtleties are vital, further hindered by a lack of domain-specific data, such\nas rheumatoid arthritis (RA). To address these issues, this paper introduces\nHengqin-RA-v1, the first large language model specifically tailored for TCM\nwith a focus on diagnosing and treating RA. We also present HQ-GCM-RA-C1, a\ncomprehensive RA-specific dataset curated from ancient Chinese medical\nliterature, classical texts, and modern clinical studies. This dataset empowers\nHengqin-RA-v1 to deliver accurate and culturally informed responses,\neffectively bridging the gaps left by general-purpose models. Extensive\nexperiments demonstrate that Hengqin-RA-v1 outperforms state-of-the-art models,\neven surpassing the diagnostic accuracy of TCM practitioners in certain cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) primarily trained on English texts, often face\nbiases and inaccuracies in Chinese contexts. Their limitations are pronounced\nin fields like Traditional Chinese Medicine (TCM), where cultural and clinical\nsubtleties are vital, further hindered by a lack of domain-specific data, such\nas rheumatoid arthritis (RA). To address these issues, this paper introduces\nHengqin-RA-v1, the first large language model specifically tailored for TCM\nwith a focus on diagnosing and treating RA. We also present HQ-GCM-RA-C1, a\ncomprehensive RA-specific dataset curated from ancient Chinese medical\nliterature, classical texts, and modern clinical studies. This dataset empowers\nHengqin-RA-v1 to deliver accurate and culturally informed responses,\neffectively bridging the gaps left by general-purpose models. Extensive\nexperiments demonstrate that Hengqin-RA-v1 outperforms state-of-the-art models,\neven surpassing the diagnostic accuracy of TCM practitioners in certain cases."
                },
                "authors": [
                    {
                        "name": "Yishen Liu"
                    },
                    {
                        "name": "Shengda Luo"
                    },
                    {
                        "name": "Zishao Zhong"
                    },
                    {
                        "name": "Tongtong Wu"
                    },
                    {
                        "name": "Jianguo Zhang"
                    },
                    {
                        "name": "Peiyao Ou"
                    },
                    {
                        "name": "Yong Liang"
                    },
                    {
                        "name": "Liang Liu"
                    },
                    {
                        "name": "Hudan Pan"
                    }
                ],
                "author_detail": {
                    "name": "Hudan Pan"
                },
                "author": "Hudan Pan",
                "arxiv_comment": "8 pages, 5 figures, AAAI-2025 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v1",
                "updated": "2025-01-05T07:41:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "11 pages, 15 figures, and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]