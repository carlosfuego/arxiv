[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.14093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14093v1",
                "updated": "2025-09-17T15:33:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    33,
                    44,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T15:33:44Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    33,
                    44,
                    2,
                    260,
                    0
                ],
                "title": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A\n  Self-Optimizing Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A\n  Self-Optimizing Framework"
                },
                "summary": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nprompting intermediate steps, improving accuracy and robustness in arithmetic,\nlogic, and commonsense tasks. However, this benefit comes with high\ncomputational costs: longer outputs increase latency, memory usage, and\nKV-cache demands. These issues are especially critical in software engineering\ntasks where concise and deterministic outputs are required. To investigate\nthese trade-offs, we conduct an empirical study based on code generation\nbenchmarks. The results reveal that longer CoT does not always help. Excessive\nreasoning often causes truncation, accuracy drops, and latency up to five times\nhigher, with failed outputs consistently longer than successful ones. These\nfindings challenge the assumption that longer reasoning is inherently better\nand highlight the need for adaptive CoT control. Motivated by this, we propose\nSEER (Self-Enhancing Efficient Reasoning), an adaptive framework that\ncompresses CoT while preserving accuracy. SEER combines Best-of-N sampling with\ntask-aware adaptive filtering, dynamically adjusting thresholds based on\npre-inference outputs to reduce verbosity and computational overhead. We then\nevaluate SEER on three software engineering tasks and one math task. On\naverage, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,\nand eliminates most infinite loops. These results demonstrate SEER as a\npractical method to make CoT-enhanced LLMs more efficient and robust, even\nunder resource constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nprompting intermediate steps, improving accuracy and robustness in arithmetic,\nlogic, and commonsense tasks. However, this benefit comes with high\ncomputational costs: longer outputs increase latency, memory usage, and\nKV-cache demands. These issues are especially critical in software engineering\ntasks where concise and deterministic outputs are required. To investigate\nthese trade-offs, we conduct an empirical study based on code generation\nbenchmarks. The results reveal that longer CoT does not always help. Excessive\nreasoning often causes truncation, accuracy drops, and latency up to five times\nhigher, with failed outputs consistently longer than successful ones. These\nfindings challenge the assumption that longer reasoning is inherently better\nand highlight the need for adaptive CoT control. Motivated by this, we propose\nSEER (Self-Enhancing Efficient Reasoning), an adaptive framework that\ncompresses CoT while preserving accuracy. SEER combines Best-of-N sampling with\ntask-aware adaptive filtering, dynamically adjusting thresholds based on\npre-inference outputs to reduce verbosity and computational overhead. We then\nevaluate SEER on three software engineering tasks and one math task. On\naverage, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,\nand eliminates most infinite loops. These results demonstrate SEER as a\npractical method to make CoT-enhanced LLMs more efficient and robust, even\nunder resource constraints."
                },
                "authors": [
                    {
                        "name": "Kerui Huang"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Tongtong Xu"
                    },
                    {
                        "name": "Lingfeng Bao"
                    },
                    {
                        "name": "Xin Xia"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xia"
                },
                "author": "Xin Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14041v1",
                "updated": "2025-09-17T14:42:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    42,
                    38,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T14:42:38Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    42,
                    38,
                    2,
                    260,
                    0
                ],
                "title": "A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval\n  Prediction For Instruction Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval\n  Prediction For Instruction Caching"
                },
                "summary": "Modern mobile CPU software pose challenges for conventional instruction cache\nreplacement policies due to their complex runtime behavior causing high reuse\ndistance between executions of the same instruction. Mobile code commonly\nsuffers from large amounts of stalls in the CPU frontend and thus starvation of\nthe rest of the CPU resources. Complexity of these applications and their code\nfootprint are projected to grow at a rate faster than available on-chip memory\ndue to power and area constraints, making conventional hardware-centric methods\nfor managing instruction caches to be inadequate. We present a novel\nsoftware-hardware co-design approach called TRRIP (Temperature-based\nRe-Reference Interval Prediction) that enables the compiler to analyze,\nclassify, and transform code based on \"temperature\" (hot/cold), and to provide\nthe hardware with a summary of code temperature information through a\nwell-defined OS interface based on using code page attributes. TRRIP's\nlightweight hardware extension employs code temperature attributes to optimize\nthe instruction cache replacement policy resulting in the eviction rate\nreduction of hot code. TRRIP is designed to be practical and adoptable in real\nmobile systems that have strict feature requirements on both the software and\nhardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5%\nresulting in geomean speedup of 3.9%, on top of RRIP cache replacement running\nmobile code already optimized using PGO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern mobile CPU software pose challenges for conventional instruction cache\nreplacement policies due to their complex runtime behavior causing high reuse\ndistance between executions of the same instruction. Mobile code commonly\nsuffers from large amounts of stalls in the CPU frontend and thus starvation of\nthe rest of the CPU resources. Complexity of these applications and their code\nfootprint are projected to grow at a rate faster than available on-chip memory\ndue to power and area constraints, making conventional hardware-centric methods\nfor managing instruction caches to be inadequate. We present a novel\nsoftware-hardware co-design approach called TRRIP (Temperature-based\nRe-Reference Interval Prediction) that enables the compiler to analyze,\nclassify, and transform code based on \"temperature\" (hot/cold), and to provide\nthe hardware with a summary of code temperature information through a\nwell-defined OS interface based on using code page attributes. TRRIP's\nlightweight hardware extension employs code temperature attributes to optimize\nthe instruction cache replacement policy resulting in the eviction rate\nreduction of hot code. TRRIP is designed to be practical and adoptable in real\nmobile systems that have strict feature requirements on both the software and\nhardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5%\nresulting in geomean speedup of 3.9%, on top of RRIP cache replacement running\nmobile code already optimized using PGO."
                },
                "authors": [
                    {
                        "name": "Henry Kao"
                    },
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Prabhdeep Singh Soni"
                    },
                    {
                        "name": "Ali Sedaghati"
                    },
                    {
                        "name": "Fang Su"
                    },
                    {
                        "name": "Bryan Chan"
                    },
                    {
                        "name": "Maziar Goudarzi"
                    },
                    {
                        "name": "Reza Azimi"
                    }
                ],
                "author_detail": {
                    "name": "Reza Azimi"
                },
                "author": "Reza Azimi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13848v1",
                "updated": "2025-09-17T09:24:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    24,
                    40,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T09:24:40Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    24,
                    40,
                    2,
                    260,
                    0
                ],
                "title": "SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation"
                },
                "summary": "Feature caching has recently emerged as a promising method for diffusion\nmodel acceleration. It effectively alleviates the inefficiency problem caused\nby high computational requirements by caching similar features in the inference\nprocess of the diffusion model. In this paper, we analyze existing feature\ncaching methods from the perspective of information utilization, and point out\nthat relying solely on historical information will lead to constrained accuracy\nand speed performance. And we propose a novel paradigm that introduces future\ninformation via self-speculation based on the information similarity at the\nsame time step across different iteration times. Based on this paradigm, we\npresent \\textit{SpecDiff}, a training-free multi-level feature caching strategy\nincluding a cached feature selection algorithm and a multi-level feature\nclassification algorithm. (1) Feature selection algorithm based on\nself-speculative information. \\textit{SpecDiff} determines a dynamic importance\nscore for each token based on self-speculative information and historical\ninformation, and performs cached feature selection through the importance\nscore. (2) Multi-level feature classification algorithm based on feature\nimportance scores. \\textit{SpecDiff} classifies tokens by leveraging the\ndifferences in feature importance scores and introduces a multi-level feature\ncalculation strategy. Extensive experiments show that \\textit{SpecDiff}\nachieves average 2.80 \\times, 2.74 \\times , and 3.17\\times speedup with\nnegligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow\non NVIDIA A800-80GB GPU. By merging speculative and historical information,\n\\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing\nthe Pareto frontier of speedup and accuracy in the efficient diffusion model\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching has recently emerged as a promising method for diffusion\nmodel acceleration. It effectively alleviates the inefficiency problem caused\nby high computational requirements by caching similar features in the inference\nprocess of the diffusion model. In this paper, we analyze existing feature\ncaching methods from the perspective of information utilization, and point out\nthat relying solely on historical information will lead to constrained accuracy\nand speed performance. And we propose a novel paradigm that introduces future\ninformation via self-speculation based on the information similarity at the\nsame time step across different iteration times. Based on this paradigm, we\npresent \\textit{SpecDiff}, a training-free multi-level feature caching strategy\nincluding a cached feature selection algorithm and a multi-level feature\nclassification algorithm. (1) Feature selection algorithm based on\nself-speculative information. \\textit{SpecDiff} determines a dynamic importance\nscore for each token based on self-speculative information and historical\ninformation, and performs cached feature selection through the importance\nscore. (2) Multi-level feature classification algorithm based on feature\nimportance scores. \\textit{SpecDiff} classifies tokens by leveraging the\ndifferences in feature importance scores and introduces a multi-level feature\ncalculation strategy. Extensive experiments show that \\textit{SpecDiff}\nachieves average 2.80 \\times, 2.74 \\times , and 3.17\\times speedup with\nnegligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow\non NVIDIA A800-80GB GPU. By merging speculative and historical information,\n\\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing\nthe Pareto frontier of speedup and accuracy in the efficient diffusion model\ninference."
                },
                "authors": [
                    {
                        "name": "Jiayi Pan"
                    },
                    {
                        "name": "Jiaming Xu"
                    },
                    {
                        "name": "Yongkang Zhou"
                    },
                    {
                        "name": "Guohao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Guohao Dai"
                },
                "author": "Guohao Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13789v2",
                "updated": "2025-09-18T04:57:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    4,
                    57,
                    32,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-17T07:58:36Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    58,
                    36,
                    2,
                    260,
                    0
                ],
                "title": "BWCache: Accelerating Video Diffusion Transformers through Block-Wise\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BWCache: Accelerating Video Diffusion Transformers through Block-Wise\n  Caching"
                },
                "summary": "Recent advancements in Diffusion Transformers (DiTs) have established them as\nthe state-of-the-art method for video generation. However, their inherently\nsequential denoising process results in inevitable latency, limiting real-world\napplicability. Existing acceleration methods either compromise visual quality\ndue to architectural modifications or fail to reuse intermediate features at\nproper granularity. Our analysis reveals that DiT blocks are the primary\ncontributors to inference latency. Across diffusion timesteps, the feature\nvariations of DiT blocks exhibit a U-shaped pattern with high similarity during\nintermediate timesteps, which suggests substantial computational redundancy. In\nthis paper, we propose Block-Wise Caching (BWCache), a training-free method to\naccelerate DiT-based video generation. BWCache dynamically caches and reuses\nfeatures from DiT blocks across diffusion timesteps. Furthermore, we introduce\na similarity indicator that triggers feature reuse only when the differences\nbetween block features at adjacent timesteps fall below a threshold, thereby\nminimizing redundant computations while maintaining visual fidelity. Extensive\nexperiments on several video diffusion models demonstrate that BWCache achieves\nup to 2.24$\\times$ speedup with comparable visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Diffusion Transformers (DiTs) have established them as\nthe state-of-the-art method for video generation. However, their inherently\nsequential denoising process results in inevitable latency, limiting real-world\napplicability. Existing acceleration methods either compromise visual quality\ndue to architectural modifications or fail to reuse intermediate features at\nproper granularity. Our analysis reveals that DiT blocks are the primary\ncontributors to inference latency. Across diffusion timesteps, the feature\nvariations of DiT blocks exhibit a U-shaped pattern with high similarity during\nintermediate timesteps, which suggests substantial computational redundancy. In\nthis paper, we propose Block-Wise Caching (BWCache), a training-free method to\naccelerate DiT-based video generation. BWCache dynamically caches and reuses\nfeatures from DiT blocks across diffusion timesteps. Furthermore, we introduce\na similarity indicator that triggers feature reuse only when the differences\nbetween block features at adjacent timesteps fall below a threshold, thereby\nminimizing redundant computations while maintaining visual fidelity. Extensive\nexperiments on several video diffusion models demonstrate that BWCache achieves\nup to 2.24$\\times$ speedup with comparable visual quality."
                },
                "authors": [
                    {
                        "name": "Hanshuai Cui"
                    },
                    {
                        "name": "Zhiqing Tang"
                    },
                    {
                        "name": "Zhifei Xu"
                    },
                    {
                        "name": "Zhi Yao"
                    },
                    {
                        "name": "Wenyi Zeng"
                    },
                    {
                        "name": "Weijia Jia"
                    }
                ],
                "author_detail": {
                    "name": "Weijia Jia"
                },
                "author": "Weijia Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13604v1",
                "updated": "2025-09-17T00:28:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    0,
                    28,
                    49,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T00:28:49Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    0,
                    28,
                    49,
                    2,
                    260,
                    0
                ],
                "title": "A Framework for Multi-source Prefetching Through Adaptive Weight",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Multi-source Prefetching Through Adaptive Weight"
                },
                "summary": "The World Wide Web has come to be a great part of our daily life, yet user\nobserved latency is still a problem that needs a proper means of handling. Even\nthough earlier attempts focused on caching as the chief solution to tackling\nthis issue, its success was extremely limited. Prefetching has come to be the\nprimary technique in supplementing caching towards soothing the latency problem\nassociated with the contemporary Internet. However, existing approaches in\nprefetching are extremely limited in their ability to employ application level\nweb document relationship which is often visible only to the content developer.\nThis is because most approaches are access history based schemes that make\nfuture users' access prediction only based on past user access. Attempts to\nincorporate prefetching schemes that utilize semantic information with those\nthat use users past access history are extremely limited in their\nextensibility. In this work we present a novel framework that enables\nintegration of schemes from both worlds of prefetching without the need for a\nmajor modification to the algorithms. When there is a need/possibility to\ncapture new application level context, a new algorithm could be developed to do\nso and then it can be integrated into the framework. Since each participating\nscheme is merely viewed as an algorithm that produces a list of candidate\nobjects that are likely to be accessed in the near future, the framework can\nentertain any one of the existing prefetching schemes. With its adaptive weight\nmanagement technique the framework adjusts the effect of each algorithm in the\noverall prediction to parallel with its observed performance so far. We have\nfound this formwork to be less aggressive than its contemporary counterparts\nwhich is extremely important for resource constrained mobile devices that have\ncome to be the major means of access by users of the current web.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The World Wide Web has come to be a great part of our daily life, yet user\nobserved latency is still a problem that needs a proper means of handling. Even\nthough earlier attempts focused on caching as the chief solution to tackling\nthis issue, its success was extremely limited. Prefetching has come to be the\nprimary technique in supplementing caching towards soothing the latency problem\nassociated with the contemporary Internet. However, existing approaches in\nprefetching are extremely limited in their ability to employ application level\nweb document relationship which is often visible only to the content developer.\nThis is because most approaches are access history based schemes that make\nfuture users' access prediction only based on past user access. Attempts to\nincorporate prefetching schemes that utilize semantic information with those\nthat use users past access history are extremely limited in their\nextensibility. In this work we present a novel framework that enables\nintegration of schemes from both worlds of prefetching without the need for a\nmajor modification to the algorithms. When there is a need/possibility to\ncapture new application level context, a new algorithm could be developed to do\nso and then it can be integrated into the framework. Since each participating\nscheme is merely viewed as an algorithm that produces a list of candidate\nobjects that are likely to be accessed in the near future, the framework can\nentertain any one of the existing prefetching schemes. With its adaptive weight\nmanagement technique the framework adjusts the effect of each algorithm in the\noverall prediction to parallel with its observed performance so far. We have\nfound this formwork to be less aggressive than its contemporary counterparts\nwhich is extremely important for resource constrained mobile devices that have\ncome to be the major means of access by users of the current web."
                },
                "authors": [
                    {
                        "name": "Yoseph Berhanu Alebachew"
                    },
                    {
                        "name": "Mulugeta Libsie"
                    }
                ],
                "author_detail": {
                    "name": "Mulugeta Libsie"
                },
                "author": "Mulugeta Libsie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21492v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21492v3",
                "updated": "2025-09-16T23:56:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    23,
                    56,
                    55,
                    1,
                    259,
                    0
                ],
                "published": "2025-07-29T04:21:11Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    4,
                    21,
                    11,
                    1,
                    210,
                    0
                ],
                "title": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist"
                },
                "summary": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts."
                },
                "authors": [
                    {
                        "name": "Yicong Luo"
                    },
                    {
                        "name": "Senhe Hao"
                    },
                    {
                        "name": "Brian Wheatman"
                    },
                    {
                        "name": "Prashant Pandey"
                    },
                    {
                        "name": "Helen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Helen Xu"
                },
                "author": "Helen Xu",
                "arxiv_doi": "10.1145/3754598.3754655",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754655",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.21492v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21492v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Original paper was accepted into ICPP 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08256v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08256v2",
                "updated": "2025-09-16T23:15:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    23,
                    15,
                    44,
                    1,
                    259,
                    0
                ],
                "published": "2025-05-28T05:22:44Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    5,
                    22,
                    44,
                    2,
                    148,
                    0
                ],
                "title": "FIER: Fine-Grained and Efficient KV Cache Retrieval for Long-context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FIER: Fine-Grained and Efficient KV Cache Retrieval for Long-context LLM\n  Inference"
                },
                "summary": "The Key-Value (KV) cache reading latency increases significantly with context\nlengths, hindering the efficiency of long-context LLM inference. To address\nthis, previous works propose retaining a small fraction of KV cache based on\ntoken importance. For example, KV eviction uses static heuristics to retain\ntokens, while KV retrieval dynamically selects query-relevant tokens for more\nadaptive cache management. However, we observe that important tokens are often\nsparsely distributed across the long context. This sparsity makes existing\npage-level KV retrieval inaccurate, as each page may include irrelevant tokens\nand miss critical ones. In this work, we propose Fier, a\n\\underline{Fi}ne-Grained and \\underline{E}fficient KV cache\n\\underline{R}etrieval method. Fier uses 1-bit quantized keys to estimate the\nimportance of each token, resulting in efficient and precise retrieval.\nExperiments show that Fier matches full KV performance using only 11\\% of the\ncache budget across various long-context tasks, reducing decoding latency by\n1.2$\\times$ to 1.5$\\times$.Code is available at\nhttps://github.com/SimWangArizona/FIER",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache reading latency increases significantly with context\nlengths, hindering the efficiency of long-context LLM inference. To address\nthis, previous works propose retaining a small fraction of KV cache based on\ntoken importance. For example, KV eviction uses static heuristics to retain\ntokens, while KV retrieval dynamically selects query-relevant tokens for more\nadaptive cache management. However, we observe that important tokens are often\nsparsely distributed across the long context. This sparsity makes existing\npage-level KV retrieval inaccurate, as each page may include irrelevant tokens\nand miss critical ones. In this work, we propose Fier, a\n\\underline{Fi}ne-Grained and \\underline{E}fficient KV cache\n\\underline{R}etrieval method. Fier uses 1-bit quantized keys to estimate the\nimportance of each token, resulting in efficient and precise retrieval.\nExperiments show that Fier matches full KV performance using only 11\\% of the\ncache budget across various long-context tasks, reducing decoding latency by\n1.2$\\times$ to 1.5$\\times$.Code is available at\nhttps://github.com/SimWangArizona/FIER"
                },
                "authors": [
                    {
                        "name": "Dongwei Wang"
                    },
                    {
                        "name": "Zijie Liu"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Yuxin Ren"
                    },
                    {
                        "name": "Jianing Deng"
                    },
                    {
                        "name": "Jingtong Hu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Huanrui Yang"
                    }
                ],
                "author_detail": {
                    "name": "Huanrui Yang"
                },
                "author": "Huanrui Yang",
                "arxiv_comment": "EMNLP2025 Camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08256v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08256v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v3",
                "updated": "2025-09-16T10:33:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    33,
                    29,
                    1,
                    259,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_doi": "10.1145/3744916.3764523",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3744916.3764523",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.08523v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ICSE '26 (The 48th IEEE/ACM International Conference on\n  Software Engineering)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12900v1",
                "updated": "2025-09-16T09:54:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    54,
                    58,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T09:54:58Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    54,
                    58,
                    1,
                    259,
                    0
                ],
                "title": "Topology and Fragility of European High-Voltage Networks: A\n  Cross-Country Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topology and Fragility of European High-Voltage Networks: A\n  Cross-Country Comparative Analysis"
                },
                "summary": "Reliable electricity supply depends on the seamless operation of high-voltage\ngrid infrastructure spanning both transmission and sub-transmission levels.\nBeneath this apparent uniformity lies a striking structural diversity, which\nleaves a clear imprint on system vulnerability. In this paper, we present\nharmonized topological models of the high-voltage grids of 15 European\ncountries, integrating all elements at voltage levels above 110 kV. Topological\nanalysis of these networks reveals a simple yet robust pattern: node degree\ndistributions consistently follow an exponential decay, but the rate of decay\nvaries significantly across countries. Through a detailed and systematic\nevaluation of network tolerance to node and edge removals, we show that the\ndecay rate delineates the boundary between systems that are more resilient to\nfailures and those that are prone to large-scale disruptions. Furthermore, we\ndemonstrate that this numerical boundary is highly sensitive to which layers of\nthe infrastructure are included in the models. To our knowledge, this study\nprovides the first quantitative cross-country comparison of 15 European\nhigh-voltage networks, linking topological properties with vulnerability\ncharacteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable electricity supply depends on the seamless operation of high-voltage\ngrid infrastructure spanning both transmission and sub-transmission levels.\nBeneath this apparent uniformity lies a striking structural diversity, which\nleaves a clear imprint on system vulnerability. In this paper, we present\nharmonized topological models of the high-voltage grids of 15 European\ncountries, integrating all elements at voltage levels above 110 kV. Topological\nanalysis of these networks reveals a simple yet robust pattern: node degree\ndistributions consistently follow an exponential decay, but the rate of decay\nvaries significantly across countries. Through a detailed and systematic\nevaluation of network tolerance to node and edge removals, we show that the\ndecay rate delineates the boundary between systems that are more resilient to\nfailures and those that are prone to large-scale disruptions. Furthermore, we\ndemonstrate that this numerical boundary is highly sensitive to which layers of\nthe infrastructure are included in the models. To our knowledge, this study\nprovides the first quantitative cross-country comparison of 15 European\nhigh-voltage networks, linking topological properties with vulnerability\ncharacteristics."
                },
                "authors": [
                    {
                        "name": "Bálint Hartmann"
                    },
                    {
                        "name": "Michelle T. Cirunay"
                    }
                ],
                "author_detail": {
                    "name": "Michelle T. Cirunay"
                },
                "author": "Michelle T. Cirunay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12867v1",
                "updated": "2025-09-16T09:22:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    22,
                    21,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T09:22:21Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    22,
                    21,
                    1,
                    259,
                    0
                ],
                "title": "Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use"
                },
                "summary": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding and reasoning, yet they remain limited when tackling\nreal-world tasks that require up-to-date knowledge, precise operations, or\nspecialized tool use. To address this, we propose Tool-R1, a reinforcement\nlearning framework that enables LLMs to perform general, compositional, and\nmulti-step tool use by generating executable Python code. Tool-R1 supports\nintegration of user-defined tools and standard libraries, with variable sharing\nacross steps to construct coherent workflows. An outcome-based reward function,\ncombining LLM-based answer judgment and code execution success, guides policy\noptimization. To improve training efficiency, we maintain a dynamic sample\nqueue to cache and reuse high-quality trajectories, reducing the overhead of\ncostly online sampling. Experiments on the GAIA benchmark show that Tool-R1\nsubstantially improves both accuracy and robustness, achieving about 10\\% gain\nover strong baselines, with larger improvements on complex multi-step tasks.\nThese results highlight the potential of Tool-R1 for enabling reliable and\nefficient tool-augmented reasoning in real-world applications. Our code will be\navailable at https://github.com/YBYBZhang/Tool-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding and reasoning, yet they remain limited when tackling\nreal-world tasks that require up-to-date knowledge, precise operations, or\nspecialized tool use. To address this, we propose Tool-R1, a reinforcement\nlearning framework that enables LLMs to perform general, compositional, and\nmulti-step tool use by generating executable Python code. Tool-R1 supports\nintegration of user-defined tools and standard libraries, with variable sharing\nacross steps to construct coherent workflows. An outcome-based reward function,\ncombining LLM-based answer judgment and code execution success, guides policy\noptimization. To improve training efficiency, we maintain a dynamic sample\nqueue to cache and reuse high-quality trajectories, reducing the overhead of\ncostly online sampling. Experiments on the GAIA benchmark show that Tool-R1\nsubstantially improves both accuracy and robustness, achieving about 10\\% gain\nover strong baselines, with larger improvements on complex multi-step tasks.\nThese results highlight the potential of Tool-R1 for enabling reliable and\nefficient tool-augmented reasoning in real-world applications. Our code will be\navailable at https://github.com/YBYBZhang/Tool-R1."
                },
                "authors": [
                    {
                        "name": "Yabo Zhang"
                    },
                    {
                        "name": "Yihan Zeng"
                    },
                    {
                        "name": "Qingyun Li"
                    },
                    {
                        "name": "Zhen Hu"
                    },
                    {
                        "name": "Kavin Han"
                    },
                    {
                        "name": "Wangmeng Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Wangmeng Zuo"
                },
                "author": "Wangmeng Zuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12817v1",
                "updated": "2025-09-16T08:36:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    8,
                    36,
                    5,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T08:36:05Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    8,
                    36,
                    5,
                    1,
                    259,
                    0
                ],
                "title": "SAGA: Selective Adaptive Gating for Efficient and Expressive Linear\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAGA: Selective Adaptive Gating for Efficient and Expressive Linear\n  Attention"
                },
                "summary": "While Transformer architecture excel at modeling long-range dependencies\ncontributing to its widespread adoption in vision tasks the quadratic\ncomplexity of softmax-based attention mechanisms imposes a major bottleneck,\nparticularly when processing high-resolution images. Linear attention presents\na promising alternative by reformulating the attention computation from $(QK)V$\nto $Q(KV)$, thereby reducing the complexity from $\\mathcal{O}(N^2)$ to\n$\\mathcal{O}(N)$ while preserving the global receptive field. However, most\nexisting methods compress historical key-value (KV) information uniformly,\nwhich can lead to feature redundancy and the loss of directional alignment with\nthe query (Q). This uniform compression results in low-rank $KV$ feature maps,\ncontributing to a performance gap compared to softmax attention. To mitigate\nthis limitation, we propose \\textbf{S}elective \\textbf{A}daptive\n\\textbf{GA}ting for Efficient and Expressive Linear Attention (SAGA) , which\nintroduces input-adaptive learnable gates to selectively modulate information\naggregation into the $KV$ feature map. These gates enhance semantic diversity\nand alleviate the low-rank constraint inherent in conventional linear\nattention. Additionally, we propose an efficient Hadamard-product decomposition\nmethod for gate computation, which introduces no additional memory overhead.\nExperiments demonstrate that SAGA achieves a 1.76$\\times$ improvement in\nthroughput and a 2.69$\\times$ reduction in peak GPU memory compared to PVT-T at\na resolution of $1280 \\times 1280$. Moreover, it improves top-1 accuracy by up\nto 4.4\\% on the ImageNet dataset, demonstrating both computational efficiency\nand model effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer architecture excel at modeling long-range dependencies\ncontributing to its widespread adoption in vision tasks the quadratic\ncomplexity of softmax-based attention mechanisms imposes a major bottleneck,\nparticularly when processing high-resolution images. Linear attention presents\na promising alternative by reformulating the attention computation from $(QK)V$\nto $Q(KV)$, thereby reducing the complexity from $\\mathcal{O}(N^2)$ to\n$\\mathcal{O}(N)$ while preserving the global receptive field. However, most\nexisting methods compress historical key-value (KV) information uniformly,\nwhich can lead to feature redundancy and the loss of directional alignment with\nthe query (Q). This uniform compression results in low-rank $KV$ feature maps,\ncontributing to a performance gap compared to softmax attention. To mitigate\nthis limitation, we propose \\textbf{S}elective \\textbf{A}daptive\n\\textbf{GA}ting for Efficient and Expressive Linear Attention (SAGA) , which\nintroduces input-adaptive learnable gates to selectively modulate information\naggregation into the $KV$ feature map. These gates enhance semantic diversity\nand alleviate the low-rank constraint inherent in conventional linear\nattention. Additionally, we propose an efficient Hadamard-product decomposition\nmethod for gate computation, which introduces no additional memory overhead.\nExperiments demonstrate that SAGA achieves a 1.76$\\times$ improvement in\nthroughput and a 2.69$\\times$ reduction in peak GPU memory compared to PVT-T at\na resolution of $1280 \\times 1280$. Moreover, it improves top-1 accuracy by up\nto 4.4\\% on the ImageNet dataset, demonstrating both computational efficiency\nand model effectiveness."
                },
                "authors": [
                    {
                        "name": "Yuan Cao"
                    },
                    {
                        "name": "Dong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dong Wang"
                },
                "author": "Dong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11156v2",
                "updated": "2025-09-16T07:49:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    7,
                    49,
                    41,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-14T08:22:37Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    22,
                    37,
                    6,
                    257,
                    0
                ],
                "title": "Adaptive K-PackCache: Cost-Centric Data Caching in Cloud",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive K-PackCache: Cost-Centric Data Caching in Cloud"
                },
                "summary": "Recent advances in data analytics have enabled the accurate prediction of\nuser access patterns, giving rise to the idea of packed caching delivering\nmultiple co accessed data items together as a bundle. This improves caching\nefficiency, as accessing one item often implies the need for others. Prior work\nhas explored only 2 item pairwise packing. In this paper, we extend the concept\nto general K packing, allowing variable size bundles for improved flexibility\nand performance. We formulate the K PackCache problem from a content delivery\nnetwork CDN operator perspective, aiming to minimize total cost comprising two\ncomponents: transfer cost modeled as a base cost plus a linearly increasing\nterm with the number of items packed, and memory rental cost for caching, which\ndepends on how long and how much is stored. Overpacking increases cost due to\nlow utility, underpacking leads to missed sharing opportunities. We propose an\nonline algorithm, Adaptive K PackCache AKPC, which dynamically forms, merges,\nand splits data cliques based on user access patterns and content correlation.\nOur approach supports batch requests, enables approximate clique merging, and\noffers a formal competitive guarantee. Through extensive evaluation on the\nNetflix and Spotify datasets, AKPC reduces total cost by up to 63 and 55\npercentage over online baselines, respectively, and achieves performance within\n15 and 13 percentage of the optimal. This demonstrates its scalability and\neffectiveness for real world caching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in data analytics have enabled the accurate prediction of\nuser access patterns, giving rise to the idea of packed caching delivering\nmultiple co accessed data items together as a bundle. This improves caching\nefficiency, as accessing one item often implies the need for others. Prior work\nhas explored only 2 item pairwise packing. In this paper, we extend the concept\nto general K packing, allowing variable size bundles for improved flexibility\nand performance. We formulate the K PackCache problem from a content delivery\nnetwork CDN operator perspective, aiming to minimize total cost comprising two\ncomponents: transfer cost modeled as a base cost plus a linearly increasing\nterm with the number of items packed, and memory rental cost for caching, which\ndepends on how long and how much is stored. Overpacking increases cost due to\nlow utility, underpacking leads to missed sharing opportunities. We propose an\nonline algorithm, Adaptive K PackCache AKPC, which dynamically forms, merges,\nand splits data cliques based on user access patterns and content correlation.\nOur approach supports batch requests, enables approximate clique merging, and\noffers a formal competitive guarantee. Through extensive evaluation on the\nNetflix and Spotify datasets, AKPC reduces total cost by up to 63 and 55\npercentage over online baselines, respectively, and achieves performance within\n15 and 13 percentage of the optimal. This demonstrates its scalability and\neffectiveness for real world caching systems."
                },
                "authors": [
                    {
                        "name": "Suvarthi Sarkar"
                    },
                    {
                        "name": "Aadarshraj Sah"
                    },
                    {
                        "name": "Poddutoori Sweeya Reddy"
                    },
                    {
                        "name": "Aryabartta Sahu"
                    }
                ],
                "author_detail": {
                    "name": "Aryabartta Sahu"
                },
                "author": "Aryabartta Sahu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13231v2",
                "updated": "2025-09-15T14:40:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    40,
                    16,
                    0,
                    258,
                    0
                ],
                "published": "2025-08-17T19:07:08Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    19,
                    7,
                    8,
                    6,
                    229,
                    0
                ],
                "title": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System"
                },
                "summary": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference."
                },
                "authors": [
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Kaoutar El Maghraoui"
                    },
                    {
                        "name": "Naigang Wang"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "IEEE Computer Architecture Letter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11815v1",
                "updated": "2025-09-15T11:53:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    53,
                    56,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T11:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    53,
                    56,
                    0,
                    258,
                    0
                ],
                "title": "SpecVLM: Fast Speculative Decoding in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecVLM: Fast Speculative Decoding in Vision-Language Models"
                },
                "summary": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM."
                },
                "authors": [
                    {
                        "name": "Haiduo Huang"
                    },
                    {
                        "name": "Fuwei Yang"
                    },
                    {
                        "name": "Zhenhua Liu"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Pengju Ren"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11628v1",
                "updated": "2025-09-15T06:46:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    6,
                    46,
                    22,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T06:46:22Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    6,
                    46,
                    22,
                    0,
                    258,
                    0
                ],
                "title": "SpeCa: Accelerating Diffusion Transformers with Speculative Feature\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeCa: Accelerating Diffusion Transformers with Speculative Feature\n  Caching"
                },
                "summary": "Diffusion models have revolutionized high-fidelity image and video synthesis,\nyet their computational demands remain prohibitive for real-time applications.\nThese models face two fundamental challenges: strict temporal dependencies\npreventing parallelization, and computationally intensive forward passes\nrequired at each denoising step. Drawing inspiration from speculative decoding\nin large language models, we present SpeCa, a novel 'Forecast-then-verify'\nacceleration framework that effectively addresses both limitations. SpeCa's\ncore innovation lies in introducing Speculative Sampling to diffusion models,\npredicting intermediate features for subsequent timesteps based on fully\ncomputed reference timesteps. Our approach implements a parameter-free\nverification mechanism that efficiently evaluates prediction reliability,\nenabling real-time decisions to accept or reject each prediction while\nincurring negligible computational overhead. Furthermore, SpeCa introduces\nsample-adaptive computation allocation that dynamically modulates resources\nbased on generation complexity, allocating reduced computation for simpler\nsamples while preserving intensive processing for complex instances.\nExperiments demonstrate 6.34x acceleration on FLUX with minimal quality\ndegradation (5.5% drop), 7.3x speedup on DiT while preserving generation\nfidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The\nverification mechanism incurs minimal overhead (1.67%-3.5% of full inference\ncosts), establishing a new paradigm for efficient diffusion model inference\nwhile maintaining generation quality even at aggressive acceleration ratios.\nOur codes have been released in Github:\n\\textbf{https://github.com/Shenyi-Z/Cache4Diffusion}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have revolutionized high-fidelity image and video synthesis,\nyet their computational demands remain prohibitive for real-time applications.\nThese models face two fundamental challenges: strict temporal dependencies\npreventing parallelization, and computationally intensive forward passes\nrequired at each denoising step. Drawing inspiration from speculative decoding\nin large language models, we present SpeCa, a novel 'Forecast-then-verify'\nacceleration framework that effectively addresses both limitations. SpeCa's\ncore innovation lies in introducing Speculative Sampling to diffusion models,\npredicting intermediate features for subsequent timesteps based on fully\ncomputed reference timesteps. Our approach implements a parameter-free\nverification mechanism that efficiently evaluates prediction reliability,\nenabling real-time decisions to accept or reject each prediction while\nincurring negligible computational overhead. Furthermore, SpeCa introduces\nsample-adaptive computation allocation that dynamically modulates resources\nbased on generation complexity, allocating reduced computation for simpler\nsamples while preserving intensive processing for complex instances.\nExperiments demonstrate 6.34x acceleration on FLUX with minimal quality\ndegradation (5.5% drop), 7.3x speedup on DiT while preserving generation\nfidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The\nverification mechanism incurs minimal overhead (1.67%-3.5% of full inference\ncosts), establishing a new paradigm for efficient diffusion model inference\nwhile maintaining generation quality even at aggressive acceleration ratios.\nOur codes have been released in Github:\n\\textbf{https://github.com/Shenyi-Z/Cache4Diffusion}"
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Fei Ren"
                    },
                    {
                        "name": "Shaobo Wang"
                    },
                    {
                        "name": "Kaixin Li"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_doi": "10.1145/3746027.3755331",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746027.3755331",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.11628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 9 figures, ACM Multimedia 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14089v2",
                "updated": "2025-09-15T01:15:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    1,
                    15,
                    50,
                    0,
                    258,
                    0
                ],
                "published": "2025-04-18T22:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average."
                },
                "authors": [
                    {
                        "name": "Kang He"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06261v2",
                "updated": "2025-09-15T00:51:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    0,
                    51,
                    47,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-08T00:57:50Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    0,
                    57,
                    50,
                    0,
                    251,
                    0
                ],
                "title": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving"
                },
                "summary": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems."
                },
                "authors": [
                    {
                        "name": "Kyungmin Bin"
                    },
                    {
                        "name": "Seungbeom Choi"
                    },
                    {
                        "name": "Jimyoung Son"
                    },
                    {
                        "name": "Jieun Choi"
                    },
                    {
                        "name": "Daseul Bae"
                    },
                    {
                        "name": "Daehyeon Baek"
                    },
                    {
                        "name": "Kihyo Moon"
                    },
                    {
                        "name": "Minsung Jang"
                    },
                    {
                        "name": "Hyojung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hyojung Lee"
                },
                "author": "Hyojung Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11239v1",
                "updated": "2025-09-14T12:29:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    12,
                    29,
                    49,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T12:29:49Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    12,
                    29,
                    49,
                    6,
                    257,
                    0
                ],
                "title": "Multi-Layer Perceptron-Based Relay Node Selection for Next-Generation\n  Intelligent Delay-Tolerant Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Layer Perceptron-Based Relay Node Selection for Next-Generation\n  Intelligent Delay-Tolerant Networks"
                },
                "summary": "Delay Tolerant Networks (DTNs) are critical for emergency communication in\nhighly dynamic and challenging scenarios characterized by intermittent\nconnectivity, frequent disruptions, and unpredictable node mobility. While some\nprotocols are widely adopted for simplicity and low overhead, their static\nreplication strategy lacks the ability to adaptively distinguish high-quality\nrelay nodes, often leading to inefficient and suboptimal message dissemination.\nTo address this challenge, we propose a novel intelligent routing enhancement\nthat integrates machine learning-based node evaluation into the Spray and Wait\nframework. Several dynamic, core features are extracted from simulation logs\nand are used to train multiple classifiers - Multi-Layer Perceptron (MLP),\nSupport Vector Machine (SVM), and Random Forest (RF) - to predict whether a\nnode is suitable as a relay under dynamic conditions. The trained models are\ndeployed via a lightweight Flask-based RESTful API, enabling real-time,\nadaptive predictions. We implement the enhanced router MLPBasedSprayRouter,\nwhich selectively forwards messages based on the predicted relay quality. A\ncaching mechanism is incorporated to reduce computational overhead and ensure\nstable, low-latency inference. Extensive experiments under realistic emergency\nmobility scenarios demonstrate that the proposed framework significantly\nimproves delivery ratio while reducing average latency compared to the baseline\nprotocols. Among all evaluated classifiers, MLP achieved the most robust\nperformance, consistently outperforming both SVM and RF in terms of accuracy,\nadaptability, and inference speed. These results confirm the novelty and\npracticality of integrating machine learning into DTN routing, paving the way\nfor resilient and intelligent communication systems in smart cities, disaster\nrecovery, and other dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delay Tolerant Networks (DTNs) are critical for emergency communication in\nhighly dynamic and challenging scenarios characterized by intermittent\nconnectivity, frequent disruptions, and unpredictable node mobility. While some\nprotocols are widely adopted for simplicity and low overhead, their static\nreplication strategy lacks the ability to adaptively distinguish high-quality\nrelay nodes, often leading to inefficient and suboptimal message dissemination.\nTo address this challenge, we propose a novel intelligent routing enhancement\nthat integrates machine learning-based node evaluation into the Spray and Wait\nframework. Several dynamic, core features are extracted from simulation logs\nand are used to train multiple classifiers - Multi-Layer Perceptron (MLP),\nSupport Vector Machine (SVM), and Random Forest (RF) - to predict whether a\nnode is suitable as a relay under dynamic conditions. The trained models are\ndeployed via a lightweight Flask-based RESTful API, enabling real-time,\nadaptive predictions. We implement the enhanced router MLPBasedSprayRouter,\nwhich selectively forwards messages based on the predicted relay quality. A\ncaching mechanism is incorporated to reduce computational overhead and ensure\nstable, low-latency inference. Extensive experiments under realistic emergency\nmobility scenarios demonstrate that the proposed framework significantly\nimproves delivery ratio while reducing average latency compared to the baseline\nprotocols. Among all evaluated classifiers, MLP achieved the most robust\nperformance, consistently outperforming both SVM and RF in terms of accuracy,\nadaptability, and inference speed. These results confirm the novelty and\npracticality of integrating machine learning into DTN routing, paving the way\nfor resilient and intelligent communication systems in smart cities, disaster\nrecovery, and other dynamic environments."
                },
                "authors": [
                    {
                        "name": "Zhekun Huang"
                    },
                    {
                        "name": "Milena Radenkovic"
                    }
                ],
                "author_detail": {
                    "name": "Milena Radenkovic"
                },
                "author": "Milena Radenkovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11181v1",
                "updated": "2025-09-14T09:26:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    9,
                    26,
                    44,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T09:26:44Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    9,
                    26,
                    44,
                    6,
                    257,
                    0
                ],
                "title": "Dislocation response to electric fields in strontium titanate: A\n  mesoscale indentation study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dislocation response to electric fields in strontium titanate: A\n  mesoscale indentation study"
                },
                "summary": "Dislocations in perovskite oxides have drawn increasing research interest due\nto their potential of tuning functional properties of electroceramics. Open\nquestions remain regarding the behavior of dislocations concerning their\nstability under strong externally applied electric fields. In this study, we\ninvestigate the dielectric breakdown strength of nominally undoped SrTiO3\ncrystals after the introduction of high-density dislocations. The\ndislocation-rich samples are prepared using the Brinell scratching method, and\nthey consistently exhibit lower dielectric breakdown strength as well as a\nlarger scatter in the breakdown probability. We also study the impact of\nelectric field on the introduction and movement of dislocations in SrTiO3\ncrystals using Brinell indentation coupled with an electric field of 2 kV/mm.\nNo changes on the dislocation plastic zone size, depth, and dislocation\ndistribution are observed under this electric field. Based on the charge state\nof the dislocations in SrTiO3 as well as the electrical and thermal\nconductivity modified by dislocations, we discuss the forces induced by the\nelectric field to act on the dislocations to underline the possible mechanisms\nfor such dislocation behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dislocations in perovskite oxides have drawn increasing research interest due\nto their potential of tuning functional properties of electroceramics. Open\nquestions remain regarding the behavior of dislocations concerning their\nstability under strong externally applied electric fields. In this study, we\ninvestigate the dielectric breakdown strength of nominally undoped SrTiO3\ncrystals after the introduction of high-density dislocations. The\ndislocation-rich samples are prepared using the Brinell scratching method, and\nthey consistently exhibit lower dielectric breakdown strength as well as a\nlarger scatter in the breakdown probability. We also study the impact of\nelectric field on the introduction and movement of dislocations in SrTiO3\ncrystals using Brinell indentation coupled with an electric field of 2 kV/mm.\nNo changes on the dislocation plastic zone size, depth, and dislocation\ndistribution are observed under this electric field. Based on the charge state\nof the dislocations in SrTiO3 as well as the electrical and thermal\nconductivity modified by dislocations, we discuss the forces induced by the\nelectric field to act on the dislocations to underline the possible mechanisms\nfor such dislocation behavior."
                },
                "authors": [
                    {
                        "name": "Alexander Frisch"
                    },
                    {
                        "name": "Daniel Isaia"
                    },
                    {
                        "name": "Oliver Preuß"
                    },
                    {
                        "name": "Xufei Fang"
                    }
                ],
                "author_detail": {
                    "name": "Xufei Fang"
                },
                "author": "Xufei Fang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11155v1",
                "updated": "2025-09-14T08:20:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    20,
                    48,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T08:20:48Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    20,
                    48,
                    6,
                    257,
                    0
                ],
                "title": "AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient\n  Inference in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient\n  Inference in LLMs"
                },
                "summary": "The quadratic complexity of the attention mechanism remains a fundamental\nbarrier to scaling Large Language Models (LLMs) to longer contexts, creating a\ncritical bottleneck in both computation and memory. To address this, we\nintroduce AQUA (Attention via QUery mAgnitudes) a novel and versatile\napproximation strategy that significantly reduces the cost of attention with a\ngraceful performance trade-off. Our method operates in two phases: an efficient\noffline step where we compute a universal, language agnostic projection matrix\nvia SVD on a calibration dataset, and an online inference step where we project\nquery and key vectors and dynamically select a sparse subset of dimensions\nbased on the query's magnitude. We provide a formal theoretical analysis of\nAQUA, establishing the break-even point at which it becomes more\ncomputationally efficient than standard attention. Our empirical evaluations on\nstate-of-the-art models like Llama-3.1-8B demonstrate that a 25% reduction in\nthe attention dot-product computation can be achieved with a statistically\ninsignificant impact on performance across a wide range of benchmarks. We\nfurther showcase the versatility of AQUA by demonstrating its ability to\nsynergistically accelerate existing token eviction methods like H2O and to\ndirectly reduce KV-cache memory size. By offering a controllable knob to\nbalance efficiency and accuracy, AQUA provides a practical and powerful tool\nfor making large-scale LLM inference more accessible and sustainable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic complexity of the attention mechanism remains a fundamental\nbarrier to scaling Large Language Models (LLMs) to longer contexts, creating a\ncritical bottleneck in both computation and memory. To address this, we\nintroduce AQUA (Attention via QUery mAgnitudes) a novel and versatile\napproximation strategy that significantly reduces the cost of attention with a\ngraceful performance trade-off. Our method operates in two phases: an efficient\noffline step where we compute a universal, language agnostic projection matrix\nvia SVD on a calibration dataset, and an online inference step where we project\nquery and key vectors and dynamically select a sparse subset of dimensions\nbased on the query's magnitude. We provide a formal theoretical analysis of\nAQUA, establishing the break-even point at which it becomes more\ncomputationally efficient than standard attention. Our empirical evaluations on\nstate-of-the-art models like Llama-3.1-8B demonstrate that a 25% reduction in\nthe attention dot-product computation can be achieved with a statistically\ninsignificant impact on performance across a wide range of benchmarks. We\nfurther showcase the versatility of AQUA by demonstrating its ability to\nsynergistically accelerate existing token eviction methods like H2O and to\ndirectly reduce KV-cache memory size. By offering a controllable knob to\nbalance efficiency and accuracy, AQUA provides a practical and powerful tool\nfor making large-scale LLM inference more accessible and sustainable."
                },
                "authors": [
                    {
                        "name": "Santhosh G S"
                    },
                    {
                        "name": "Saurav Prakash"
                    },
                    {
                        "name": "Balaraman Ravindran"
                    }
                ],
                "author_detail": {
                    "name": "Balaraman Ravindran"
                },
                "author": "Balaraman Ravindran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10798v1",
                "updated": "2025-09-13T03:34:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    13,
                    3,
                    34,
                    12,
                    5,
                    256,
                    0
                ],
                "published": "2025-09-13T03:34:12Z",
                "published_parsed": [
                    2025,
                    9,
                    13,
                    3,
                    34,
                    12,
                    5,
                    256,
                    0
                ],
                "title": "Judge Q: Trainable Queries for Optimized Information Retention in KV\n  Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judge Q: Trainable Queries for Optimized Information Retention in KV\n  Cache Eviction"
                },
                "summary": "Large language models (LLMs) utilize key-value (KV) cache to store historical\ninformation during sequence processing. The size of KV cache grows linearly as\nthe length of the sequence extends, which seriously affects memory usage and\ndecoding efficiency. Current methods for KV cache eviction typically utilize\nthe last window from the pre-filling phase as queries to compute the KV\nimportance scores for eviction. Although this scheme is simple to implement, it\ntends to overly focus on local information, potentially leading to the neglect\nor omission of crucial global information. To mitigate this issue, we propose\nJudge Q, a novel training method which incorporates a soft token list. This\nmethod only tunes the model's embedding layer at a low training cost. By\nconcatenating the soft token list at the end of the input sequence, we train\nthese tokens' attention map to the original input sequence to align with that\nof the actual decoded tokens. In this way, the queries corresponding to the\nsoft tokens can effectively capture global information and better evaluate the\nimportance of the keys and values within the KV cache, thus maintaining\ndecoding quality when KV cache is evicted. Under the same eviction budget, our\nmethod exhibits less performance degradation compared to existing eviction\napproaches. We validate our approach through experiments conducted on models\nsuch as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks\nincluding LongBench, RULER, and Needle-in-a-Haystack. Results indicate an\nimprovement of approximately 1 point on the LongBench and over 3 points on\nRULER. This proposed methodology can be seamlessly integrated into existing\nopen-source models with minimal training overhead, thereby enhancing\nperformance in KV cache eviction scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) utilize key-value (KV) cache to store historical\ninformation during sequence processing. The size of KV cache grows linearly as\nthe length of the sequence extends, which seriously affects memory usage and\ndecoding efficiency. Current methods for KV cache eviction typically utilize\nthe last window from the pre-filling phase as queries to compute the KV\nimportance scores for eviction. Although this scheme is simple to implement, it\ntends to overly focus on local information, potentially leading to the neglect\nor omission of crucial global information. To mitigate this issue, we propose\nJudge Q, a novel training method which incorporates a soft token list. This\nmethod only tunes the model's embedding layer at a low training cost. By\nconcatenating the soft token list at the end of the input sequence, we train\nthese tokens' attention map to the original input sequence to align with that\nof the actual decoded tokens. In this way, the queries corresponding to the\nsoft tokens can effectively capture global information and better evaluate the\nimportance of the keys and values within the KV cache, thus maintaining\ndecoding quality when KV cache is evicted. Under the same eviction budget, our\nmethod exhibits less performance degradation compared to existing eviction\napproaches. We validate our approach through experiments conducted on models\nsuch as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks\nincluding LongBench, RULER, and Needle-in-a-Haystack. Results indicate an\nimprovement of approximately 1 point on the LongBench and over 3 points on\nRULER. This proposed methodology can be seamlessly integrated into existing\nopen-source models with minimal training overhead, thereby enhancing\nperformance in KV cache eviction scenarios."
                },
                "authors": [
                    {
                        "name": "Yijun Liu"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Shiyu Ji"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10372v1",
                "updated": "2025-09-12T16:05:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    27,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T16:05:27Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    27,
                    4,
                    255,
                    0
                ],
                "title": "MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging\n  Bit-Slice-enabled Sparsity and Repetitiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging\n  Bit-Slice-enabled Sparsity and Repetitiveness"
                },
                "summary": "Large language models (LLMs) face significant inference latency due to\ninefficiencies in GEMM operations, weight access, and KV cache access,\nespecially in real-time scenarios. This highlights the need for a versatile\ncompute-memory efficient accelerator. Unfortunately, existing Transformer\naccelerators struggle to address both aspects simultaneously, as they focus on\nvalue-level processing, missing fine-grained opportunities to optimize\ncomputation and memory collaboratively. This paper introduces MCBP, a\nbit-grained compute-memory efficient algorithm-hardware co-design that\nleverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM\ninference. MCBP features three key innovations: 1) BS-repetitiveness-enabled\ncomputation reduction (BRCR), which eliminates redundant GEMM computations via\nleveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state\ncoding (BSTC), which reduces weight access via exploiting significant sparsity\nin high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),\nwhich reduces KV cache access by leveraging early-termination-based bit-grained\nprediction. These techniques, supported by custom accelerator designs,\neffectively alleviate the burden in GEMM, weight access, and KV cache access.\nExtensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up\nand 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA\nTransformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than\nSpatten, FACT and SOFA, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face significant inference latency due to\ninefficiencies in GEMM operations, weight access, and KV cache access,\nespecially in real-time scenarios. This highlights the need for a versatile\ncompute-memory efficient accelerator. Unfortunately, existing Transformer\naccelerators struggle to address both aspects simultaneously, as they focus on\nvalue-level processing, missing fine-grained opportunities to optimize\ncomputation and memory collaboratively. This paper introduces MCBP, a\nbit-grained compute-memory efficient algorithm-hardware co-design that\nleverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM\ninference. MCBP features three key innovations: 1) BS-repetitiveness-enabled\ncomputation reduction (BRCR), which eliminates redundant GEMM computations via\nleveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state\ncoding (BSTC), which reduces weight access via exploiting significant sparsity\nin high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),\nwhich reduces KV cache access by leveraging early-termination-based bit-grained\nprediction. These techniques, supported by custom accelerator designs,\neffectively alleviate the burden in GEMM, weight access, and KV cache access.\nExtensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up\nand 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA\nTransformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than\nSpatten, FACT and SOFA, respectively."
                },
                "authors": [
                    {
                        "name": "Huizheng Wang"
                    },
                    {
                        "name": "Zichuan Wang"
                    },
                    {
                        "name": "Zhiheng Yue"
                    },
                    {
                        "name": "Yousheng Long"
                    },
                    {
                        "name": "Taiquan Wei"
                    },
                    {
                        "name": "Jianxun Yang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Shaojun Wei"
                    },
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Shouyi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Shouyi Yin"
                },
                "author": "Shouyi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10312v1",
                "updated": "2025-09-12T14:53:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    53,
                    45,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    53,
                    45,
                    4,
                    255,
                    0
                ],
                "title": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion\n  Transformers with Cluster-Driven Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion\n  Transformers with Cluster-Driven Feature Caching"
                },
                "summary": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion."
                },
                "authors": [
                    {
                        "name": "Zhixin Zheng"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Shaobo Wang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "11 pages, 11 figures; Accepted by ACM MM2025; Mainly focus on feature\n  caching for diffusion transformers acceleration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10251v1",
                "updated": "2025-09-12T13:49:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    49,
                    27,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T13:49:27Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    49,
                    27,
                    4,
                    255,
                    0
                ],
                "title": "XBOF: A Cost-Efficient CXL JBOF with Inter-SSD Compute Resource Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XBOF: A Cost-Efficient CXL JBOF with Inter-SSD Compute Resource Sharing"
                },
                "summary": "Enterprise SSDs integrate numerous computing resources (e.g., ARM processor\nand onboard DRAM) to satisfy the ever-increasing performance requirements of\nI/O bursts. While these resources substantially elevate the monetary costs of\nSSDs, the sporadic nature of I/O bursts causes severe SSD resource\nunderutilization in just a bunch of flash (JBOF) level. Tackling this\nchallenge, we propose XBOF, a cost-efficient JBOF design, which only reserves\nmoderate computing resources in SSDs at low monetary cost, while achieving\ndemanded I/O performance through efficient inter-SSD resource sharing.\nSpecifically, XBOF first disaggregates SSD architecture into multiple disjoint\nparts based on their functionality, enabling fine-grained SSD internal resource\nmanagement. XBOF then employs a decentralized scheme to manage these\ndisaggregated resources and harvests the computing resources of idle SSDs to\nassist busy SSDs in handling I/O bursts. This idea is facilitated by the\ncache-coherent capability of Compute eXpress Link (CXL), with which the busy\nSSDs can directly utilize the harvested computing resources to accelerate\nmetadata processing. The evaluation results show that XBOF improves SSD\nresource utilization by 50.4% and saves 19.0% monetary costs with a negligible\nperformance loss, compared to existing JBOF designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise SSDs integrate numerous computing resources (e.g., ARM processor\nand onboard DRAM) to satisfy the ever-increasing performance requirements of\nI/O bursts. While these resources substantially elevate the monetary costs of\nSSDs, the sporadic nature of I/O bursts causes severe SSD resource\nunderutilization in just a bunch of flash (JBOF) level. Tackling this\nchallenge, we propose XBOF, a cost-efficient JBOF design, which only reserves\nmoderate computing resources in SSDs at low monetary cost, while achieving\ndemanded I/O performance through efficient inter-SSD resource sharing.\nSpecifically, XBOF first disaggregates SSD architecture into multiple disjoint\nparts based on their functionality, enabling fine-grained SSD internal resource\nmanagement. XBOF then employs a decentralized scheme to manage these\ndisaggregated resources and harvests the computing resources of idle SSDs to\nassist busy SSDs in handling I/O bursts. This idea is facilitated by the\ncache-coherent capability of Compute eXpress Link (CXL), with which the busy\nSSDs can directly utilize the harvested computing resources to accelerate\nmetadata processing. The evaluation results show that XBOF improves SSD\nresource utilization by 50.4% and saves 19.0% monetary costs with a negligible\nperformance loss, compared to existing JBOF designs."
                },
                "authors": [
                    {
                        "name": "Shushu Yi"
                    },
                    {
                        "name": "Yuda An"
                    },
                    {
                        "name": "Li Peng"
                    },
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Jieming Yin"
                    },
                    {
                        "name": "Guangyan Zhang"
                    },
                    {
                        "name": "Wenfei Wu"
                    },
                    {
                        "name": "Diyu Zhou"
                    },
                    {
                        "name": "Zhenlin Wang"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10016v1",
                "updated": "2025-09-12T07:20:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    20,
                    53,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T07:20:53Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    20,
                    53,
                    4,
                    255,
                    0
                ],
                "title": "SvalMIZ-25 Svalbard Marginal Ice Zone Campaign 2025 -- Cruise Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SvalMIZ-25 Svalbard Marginal Ice Zone Campaign 2025 -- Cruise Report"
                },
                "summary": "The coupling of weather, sea-ice, ocean, and wave forecasting systems has\nbeen a long-standing research focus to improve Arctic forecasting systems and\ntheir realism and is also a priority of international initiatives such as the\nWMO research project PCAPS. The goal of the Svalbard Marginal Ice Zone 2025\nCampaign (SvalMIZ-25) was to observe and better understand the complex\ninterplay between atmosphere, waves, and sea-ice in the winter Marginal Ice\nZone (MIZ) in order to advance predictive skill of coupled Arctic forecasting\nsystems. The main objective has been to set up a network of observations with a\nspatial distribution that allows for a representative comparison between in\nsitu observations and gridded model data. The observed variables include air\nand surface temperature, sea-ice drift, and wave energy spectra. With the\nsupport of the Norwegian Coast Guard, we participated in the research cruise\nwith KV Svalbard from 22.April - 11.May 2025. In total 21 buoys were deployed\nin the Marginal Ice Zone north of the Svalbard Archipelago.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coupling of weather, sea-ice, ocean, and wave forecasting systems has\nbeen a long-standing research focus to improve Arctic forecasting systems and\ntheir realism and is also a priority of international initiatives such as the\nWMO research project PCAPS. The goal of the Svalbard Marginal Ice Zone 2025\nCampaign (SvalMIZ-25) was to observe and better understand the complex\ninterplay between atmosphere, waves, and sea-ice in the winter Marginal Ice\nZone (MIZ) in order to advance predictive skill of coupled Arctic forecasting\nsystems. The main objective has been to set up a network of observations with a\nspatial distribution that allows for a representative comparison between in\nsitu observations and gridded model data. The observed variables include air\nand surface temperature, sea-ice drift, and wave energy spectra. With the\nsupport of the Norwegian Coast Guard, we participated in the research cruise\nwith KV Svalbard from 22.April - 11.May 2025. In total 21 buoys were deployed\nin the Marginal Ice Zone north of the Svalbard Archipelago."
                },
                "authors": [
                    {
                        "name": "M. Müller"
                    },
                    {
                        "name": "J. Rabault"
                    },
                    {
                        "name": "C. Palerme"
                    },
                    {
                        "name": "J. Tjernström"
                    }
                ],
                "author_detail": {
                    "name": "J. Tjernström"
                },
                "author": "J. Tjernström",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09754v1",
                "updated": "2025-09-11T16:48:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    48,
                    24,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T16:48:24Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    48,
                    24,
                    3,
                    254,
                    0
                ],
                "title": "LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation"
                },
                "summary": "KV Cache is commonly used to accelerate LLM inference with long contexts, yet\nits high memory demand drives the need for cache compression. Existing\ncompression methods, however, are largely heuristic and lack dynamic budget\nallocation. To address this limitation, we introduce a unified framework for\ncache compression by minimizing information loss in Transformer residual\nstreams. Building on it, we analyze the layer attention output loss and derive\na new metric to compare cache entries across heads, enabling layer-wise\ncompression with dynamic head budgets. Additionally, by contrasting cross-layer\ninformation, we also achieve dynamic layer budgets. LAVa is the first unified\nstrategy for cache eviction and dynamic budget allocation that, unlike prior\nmethods, does not rely on training or the combination of multiple strategies.\nExperiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and\nInfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a\nnew insight: dynamic layer budgets are crucial for generation tasks (e.g., code\ncompletion), while dynamic head budgets play a key role in extraction tasks\n(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently\nmaintains top performance across task types. Our code is available at\nhttps://github.com/MGDDestiny/Lava.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache is commonly used to accelerate LLM inference with long contexts, yet\nits high memory demand drives the need for cache compression. Existing\ncompression methods, however, are largely heuristic and lack dynamic budget\nallocation. To address this limitation, we introduce a unified framework for\ncache compression by minimizing information loss in Transformer residual\nstreams. Building on it, we analyze the layer attention output loss and derive\na new metric to compare cache entries across heads, enabling layer-wise\ncompression with dynamic head budgets. Additionally, by contrasting cross-layer\ninformation, we also achieve dynamic layer budgets. LAVa is the first unified\nstrategy for cache eviction and dynamic budget allocation that, unlike prior\nmethods, does not rely on training or the combination of multiple strategies.\nExperiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and\nInfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a\nnew insight: dynamic layer budgets are crucial for generation tasks (e.g., code\ncompletion), while dynamic head budgets play a key role in extraction tasks\n(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently\nmaintains top performance across task types. Our code is available at\nhttps://github.com/MGDDestiny/Lava."
                },
                "authors": [
                    {
                        "name": "Yiqun Shen"
                    },
                    {
                        "name": "Song Yuan"
                    },
                    {
                        "name": "Zhengze Zhang"
                    },
                    {
                        "name": "Xiaoliang Wang"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Nguyen Cam-Tu"
                    }
                ],
                "author_detail": {
                    "name": "Nguyen Cam-Tu"
                },
                "author": "Nguyen Cam-Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09525v1",
                "updated": "2025-09-11T15:06:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    6,
                    3,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T15:06:03Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    6,
                    3,
                    3,
                    254,
                    0
                ],
                "title": "TrEnv: Transparently Share Serverless Execution Environments Across\n  Different Functions and Nodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrEnv: Transparently Share Serverless Execution Environments Across\n  Different Functions and Nodes"
                },
                "summary": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B."
                },
                "authors": [
                    {
                        "name": "Jialiang Huang"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Sixing Lin"
                    },
                    {
                        "name": "Kang Chen"
                    },
                    {
                        "name": "Jinlei Jiang"
                    },
                    {
                        "name": "Xia Liao"
                    },
                    {
                        "name": "Yingdi Shan"
                    },
                    {
                        "name": "Yongwei Wu"
                    },
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Mengting Lu"
                    },
                    {
                        "name": "Tao Ma"
                    },
                    {
                        "name": "Haifeng Gong"
                    },
                    {
                        "name": "Mingxing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mingxing Zhang"
                },
                "author": "Mingxing Zhang",
                "arxiv_comment": "38 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09494v1",
                "updated": "2025-09-11T14:34:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    34,
                    1,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T14:34:01Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    34,
                    1,
                    3,
                    254,
                    0
                ],
                "title": "In-Loop Filtering Using Learned Look-Up Tables for Video Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering Using Learned Look-Up Tables for Video Coding"
                },
                "summary": "In-loop filtering (ILF) is a key technology in video coding standards to\nreduce artifacts and enhance visual quality. Recently, neural network-based ILF\nschemes have achieved remarkable coding gains, emerging as a powerful candidate\nfor next-generation video coding standards. However, the use of deep neural\nnetworks (DNN) brings significant computational and time complexity or high\ndemands for dedicated hardware, making it challenging for general use. To\naddress this limitation, we study a practical ILF solution by adopting look-up\ntables (LUTs). After training a DNN with a restricted reference range for ILF,\nall possible inputs are traversed, and the output values of the DNN are cached\ninto LUTs. During the coding process, the filtering process is performed by\nsimply retrieving the filtered pixel through locating the input pixels and\ninterpolating between the cached values, instead of relying on heavy inference\ncomputations. In this paper, we propose a universal LUT-based ILF framework,\ntermed LUT-ILF++. First, we introduce the cooperation of multiple kinds of\nfiltering LUTs and propose a series of customized indexing mechanisms to enable\nbetter filtering reference perception with limited storage consumption. Second,\nwe propose the cross-component indexing mechanism to enable the filtering of\ndifferent color components jointly. Third, in order to make our solution\npractical for coding uses, we propose the LUT compaction scheme to enable the\nLUT pruning, achieving a lower storage cost of the entire solution. The\nproposed framework is implemented in the VVC reference software. Experimental\nresults show that the proposed framework achieves on average 0.82%/2.97%/1.63%\nand 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI\nand RA configurations, respectively. Compared to DNN-based solutions, our\nproposed solution has much lower time complexity and storage cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology in video coding standards to\nreduce artifacts and enhance visual quality. Recently, neural network-based ILF\nschemes have achieved remarkable coding gains, emerging as a powerful candidate\nfor next-generation video coding standards. However, the use of deep neural\nnetworks (DNN) brings significant computational and time complexity or high\ndemands for dedicated hardware, making it challenging for general use. To\naddress this limitation, we study a practical ILF solution by adopting look-up\ntables (LUTs). After training a DNN with a restricted reference range for ILF,\nall possible inputs are traversed, and the output values of the DNN are cached\ninto LUTs. During the coding process, the filtering process is performed by\nsimply retrieving the filtered pixel through locating the input pixels and\ninterpolating between the cached values, instead of relying on heavy inference\ncomputations. In this paper, we propose a universal LUT-based ILF framework,\ntermed LUT-ILF++. First, we introduce the cooperation of multiple kinds of\nfiltering LUTs and propose a series of customized indexing mechanisms to enable\nbetter filtering reference perception with limited storage consumption. Second,\nwe propose the cross-component indexing mechanism to enable the filtering of\ndifferent color components jointly. Third, in order to make our solution\npractical for coding uses, we propose the LUT compaction scheme to enable the\nLUT pruning, achieving a lower storage cost of the entire solution. The\nproposed framework is implemented in the VVC reference software. Experimental\nresults show that the proposed framework achieves on average 0.82%/2.97%/1.63%\nand 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI\nand RA configurations, respectively. Compared to DNN-based solutions, our\nproposed solution has much lower time complexity and storage cost."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Jialin Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05211v2",
                "updated": "2025-09-11T12:06:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    6,
                    49,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-07T09:47:21Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "title": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization"
                },
                "summary": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference."
                },
                "authors": [
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Runsen Xu"
                    },
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19880v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19880v2",
                "updated": "2025-09-11T10:20:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    10,
                    20,
                    20,
                    3,
                    254,
                    0
                ],
                "published": "2025-05-26T12:06:12Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    6,
                    12,
                    0,
                    146,
                    0
                ],
                "title": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing"
                },
                "summary": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead."
                },
                "authors": [
                    {
                        "name": "Saman Akbari"
                    },
                    {
                        "name": "Manfred Hauswirth"
                    }
                ],
                "author_detail": {
                    "name": "Manfred Hauswirth"
                },
                "author": "Manfred Hauswirth",
                "arxiv_doi": "10.1109/CLOUD67622.2025.00051",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/CLOUD67622.2025.00051",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.19880v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19880v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the 2025 IEEE 18th International Conference on Cloud\n  Computing (CLOUD)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19740v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19740v3",
                "updated": "2025-09-11T06:45:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    45,
                    58,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-27T10:11:27Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval"
                },
                "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19740v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19740v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01085v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01085v3",
                "updated": "2025-09-11T06:16:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    16,
                    31,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-01T03:16:52Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    16,
                    52,
                    0,
                    244,
                    0
                ],
                "title": "Bidirectional Sparse Attention for Faster Video Diffusion Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional Sparse Attention for Faster Video Diffusion Training"
                },
                "summary": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention."
                },
                "authors": [
                    {
                        "name": "Chenlu Zhan"
                    },
                    {
                        "name": "Wen Li"
                    },
                    {
                        "name": "Chuyu Shen"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Suhui Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01085v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01085v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09094v1",
                "updated": "2025-09-11T02:00:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    2,
                    0,
                    27,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T02:00:27Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    2,
                    0,
                    27,
                    3,
                    254,
                    0
                ],
                "title": "Coherence-Aware Task Graph Modeling for Realistic Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherence-Aware Task Graph Modeling for Realistic Application"
                },
                "summary": "As multicore systems continue to scale, cache coherence has emerged as a\ncritical determinant of system performance, with coherence behavior and task\nexecution closely intertwined, reshaping inter-task dependencies. Task graph\nmodeling provides a structured way to capture such dependencies and serves as\nthe foundation for many system-level design strategies. However, these\nstrategies typically rely on predefined task graphs, while many real-world\napplications lack explicit graphs and exhibit dynamic, data-dependent behavior,\nlimiting the effectiveness of static approaches. To address this, several task\ngraph modeling methods for realistic workloads have been developed. Yet, they\neither rely on implicit techniques that use application-specific features\nwithout producing explicit graphs, or they generate graphs tailored to fixed\nscheduling models, which limits generality. More importantly, they often\noverlook coherence interactions, creating a gap between design assumptions and\nactual runtime behavior. To overcome these limitations, we propose CoTAM, a\nCoherence-Aware Task Graph Modeling framework for realistic workloads that\nconstructs a unified task graph reflecting runtime behavior. CoTAM analyzes the\nimpact of coherence by decoupling its effects from overall execution,\nquantifies its influence through a learned weighting scheme, and infers\ninter-task dependencies for coherence-aware graph generation. Extensive\nexperiments show that CoTAM outperforms implicit methods, bridging the gap\nbetween dynamic workload behavior and existing designs while demonstrating the\nimportance of incorporating cache coherence into task graph modeling for\naccurate and generalizable system-level analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multicore systems continue to scale, cache coherence has emerged as a\ncritical determinant of system performance, with coherence behavior and task\nexecution closely intertwined, reshaping inter-task dependencies. Task graph\nmodeling provides a structured way to capture such dependencies and serves as\nthe foundation for many system-level design strategies. However, these\nstrategies typically rely on predefined task graphs, while many real-world\napplications lack explicit graphs and exhibit dynamic, data-dependent behavior,\nlimiting the effectiveness of static approaches. To address this, several task\ngraph modeling methods for realistic workloads have been developed. Yet, they\neither rely on implicit techniques that use application-specific features\nwithout producing explicit graphs, or they generate graphs tailored to fixed\nscheduling models, which limits generality. More importantly, they often\noverlook coherence interactions, creating a gap between design assumptions and\nactual runtime behavior. To overcome these limitations, we propose CoTAM, a\nCoherence-Aware Task Graph Modeling framework for realistic workloads that\nconstructs a unified task graph reflecting runtime behavior. CoTAM analyzes the\nimpact of coherence by decoupling its effects from overall execution,\nquantifies its influence through a learned weighting scheme, and infers\ninter-task dependencies for coherence-aware graph generation. Extensive\nexperiments show that CoTAM outperforms implicit methods, bridging the gap\nbetween dynamic workload behavior and existing designs while demonstrating the\nimportance of incorporating cache coherence into task graph modeling for\naccurate and generalizable system-level analysis."
                },
                "authors": [
                    {
                        "name": "Guochu Xiong"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Weichen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weichen Liu"
                },
                "author": "Weichen Liu",
                "arxiv_doi": "10.1145/3742875.3754678",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3742875.3754678",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.09094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by MEMOCODE'25, 10 pages",
                "arxiv_journal_ref": "International Symposium on Formal Methods and Models for System\n  Design (MEMOCODE '25), September 28-October 3, 2025, Taipei, Taiwan",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23674v2",
                "updated": "2025-09-10T17:59:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    59,
                    8,
                    2,
                    253,
                    0
                ],
                "published": "2025-07-31T15:50:57Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses"
                },
                "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience."
                },
                "authors": [
                    {
                        "name": "Muhammad Taha Cheema"
                    },
                    {
                        "name": "Abeer Aamir"
                    },
                    {
                        "name": "Khawaja Gul Muhammad"
                    },
                    {
                        "name": "Naveed Anwar Bhatti"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08696v1",
                "updated": "2025-09-10T15:41:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    41,
                    15,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T15:41:15Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    41,
                    15,
                    2,
                    253,
                    0
                ],
                "title": "Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer\n  Layer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer\n  Layer Caching"
                },
                "summary": "This paper presents a method to accelerate the inference process of diffusion\ntransformer (DiT)-based text-to-speech (TTS) models by applying a selective\ncaching mechanism to transformer layers. Specifically, I integrate SmoothCache\ninto the F5-TTS architecture, focusing on caching outputs of self-attention and\nfeed-forward network layers to reduce redundant computations during the\ndenoising process. A calibration phase is introduced to analyze L1 relative\nerrors between timesteps, guiding the selection of cache schedules that\nminimize quality degradation. To address the problem of inter-layer dependency,\na unified caching schedule is adopted, applying the cache pattern derived from\nself-attention layers to both layer types. Experiments on LibriSpeech-PC and\nSeed-TTS datasets evaluate various cache thresholds and denoising step\nconfigurations. Results show that caching at higher denoising steps reduces\ninference time without compromising output quality, whereas caching at lower\nsteps can negatively impact synthesis quality similarly to reducing the total\nnumber of denoising steps. Objective and subjective metrics confirm the\neffectiveness of SmoothCache in maintaining performance while improving\ncomputational efficiency. Comparisons between cached inference and reduced-step\ninference further highlight the benefits of selective caching, especially under\nhigh-step configurations. This work demonstrates that transformer layer caching\nis a practical solution for optimizing diffusion transformer-based TTS models\nwithout requiring architectural changes or retraining. Example inference\nresults can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a method to accelerate the inference process of diffusion\ntransformer (DiT)-based text-to-speech (TTS) models by applying a selective\ncaching mechanism to transformer layers. Specifically, I integrate SmoothCache\ninto the F5-TTS architecture, focusing on caching outputs of self-attention and\nfeed-forward network layers to reduce redundant computations during the\ndenoising process. A calibration phase is introduced to analyze L1 relative\nerrors between timesteps, guiding the selection of cache schedules that\nminimize quality degradation. To address the problem of inter-layer dependency,\na unified caching schedule is adopted, applying the cache pattern derived from\nself-attention layers to both layer types. Experiments on LibriSpeech-PC and\nSeed-TTS datasets evaluate various cache thresholds and denoising step\nconfigurations. Results show that caching at higher denoising steps reduces\ninference time without compromising output quality, whereas caching at lower\nsteps can negatively impact synthesis quality similarly to reducing the total\nnumber of denoising steps. Objective and subjective metrics confirm the\neffectiveness of SmoothCache in maintaining performance while improving\ncomputational efficiency. Comparisons between cached inference and reduced-step\ninference further highlight the benefits of selective caching, especially under\nhigh-step configurations. This work demonstrates that transformer layer caching\nis a practical solution for optimizing diffusion transformer-based TTS models\nwithout requiring architectural changes or retraining. Example inference\nresults can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ ."
                },
                "authors": [
                    {
                        "name": "Siratish Sakpiboonchit"
                    }
                ],
                "author_detail": {
                    "name": "Siratish Sakpiboonchit"
                },
                "author": "Siratish Sakpiboonchit",
                "arxiv_comment": "9 pages, 2 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08542v1",
                "updated": "2025-09-10T12:46:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    46,
                    29,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T12:46:29Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    46,
                    29,
                    2,
                    253,
                    0
                ],
                "title": "BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter\n  1.58-bit LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter\n  1.58-bit LLM Inference"
                },
                "summary": "Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy\nefficiency for CNNs by eliminating runtime weight updates. However, their\nscalability to Large Language Models (LLMs) is fundamentally constrained by\ntheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA\nseries - demands more than 1,000 cm2 of silicon area even in advanced CMOS\nnodes. This paper presents BitROM, the first CiROM-based accelerator that\novercomes this limitation through co-design with BitNet's 1.58-bit quantization\nmodel, enabling practical and efficient LLM inference at the edge. BitROM\nintroduces three key innovations: 1) a novel Bidirectional ROM Array that\nstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator\noptimized for ternary-weight computations; and 3) an integrated Decode-Refresh\n(DR) eDRAM that supports on-die KV-cache management, significantly reducing\nexternal memory access during decoding. In addition, BitROM integrates\nLoRA-based adapters to enable efficient transfer learning across various\ndownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit\ndensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over\nprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%\nreduction in external DRAM access, further enhancing deployment efficiency for\nLLMs in edge applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy\nefficiency for CNNs by eliminating runtime weight updates. However, their\nscalability to Large Language Models (LLMs) is fundamentally constrained by\ntheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA\nseries - demands more than 1,000 cm2 of silicon area even in advanced CMOS\nnodes. This paper presents BitROM, the first CiROM-based accelerator that\novercomes this limitation through co-design with BitNet's 1.58-bit quantization\nmodel, enabling practical and efficient LLM inference at the edge. BitROM\nintroduces three key innovations: 1) a novel Bidirectional ROM Array that\nstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator\noptimized for ternary-weight computations; and 3) an integrated Decode-Refresh\n(DR) eDRAM that supports on-die KV-cache management, significantly reducing\nexternal memory access during decoding. In addition, BitROM integrates\nLoRA-based adapters to enable efficient transfer learning across various\ndownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit\ndensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over\nprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%\nreduction in external DRAM access, further enhancing deployment efficiency for\nLLMs in edge applications."
                },
                "authors": [
                    {
                        "name": "Wenlun Zhang"
                    },
                    {
                        "name": "Xinyu Li"
                    },
                    {
                        "name": "Shimpei Ando"
                    },
                    {
                        "name": "Kentaro Yoshioka"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Yoshioka"
                },
                "author": "Kentaro Yoshioka",
                "arxiv_comment": "Accepted to ASP-DAC 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08342v1",
                "updated": "2025-09-10T07:28:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    28,
                    24,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T07:28:24Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    28,
                    24,
                    2,
                    253,
                    0
                ],
                "title": "Accelerating Mixture-of-Expert Inference with Adaptive Expert Split\n  Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Mixture-of-Expert Inference with Adaptive Expert Split\n  Mechanism"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a promising architecture for modern\nlarge language models (LLMs). However, massive parameters impose heavy GPU\nmemory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.\nOffloading the expert parameters to CPU RAM offers an effective way to\nalleviate the VRAM requirements for MoE inference. Existing approaches\ntypically cache a small subset of experts in VRAM and dynamically prefetch\nexperts from RAM during inference, leading to significant degradation in\ninference speed due to the poor cache hit rate and substantial expert loading\nlatency. In this work, we propose MoEpic, an efficient MoE inference system\nwith a novel expert split mechanism. Specifically, each expert is vertically\ndivided into two segments: top and bottom. MoEpic caches the top segment of hot\nexperts, so that more experts will be stored under the limited VRAM budget,\nthereby improving the cache hit rate. During each layer's inference, MoEpic\npredicts and prefetches the activated experts for the next layer. Since the top\nsegments of cached experts are exempt from fetching, the loading time is\nreduced, which allows efficient transfer-computation overlap. Nevertheless, the\nperformance of MoEpic critically depends on the cache configuration (i.e., each\nlayer's VRAM budget and expert split ratio). To this end, we propose a\ndivide-and-conquer algorithm based on fixed-point iteration for adaptive cache\nconfiguration. Extensive experiments on popular MoE LLMs demonstrate that\nMoEpic can save about half of the GPU cost, while lowering the inference\nlatency by about 37.51%-65.73% compared to the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a promising architecture for modern\nlarge language models (LLMs). However, massive parameters impose heavy GPU\nmemory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.\nOffloading the expert parameters to CPU RAM offers an effective way to\nalleviate the VRAM requirements for MoE inference. Existing approaches\ntypically cache a small subset of experts in VRAM and dynamically prefetch\nexperts from RAM during inference, leading to significant degradation in\ninference speed due to the poor cache hit rate and substantial expert loading\nlatency. In this work, we propose MoEpic, an efficient MoE inference system\nwith a novel expert split mechanism. Specifically, each expert is vertically\ndivided into two segments: top and bottom. MoEpic caches the top segment of hot\nexperts, so that more experts will be stored under the limited VRAM budget,\nthereby improving the cache hit rate. During each layer's inference, MoEpic\npredicts and prefetches the activated experts for the next layer. Since the top\nsegments of cached experts are exempt from fetching, the loading time is\nreduced, which allows efficient transfer-computation overlap. Nevertheless, the\nperformance of MoEpic critically depends on the cache configuration (i.e., each\nlayer's VRAM budget and expert split ratio). To this end, we propose a\ndivide-and-conquer algorithm based on fixed-point iteration for adaptive cache\nconfiguration. Extensive experiments on popular MoE LLMs demonstrate that\nMoEpic can save about half of the GPU cost, while lowering the inference\nlatency by about 37.51%-65.73% compared to the baselines."
                },
                "authors": [
                    {
                        "name": "Jiaming Yan"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08315v1",
                "updated": "2025-09-10T06:32:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    6,
                    32,
                    49,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T06:32:49Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    6,
                    32,
                    49,
                    2,
                    253,
                    0
                ],
                "title": "EvolKV: Evolutionary KV Cache Compression for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvolKV: Evolutionary KV Cache Compression for LLM Inference"
                },
                "summary": "Existing key-value (KV) cache compression methods typically rely on\nheuristics, such as uniform cache allocation across layers or static eviction\npolicies, however, they ignore the critical interplays among layer-specific\nfeature patterns and task performance, which can lead to degraded\ngeneralization. In this paper, we propose EvolKV, an adaptive framework for\nlayer-wise, task-driven KV cache compression that jointly optimizes the memory\nefficiency and task performance. By reformulating cache allocation as a\nmulti-objective optimization problem, EvolKV leverages evolutionary search to\ndynamically configure layer budgets while directly maximizing downstream\nperformance. Extensive experiments on 11 tasks demonstrate that our approach\noutperforms all baseline methods across a wide range of KV cache budgets on\nlong-context tasks and surpasses heuristic baselines by up to 7 percentage\npoints on GSM8K. Notably, EvolKV achieves superior performance over the full KV\ncache setting on code completion while utilizing only 1.5% of the original\nbudget, suggesting the untapped potential in learned compression strategies for\nKV cache budget allocation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing key-value (KV) cache compression methods typically rely on\nheuristics, such as uniform cache allocation across layers or static eviction\npolicies, however, they ignore the critical interplays among layer-specific\nfeature patterns and task performance, which can lead to degraded\ngeneralization. In this paper, we propose EvolKV, an adaptive framework for\nlayer-wise, task-driven KV cache compression that jointly optimizes the memory\nefficiency and task performance. By reformulating cache allocation as a\nmulti-objective optimization problem, EvolKV leverages evolutionary search to\ndynamically configure layer budgets while directly maximizing downstream\nperformance. Extensive experiments on 11 tasks demonstrate that our approach\noutperforms all baseline methods across a wide range of KV cache budgets on\nlong-context tasks and surpasses heuristic baselines by up to 7 percentage\npoints on GSM8K. Notably, EvolKV achieves superior performance over the full KV\ncache setting on code completion while utilizing only 1.5% of the original\nbudget, suggesting the untapped potential in learned compression strategies for\nKV cache budget allocation."
                },
                "authors": [
                    {
                        "name": "Bohan Yu"
                    },
                    {
                        "name": "Yekun Chai"
                    }
                ],
                "author_detail": {
                    "name": "Yekun Chai"
                },
                "author": "Yekun Chai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v3",
                "updated": "2025-09-09T13:30:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    30,
                    17,
                    1,
                    252,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "Accepted by EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07379v1",
                "updated": "2025-09-09T04:00:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    4,
                    0,
                    43,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T04:00:43Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    4,
                    0,
                    43,
                    1,
                    252,
                    0
                ],
                "title": "DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for\n  Efficient MoE LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for\n  Efficient MoE LLM Inference"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of deep learning tasks. Mixture of Experts (MoE) further enhances\ntheir capabilities by increasing model width through sparsely activated expert\nbranches, which keeps inference computation efficient. However, the large\nnumber of expert weights introduces significant GPU memory pressure, especially\nin resource-constrained environments such as single-GPU servers. More\nimportantly, MoE inference consists of two fundamentally different stages: a\nprefill stage where most experts are activated densely, and a decode stage\nwhere only a few experts are triggered sparsely. Treating these stages with a\nuniform scheduling strategy often leads to suboptimal latency and memory usage.\nTo address this, we propose DuoServe-MoE, an inference serving system that\nexplicitly separates prefill and decode stages and applies tailored expert\nscheduling strategies to each. In the prefill stage, DuoServe-MoE uses a\ntwo-stream CUDA pipeline that overlaps expert weight prefetching with the\ncomputation of non-MoE layers, limiting expert residency in GPU memory. In the\ndecode stage, a lightweight layer-level predictor trained offline from\nactivation traces is used to prefetch only the most likely activated experts,\nwithout requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B\nand 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to\n7.54 times while keeping peak memory usage at only 15 percent of the full model\nsize.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of deep learning tasks. Mixture of Experts (MoE) further enhances\ntheir capabilities by increasing model width through sparsely activated expert\nbranches, which keeps inference computation efficient. However, the large\nnumber of expert weights introduces significant GPU memory pressure, especially\nin resource-constrained environments such as single-GPU servers. More\nimportantly, MoE inference consists of two fundamentally different stages: a\nprefill stage where most experts are activated densely, and a decode stage\nwhere only a few experts are triggered sparsely. Treating these stages with a\nuniform scheduling strategy often leads to suboptimal latency and memory usage.\nTo address this, we propose DuoServe-MoE, an inference serving system that\nexplicitly separates prefill and decode stages and applies tailored expert\nscheduling strategies to each. In the prefill stage, DuoServe-MoE uses a\ntwo-stream CUDA pipeline that overlaps expert weight prefetching with the\ncomputation of non-MoE layers, limiting expert residency in GPU memory. In the\ndecode stage, a lightweight layer-level predictor trained offline from\nactivation traces is used to prefetch only the most likely activated experts,\nwithout requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B\nand 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to\n7.54 times while keeping peak memory usage at only 15 percent of the full model\nsize."
                },
                "authors": [
                    {
                        "name": "Yuning Zhang"
                    },
                    {
                        "name": "Grant Pinkert"
                    },
                    {
                        "name": "Nan Yang"
                    },
                    {
                        "name": "Yanli Li"
                    },
                    {
                        "name": "Dong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yuan"
                },
                "author": "Dong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01742v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01742v2",
                "updated": "2025-09-09T00:15:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    0,
                    15,
                    5,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-01T19:49:21Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    19,
                    49,
                    21,
                    0,
                    244,
                    0
                ],
                "title": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators"
                },
                "summary": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations:\n(i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache\nto accelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations:\n(i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache\nto accelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook."
                },
                "authors": [
                    {
                        "name": "Yitong Guo"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Haobin Hiroki Chen"
                    },
                    {
                        "name": "Yukui Luo"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Chenghong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chenghong Wang"
                },
                "author": "Chenghong Wang",
                "arxiv_comment": "Accepted by CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01742v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01742v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06949v1",
                "updated": "2025-09-08T17:58:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:58:06Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models"
                },
                "summary": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL"
                },
                "authors": [
                    {
                        "name": "Yinjie Wang"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ke Shen"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03377v2",
                "updated": "2025-09-08T17:22:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    22,
                    17,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-03T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    53,
                    45,
                    2,
                    246,
                    0
                ],
                "title": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing"
                },
                "summary": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v4",
                "updated": "2025-09-08T13:34:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    34,
                    54,
                    0,
                    251,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AGI/AMD-Hybrid-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AGI/AMD-Hybrid-Models."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06579v1",
                "updated": "2025-09-08T11:49:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    49,
                    51,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T11:49:51Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    49,
                    51,
                    0,
                    251,
                    0
                ],
                "title": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View\n  Synthesis"
                },
                "summary": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html."
                },
                "authors": [
                    {
                        "name": "Xin Kong"
                    },
                    {
                        "name": "Daniel Watson"
                    },
                    {
                        "name": "Yannick Strümpler"
                    },
                    {
                        "name": "Michael Niemeyer"
                    },
                    {
                        "name": "Federico Tombari"
                    }
                ],
                "author_detail": {
                    "name": "Federico Tombari"
                },
                "author": "Federico Tombari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06493v1",
                "updated": "2025-09-08T09:54:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T09:54:18Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers"
                },
                "summary": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search."
                },
                "authors": [
                    {
                        "name": "Ran Xin"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Yanchen Nie"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Xia Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xia Xiao"
                },
                "author": "Xia Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09822v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09822v4",
                "updated": "2025-09-08T09:09:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    9,
                    36,
                    0,
                    251,
                    0
                ],
                "published": "2025-08-13T13:54:51Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "title": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining"
                },
                "summary": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/"
                },
                "authors": [
                    {
                        "name": "Zijian Song"
                    },
                    {
                        "name": "Sihan Qin"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Guangrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangrun Wang"
                },
                "author": "Guangrun Wang",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09822v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09822v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06444v1",
                "updated": "2025-09-08T08:44:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    44,
                    24,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T08:44:24Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    44,
                    24,
                    0,
                    251,
                    0
                ],
                "title": "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for\n  Heterogeneous and Privacy-Sensitive Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for\n  Heterogeneous and Privacy-Sensitive Data"
                },
                "summary": "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments."
                },
                "authors": [
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Hainan Zhang"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Hong-Wei Zheng"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06436v1",
                "updated": "2025-09-08T08:34:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T08:34:02Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "title": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning"
                },
                "summary": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Xiaofei Xu"
                    },
                    {
                        "name": "Ke Deng"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Lin Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lin Tian"
                },
                "author": "Lin Tian",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06047v1",
                "updated": "2025-09-07T13:15:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    7,
                    13,
                    15,
                    17,
                    6,
                    250,
                    0
                ],
                "published": "2025-09-07T13:15:17Z",
                "published_parsed": [
                    2025,
                    9,
                    7,
                    13,
                    15,
                    17,
                    6,
                    250,
                    0
                ],
                "title": "A facile vector substrate platform via BaTiO3 membrane transfer enables\n  high quality solution processed epitaxial PZT on silicon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A facile vector substrate platform via BaTiO3 membrane transfer enables\n  high quality solution processed epitaxial PZT on silicon"
                },
                "summary": "The direct integration of high-performance ferroelectric oxides with silicon\nremains challenging due to lattice mismatch, thermal incompatibility, and the\nneed for high-temperature epitaxial growth. Here, a hybrid integration approach\nis demonstrated in which crystalline BaTiO3 (BTO) membranes are first\ntransferred onto Pt coated Si substrates and subsequently used as vector\nsubstrates (VS) for the growth of epitaxial (001) Pb(Zr0.52Ti0.48)O3 (PZT) thin\nfilms via chemical solution deposition (CSD). A KI and HCl based etchant\nenables rapid and complete dissolution of the SrVO3 sacrificial layer in about\n30 minutes, reducing the release time from days to minutes compared with\nconventional water based approaches to dissolve AVO3 and AMoO3 (A is Ca, Sr,\nBa). The BTO VS imposes dominant (00l) out of plane orientation and in plane\ncube on cube epitaxy in the overlying PZT. Devices exhibit remnant polarization\n10 to 12 micro coulomb/cm2 and coercive field of 100 kV/cm, with stable\nswitching to 10^8 cycles on the VS. From piezoelectric butterfly loops, we\nextract effective d33 of 70 pm/V for PZT on VS, and 54 pm/V for PZT grown on\nconventional Pt Si substrates. This approach demonstrates a scalable and cost\neffective route for integrating functional ferroelectric materials onto silicon\nand offers a promising platform for future CMOS compatible oxide electronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The direct integration of high-performance ferroelectric oxides with silicon\nremains challenging due to lattice mismatch, thermal incompatibility, and the\nneed for high-temperature epitaxial growth. Here, a hybrid integration approach\nis demonstrated in which crystalline BaTiO3 (BTO) membranes are first\ntransferred onto Pt coated Si substrates and subsequently used as vector\nsubstrates (VS) for the growth of epitaxial (001) Pb(Zr0.52Ti0.48)O3 (PZT) thin\nfilms via chemical solution deposition (CSD). A KI and HCl based etchant\nenables rapid and complete dissolution of the SrVO3 sacrificial layer in about\n30 minutes, reducing the release time from days to minutes compared with\nconventional water based approaches to dissolve AVO3 and AMoO3 (A is Ca, Sr,\nBa). The BTO VS imposes dominant (00l) out of plane orientation and in plane\ncube on cube epitaxy in the overlying PZT. Devices exhibit remnant polarization\n10 to 12 micro coulomb/cm2 and coercive field of 100 kV/cm, with stable\nswitching to 10^8 cycles on the VS. From piezoelectric butterfly loops, we\nextract effective d33 of 70 pm/V for PZT on VS, and 54 pm/V for PZT grown on\nconventional Pt Si substrates. This approach demonstrates a scalable and cost\neffective route for integrating functional ferroelectric materials onto silicon\nand offers a promising platform for future CMOS compatible oxide electronics."
                },
                "authors": [
                    {
                        "name": "Asraful Haque"
                    },
                    {
                        "name": "Antony Jeyaseelan"
                    },
                    {
                        "name": "Shubham Kumar Parate"
                    },
                    {
                        "name": "Srinivasan Raghavan"
                    },
                    {
                        "name": "Pavan Nukala"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Nukala"
                },
                "arxiv_affiliation": "Centre for Nanoscience and Engineering, Indian Institute of Science, Bengaluru, India",
                "author": "Pavan Nukala",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13863v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13863v2",
                "updated": "2025-09-06T05:58:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    6,
                    5,
                    58,
                    51,
                    5,
                    249,
                    0
                ],
                "published": "2025-08-19T14:30:41Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    30,
                    41,
                    1,
                    231,
                    0
                ],
                "title": "Tight Cache Contention Analysis for WCET Estimation on Multicore Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tight Cache Contention Analysis for WCET Estimation on Multicore Systems"
                },
                "summary": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Shenlin Cai"
                    },
                    {
                        "name": "Yaowei Liang"
                    },
                    {
                        "name": "Chen Jie"
                    },
                    {
                        "name": "Yinjie Fang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Guoquan Zhang"
                    },
                    {
                        "name": "Yaoyao Gu"
                    },
                    {
                        "name": "Xiang Xiao"
                    },
                    {
                        "name": "Wei Qin"
                    },
                    {
                        "name": "Xiangzhen Ouyang"
                    },
                    {
                        "name": "Wanli Chang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Chang"
                },
                "author": "Wanli Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13863v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13863v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05207v1",
                "updated": "2025-09-05T16:10:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    20,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T16:10:20Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    20,
                    4,
                    248,
                    0
                ],
                "title": "RapidGNN: Energy and Communication-Efficient Distributed Training on\n  Large-Scale Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RapidGNN: Energy and Communication-Efficient Distributed Training on\n  Large-Scale Graph Neural Networks"
                },
                "summary": "Graph Neural Networks (GNNs) have become popular across a diverse set of\ntasks in exploring structural relationships between entities. However, due to\nthe highly connected structure of the datasets, distributed training of GNNs on\nlarge-scale graphs poses significant challenges. Traditional sampling-based\napproaches mitigate the computational loads, yet the communication overhead\nremains a challenge. This paper presents RapidGNN, a distributed GNN training\nframework with deterministic sampling-based scheduling to enable efficient\ncache construction and prefetching of remote features. Evaluation on benchmark\ngraph datasets demonstrates RapidGNN's effectiveness across different scales\nand topologies. RapidGNN improves end-to-end training throughput by 2.46x to\n3.00x on average over baseline methods across the benchmark datasets, while\ncutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further\ndemonstrates near-linear scalability with an increasing number of computing\nunits efficiently. Furthermore, it achieves increased energy efficiency over\nthe baseline methods for both CPU and GPU by 44% and 32%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have become popular across a diverse set of\ntasks in exploring structural relationships between entities. However, due to\nthe highly connected structure of the datasets, distributed training of GNNs on\nlarge-scale graphs poses significant challenges. Traditional sampling-based\napproaches mitigate the computational loads, yet the communication overhead\nremains a challenge. This paper presents RapidGNN, a distributed GNN training\nframework with deterministic sampling-based scheduling to enable efficient\ncache construction and prefetching of remote features. Evaluation on benchmark\ngraph datasets demonstrates RapidGNN's effectiveness across different scales\nand topologies. RapidGNN improves end-to-end training throughput by 2.46x to\n3.00x on average over baseline methods across the benchmark datasets, while\ncutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further\ndemonstrates near-linear scalability with an increasing number of computing\nunits efficiently. Furthermore, it achieves increased energy efficiency over\nthe baseline methods for both CPU and GPU by 44% and 32%, respectively."
                },
                "authors": [
                    {
                        "name": "Arefin Niam"
                    },
                    {
                        "name": "Tevfik Kosar"
                    },
                    {
                        "name": "M S Q Zulkar Nine"
                    }
                ],
                "author_detail": {
                    "name": "M S Q Zulkar Nine"
                },
                "author": "M S Q Zulkar Nine",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2505.10806",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05165v1",
                "updated": "2025-09-05T14:58:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T14:58:24Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "title": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens"
                },
                "summary": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment."
                },
                "authors": [
                    {
                        "name": "Dmitry Akulov"
                    },
                    {
                        "name": "Mohamed Sana"
                    },
                    {
                        "name": "Antonio De Domenico"
                    },
                    {
                        "name": "Tareq Si Salem"
                    },
                    {
                        "name": "Nicola Piovesan"
                    },
                    {
                        "name": "Fadhel Ayed"
                    }
                ],
                "author_detail": {
                    "name": "Fadhel Ayed"
                },
                "author": "Fadhel Ayed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09758v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09758v2",
                "updated": "2025-09-05T10:39:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    39,
                    3,
                    4,
                    248,
                    0
                ],
                "published": "2025-06-11T14:03:13Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "title": "Mainframe-Style Channel Controllers for Modern Disaggregated Memory\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainframe-Style Channel Controllers for Modern Disaggregated Memory\n  Systems"
                },
                "summary": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs."
                },
                "authors": [
                    {
                        "name": "Zikai Liu"
                    },
                    {
                        "name": "Jasmin Schult"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "arxiv_doi": "10.1145/3725783.3764403",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725783.3764403",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.09758v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09758v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Camera-ready authors' version for APSys'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04377v1",
                "updated": "2025-09-04T16:40:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T16:40:01Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "title": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference"
                },
                "summary": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks."
                },
                "authors": [
                    {
                        "name": "Krishna Teja Chitty-Venkata"
                    },
                    {
                        "name": "Jie Ye"
                    },
                    {
                        "name": "Xian-He Sun"
                    },
                    {
                        "name": "Anthony Kougkas"
                    },
                    {
                        "name": "Murali Emani"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Nicolae"
                },
                "author": "Bogdan Nicolae",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12084v2",
                "updated": "2025-09-04T15:21:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    21,
                    11,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-21T12:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "title": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis"
                },
                "summary": "This study presents a comprehensive multi-level analysis of the NVIDIA Hopper\nGPU architecture, focusing on its performance characteristics and novel\nfeatures. We benchmark Hopper's memory subsystem, highlighting improvements in\nthe L2 partitioned cache and global memory access compared to Ampere and Ada\nLovelace. The evaluation of Hopper's fourth-generation tensor cores reveals the\nbenefits of FP8 precision and asynchronous wgmma instructions for matrix\noperations. Additionally, we investigate the performance of DPX instructions\nfor dynamic programming, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. Through multi-level evaluation, we discover that the Hopper\narchitecture demonstrates significant acceleration potential in real-world\napplications. For instance, the asynchronous programming model supported by TMA\nachieves a 1.5x speedup in matrix multiplication, FP8 delivers nearly double\nthe performance of FP16, and DPX instructions accelerate a computational\nbiology algorithm by at least 4.75x. Our findings provide actionable insights\nfor optimizing compute-intensive workloads, from AI training to bioinformatics,\non Hopper GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a comprehensive multi-level analysis of the NVIDIA Hopper\nGPU architecture, focusing on its performance characteristics and novel\nfeatures. We benchmark Hopper's memory subsystem, highlighting improvements in\nthe L2 partitioned cache and global memory access compared to Ampere and Ada\nLovelace. The evaluation of Hopper's fourth-generation tensor cores reveals the\nbenefits of FP8 precision and asynchronous wgmma instructions for matrix\noperations. Additionally, we investigate the performance of DPX instructions\nfor dynamic programming, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. Through multi-level evaluation, we discover that the Hopper\narchitecture demonstrates significant acceleration potential in real-world\napplications. For instance, the asynchronous programming model supported by TMA\nachieves a 1.5x speedup in matrix multiplication, FP8 delivers nearly double\nthe performance of FP16, and DPX instructions accelerate a computational\nbiology algorithm by at least 4.75x. Our findings provide actionable insights\nfor optimizing compute-intensive workloads, from AI training to bioinformatics,\non Hopper GPUs."
                },
                "authors": [
                    {
                        "name": "Weile Luo"
                    },
                    {
                        "name": "Ruibo Fan"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Hongyuan Liu"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2402.13499",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04185v1",
                "updated": "2025-09-04T13:02:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T13:02:39Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "title": "Set Block Decoding is a Language Model Inference Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Set Block Decoding is a Language Model Inference Accelerator"
                },
                "summary": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training."
                },
                "authors": [
                    {
                        "name": "Itai Gat"
                    },
                    {
                        "name": "Heli Ben-Hamu"
                    },
                    {
                        "name": "Marton Havasi"
                    },
                    {
                        "name": "Daniel Haziza"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Gabriel Synnaeve"
                    },
                    {
                        "name": "David Lopez-Paz"
                    },
                    {
                        "name": "Brian Karrer"
                    },
                    {
                        "name": "Yaron Lipman"
                    }
                ],
                "author_detail": {
                    "name": "Yaron Lipman"
                },
                "author": "Yaron Lipman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04180v1",
                "updated": "2025-09-04T12:54:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    54,
                    32,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T12:54:32Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    54,
                    32,
                    3,
                    247,
                    0
                ],
                "title": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision"
                },
                "summary": "AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}."
                },
                "authors": [
                    {
                        "name": "Safouane El Ghazouali"
                    },
                    {
                        "name": "Umberto Michelucci"
                    }
                ],
                "author_detail": {
                    "name": "Umberto Michelucci"
                },
                "author": "Umberto Michelucci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04010v1",
                "updated": "2025-09-04T08:41:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    41,
                    6,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T08:41:06Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    41,
                    6,
                    3,
                    247,
                    0
                ],
                "title": "Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and\n  Lessons Learned",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and\n  Lessons Learned"
                },
                "summary": "The PQDSS standardization process requires cryptographic primitives to be\nfree from vulnerabilities, including timing and cache side-channels. Resistance\nto timing leakage is therefore an essential property, and achieving this\ntypically relies on software implementations that follow constant-time\nprinciples. Moreover, ensuring that all implementations are constant-time is\ncrucial for fair performance comparisons, as secure implementations often incur\nadditional overhead. Such analysis also helps identify scheme proposals that\nare inherently difficult to implement in constant time. Because constant-time\nproperties can be broken during compilation, it is often necessary to analyze\nthe compiled binary directly. Since manual binary analysis is extremely\nchallenging, automated analysis becomes highly important. Although several\ntools exist to assist with such analysis, they often have usability limitations\nand are difficult to set up correctly. To support the developers besides the\nNIST committee in verifying candidates, we developed a toolchain that automates\nconfiguration, execution, and result analysis for several widely used\nconstant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify\nconstant-time policy compliance at the binary level, and dudect and RTLF to\ndetect side-channel vulnerabilities through statistical analysis of execution\ntime behavior. We demonstrate its effectiveness and practicability by\nevaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26\nissues in total to the respective developers, and 5 of them have already been\nfixed. We also discuss our different findings, as well as the benefits of\nshortcomings of the different tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The PQDSS standardization process requires cryptographic primitives to be\nfree from vulnerabilities, including timing and cache side-channels. Resistance\nto timing leakage is therefore an essential property, and achieving this\ntypically relies on software implementations that follow constant-time\nprinciples. Moreover, ensuring that all implementations are constant-time is\ncrucial for fair performance comparisons, as secure implementations often incur\nadditional overhead. Such analysis also helps identify scheme proposals that\nare inherently difficult to implement in constant time. Because constant-time\nproperties can be broken during compilation, it is often necessary to analyze\nthe compiled binary directly. Since manual binary analysis is extremely\nchallenging, automated analysis becomes highly important. Although several\ntools exist to assist with such analysis, they often have usability limitations\nand are difficult to set up correctly. To support the developers besides the\nNIST committee in verifying candidates, we developed a toolchain that automates\nconfiguration, execution, and result analysis for several widely used\nconstant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify\nconstant-time policy compliance at the binary level, and dudect and RTLF to\ndetect side-channel vulnerabilities through statistical analysis of execution\ntime behavior. We demonstrate its effectiveness and practicability by\nevaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26\nissues in total to the respective developers, and 5 of them have already been\nfixed. We also discuss our different findings, as well as the benefits of\nshortcomings of the different tools."
                },
                "authors": [
                    {
                        "name": "Olivier Adjonyo"
                    },
                    {
                        "name": "Sebastien Bardin"
                    },
                    {
                        "name": "Emanuele Bellini"
                    },
                    {
                        "name": "Gilbert Ndollane Dione"
                    },
                    {
                        "name": "Mahmudul Faisal Al Ameen"
                    },
                    {
                        "name": "Robert Merget"
                    },
                    {
                        "name": "Frederic Recoules"
                    },
                    {
                        "name": "Yanis Sellami"
                    }
                ],
                "author_detail": {
                    "name": "Yanis Sellami"
                },
                "author": "Yanis Sellami",
                "arxiv_comment": "20 pages, 1 figure, to be published and presented at Sixth PQC\n  Standardization Conference by NIST, partially supported by the \"France 2030\"\n  government investment plan managed by the French National Research Agency,\n  under the reference ANR-22-PECY-0005",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12689v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12689v3",
                "updated": "2025-09-04T06:20:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    6,
                    20,
                    55,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-22T07:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "IC-Cache: Efficient Large Language Model Serving via In-context Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IC-Cache: Efficient Large Language Model Serving via In-context Caching"
                },
                "summary": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 70% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge transfer among requests. However, naively caching and reusing past\nresponses leads to a big quality drop. In this paper, we introduce IC-Cache, a\ncaching system that enables live LLM capability augmentation to improve serving\nefficiency: by leveraging historical request-response pairs from larger models\nas in-context examples, IC-Cache empowers small LLMs to imitate and even exceed\nthe compositional abilities (e.g., reasoning) of their larger counterparts,\nenabling selective offloading of requests to reduce cost and latency. Achieving\nthis live augmentation at scale introduces intricate trade-offs between\nresponse quality, latency, and system throughput. For a new request, IC-Cache\nefficiently selects similar, high-utility examples to prepend them to the new\nrequest's input. At scale, it adaptively routes requests across LLMs of varying\ncapabilities, accounting for response quality and serving loads. IC-Cache\nemploys a cost-aware cache replay mechanism that refines example quality\noffline to maximize online cache utility and efficiency. Evaluations on\nmillions of realistic requests demonstrate that IC-Cache improves LLM serving\nthroughput by 1.4-5.9x and reduces latency by 28-71% without hurting response\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 70% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge transfer among requests. However, naively caching and reusing past\nresponses leads to a big quality drop. In this paper, we introduce IC-Cache, a\ncaching system that enables live LLM capability augmentation to improve serving\nefficiency: by leveraging historical request-response pairs from larger models\nas in-context examples, IC-Cache empowers small LLMs to imitate and even exceed\nthe compositional abilities (e.g., reasoning) of their larger counterparts,\nenabling selective offloading of requests to reduce cost and latency. Achieving\nthis live augmentation at scale introduces intricate trade-offs between\nresponse quality, latency, and system throughput. For a new request, IC-Cache\nefficiently selects similar, high-utility examples to prepend them to the new\nrequest's input. At scale, it adaptively routes requests across LLMs of varying\ncapabilities, accounting for response quality and serving loads. IC-Cache\nemploys a cost-aware cache replay mechanism that refines example quality\noffline to maximize online cache utility and efficiency. Evaluations on\nmillions of realistic requests demonstrate that IC-Cache improves LLM serving\nthroughput by 1.4-5.9x and reduces latency by 28-71% without hurting response\nquality."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Nikhil Sarda"
                    },
                    {
                        "name": "Lillian Tsai"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Yanqi Zhou"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Henry M. Levy"
                    },
                    {
                        "name": "David Culler"
                    }
                ],
                "author_detail": {
                    "name": "David Culler"
                },
                "author": "David Culler",
                "arxiv_doi": "10.1145/3731569.3764829",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731569.3764829",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.12689v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12689v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01228v2",
                "updated": "2025-09-03T20:54:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    20,
                    54,
                    57,
                    2,
                    246,
                    0
                ],
                "published": "2024-10-02T04:12:13Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    4,
                    12,
                    13,
                    2,
                    276,
                    0
                ],
                "title": "ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline\n  Co-Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline\n  Co-Serving"
                },
                "summary": "Large language model (LLM) serving demands low latency and high throughput,\nbut high load variability makes it challenging to achieve high GPU utilization.\nIn this paper, we identify a synergetic but overlooked opportunity to co-serve\nlatency-critical online requests alongside latency-tolerant offline tasks such\nas model benchmarking. While promising, existing serving systems fail to\nco-serve them efficiently, as their coarse-grained resource management at the\nrequest or iteration level cannot harvest millisecond-level GPU idle cycles\nwithout introducing interference that violates online latency objectives.\nConServe is a new LLM co-serving system that achieves high throughput and\nstrong online latency guarantees by managing resources at finer granularities.\nConServe introduces three techniques: (1) a latency-aware token-level scheduler\nthat precisely sizes offline batches and tokens to fit within online latency\nobjectives; (2) sub-iteration, layer-wise preemption that allows offline tasks\nto yield to online load spikes; and (3) incremental KV cache management that\nenables preempting and resuming offline requests at near-zero cost. Evaluations\nwith Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe\ndelivers an average of 2.2$\\times$ higher throughput and reduces online serving\ntail latency by 2.9$\\times$ on average compared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving demands low latency and high throughput,\nbut high load variability makes it challenging to achieve high GPU utilization.\nIn this paper, we identify a synergetic but overlooked opportunity to co-serve\nlatency-critical online requests alongside latency-tolerant offline tasks such\nas model benchmarking. While promising, existing serving systems fail to\nco-serve them efficiently, as their coarse-grained resource management at the\nrequest or iteration level cannot harvest millisecond-level GPU idle cycles\nwithout introducing interference that violates online latency objectives.\nConServe is a new LLM co-serving system that achieves high throughput and\nstrong online latency guarantees by managing resources at finer granularities.\nConServe introduces three techniques: (1) a latency-aware token-level scheduler\nthat precisely sizes offline batches and tokens to fit within online latency\nobjectives; (2) sub-iteration, layer-wise preemption that allows offline tasks\nto yield to online load spikes; and (3) incremental KV cache management that\nenables preempting and resuming offline requests at near-zero cost. Evaluations\nwith Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe\ndelivers an average of 2.2$\\times$ higher throughput and reduces online serving\ntail latency by 2.9$\\times$ on average compared to state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Yifan Qiao"
                    },
                    {
                        "name": "Shu Anzai"
                    },
                    {
                        "name": "Shan Yu"
                    },
                    {
                        "name": "Haoran Ma"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Miryung Kim"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jiarong Xing"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Harry Xu"
                    }
                ],
                "author_detail": {
                    "name": "Harry Xu"
                },
                "author": "Harry Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03394v1",
                "updated": "2025-09-03T15:15:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    44,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T15:15:44Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    44,
                    2,
                    246,
                    0
                ],
                "title": "CloudFormer: An Attention-based Performance Prediction for Public Clouds\n  with Unknown Workload",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CloudFormer: An Attention-based Performance Prediction for Public Clouds\n  with Unknown Workload"
                },
                "summary": "Cloud platforms are increasingly relied upon to host diverse,\nresource-intensive workloads due to their scalability, flexibility, and\ncost-efficiency. In multi-tenant cloud environments, virtual machines are\nconsolidated on shared physical servers to improve resource utilization. While\nvirtualization guarantees resource partitioning for CPU, memory, and storage,\nit cannot ensure performance isolation. Competition for shared resources such\nas last-level cache, memory bandwidth, and network interfaces often leads to\nsevere performance degradation. Existing management techniques, including VM\nscheduling and resource provisioning, require accurate performance prediction\nto mitigate interference. However, this remains challenging in public clouds\ndue to the black-box nature of VMs and the highly dynamic nature of workloads.\nTo address these limitations, we propose CloudFormer, a dual-branch\nTransformer-based model designed to predict VM performance degradation in\nblack-box environments. CloudFormer jointly models temporal dynamics and\nsystem-level interactions, leveraging 206 system metrics at one-second\nresolution across both static and dynamic scenarios. This design enables the\nmodel to capture transient interference effects and adapt to varying workload\nconditions without scenario-specific tuning. Complementing the methodology, we\nprovide a fine-grained dataset that significantly expands the temporal\nresolution and metric diversity compared to existing benchmarks. Experimental\nresults demonstrate that CloudFormer consistently outperforms state-of-the-art\nbaselines across multiple evaluation metrics, achieving robust generalization\nacross diverse and previously unseen workloads. Notably, CloudFormer attains a\nmean absolute error (MAE) of just 7.8%, representing a substantial improvement\nin predictive accuracy and outperforming existing methods at least by 28%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms are increasingly relied upon to host diverse,\nresource-intensive workloads due to their scalability, flexibility, and\ncost-efficiency. In multi-tenant cloud environments, virtual machines are\nconsolidated on shared physical servers to improve resource utilization. While\nvirtualization guarantees resource partitioning for CPU, memory, and storage,\nit cannot ensure performance isolation. Competition for shared resources such\nas last-level cache, memory bandwidth, and network interfaces often leads to\nsevere performance degradation. Existing management techniques, including VM\nscheduling and resource provisioning, require accurate performance prediction\nto mitigate interference. However, this remains challenging in public clouds\ndue to the black-box nature of VMs and the highly dynamic nature of workloads.\nTo address these limitations, we propose CloudFormer, a dual-branch\nTransformer-based model designed to predict VM performance degradation in\nblack-box environments. CloudFormer jointly models temporal dynamics and\nsystem-level interactions, leveraging 206 system metrics at one-second\nresolution across both static and dynamic scenarios. This design enables the\nmodel to capture transient interference effects and adapt to varying workload\nconditions without scenario-specific tuning. Complementing the methodology, we\nprovide a fine-grained dataset that significantly expands the temporal\nresolution and metric diversity compared to existing benchmarks. Experimental\nresults demonstrate that CloudFormer consistently outperforms state-of-the-art\nbaselines across multiple evaluation metrics, achieving robust generalization\nacross diverse and previously unseen workloads. Notably, CloudFormer attains a\nmean absolute error (MAE) of just 7.8%, representing a substantial improvement\nin predictive accuracy and outperforming existing methods at least by 28%."
                },
                "authors": [
                    {
                        "name": "Amirhossein Shahbazinia"
                    },
                    {
                        "name": "Darong Huang"
                    },
                    {
                        "name": "Luis Costero"
                    },
                    {
                        "name": "David Atienza"
                    }
                ],
                "author_detail": {
                    "name": "David Atienza"
                },
                "author": "David Atienza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00079v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00079v4",
                "updated": "2025-09-03T14:56:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    56,
                    29,
                    2,
                    246,
                    0
                ],
                "published": "2024-06-24T02:05:32Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    2,
                    5,
                    32,
                    0,
                    176,
                    0
                ],
                "title": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving"
                },
                "summary": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests."
                },
                "authors": [
                    {
                        "name": "Ruoyu Qin"
                    },
                    {
                        "name": "Zheming Li"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yongwei Wu"
                    },
                    {
                        "name": "Weimin Zheng"
                    },
                    {
                        "name": "Xinran Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xinran Xu"
                },
                "author": "Xinran Xu",
                "arxiv_comment": "23 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00079v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00079v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04416v2",
                "updated": "2025-09-03T14:28:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    28,
                    23,
                    2,
                    246,
                    0
                ],
                "published": "2025-07-06T15:08:49Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "title": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling"
                },
                "summary": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT."
                },
                "authors": [
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Anunay Yadav"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03560v1",
                "updated": "2025-09-03T11:23:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    23,
                    35,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T11:23:35Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    23,
                    35,
                    2,
                    246,
                    0
                ],
                "title": "A Cegar-centric Bounded Reachability Analysis for Compositional Affine\n  Hybrid Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Cegar-centric Bounded Reachability Analysis for Compositional Affine\n  Hybrid Systems"
                },
                "summary": "Reachability analysis of compositional hybrid systems, where individual\ncomponents are modeled as hybrid automata, poses unique challenges. In addition\nto preserving the compositional semantics while computing system behaviors,\nalgorithms have to cater to the explosion in the number of locations in the\nparallel product automaton. In this paper, we propose a bounded reachability\nanalysis algorithm for compositional hybrid systems with piecewise affine\ndynamics, based on the principle of counterexample guided abstraction\nrefinement (CEGAR). In particular, the algorithm searches for a counterexample\nin the discrete abstraction of the composition model, without explicitly\ncomputing a product automaton. When a counterexample is discovered in the\nabstraction, its validity is verified by a refinement of the state-space guided\nby the abstract counterexample. The state-space refinement is through a\nsymbolic reachability analysis, particularly using a state-of-the-art algorithm\nwith support functions as the continuous state representation. In addition, the\nalgorithm mixes different semantics of composition with the objective of\nimproved efficiency. Step compositional semantics is followed while exploring\nthe abstract (discrete) state-space, while shallow compositional semantics is\nfollowed during state-space refinement with symbolic reachability analysis.\nOptimizations such as caching the results of the symbolic reachability\nanalysis, which can be later reused, have been proposed. We implement this\nalgorithm in the tool SAT-Reach and demonstrate the scalability benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reachability analysis of compositional hybrid systems, where individual\ncomponents are modeled as hybrid automata, poses unique challenges. In addition\nto preserving the compositional semantics while computing system behaviors,\nalgorithms have to cater to the explosion in the number of locations in the\nparallel product automaton. In this paper, we propose a bounded reachability\nanalysis algorithm for compositional hybrid systems with piecewise affine\ndynamics, based on the principle of counterexample guided abstraction\nrefinement (CEGAR). In particular, the algorithm searches for a counterexample\nin the discrete abstraction of the composition model, without explicitly\ncomputing a product automaton. When a counterexample is discovered in the\nabstraction, its validity is verified by a refinement of the state-space guided\nby the abstract counterexample. The state-space refinement is through a\nsymbolic reachability analysis, particularly using a state-of-the-art algorithm\nwith support functions as the continuous state representation. In addition, the\nalgorithm mixes different semantics of composition with the objective of\nimproved efficiency. Step compositional semantics is followed while exploring\nthe abstract (discrete) state-space, while shallow compositional semantics is\nfollowed during state-space refinement with symbolic reachability analysis.\nOptimizations such as caching the results of the symbolic reachability\nanalysis, which can be later reused, have been proposed. We implement this\nalgorithm in the tool SAT-Reach and demonstrate the scalability benefits."
                },
                "authors": [
                    {
                        "name": "Atanu Kundu"
                    },
                    {
                        "name": "Pratyay Sarkar"
                    },
                    {
                        "name": "Rajarshi Ray"
                    }
                ],
                "author_detail": {
                    "name": "Rajarshi Ray"
                },
                "author": "Rajarshi Ray",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03136v1",
                "updated": "2025-09-03T08:38:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T08:38:40Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "title": "Adaptive KV-Cache Compression without Manually Setting Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive KV-Cache Compression without Manually Setting Budget"
                },
                "summary": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable."
                },
                "authors": [
                    {
                        "name": "Chenxia Tang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20353v2",
                "updated": "2025-09-03T06:56:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    6,
                    56,
                    21,
                    2,
                    246,
                    0
                ],
                "published": "2025-05-26T05:58:49Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    58,
                    49,
                    0,
                    146,
                    0
                ],
                "title": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation"
                },
                "summary": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Ben Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v2",
                "updated": "2025-09-02T18:10:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    18,
                    10,
                    0,
                    1,
                    245,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    },
                    {
                        "name": "Yuchen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Liu"
                },
                "author": "Yuchen Liu",
                "arxiv_comment": "Under Major Revision in IEEE Network",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02532v1",
                "updated": "2025-09-02T17:35:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    17,
                    35,
                    42,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T17:35:42Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    17,
                    35,
                    42,
                    1,
                    245,
                    0
                ],
                "title": "A Novel Coded Caching Scheme for Partially Cooperative Device-to-Device\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Coded Caching Scheme for Partially Cooperative Device-to-Device\n  Networks"
                },
                "summary": "Device-to-device (D2D) communication is one of the most promising techniques\nfor future wireless cellular communication systems. This paper considers coded\ncaching in a partially cooperative wireless D2D network, where only a subset of\nusers transmit during delivery, while all users request files. The\nnon-transmitting users are referred to as selfish users. All existing schemes\nthat do not require knowledge of the identity of selfish users before content\nplacement are limited to the high-memory regime, particularly when the number\nof selfish users is large. We propose a novel coded caching scheme for a\npartially cooperative D2D network that operates in all feasible memory regimes,\nregardless of the number of selfish users. We also derive a lower bound on the\ntransmission load of a partially cooperative D2D coded caching scheme. Using\nthis bound, the proposed scheme is shown to be optimal in the high-memory\nregime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Device-to-device (D2D) communication is one of the most promising techniques\nfor future wireless cellular communication systems. This paper considers coded\ncaching in a partially cooperative wireless D2D network, where only a subset of\nusers transmit during delivery, while all users request files. The\nnon-transmitting users are referred to as selfish users. All existing schemes\nthat do not require knowledge of the identity of selfish users before content\nplacement are limited to the high-memory regime, particularly when the number\nof selfish users is large. We propose a novel coded caching scheme for a\npartially cooperative D2D network that operates in all feasible memory regimes,\nregardless of the number of selfish users. We also derive a lower bound on the\ntransmission load of a partially cooperative D2D coded caching scheme. Using\nthis bound, the proposed scheme is shown to be optimal in the high-memory\nregime."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "7 pages and 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v5",
                "updated": "2025-09-02T16:39:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    39,
                    56,
                    1,
                    245,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation"
                },
                "summary": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. However, existing Ethernet-based solutions, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilization due to both increasing traffic demands and\nthe expanding scale of datacenter topologies, which also exacerbate network\nfailures. To address these limitations, we propose REPS, a lightweight\ndecentralized per-packet adaptive load balancing algorithm designed to optimize\nnetwork utilization while ensuring rapid recovery from link failures. REPS\nadapts to network conditions by caching good-performing paths. In case of a\nnetwork failure, REPS re-routes traffic away from it in less than 100\nmicroseconds. REPS is designed to be deployed with next-generation out-of-order\ntransports, such as Ultra Ethernet, and uses less than 25 bytes of\nper-connection state regardless of the topology size. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. However, existing Ethernet-based solutions, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilization due to both increasing traffic demands and\nthe expanding scale of datacenter topologies, which also exacerbate network\nfailures. To address these limitations, we propose REPS, a lightweight\ndecentralized per-packet adaptive load balancing algorithm designed to optimize\nnetwork utilization while ensuring rapid recovery from link failures. REPS\nadapts to network conditions by caching good-performing paths. In case of a\nnetwork failure, REPS re-routes traffic away from it in less than 100\nmicroseconds. REPS is designed to be deployed with next-generation out-of-order\ntransports, such as Ultra Ethernet, and uses less than 25 bytes of\nper-connection state regardless of the topology size. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02480v1",
                "updated": "2025-09-02T16:30:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    30,
                    49,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T16:30:49Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    30,
                    49,
                    1,
                    245,
                    0
                ],
                "title": "MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to\n  Break the GPU Memory Wall",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to\n  Break the GPU Memory Wall"
                },
                "summary": "Training LLMs larger than the aggregated memory of multiple GPUs is\nincreasingly necessary due to the faster growth of LLM sizes compared to GPU\nmemory. To this end, multi-tier host memory or disk offloading techniques are\nproposed by state of art. Despite advanced asynchronous multi-tier read/write\nstrategies, such offloading strategies result in significant I/O overheads in\nthe critical path of training, resulting in slower iterations. To this end, we\npropose MLP-Offload, a novel multi-level, multi-path offloading engine\nspecifically designed for optimizing LLM training on resource-constrained\nsetups by mitigating I/O bottlenecks. We make several key observations that\ndrive the design of MLP-Offload, such as I/O overheads during the update\ndominate the iteration time; I/O bandwidth of the third-level remote storage\ntier remains unutilized; and, contention due to concurrent offloading amplifies\nI/O bottlenecks. Driven by these insights, we design and implement MLP-Offload\nto offload the optimizer states across multiple tiers in a cache-efficient and\nconcurrency-controlled fashion to mitigate I/O bottlenecks during the backward\nand update phases. Evaluations on models up to 280B parameters shows that\nMLP-Offload achieves 2.5$\\times$ faster iterations compared to the\nstate-of-the-art LLM training runtimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLMs larger than the aggregated memory of multiple GPUs is\nincreasingly necessary due to the faster growth of LLM sizes compared to GPU\nmemory. To this end, multi-tier host memory or disk offloading techniques are\nproposed by state of art. Despite advanced asynchronous multi-tier read/write\nstrategies, such offloading strategies result in significant I/O overheads in\nthe critical path of training, resulting in slower iterations. To this end, we\npropose MLP-Offload, a novel multi-level, multi-path offloading engine\nspecifically designed for optimizing LLM training on resource-constrained\nsetups by mitigating I/O bottlenecks. We make several key observations that\ndrive the design of MLP-Offload, such as I/O overheads during the update\ndominate the iteration time; I/O bandwidth of the third-level remote storage\ntier remains unutilized; and, contention due to concurrent offloading amplifies\nI/O bottlenecks. Driven by these insights, we design and implement MLP-Offload\nto offload the optimizer states across multiple tiers in a cache-efficient and\nconcurrency-controlled fashion to mitigate I/O bottlenecks during the backward\nand update phases. Evaluations on models up to 280B parameters shows that\nMLP-Offload achieves 2.5$\\times$ faster iterations compared to the\nstate-of-the-art LLM training runtimes."
                },
                "authors": [
                    {
                        "name": "Avinash Maurya"
                    },
                    {
                        "name": "M. Mustafa Rafique"
                    },
                    {
                        "name": "Franck Cappello"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Nicolae"
                },
                "author": "Bogdan Nicolae",
                "arxiv_doi": "10.1145/3712285.3759864",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3712285.3759864",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SC'25: The International Conference for High Performance Computing,\n  Networking, Storage and Analysis",
                "arxiv_journal_ref": "SC'25: The International Conference for High Performance\n  Computing, Networking, Storage and Analysis, 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.0; E.2; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02408v1",
                "updated": "2025-09-02T15:19:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    15,
                    19,
                    6,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T15:19:06Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    15,
                    19,
                    6,
                    1,
                    245,
                    0
                ],
                "title": "Cache Management for Mixture-of-Experts LLMs -- extended version",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Management for Mixture-of-Experts LLMs -- extended version"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks. One of the main challenges towards the successful\ndeployment of LLMs is memory management, since they typically involve billions\nof parameters. To this end, architectures based on Mixture-of-Experts have been\nproposed, which aim to reduce the size of the parameters that are activated\nwhen producing a token. This raises the equally critical issue of efficiently\nmanaging the limited cache of the system, in that frequently used experts\nshould be stored in the fast cache rather than in the slower secondary memory.\n  In this work, we introduce and study a new paging problem that models expert\nmanagement optimization. Our formulation captures both the layered architecture\nof LLMs and the requirement that experts are cached efficiently. We first\npresent lower bounds on the competitive ratio of both deterministic and\nrandomized algorithms, which show that under mild assumptions, LRU-like\npolicies have good theoretical competitive performance. We then propose a\nlayer-based extension of LRU that is tailored to the problem at hand.\n  Extensive simulations on both synthetic datasets and actual traces of MoE\nusage show that our algorithm outperforms policies for the classic paging\nproblem, such as the standard LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks. One of the main challenges towards the successful\ndeployment of LLMs is memory management, since they typically involve billions\nof parameters. To this end, architectures based on Mixture-of-Experts have been\nproposed, which aim to reduce the size of the parameters that are activated\nwhen producing a token. This raises the equally critical issue of efficiently\nmanaging the limited cache of the system, in that frequently used experts\nshould be stored in the fast cache rather than in the slower secondary memory.\n  In this work, we introduce and study a new paging problem that models expert\nmanagement optimization. Our formulation captures both the layered architecture\nof LLMs and the requirement that experts are cached efficiently. We first\npresent lower bounds on the competitive ratio of both deterministic and\nrandomized algorithms, which show that under mild assumptions, LRU-like\npolicies have good theoretical competitive performance. We then propose a\nlayer-based extension of LRU that is tailored to the problem at hand.\n  Extensive simulations on both synthetic datasets and actual traces of MoE\nusage show that our algorithm outperforms policies for the classic paging\nproblem, such as the standard LRU."
                },
                "authors": [
                    {
                        "name": "Spyros Angelopoulos"
                    },
                    {
                        "name": "Loris Marchal"
                    },
                    {
                        "name": "Adrien Obrecht"
                    },
                    {
                        "name": "Bertrand Simon"
                    }
                ],
                "author_detail": {
                    "name": "Bertrand Simon"
                },
                "author": "Bertrand Simon",
                "arxiv_doi": "10.1007/978-3-031-99872-0_2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-99872-0_2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v2",
                "updated": "2025-09-02T13:09:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    13,
                    9,
                    37,
                    1,
                    245,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing reliance on expensive vector\ndatabase lookups. To scale efficiently, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically skewed MedRAG workload reduces database calls by 78.9% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our work highlights that approximate caching\nis a viable and effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing reliance on expensive vector\ndatabase lookups. To scale efficiently, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically skewed MedRAG workload reduces database calls by 78.9% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our work highlights that approximate caching\nis a viable and effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Zhang Ji"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    }
                ],
                "author_detail": {
                    "name": "Martijn de Vos"
                },
                "author": "Martijn de Vos",
                "arxiv_doi": "10.1145/3721146.3721941",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721941",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02232v1",
                "updated": "2025-09-02T11:58:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    58,
                    6,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T11:58:06Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    58,
                    6,
                    1,
                    245,
                    0
                ],
                "title": "Efficient Geometry Compression and Communication for 3D Gaussian\n  Splatting Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Geometry Compression and Communication for 3D Gaussian\n  Splatting Point Clouds"
                },
                "summary": "Storage and transmission challenges in dynamic 3D scene representation based\non the i3DV platform, With increasing scene complexity, the explosive growth of\n3D Gaussian data volume causes excessive storage space occupancy. To address\nthis issue, we propose adopting the AVS PCRM reference software for efficient\ncompression of Gaussian point cloud geometry data. The strategy deeply\nintegrates the advanced encoding capabilities of AVS PCRM into the i3DV\nplatform, forming technical complementarity with the original rate-distortion\noptimization mechanism based on binary hash tables. On one hand, the hash table\nefficiently caches inter-frame Gaussian point transformation relationships,\nwhich allows for high-fidelity transmission within a 40 Mbps bandwidth\nconstraint. On the other hand, AVS PCRM performs precise compression on\ngeometry data. Experimental results demonstrate that the joint framework\nmaintains the advantages of fast rendering and high-quality synthesis in 3D\nGaussian technology while achieving significant 10\\%-25\\% bitrate savings on\nuniversal test sets. It provides a superior rate-distortion tradeoff solution\nfor the storage, transmission, and interaction of 3D volumetric video.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Storage and transmission challenges in dynamic 3D scene representation based\non the i3DV platform, With increasing scene complexity, the explosive growth of\n3D Gaussian data volume causes excessive storage space occupancy. To address\nthis issue, we propose adopting the AVS PCRM reference software for efficient\ncompression of Gaussian point cloud geometry data. The strategy deeply\nintegrates the advanced encoding capabilities of AVS PCRM into the i3DV\nplatform, forming technical complementarity with the original rate-distortion\noptimization mechanism based on binary hash tables. On one hand, the hash table\nefficiently caches inter-frame Gaussian point transformation relationships,\nwhich allows for high-fidelity transmission within a 40 Mbps bandwidth\nconstraint. On the other hand, AVS PCRM performs precise compression on\ngeometry data. Experimental results demonstrate that the joint framework\nmaintains the advantages of fast rendering and high-quality synthesis in 3D\nGaussian technology while achieving significant 10\\%-25\\% bitrate savings on\nuniversal test sets. It provides a superior rate-distortion tradeoff solution\nfor the storage, transmission, and interaction of 3D volumetric video."
                },
                "authors": [
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Yanting Li"
                    },
                    {
                        "name": "Luyang Tang"
                    },
                    {
                        "name": "Wei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Gao"
                },
                "author": "Wei Gao",
                "arxiv_doi": "10.1145/3680207.3765659",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3680207.3765659",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages,5 figures",
                "arxiv_journal_ref": "ACM MOBICOM 2025",
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15212v2",
                "updated": "2025-09-02T11:29:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    29,
                    34,
                    1,
                    245,
                    0
                ],
                "published": "2025-08-21T03:48:28Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    48,
                    28,
                    3,
                    233,
                    0
                ],
                "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning"
                },
                "summary": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Guanchen Li"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Emad Barsoum"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02121v1",
                "updated": "2025-09-02T09:17:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    9,
                    17,
                    40,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T09:17:40Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    9,
                    17,
                    40,
                    1,
                    245,
                    0
                ],
                "title": "Batch Query Processing and Optimization for Agentic Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch Query Processing and Optimization for Agentic Workflows"
                },
                "summary": "Large Language Models (LLMs) in agentic workflows combine multi-step\nreasoning, tool use, and collaboration across multiple specialized agents.\nExisting LLM serving engines optimize individual calls in isolation, while\nmulti-agent frameworks focus on orchestration without system-level performance\nplanning. As a result, repeated prompts, overlapping contexts, and concurrent\nexecutions create substantial redundancy and poor GPU utilization, especially\nin batch analytics scenarios. We introduce Halo, a system that brings batch\nquery processing and optimization into agentic LLM workflows. Halo represents\neach workflow as a structured query plan DAG and constructs a consolidated\ngraph for batched queries that exposes shared computation. Guided by a cost\nmodel that jointly considers prefill and decode costs, cache reuse, and GPU\nplacement, Halo performs plan-level optimization to minimize redundant\nexecution. Its runtime integrates adaptive batching, KV-cache sharing and\nmigration, along with compute-communication overlap to maximize hardware\nefficiency. Evaluation across six benchmarks shows that Halo achieves up to\n18.6x speedup for batch inference and 4.7x throughput improvement under online\nserving, scaling to workloads of tens of thousands of queries and complex\ngraphs. These gains are achieved without compromising output quality. By\nunifying query optimization with LLM serving, Halo enables efficient agentic\nworkflows in data analytics and decision-making applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) in agentic workflows combine multi-step\nreasoning, tool use, and collaboration across multiple specialized agents.\nExisting LLM serving engines optimize individual calls in isolation, while\nmulti-agent frameworks focus on orchestration without system-level performance\nplanning. As a result, repeated prompts, overlapping contexts, and concurrent\nexecutions create substantial redundancy and poor GPU utilization, especially\nin batch analytics scenarios. We introduce Halo, a system that brings batch\nquery processing and optimization into agentic LLM workflows. Halo represents\neach workflow as a structured query plan DAG and constructs a consolidated\ngraph for batched queries that exposes shared computation. Guided by a cost\nmodel that jointly considers prefill and decode costs, cache reuse, and GPU\nplacement, Halo performs plan-level optimization to minimize redundant\nexecution. Its runtime integrates adaptive batching, KV-cache sharing and\nmigration, along with compute-communication overlap to maximize hardware\nefficiency. Evaluation across six benchmarks shows that Halo achieves up to\n18.6x speedup for batch inference and 4.7x throughput improvement under online\nserving, scaling to workloads of tens of thousands of queries and complex\ngraphs. These gains are achieved without compromising output quality. By\nunifying query optimization with LLM serving, Halo enables efficient agentic\nworkflows in data analytics and decision-making applications."
                },
                "authors": [
                    {
                        "name": "Junyi Shen"
                    },
                    {
                        "name": "Noppanat Wadlom"
                    },
                    {
                        "name": "Yao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Lu"
                },
                "author": "Yao Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02004v1",
                "updated": "2025-09-02T06:40:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    6,
                    40,
                    45,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T06:40:45Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    6,
                    40,
                    45,
                    1,
                    245,
                    0
                ],
                "title": "Augmented Shuffle Differential Privacy Protocols for Large-Domain\n  Categorical and Key-Value Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Shuffle Differential Privacy Protocols for Large-Domain\n  Categorical and Key-Value Data"
                },
                "summary": "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols."
                },
                "authors": [
                    {
                        "name": "Takao Murakami"
                    },
                    {
                        "name": "Yuichi Sei"
                    },
                    {
                        "name": "Reo Eriguchi"
                    }
                ],
                "author_detail": {
                    "name": "Reo Eriguchi"
                },
                "author": "Reo Eriguchi",
                "arxiv_comment": "Full version of the paper accepted at NDSS 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01395v1",
                "updated": "2025-09-01T11:41:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    41,
                    10,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T11:41:10Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    41,
                    10,
                    0,
                    244,
                    0
                ],
                "title": "LLMs cannot spot math errors, even when allowed to peek into the\n  solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs cannot spot math errors, even when allowed to peek into the\n  solution"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance."
                },
                "authors": [
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "Accepted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15779v2",
                "updated": "2025-09-01T07:26:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    7,
                    26,
                    57,
                    0,
                    244,
                    0
                ],
                "published": "2025-02-17T08:12:34Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    12,
                    34,
                    0,
                    48,
                    0
                ],
                "title": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer"
                },
                "summary": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP."
                },
                "authors": [
                    {
                        "name": "Euntae Choi"
                    },
                    {
                        "name": "Sumin Song"
                    },
                    {
                        "name": "Woosang Lim"
                    },
                    {
                        "name": "Sungjoo Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Sungjoo Yoo"
                },
                "author": "Sungjoo Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v3",
                "updated": "2025-09-01T03:51:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    51,
                    9,
                    0,
                    244,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01092v1",
                "updated": "2025-09-01T03:31:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T03:31:44Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "title": "REFRAG: Rethinking RAG based Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REFRAG: Rethinking RAG based Decoding"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes."
                },
                "authors": [
                    {
                        "name": "Xiaoqiang Lin"
                    },
                    {
                        "name": "Aritra Ghosh"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Anshumali Shrivastava"
                    },
                    {
                        "name": "Vijai Mohan"
                    }
                ],
                "author_detail": {
                    "name": "Vijai Mohan"
                },
                "author": "Vijai Mohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06133v2",
                "updated": "2025-08-31T15:09:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    15,
                    9,
                    36,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-08T08:54:21Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "title": "LLM Serving Optimization with Variable Prefill and Decode Lengths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Serving Optimization with Variable Prefill and Decode Lengths"
                },
                "summary": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency."
                },
                "authors": [
                    {
                        "name": "Meixuan Wang"
                    },
                    {
                        "name": "Yinyu Ye"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00883v1",
                "updated": "2025-08-31T14:51:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    51,
                    19,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-31T14:51:19Z",
                "published_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    51,
                    19,
                    6,
                    243,
                    0
                ],
                "title": "Accelerating Latency-Critical Applications with AI-Powered\n  Semi-Automatic Fine-Grained Parallelization on SMT Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Latency-Critical Applications with AI-Powered\n  Semi-Automatic Fine-Grained Parallelization on SMT Processors"
                },
                "summary": "Latency-critical applications tend to show low utilization of functional\nunits due to frequent cache misses and mispredictions during speculative\nexecution in high-performance superscalar processors. However, due to\nsignificant impact on single-thread performance, Simultaneous Multithreading\n(SMT) technology is rarely used with heavy threads of latency-critical\napplications. In this paper, we explore utilization of SMT technology to\nsupport fine-grained parallelization of latency-critical applications.\nFollowing the advancements in the development of Large Language Models (LLMs),\nwe introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we\nextend AI Coding Agent in Cursor IDE with additional tools connected through\nModel Context Protocol, enabling end-to-end AI Agent for parallelization.\nAdditional connected tools enable LLM-guided hotspot detection, collection of\ndynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance\nsimulation to estimate performance gains. We apply Aira with Relic parallel\nframework for fine-grained task parallelism on SMT cores to parallelize\nlatency-critical benchmarks representing real-world applications used in\nindustry. We show 17% geomean performance gain from parallelization of\nlatency-critical benchmarks using Aira with Relic framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency-critical applications tend to show low utilization of functional\nunits due to frequent cache misses and mispredictions during speculative\nexecution in high-performance superscalar processors. However, due to\nsignificant impact on single-thread performance, Simultaneous Multithreading\n(SMT) technology is rarely used with heavy threads of latency-critical\napplications. In this paper, we explore utilization of SMT technology to\nsupport fine-grained parallelization of latency-critical applications.\nFollowing the advancements in the development of Large Language Models (LLMs),\nwe introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we\nextend AI Coding Agent in Cursor IDE with additional tools connected through\nModel Context Protocol, enabling end-to-end AI Agent for parallelization.\nAdditional connected tools enable LLM-guided hotspot detection, collection of\ndynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance\nsimulation to estimate performance gains. We apply Aira with Relic parallel\nframework for fine-grained task parallelism on SMT cores to parallelize\nlatency-critical benchmarks representing real-world applications used in\nindustry. We show 17% geomean performance gain from parallelization of\nlatency-critical benchmarks using Aira with Relic framework."
                },
                "authors": [
                    {
                        "name": "Denis Los"
                    },
                    {
                        "name": "Igor Petushkov"
                    }
                ],
                "author_detail": {
                    "name": "Igor Petushkov"
                },
                "author": "Igor Petushkov",
                "arxiv_journal_ref": "International Journal of Open Information Technologies, vol. 13,\n  no. 9, pp. 129-134, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10431v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10431v3",
                "updated": "2025-08-31T05:43:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    5,
                    43,
                    55,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-14T08:04:15Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    4,
                    15,
                    3,
                    226,
                    0
                ],
                "title": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches"
                },
                "summary": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE."
                },
                "authors": [
                    {
                        "name": "Chris Cao"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "arxiv_comment": "This version includes updated analysis of RCO Bugs (one additional\n  bug identified). Appendix added with code snippets for bug fixes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10431v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10431v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00625v1",
                "updated": "2025-08-30T22:47:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    22,
                    47,
                    15,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T22:47:15Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    22,
                    47,
                    15,
                    5,
                    242,
                    0
                ],
                "title": "NetGent: Agent-Based Automation of Network Application Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NetGent: Agent-Based Automation of Network Application Workflows"
                },
                "summary": "We present NetGent, an AI-agent framework for automating complex application\nworkflows to generate realistic network traffic datasets. Developing\ngeneralizable ML models for networking requires data collection from network\nenvironments with traffic that results from a diverse set of real-world web\napplications. However, using existing browser automation tools that are\ndiverse, repeatable, realistic, and efficient remains fragile and costly.\nNetGent addresses this challenge by allowing users to specify workflows as\nnatural-language rules that define state-dependent actions. These abstract\nspecifications are compiled into nondeterministic finite automata (NFAs), which\na state synthesis component translates into reusable, executable code. This\ndesign enables deterministic replay, reduces redundant LLM calls through state\ncaching, and adapts quickly when application interfaces change. In experiments,\nNetGent automated more than 50+ workflows spanning video-on-demand streaming,\nlive video streaming, video conferencing, social media, and web scraping,\nproducing realistic traffic traces while remaining robust to UI variability. By\ncombining the flexibility of language-based agents with the reliability of\ncompiled execution, NetGent provides a scalable foundation for generating the\ndiverse, repeatable datasets needed to advance ML in networking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present NetGent, an AI-agent framework for automating complex application\nworkflows to generate realistic network traffic datasets. Developing\ngeneralizable ML models for networking requires data collection from network\nenvironments with traffic that results from a diverse set of real-world web\napplications. However, using existing browser automation tools that are\ndiverse, repeatable, realistic, and efficient remains fragile and costly.\nNetGent addresses this challenge by allowing users to specify workflows as\nnatural-language rules that define state-dependent actions. These abstract\nspecifications are compiled into nondeterministic finite automata (NFAs), which\na state synthesis component translates into reusable, executable code. This\ndesign enables deterministic replay, reduces redundant LLM calls through state\ncaching, and adapts quickly when application interfaces change. In experiments,\nNetGent automated more than 50+ workflows spanning video-on-demand streaming,\nlive video streaming, video conferencing, social media, and web scraping,\nproducing realistic traffic traces while remaining robust to UI variability. By\ncombining the flexibility of language-based agents with the reliability of\ncompiled execution, NetGent provides a scalable foundation for generating the\ndiverse, repeatable datasets needed to advance ML in networking."
                },
                "authors": [
                    {
                        "name": "Jaber Daneshamooz"
                    },
                    {
                        "name": "Eugene Vuong"
                    },
                    {
                        "name": "Laasya Koduru"
                    },
                    {
                        "name": "Sanjay Chandrasekaran"
                    },
                    {
                        "name": "Arpit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Arpit Gupta"
                },
                "author": "Arpit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00579v1",
                "updated": "2025-08-30T18:25:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    18,
                    25,
                    19,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T18:25:19Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    18,
                    25,
                    19,
                    5,
                    242,
                    0
                ],
                "title": "KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for\n  KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for\n  KV Cache"
                },
                "summary": "Transformer-based large language models (LLMs) demonstrate impressive\npotential in various practical applications. However, long context inference\nposes a significant challenge due to the enormous memory requirements of the\nkey-value (KV) cache, which can scale to multiple gigabytes as sequence length\nand batch size increase. In this paper, we present KVComp, a generic and\nefficient KV cache management framework optimized for long-text generation that\nsynergistically works with both latency-critical and throughput-critical\ninference systems. KVComp employs novel lossy compression techniques\nspecifically designed for KV cache data characteristics, featuring careful\nco-design of compression algorithms and system architecture. Our approach\nmaintains compatibility with the growing nature of KV cache while preserving\nhigh computational efficiency. Experimental results show that KVComp achieves\non average 47\\% and up to 83\\% higher memory reduction rate compared to\nexisting methods with little/no model accuracy degradation. Furthermore, KVComp\nachieves extremely high execution throughput, effectively reducing\ndecompression overhead and, in some cases, even accelerating the matrix-vector\nmultiplication operation and outperform cuBLAS-based attention kernels with\nless data movement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) demonstrate impressive\npotential in various practical applications. However, long context inference\nposes a significant challenge due to the enormous memory requirements of the\nkey-value (KV) cache, which can scale to multiple gigabytes as sequence length\nand batch size increase. In this paper, we present KVComp, a generic and\nefficient KV cache management framework optimized for long-text generation that\nsynergistically works with both latency-critical and throughput-critical\ninference systems. KVComp employs novel lossy compression techniques\nspecifically designed for KV cache data characteristics, featuring careful\nco-design of compression algorithms and system architecture. Our approach\nmaintains compatibility with the growing nature of KV cache while preserving\nhigh computational efficiency. Experimental results show that KVComp achieves\non average 47\\% and up to 83\\% higher memory reduction rate compared to\nexisting methods with little/no model accuracy degradation. Furthermore, KVComp\nachieves extremely high execution throughput, effectively reducing\ndecompression overhead and, in some cases, even accelerating the matrix-vector\nmultiplication operation and outperform cuBLAS-based attention kernels with\nless data movement."
                },
                "authors": [
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Taolue Yang"
                    },
                    {
                        "name": "Youyuan Liu"
                    },
                    {
                        "name": "Chengming Zhang"
                    },
                    {
                        "name": "Xubin He"
                    },
                    {
                        "name": "Sian Jin"
                    }
                ],
                "author_detail": {
                    "name": "Sian Jin"
                },
                "author": "Sian Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.13777v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.13777v2",
                "updated": "2025-08-30T14:49:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    14,
                    49,
                    34,
                    5,
                    242,
                    0
                ],
                "published": "2023-10-20T19:22:58Z",
                "published_parsed": [
                    2023,
                    10,
                    20,
                    19,
                    22,
                    58,
                    4,
                    293,
                    0
                ],
                "title": "Discrete and Continuous Caching Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete and Continuous Caching Games"
                },
                "summary": "We investigate a discrete search game called the Multiple Caching Game where\nthe searcher's aim is to find all of a set of $d$ treasures hidden in $n$\nlocations. Allowed queries are sets of locations of size $k$, and the searcher\nwins if in all $d$ queries, at least one treasure is hidden in one of the $k$\npicked locations. P\\'alv\\\"olgyi showed that the value of the game is at most\n$\\frac{k^d}{\\binom{n+d-1}{d}}$, with equality for large enough $n$. We\nconjecture the exact cases of equality. We also investigate variants of the\ngame and show an example where their values are different, answering a question\nof P\\'alv\\\"olgyi.\n  This game is closely related to a continuous variant, Alpern's Caching Game,\nbased on which we define other continous variants of the multiple caching game\nand examine their values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate a discrete search game called the Multiple Caching Game where\nthe searcher's aim is to find all of a set of $d$ treasures hidden in $n$\nlocations. Allowed queries are sets of locations of size $k$, and the searcher\nwins if in all $d$ queries, at least one treasure is hidden in one of the $k$\npicked locations. P\\'alv\\\"olgyi showed that the value of the game is at most\n$\\frac{k^d}{\\binom{n+d-1}{d}}$, with equality for large enough $n$. We\nconjecture the exact cases of equality. We also investigate variants of the\ngame and show an example where their values are different, answering a question\nof P\\'alv\\\"olgyi.\n  This game is closely related to a continuous variant, Alpern's Caching Game,\nbased on which we define other continous variants of the multiple caching game\nand examine their values."
                },
                "authors": [
                    {
                        "name": "Áron Jánosik"
                    },
                    {
                        "name": "Csenge Miklós"
                    },
                    {
                        "name": "Dániel G. Simon"
                    },
                    {
                        "name": "Kristóf Zólomy"
                    }
                ],
                "author_detail": {
                    "name": "Kristóf Zólomy"
                },
                "author": "Kristóf Zólomy",
                "arxiv_doi": "10.1142/S0219198925500057",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1142/S0219198925500057",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.13777v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.13777v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "International Game Theory Review 27 (3), 2025",
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91A05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v3",
                "updated": "2025-08-30T09:35:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    9,
                    35,
                    22,
                    5,
                    242,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "DiffKV: Differentiated Memory Management for Large Language Models with\n  Parallel KV Compaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffKV: Differentiated Memory Management for Large Language Models with\n  Parallel KV Compaction"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable capabilities but face\nsubstantial serving costs due to their high memory demands, with the key-value\n(KV) cache being a primary bottleneck. State-of-the-art KV cache compression\ntechniques, such as quantization and pruning, apply uniform treatment to both\nkeys and values, and discard unimportant tokens entirely, overlooking the\nfine-grained distinctions in the significance of individual KV cache\ncomponents. To address such limitations, we introduce \\textit{DiffKV}, a novel\nframework for efficient KV cache compression that exploits three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. These levels of\ndifferentiation introduce irregular memory usage patterns across different\nrequests and attention heads, posing significant scalability challenges for\nmemory management. To address these challenges, DiffKV proposes an on-GPU\nmemory manager that compacts fragmented free memory list into contiguous\nregions in parallel, effectively translating sparsity in the KV cache into\nperformance gains. We evaluate DiffKV on several mainstream LLMs, including the\nemerging thinking models that generate extended chains of thought. DiffKV is\nable to compress the KV cache by $2.7\\times$ to $5.7\\times$ with near-lossless\naccuracy on complex workloads requiring sophisticated reasoning and\nlong-generation capabilities, and enhances throughput by $1.9\\times$ to\n$5.4\\times$. Source codes of DiffKV are available at\nhttps://github.com/zyqCSL/DiffKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable capabilities but face\nsubstantial serving costs due to their high memory demands, with the key-value\n(KV) cache being a primary bottleneck. State-of-the-art KV cache compression\ntechniques, such as quantization and pruning, apply uniform treatment to both\nkeys and values, and discard unimportant tokens entirely, overlooking the\nfine-grained distinctions in the significance of individual KV cache\ncomponents. To address such limitations, we introduce \\textit{DiffKV}, a novel\nframework for efficient KV cache compression that exploits three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. These levels of\ndifferentiation introduce irregular memory usage patterns across different\nrequests and attention heads, posing significant scalability challenges for\nmemory management. To address these challenges, DiffKV proposes an on-GPU\nmemory manager that compacts fragmented free memory list into contiguous\nregions in parallel, effectively translating sparsity in the KV cache into\nperformance gains. We evaluate DiffKV on several mainstream LLMs, including the\nemerging thinking models that generate extended chains of thought. DiffKV is\nable to compress the KV cache by $2.7\\times$ to $5.7\\times$ with near-lossless\naccuracy on complex workloads requiring sophisticated reasoning and\nlong-generation capabilities, and enhances throughput by $1.9\\times$ to\n$5.4\\times$. Source codes of DiffKV are available at\nhttps://github.com/zyqCSL/DiffKV."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "SOSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00419v1",
                "updated": "2025-08-30T08:57:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    8,
                    57,
                    53,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T08:57:53Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    8,
                    57,
                    53,
                    5,
                    242,
                    0
                ],
                "title": "LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging\n  and KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging\n  and KV Cache Compression"
                },
                "summary": "In this paper, we introduce LightVLM, a simple but effective method that can\nbe seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly\naccelerate the inference process in a training-free manner. We divide the\ninference procedure of VLMs into two stages, i.e., encoding and decoding, and\npropose to simultaneously accelerate VLMs in both stages to largely improve\nmodel efficiency. During encoding, we propose pyramid token merging to reduce\ntokens of different LLM layers in a hierarchical manner by finally only keeping\na few dominant tokens to achieve high efficiency. During decoding, aimed at\nreducing the high latency of outputting long sequences, we propose KV Cache\ncompression to remove unnecessary caches to increase the network throughput.\nExperimental results show that LightVLM successfully retains 100% performance\nwhen only preserving 35% image tokens, and maintains around 98% performance\nwhen keeping only 3% image tokens. LightVLM could 2.02$\\times$ the network\nthroughput and reduce the prefilling time by 3.65$\\times$. LightVLM also makes\nlarge VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to\ninfer faster than significantly smaller models (e.g., InternVL2.5 8B),\nhopefully facilitating the real-world deployment. When generating long text\nsequences (e.g., 4096 tokens), LightVLM could reduce the inference time by\n3.21$\\times$, largely outperforming existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce LightVLM, a simple but effective method that can\nbe seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly\naccelerate the inference process in a training-free manner. We divide the\ninference procedure of VLMs into two stages, i.e., encoding and decoding, and\npropose to simultaneously accelerate VLMs in both stages to largely improve\nmodel efficiency. During encoding, we propose pyramid token merging to reduce\ntokens of different LLM layers in a hierarchical manner by finally only keeping\na few dominant tokens to achieve high efficiency. During decoding, aimed at\nreducing the high latency of outputting long sequences, we propose KV Cache\ncompression to remove unnecessary caches to increase the network throughput.\nExperimental results show that LightVLM successfully retains 100% performance\nwhen only preserving 35% image tokens, and maintains around 98% performance\nwhen keeping only 3% image tokens. LightVLM could 2.02$\\times$ the network\nthroughput and reduce the prefilling time by 3.65$\\times$. LightVLM also makes\nlarge VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to\ninfer faster than significantly smaller models (e.g., InternVL2.5 8B),\nhopefully facilitating the real-world deployment. When generating long text\nsequences (e.g., 4096 tokens), LightVLM could reduce the inference time by\n3.21$\\times$, largely outperforming existing methods."
                },
                "authors": [
                    {
                        "name": "Lianyu Hu"
                    },
                    {
                        "name": "Fanhua Shang"
                    },
                    {
                        "name": "Wei Feng"
                    },
                    {
                        "name": "Liang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wan"
                },
                "author": "Liang Wan",
                "arxiv_comment": "EMNLP2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00388v1",
                "updated": "2025-08-30T06:56:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    6,
                    56,
                    28,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T06:56:28Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    6,
                    56,
                    28,
                    5,
                    242,
                    0
                ],
                "title": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV\n  Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV\n  Cache Eviction"
                },
                "summary": "Efficient Key-Value (KV) cache management is essential for processing long\ntext sequences in large language models (LLMs), where memory constraints often\nlimit performance. Conventional KV eviction strategies, such as top-k selection\nbased on attention scores, depend on static heuristics that fail to capture the\nevolving implicit dependencies among tokens during inference. To overcome this,\nwe propose GraphKV, a graph-based framework that redefines token selection for\nKV cache compression. In GraphKV, tokens are modeled as nodes with importance\nscores, and edges represent their similarity relationships. Through a\ndecay-signal-propagation mechanism, token importance is dynamically updated by\npropagating information across the graph, enabling adaptive retention of the\nmost contextually significant tokens. GraphKV can be seamlessly utilized in\nexisting KV cache eviction methods such as SnapKV and PyramidKV in a\nplug-and-play manner. Codes will be released on Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Key-Value (KV) cache management is essential for processing long\ntext sequences in large language models (LLMs), where memory constraints often\nlimit performance. Conventional KV eviction strategies, such as top-k selection\nbased on attention scores, depend on static heuristics that fail to capture the\nevolving implicit dependencies among tokens during inference. To overcome this,\nwe propose GraphKV, a graph-based framework that redefines token selection for\nKV cache compression. In GraphKV, tokens are modeled as nodes with importance\nscores, and edges represent their similarity relationships. Through a\ndecay-signal-propagation mechanism, token importance is dynamically updated by\npropagating information across the graph, enabling adaptive retention of the\nmost contextually significant tokens. GraphKV can be seamlessly utilized in\nexisting KV cache eviction methods such as SnapKV and PyramidKV in a\nplug-and-play manner. Codes will be released on Github."
                },
                "authors": [
                    {
                        "name": "Xuelin Li"
                    },
                    {
                        "name": "Xiangqi Jin"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11435v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11435v2",
                "updated": "2025-08-29T20:39:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    20,
                    39,
                    21,
                    4,
                    241,
                    0
                ],
                "published": "2025-04-15T17:51:39Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    51,
                    39,
                    1,
                    105,
                    0
                ],
                "title": "Robust Containment Queries over Collections of Trimmed NURBS Surfaces\n  via Generalized Winding Numbers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Containment Queries over Collections of Trimmed NURBS Surfaces\n  via Generalized Winding Numbers"
                },
                "summary": "We propose a containment query that is robust to the watertightness of\nregions bound by trimmed NURBS surfaces, as this property is difficult to\nguarantee for in-the-wild CAD models. Containment is determined through the\ngeneralized winding number (GWN), a mathematical construction that is\nindifferent to the arrangement of surfaces in the shape. Applying contemporary\ntechniques for the 3D GWN to trimmed NURBS surfaces requires some form of\ngeometric discretization, introducing computational inefficiency to the\nalgorithm and even risking containment misclassifications near the surface. In\ncontrast, our proposed method uses a novel reformulation of the relevant\nsurface integral based on Stokes' theorem, which operates on the boundary and\ntrimming curves as provided through rapidly converging adaptive quadrature.\nBatches of queries are further accelerated by memoizing (i.e.\\ caching and\nreusing) quadrature node positions and tangents as they are evaluated. We\ndemonstrate that our GWN method is robust to complex trimming geometry in a CAD\nmodel, and is accurate up to arbitrary precision at arbitrary distances from\nthe surface. The derived containment query is therefore robust to model\nnon-watertightness while respecting all curved features of the input shape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a containment query that is robust to the watertightness of\nregions bound by trimmed NURBS surfaces, as this property is difficult to\nguarantee for in-the-wild CAD models. Containment is determined through the\ngeneralized winding number (GWN), a mathematical construction that is\nindifferent to the arrangement of surfaces in the shape. Applying contemporary\ntechniques for the 3D GWN to trimmed NURBS surfaces requires some form of\ngeometric discretization, introducing computational inefficiency to the\nalgorithm and even risking containment misclassifications near the surface. In\ncontrast, our proposed method uses a novel reformulation of the relevant\nsurface integral based on Stokes' theorem, which operates on the boundary and\ntrimming curves as provided through rapidly converging adaptive quadrature.\nBatches of queries are further accelerated by memoizing (i.e.\\ caching and\nreusing) quadrature node positions and tangents as they are evaluated. We\ndemonstrate that our GWN method is robust to complex trimming geometry in a CAD\nmodel, and is accurate up to arbitrary precision at arbitrary distances from\nthe surface. The derived containment query is therefore robust to model\nnon-watertightness while respecting all curved features of the input shape."
                },
                "authors": [
                    {
                        "name": "Jacob Spainhour"
                    },
                    {
                        "name": "Kenneth Weiss"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth Weiss"
                },
                "author": "Kenneth Weiss",
                "arxiv_comment": "18 Pages, 16 Figures, 1 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11435v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11435v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68U05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.3.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00202v1",
                "updated": "2025-08-29T19:23:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    23,
                    35,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T19:23:35Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    23,
                    35,
                    4,
                    241,
                    0
                ],
                "title": "From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer\n  Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer\n  Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive\n  Inference"
                },
                "summary": "Although the Transformer has become the cornerstone of modern AI, its\nautoregressive inference suffers from a linearly growing KV Cache and a\ncomputational complexity of O(N^2 d), severely hindering its ability to process\nultra-long sequences. To overcome this limitation, this paper introduces the\nTConstFormer architecture, building upon our previous work, TLinFormer.\nTConstFormer employs an innovative periodic state update mechanism to achieve a\ntruly constant-size O(1) KV Cache. The computational complexity of this\nmechanism is also O(1) in an amortized sense: it performs purely constant-time\ncomputations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single\nlinear-time global information synchronization only on the $k$-th step.\nTheoretical calculations and experimental results demonstrate that TConstFormer\nexhibits an overwhelming advantage over baseline models in terms of speed,\nmemory efficiency, and overall performance on long-text inference tasks. This\nbreakthrough paves the way for efficient and robust streaming language model\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although the Transformer has become the cornerstone of modern AI, its\nautoregressive inference suffers from a linearly growing KV Cache and a\ncomputational complexity of O(N^2 d), severely hindering its ability to process\nultra-long sequences. To overcome this limitation, this paper introduces the\nTConstFormer architecture, building upon our previous work, TLinFormer.\nTConstFormer employs an innovative periodic state update mechanism to achieve a\ntruly constant-size O(1) KV Cache. The computational complexity of this\nmechanism is also O(1) in an amortized sense: it performs purely constant-time\ncomputations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single\nlinear-time global information synchronization only on the $k$-th step.\nTheoretical calculations and experimental results demonstrate that TConstFormer\nexhibits an overwhelming advantage over baseline models in terms of speed,\nmemory efficiency, and overall performance on long-text inference tasks. This\nbreakthrough paves the way for efficient and robust streaming language model\napplications."
                },
                "authors": [
                    {
                        "name": "Zhongpan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongpan Tang"
                },
                "author": "Zhongpan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00195v1",
                "updated": "2025-08-29T19:12:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    12,
                    4,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T19:12:04Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    12,
                    4,
                    4,
                    241,
                    0
                ],
                "title": "Democratizing Agentic AI with Fast Test-Time Scaling on the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Democratizing Agentic AI with Fast Test-Time Scaling on the Edge"
                },
                "summary": "Deploying agentic AI on edge devices is crucial for privacy and\nresponsiveness, but memory constraints typically relegate these systems to\nsmaller Large Language Models (LLMs) with inferior reasoning capabilities.\nTest-Time Scaling (TTS) can bridge this reasoning gap by dedicating more\ncompute during inference, but existing methods incur prohibitive overhead on\nedge hardware. To overcome this, we introduce FlashTTS, a serving system that\nmakes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces\nthree synergistic optimizations: (i) Speculative Beam Extension to mitigate\nsystem stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model\nMemory Allocation to dynamically balance memory between generation and\nverification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache\nreuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on\na single consumer GPU (24 GB) to match the accuracy and latency of large cloud\nmodels. Our evaluation demonstrates that FlashTTS achieves an average 2.2x\nhigher goodput and reduces latency by 38%-68% compared to a vLLM baseline,\npaving the way for democratized, high-performance agentic AI on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying agentic AI on edge devices is crucial for privacy and\nresponsiveness, but memory constraints typically relegate these systems to\nsmaller Large Language Models (LLMs) with inferior reasoning capabilities.\nTest-Time Scaling (TTS) can bridge this reasoning gap by dedicating more\ncompute during inference, but existing methods incur prohibitive overhead on\nedge hardware. To overcome this, we introduce FlashTTS, a serving system that\nmakes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces\nthree synergistic optimizations: (i) Speculative Beam Extension to mitigate\nsystem stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model\nMemory Allocation to dynamically balance memory between generation and\nverification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache\nreuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on\na single consumer GPU (24 GB) to match the accuracy and latency of large cloud\nmodels. Our evaluation demonstrates that FlashTTS achieves an average 2.2x\nhigher goodput and reduces latency by 38%-68% compared to a vLLM baseline,\npaving the way for democratized, high-performance agentic AI on edge devices."
                },
                "authors": [
                    {
                        "name": "Hao Mark Chen"
                    },
                    {
                        "name": "Zhiwen Mo"
                    },
                    {
                        "name": "Guanxi Lu"
                    },
                    {
                        "name": "Shuang Liang"
                    },
                    {
                        "name": "Lingxiao Ma"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "author": "Hongxiang Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16217v2",
                "updated": "2025-08-29T18:45:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    18,
                    45,
                    22,
                    4,
                    241,
                    0
                ],
                "published": "2025-07-22T04:21:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Towards Compute-Optimal Many-Shot In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Compute-Optimal Many-Shot In-Context Learning"
                },
                "summary": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL."
                },
                "authors": [
                    {
                        "name": "Shahriar Golchin"
                    },
                    {
                        "name": "Yanfei Chen"
                    },
                    {
                        "name": "Rujun Han"
                    },
                    {
                        "name": "Manan Gandhi"
                    },
                    {
                        "name": "Tianli Yu"
                    },
                    {
                        "name": "Swaroop Mishra"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Chen-Yu Lee"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "arxiv_comment": "Final version; accepted at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05930v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05930v2",
                "updated": "2025-08-29T09:58:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    58,
                    17,
                    4,
                    241,
                    0
                ],
                "published": "2025-06-06T09:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    9,
                    55,
                    59,
                    4,
                    157,
                    0
                ],
                "title": "Neural Visibility Cache for Real-Time Light Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Visibility Cache for Real-Time Light Sampling"
                },
                "summary": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR)."
                },
                "authors": [
                    {
                        "name": "Jakub Bokšanský"
                    },
                    {
                        "name": "Daniel Meister"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Meister"
                },
                "author": "Daniel Meister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05930v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05930v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15683v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15683v2",
                "updated": "2025-08-29T07:40:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    7,
                    40,
                    34,
                    4,
                    241,
                    0
                ],
                "published": "2025-05-21T15:58:08Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "title": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models"
                },
                "summary": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability."
                },
                "authors": [
                    {
                        "name": "Zishuai Zhang"
                    },
                    {
                        "name": "Hainan zhang"
                    },
                    {
                        "name": "Weihua Li"
                    },
                    {
                        "name": "Qinnan zhang"
                    },
                    {
                        "name": "jin Dong"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15683v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15683v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04467v1",
                "updated": "2025-08-29T02:29:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    29,
                    52,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T02:29:52Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    29,
                    52,
                    4,
                    241,
                    0
                ],
                "title": "Enhancing LLM Efficiency: Targeted Pruning for Prefill-Decode\n  Disaggregation in Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Efficiency: Targeted Pruning for Prefill-Decode\n  Disaggregation in Inference"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the default settings, our method\nachieves a 20.56% inference speedup and a 4.95 times reduction in data\ntransmission bandwidth consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the default settings, our method\nachieves a 20.56% inference speedup and a 4.95 times reduction in data\ntransmission bandwidth consumption."
                },
                "authors": [
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Mengsi Lyu"
                    },
                    {
                        "name": "Yulong Ao"
                    },
                    {
                        "name": "Yonghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yonghua Lin"
                },
                "author": "Yonghua Lin",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12211v1",
                "updated": "2025-08-28T16:17:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    17,
                    18,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T16:17:18Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    17,
                    18,
                    3,
                    240,
                    0
                ],
                "title": "TinyServe: Query-Aware Cache Selection for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinyServe: Query-Aware Cache Selection for Efficient LLM Serving"
                },
                "summary": "Serving large language models (LLMs) efficiently remains challenging due to\nthe high memory and latency overhead of key-value (KV) cache access during\nautoregressive decoding. We present \\textbf{TinyServe}, a lightweight and\nextensible serving system for deploying tiny LLMs (e.g., TinyLLaMA, GPT2-345M)\nwith support for structured KV sparsity, plugin-based token selection, and\nhardware-efficient attention kernels. Unlike prior simulation frameworks,\nTinyServe executes real-time decoding with configurable sparsity strategies and\nfine-grained instrumentation.\n  To reduce decoding cost, we introduce a \\textit{query-aware page selection}\nmechanism that leverages bounding-box metadata to estimate attention relevance\nbetween the query and KV cache blocks. This enables selective KV loading with\nminimal overhead and no model modifications. Our fused CUDA kernel integrates\npage scoring, sparse memory access, and masked attention in a single pass.\n  Experiments show that TinyServe achieves up to \\textbf{3.4x} speedup and over\n\\textbf{2x} memory savings with negligible accuracy drop. Additional analysis\nof cache reuse, page hit rate, and multi-GPU scaling confirms its practicality\nas an efficient system-level design for LLM training and inference research on\nresource-constrained hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) efficiently remains challenging due to\nthe high memory and latency overhead of key-value (KV) cache access during\nautoregressive decoding. We present \\textbf{TinyServe}, a lightweight and\nextensible serving system for deploying tiny LLMs (e.g., TinyLLaMA, GPT2-345M)\nwith support for structured KV sparsity, plugin-based token selection, and\nhardware-efficient attention kernels. Unlike prior simulation frameworks,\nTinyServe executes real-time decoding with configurable sparsity strategies and\nfine-grained instrumentation.\n  To reduce decoding cost, we introduce a \\textit{query-aware page selection}\nmechanism that leverages bounding-box metadata to estimate attention relevance\nbetween the query and KV cache blocks. This enables selective KV loading with\nminimal overhead and no model modifications. Our fused CUDA kernel integrates\npage scoring, sparse memory access, and masked attention in a single pass.\n  Experiments show that TinyServe achieves up to \\textbf{3.4x} speedup and over\n\\textbf{2x} memory savings with negligible accuracy drop. Additional analysis\nof cache reuse, page hit rate, and multi-GPU scaling confirms its practicality\nas an efficient system-level design for LLM training and inference research on\nresource-constrained hardware."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yanxuan Yu"
                },
                "author": "Yanxuan Yu",
                "arxiv_comment": "Accepted to ACM MM as Oral Paper, also accepted to ICML MOSS\n  workshop, publicly available as https://openreview.net/forum?id=sOdtl4jLci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20865v1",
                "updated": "2025-08-28T14:58:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    47,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:58:47Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    47,
                    3,
                    240,
                    0
                ],
                "title": "Deep Multiple Quantization Network on Long Behavior Sequence for\n  Click-Through Rate Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Multiple Quantization Network on Long Behavior Sequence for\n  Click-Through Rate Prediction"
                },
                "summary": "In Click-Through Rate (CTR) prediction, the long behavior sequence,\ncomprising the user's long period of historical interactions with items has a\nvital influence on assessing the user's interest in the candidate item.\nExisting approaches strike efficiency and effectiveness through a two-stage\nparadigm: first retrieving hundreds of candidate-related items and then\nextracting interest intensity vector through target attention. However, we\nargue that the discrepancy in target attention's relevance distribution between\nthe retrieved items and the full long behavior sequence inevitably leads to a\nperformance decline. To alleviate the discrepancy, we propose the Deep Multiple\nQuantization Network (DMQN) to process long behavior sequence end-to-end\nthrough compressing the long behavior sequence. Firstly, the entire spectrum of\nlong behavior sequence will be quantized into multiple codeword sequences based\non multiple independent codebooks. Hierarchical Sequential Transduction Unit is\nincorporated to facilitate the interaction of reduced codeword sequences. Then,\nattention between the candidate and multiple codeword sequences will output the\ninterest vector. To enable online serving, intermediate representations of the\ncodeword sequences are cached, significantly reducing latency. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMQN. The A/B test in our advertising system shows that DMQN\nimproves CTR by 3.5% and RPM by 2.0%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Click-Through Rate (CTR) prediction, the long behavior sequence,\ncomprising the user's long period of historical interactions with items has a\nvital influence on assessing the user's interest in the candidate item.\nExisting approaches strike efficiency and effectiveness through a two-stage\nparadigm: first retrieving hundreds of candidate-related items and then\nextracting interest intensity vector through target attention. However, we\nargue that the discrepancy in target attention's relevance distribution between\nthe retrieved items and the full long behavior sequence inevitably leads to a\nperformance decline. To alleviate the discrepancy, we propose the Deep Multiple\nQuantization Network (DMQN) to process long behavior sequence end-to-end\nthrough compressing the long behavior sequence. Firstly, the entire spectrum of\nlong behavior sequence will be quantized into multiple codeword sequences based\non multiple independent codebooks. Hierarchical Sequential Transduction Unit is\nincorporated to facilitate the interaction of reduced codeword sequences. Then,\nattention between the candidate and multiple codeword sequences will output the\ninterest vector. To enable online serving, intermediate representations of the\ncodeword sequences are cached, significantly reducing latency. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMQN. The A/B test in our advertising system shows that DMQN\nimproves CTR by 3.5% and RPM by 2.0%."
                },
                "authors": [
                    {
                        "name": "Zhuoxing Wei"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Qingchen Xie"
                    }
                ],
                "author_detail": {
                    "name": "Qingchen Xie"
                },
                "author": "Qingchen Xie",
                "arxiv_doi": "10.1145/3726302.3730177",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730177",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.20865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, 1 figures, SIGIR 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18250v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18250v2",
                "updated": "2025-08-28T08:49:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    49,
                    24,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-25T17:41:13Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    13,
                    0,
                    237,
                    0
                ],
                "title": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study"
                },
                "summary": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM."
                },
                "authors": [
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Fernando García-Redondo"
                    },
                    {
                        "name": "Arvind Sharma"
                    },
                    {
                        "name": "Van Dai Nguyen"
                    },
                    {
                        "name": "Andrea Fantini"
                    },
                    {
                        "name": "Philippe Matagne"
                    },
                    {
                        "name": "Siddharth Rao"
                    },
                    {
                        "name": "Subhali Subhechha"
                    },
                    {
                        "name": "Lynn Verschueren"
                    },
                    {
                        "name": "Mohammed Aftab Baig"
                    },
                    {
                        "name": "Marie Garcia Bardon"
                    },
                    {
                        "name": "Geert Hellings"
                    }
                ],
                "author_detail": {
                    "name": "Geert Hellings"
                },
                "author": "Geert Hellings",
                "arxiv_comment": "Manuscript submitted to IEEE Trans. Elec. Dev. Work enabled in part\n  by NanoIC pilot line; acquisition and operation jointly funded by Chips Joint\n  Undertaking, through EU's Digital Europe (101183266) and Horizon Europe\n  programs (101183277), as well as by the participating states\n  (Belgium-Flanders, France, Germany, Finland, Ireland, Romania)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18250v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.14234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14234v1",
                "updated": "2025-09-17T17:59:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    59,
                    42,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T17:59:42Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    59,
                    42,
                    2,
                    260,
                    0
                ],
                "title": "Compute as Teacher: Turning Inference Compute Into Reference-Free\n  Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute as Teacher: Turning Inference Compute Into Reference-Free\n  Supervision"
                },
                "summary": "Where do learning signals come from when there is no ground truth in\npost-training? We propose turning exploration into supervision through Compute\nas Teacher (CaT), which converts the model's own exploration at inference-time\ninto reference-free supervision by synthesizing a single reference from a group\nof parallel rollouts and then optimizing toward it. Concretely, the current\npolicy produces a group of rollouts; a frozen anchor (the initial policy)\nreconciles omissions and contradictions to estimate a reference, turning extra\ninference-time compute into a teacher signal. We turn this into rewards in two\nregimes: (i) verifiable tasks use programmatic equivalence on final answers;\n(ii) non-verifiable tasks use self-proposed rubrics-binary, auditable criteria\nscored by an independent LLM judge, with reward given by the fraction\nsatisfied. Unlike selection methods (best-of-N, majority, perplexity, or judge\nscores), synthesis may disagree with the majority and be correct even when all\nrollouts are wrong; performance scales with the number of rollouts. As a\ntest-time procedure, CaT improves Gemma 3 4B, Qwen 3 4B, and Llama 3.1 8B (up\nto +27% on MATH-500; +12% on HealthBench). With reinforcement learning\n(CaT-RL), we obtain further gains (up to +33% and +30%), with the trained\npolicy surpassing the initial teacher signal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where do learning signals come from when there is no ground truth in\npost-training? We propose turning exploration into supervision through Compute\nas Teacher (CaT), which converts the model's own exploration at inference-time\ninto reference-free supervision by synthesizing a single reference from a group\nof parallel rollouts and then optimizing toward it. Concretely, the current\npolicy produces a group of rollouts; a frozen anchor (the initial policy)\nreconciles omissions and contradictions to estimate a reference, turning extra\ninference-time compute into a teacher signal. We turn this into rewards in two\nregimes: (i) verifiable tasks use programmatic equivalence on final answers;\n(ii) non-verifiable tasks use self-proposed rubrics-binary, auditable criteria\nscored by an independent LLM judge, with reward given by the fraction\nsatisfied. Unlike selection methods (best-of-N, majority, perplexity, or judge\nscores), synthesis may disagree with the majority and be correct even when all\nrollouts are wrong; performance scales with the number of rollouts. As a\ntest-time procedure, CaT improves Gemma 3 4B, Qwen 3 4B, and Llama 3.1 8B (up\nto +27% on MATH-500; +12% on HealthBench). With reinforcement learning\n(CaT-RL), we obtain further gains (up to +33% and +30%), with the trained\npolicy surpassing the initial teacher signal."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "Shashwat Goel"
                    },
                    {
                        "name": "Thomas Foster"
                    },
                    {
                        "name": "Parag Jain"
                    },
                    {
                        "name": "Suchin Gururangan"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Anirudh Goyal"
                    },
                    {
                        "name": "Alan Schelten"
                    }
                ],
                "author_detail": {
                    "name": "Alan Schelten"
                },
                "author": "Alan Schelten",
                "arxiv_comment": "22 pages, 8 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14233v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14233v1",
                "updated": "2025-09-17T17:59:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    59,
                    21,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T17:59:21Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    59,
                    21,
                    2,
                    260,
                    0
                ],
                "title": "Apertus: Democratizing Open and Compliant LLMs for Global Language\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apertus: Democratizing Open and Compliant LLMs for Global Language\n  Environments"
                },
                "summary": "We present Apertus, a fully open suite of large language models (LLMs)\ndesigned to address two systemic shortcomings in today's open model ecosystem:\ndata compliance and multilingual representation. Unlike many prior models that\nrelease weights without reproducible data pipelines or regard for content-owner\nrights, Apertus models are pretrained exclusively on openly available data,\nretroactively respecting robots.txt exclusions and filtering for\nnon-permissive, toxic, and personally identifiable content. To mitigate risks\nof memorization, we adopt the Goldfish objective during pretraining, strongly\nsuppressing verbatim recall of data while retaining downstream task\nperformance. The Apertus models also expand multilingual coverage, training on\n15T tokens from over 1800 languages, with ~40% of pretraining data allocated to\nnon-English content. Released at 8B and 70B scales, Apertus approaches\nstate-of-the-art results among fully open models on multilingual benchmarks,\nrivalling or surpassing open-weight counterparts. Beyond model weights, we\nrelease all scientific artifacts from our development cycle with a permissive\nlicense, including data preparation scripts, checkpoints, evaluation suites,\nand training code, enabling transparent audit and extension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Apertus, a fully open suite of large language models (LLMs)\ndesigned to address two systemic shortcomings in today's open model ecosystem:\ndata compliance and multilingual representation. Unlike many prior models that\nrelease weights without reproducible data pipelines or regard for content-owner\nrights, Apertus models are pretrained exclusively on openly available data,\nretroactively respecting robots.txt exclusions and filtering for\nnon-permissive, toxic, and personally identifiable content. To mitigate risks\nof memorization, we adopt the Goldfish objective during pretraining, strongly\nsuppressing verbatim recall of data while retaining downstream task\nperformance. The Apertus models also expand multilingual coverage, training on\n15T tokens from over 1800 languages, with ~40% of pretraining data allocated to\nnon-English content. Released at 8B and 70B scales, Apertus approaches\nstate-of-the-art results among fully open models on multilingual benchmarks,\nrivalling or surpassing open-weight counterparts. Beyond model weights, we\nrelease all scientific artifacts from our development cycle with a permissive\nlicense, including data preparation scripts, checkpoints, evaluation suites,\nand training code, enabling transparent audit and extension."
                },
                "authors": [
                    {
                        "name": "Alejandro Hernández-Cano"
                    },
                    {
                        "name": "Alexander Hägele"
                    },
                    {
                        "name": "Allen Hao Huang"
                    },
                    {
                        "name": "Angelika Romanou"
                    },
                    {
                        "name": "Antoni-Joan Solergibert"
                    },
                    {
                        "name": "Barna Pasztor"
                    },
                    {
                        "name": "Bettina Messmer"
                    },
                    {
                        "name": "Dhia Garbaya"
                    },
                    {
                        "name": "Eduard Frank Ďurech"
                    },
                    {
                        "name": "Ido Hakimi"
                    },
                    {
                        "name": "Juan García Giraldo"
                    },
                    {
                        "name": "Mete Ismayilzada"
                    },
                    {
                        "name": "Negar Foroutan"
                    },
                    {
                        "name": "Skander Moalla"
                    },
                    {
                        "name": "Tiancheng Chen"
                    },
                    {
                        "name": "Vinko Sabolčec"
                    },
                    {
                        "name": "Yixuan Xu"
                    },
                    {
                        "name": "Michael Aerni"
                    },
                    {
                        "name": "Badr AlKhamissi"
                    },
                    {
                        "name": "Ines Altemir Marinas"
                    },
                    {
                        "name": "Mohammad Hossein Amani"
                    },
                    {
                        "name": "Matin Ansaripour"
                    },
                    {
                        "name": "Ilia Badanin"
                    },
                    {
                        "name": "Harold Benoit"
                    },
                    {
                        "name": "Emanuela Boros"
                    },
                    {
                        "name": "Nicholas Browning"
                    },
                    {
                        "name": "Fabian Bösch"
                    },
                    {
                        "name": "Maximilian Böther"
                    },
                    {
                        "name": "Niklas Canova"
                    },
                    {
                        "name": "Camille Challier"
                    },
                    {
                        "name": "Clement Charmillot"
                    },
                    {
                        "name": "Jonathan Coles"
                    },
                    {
                        "name": "Jan Deriu"
                    },
                    {
                        "name": "Arnout Devos"
                    },
                    {
                        "name": "Lukas Drescher"
                    },
                    {
                        "name": "Daniil Dzenhaliou"
                    },
                    {
                        "name": "Maud Ehrmann"
                    },
                    {
                        "name": "Dongyang Fan"
                    },
                    {
                        "name": "Simin Fan"
                    },
                    {
                        "name": "Silin Gao"
                    },
                    {
                        "name": "Miguel Gila"
                    },
                    {
                        "name": "María Grandury"
                    },
                    {
                        "name": "Diba Hashemi"
                    },
                    {
                        "name": "Alexander Hoyle"
                    },
                    {
                        "name": "Jiaming Jiang"
                    },
                    {
                        "name": "Mark Klein"
                    },
                    {
                        "name": "Andrei Kucharavy"
                    },
                    {
                        "name": "Anastasiia Kucherenko"
                    },
                    {
                        "name": "Frederike Lübeck"
                    },
                    {
                        "name": "Roman Machacek"
                    },
                    {
                        "name": "Theofilos Manitaras"
                    },
                    {
                        "name": "Andreas Marfurt"
                    },
                    {
                        "name": "Kyle Matoba"
                    },
                    {
                        "name": "Simon Matrenok"
                    },
                    {
                        "name": "Henrique Mendoncça"
                    },
                    {
                        "name": "Fawzi Roberto Mohamed"
                    },
                    {
                        "name": "Syrielle Montariol"
                    },
                    {
                        "name": "Luca Mouchel"
                    },
                    {
                        "name": "Sven Najem-Meyer"
                    },
                    {
                        "name": "Jingwei Ni"
                    },
                    {
                        "name": "Gennaro Oliva"
                    },
                    {
                        "name": "Matteo Pagliardini"
                    },
                    {
                        "name": "Elia Palme"
                    },
                    {
                        "name": "Andrei Panferov"
                    },
                    {
                        "name": "Léo Paoletti"
                    },
                    {
                        "name": "Marco Passerini"
                    },
                    {
                        "name": "Ivan Pavlov"
                    },
                    {
                        "name": "Auguste Poiroux"
                    },
                    {
                        "name": "Kaustubh Ponkshe"
                    },
                    {
                        "name": "Nathan Ranchin"
                    },
                    {
                        "name": "Javi Rando"
                    },
                    {
                        "name": "Mathieu Sauser"
                    },
                    {
                        "name": "Jakhongir Saydaliev"
                    },
                    {
                        "name": "Muhammad Ali Sayfiddinov"
                    },
                    {
                        "name": "Marian Schneider"
                    },
                    {
                        "name": "Stefano Schuppli"
                    },
                    {
                        "name": "Marco Scialanga"
                    },
                    {
                        "name": "Andrei Semenov"
                    },
                    {
                        "name": "Kumar Shridhar"
                    },
                    {
                        "name": "Raghav Singhal"
                    },
                    {
                        "name": "Anna Sotnikova"
                    },
                    {
                        "name": "Alexander Sternfeld"
                    },
                    {
                        "name": "Ayush Kumar Tarun"
                    },
                    {
                        "name": "Paul Teiletche"
                    },
                    {
                        "name": "Jannis Vamvas"
                    },
                    {
                        "name": "Xiaozhe Yao"
                    },
                    {
                        "name": "Hao Zhao Alexander Ilic"
                    },
                    {
                        "name": "Ana Klimovic"
                    },
                    {
                        "name": "Andreas Krause"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    },
                    {
                        "name": "David Rosenthal"
                    },
                    {
                        "name": "Elliott Ash"
                    },
                    {
                        "name": "Florian Tramèr"
                    },
                    {
                        "name": "Joost VandeVondele"
                    },
                    {
                        "name": "Livio Veraldi"
                    },
                    {
                        "name": "Martin Rajman"
                    },
                    {
                        "name": "Thomas Schulthess"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "Antoine Bosselut"
                    },
                    {
                        "name": "Martin Jaggi"
                    },
                    {
                        "name": "Imanol Schlag"
                    }
                ],
                "author_detail": {
                    "name": "Imanol Schlag"
                },
                "author": "Imanol Schlag",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14233v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14230v1",
                "updated": "2025-09-17T17:59:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    59,
                    0,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T17:59:00Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    59,
                    0,
                    2,
                    260,
                    0
                ],
                "title": "NIRVANA: Structured pruning reimagined for large language models\n  compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NIRVANA: Structured pruning reimagined for large language models\n  compression"
                },
                "summary": "Structured pruning of large language models (LLMs) offers substantial\nefficiency improvements by removing entire hidden units, yet current approaches\noften suffer from significant performance degradation, particularly in\nzero-shot settings, and necessitate costly recovery techniques such as\nsupervised fine-tuning (SFT) or adapter insertion. To address these critical\nshortcomings, we introduce NIRVANA, a novel pruning method explicitly designed\nto balance immediate zero-shot accuracy preservation with robust fine-tuning\ncapability. Leveraging a first-order saliency criterion derived from the Neural\nTangent Kernel under Adam optimization dynamics, NIRVANA provides a\ntheoretically grounded pruning strategy that respects essential model training\nbehaviors. To further address the unique challenges posed by structured\npruning, NIRVANA incorporates an adaptive sparsity allocation mechanism across\nlayers and modules (attention vs. MLP), which adjusts pruning intensity between\nmodules in a globally balanced manner. Additionally, to mitigate the high\nsensitivity of pruning decisions to calibration data quality, we propose a\nsimple yet effective KL divergence-based calibration data selection strategy,\nensuring more reliable and task-agnostic pruning outcomes. Comprehensive\nexperiments conducted on Llama3, Qwen, and T5 models demonstrate that NIRVANA\noutperforms existing structured pruning methods under equivalent sparsity\nconstraints, providing a theoretically sound and practical approach to LLM\ncompression. The code is available at\nhttps://github.com/iDEA-iSAIL-Lab-UIUC/NIRVANA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured pruning of large language models (LLMs) offers substantial\nefficiency improvements by removing entire hidden units, yet current approaches\noften suffer from significant performance degradation, particularly in\nzero-shot settings, and necessitate costly recovery techniques such as\nsupervised fine-tuning (SFT) or adapter insertion. To address these critical\nshortcomings, we introduce NIRVANA, a novel pruning method explicitly designed\nto balance immediate zero-shot accuracy preservation with robust fine-tuning\ncapability. Leveraging a first-order saliency criterion derived from the Neural\nTangent Kernel under Adam optimization dynamics, NIRVANA provides a\ntheoretically grounded pruning strategy that respects essential model training\nbehaviors. To further address the unique challenges posed by structured\npruning, NIRVANA incorporates an adaptive sparsity allocation mechanism across\nlayers and modules (attention vs. MLP), which adjusts pruning intensity between\nmodules in a globally balanced manner. Additionally, to mitigate the high\nsensitivity of pruning decisions to calibration data quality, we propose a\nsimple yet effective KL divergence-based calibration data selection strategy,\nensuring more reliable and task-agnostic pruning outcomes. Comprehensive\nexperiments conducted on Llama3, Qwen, and T5 models demonstrate that NIRVANA\noutperforms existing structured pruning methods under equivalent sparsity\nconstraints, providing a theoretically sound and practical approach to LLM\ncompression. The code is available at\nhttps://github.com/iDEA-iSAIL-Lab-UIUC/NIRVANA."
                },
                "authors": [
                    {
                        "name": "Mengting Ai"
                    },
                    {
                        "name": "Tianxin Wei"
                    },
                    {
                        "name": "Sirui Chen"
                    },
                    {
                        "name": "Jingrui He"
                    }
                ],
                "author_detail": {
                    "name": "Jingrui He"
                },
                "author": "Jingrui He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14229v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14229v1",
                "updated": "2025-09-17T17:58:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    58,
                    28,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T17:58:28Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    58,
                    28,
                    2,
                    260,
                    0
                ],
                "title": "Spacing Test for Fused Lasso",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spacing Test for Fused Lasso"
                },
                "summary": "This study addresses the unresolved problem of selecting the regularization\nparameter in the fused lasso. In particular, we extend the framework of the\nSpacing Test proposed by Tibshirani et al. to the fused lasso, providing a\ntheoretical foundation for post-selection inference by characterizing the\nselection event as a polyhedral constraint. Based on the analysis of the\nsolution path of the fused lasso using a LARS-type algorithm, we derive exact\nconditional $p$-values for the selected change-points. Our method broadens the\napplicability of the Spacing Test from the standard lasso to fused penalty\nstructures. Furthermore, through numerical experiments comparing the proposed\nmethod with sequential versions of AIC and BIC as well as cross-validation, we\ndemonstrate that the proposed approach properly controls the type I error while\nachieving high detection power. This work offers a theoretically sound and\ncomputationally practical solution for parameter selection and post-selection\ninference in structured signal estimation problems. Keywords: Fused Lasso,\nRegularization parameter selection, Spacing Test for Lasso, Selective\ninference, Change-point detection",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study addresses the unresolved problem of selecting the regularization\nparameter in the fused lasso. In particular, we extend the framework of the\nSpacing Test proposed by Tibshirani et al. to the fused lasso, providing a\ntheoretical foundation for post-selection inference by characterizing the\nselection event as a polyhedral constraint. Based on the analysis of the\nsolution path of the fused lasso using a LARS-type algorithm, we derive exact\nconditional $p$-values for the selected change-points. Our method broadens the\napplicability of the Spacing Test from the standard lasso to fused penalty\nstructures. Furthermore, through numerical experiments comparing the proposed\nmethod with sequential versions of AIC and BIC as well as cross-validation, we\ndemonstrate that the proposed approach properly controls the type I error while\nachieving high detection power. This work offers a theoretically sound and\ncomputationally practical solution for parameter selection and post-selection\ninference in structured signal estimation problems. Keywords: Fused Lasso,\nRegularization parameter selection, Spacing Test for Lasso, Selective\ninference, Change-point detection"
                },
                "authors": [
                    {
                        "name": "Rieko Tasaka"
                    },
                    {
                        "name": "Tatsuya Kimura"
                    },
                    {
                        "name": "Joe Suzuki"
                    }
                ],
                "author_detail": {
                    "name": "Joe Suzuki"
                },
                "author": "Joe Suzuki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14229v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14229v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18614v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18614v4",
                "updated": "2025-09-18T08:19:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    8,
                    19,
                    20,
                    3,
                    261,
                    0
                ],
                "published": "2025-05-24T09:28:09Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    9,
                    28,
                    9,
                    5,
                    144,
                    0
                ],
                "title": "MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song\n  Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song\n  Translation"
                },
                "summary": "Lyrics translation requires both accurate semantic transfer and preservation\nof musical rhythm, syllabic structure, and poetic style. In animated musicals,\nthe challenge intensifies due to alignment with visual and auditory cues. We\nintroduce Multilingual Audio-Video Lyrics Benchmark for Animated Song\nTranslation (MAVL), the first multilingual, multimodal benchmark for singable\nlyrics translation. By integrating text, audio, and video, MAVL enables richer\nand more expressive translations than text-only approaches. Building on this,\nwe propose Syllable-Constrained Audio-Video LLM with Chain-of-Thought\nSylAVL-CoT, which leverages audio-video cues and enforces syllabic constraints\nto produce natural-sounding lyrics. Experimental results demonstrate that\nSylAVL-CoT significantly outperforms text-based models in singability and\ncontextual accuracy, emphasizing the value of multimodal, multilingual\napproaches for lyrics translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lyrics translation requires both accurate semantic transfer and preservation\nof musical rhythm, syllabic structure, and poetic style. In animated musicals,\nthe challenge intensifies due to alignment with visual and auditory cues. We\nintroduce Multilingual Audio-Video Lyrics Benchmark for Animated Song\nTranslation (MAVL), the first multilingual, multimodal benchmark for singable\nlyrics translation. By integrating text, audio, and video, MAVL enables richer\nand more expressive translations than text-only approaches. Building on this,\nwe propose Syllable-Constrained Audio-Video LLM with Chain-of-Thought\nSylAVL-CoT, which leverages audio-video cues and enforces syllabic constraints\nto produce natural-sounding lyrics. Experimental results demonstrate that\nSylAVL-CoT significantly outperforms text-based models in singability and\ncontextual accuracy, emphasizing the value of multimodal, multilingual\napproaches for lyrics translation."
                },
                "authors": [
                    {
                        "name": "Woohyun Cho"
                    },
                    {
                        "name": "Youngmin Kim"
                    },
                    {
                        "name": "Sunghyun Lee"
                    },
                    {
                        "name": "Youngjae Yu"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Yu"
                },
                "author": "Youngjae Yu",
                "arxiv_comment": "Accepted to EMNLP 2025, Project Page:\n  https://k1064190.github.io/papers/paper1.html, our codes and datasets are\n  available at https://github.com/k1064190/MAVL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18614v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18614v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14225v1",
                "updated": "2025-09-17T17:56:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    56,
                    20,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T17:56:20Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    56,
                    20,
                    2,
                    260,
                    0
                ],
                "title": "Defending Diffusion Models Against Membership Inference Attacks via\n  Higher-Order Langevin Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defending Diffusion Models Against Membership Inference Attacks via\n  Higher-Order Langevin Dynamics"
                },
                "summary": "Recent advances in generative artificial intelligence applications have\nraised new data security concerns. This paper focuses on defending diffusion\nmodels against membership inference attacks. This type of attack occurs when\nthe attacker can determine if a certain data point was used to train the model.\nAlthough diffusion models are intrinsically more resistant to membership\ninference attacks than other generative models, they are still susceptible. The\ndefense proposed here utilizes critically-damped higher-order Langevin\ndynamics, which introduces several auxiliary variables and a joint diffusion\nprocess along these variables. The idea is that the presence of auxiliary\nvariables mixes external randomness that helps to corrupt sensitive input data\nearlier on in the diffusion process. This concept is theoretically investigated\nand validated on a toy dataset and a speech dataset using the Area Under the\nReceiver Operating Characteristic (AUROC) curves and the FID metric.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in generative artificial intelligence applications have\nraised new data security concerns. This paper focuses on defending diffusion\nmodels against membership inference attacks. This type of attack occurs when\nthe attacker can determine if a certain data point was used to train the model.\nAlthough diffusion models are intrinsically more resistant to membership\ninference attacks than other generative models, they are still susceptible. The\ndefense proposed here utilizes critically-damped higher-order Langevin\ndynamics, which introduces several auxiliary variables and a joint diffusion\nprocess along these variables. The idea is that the presence of auxiliary\nvariables mixes external randomness that helps to corrupt sensitive input data\nearlier on in the diffusion process. This concept is theoretically investigated\nand validated on a toy dataset and a speech dataset using the Area Under the\nReceiver Operating Characteristic (AUROC) curves and the FID metric."
                },
                "authors": [
                    {
                        "name": "Benjamin Sterling"
                    },
                    {
                        "name": "Yousef El-Laham"
                    },
                    {
                        "name": "Mónica F. Bugallo"
                    }
                ],
                "author_detail": {
                    "name": "Mónica F. Bugallo"
                },
                "author": "Mónica F. Bugallo",
                "arxiv_comment": "5 pages, 2 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14221v1",
                "updated": "2025-09-17T17:53:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    53,
                    43,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T17:53:43Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    53,
                    43,
                    2,
                    260,
                    0
                ],
                "title": "GEM-Bench: A Benchmark for Ad-Injected Response Generation within\n  Generative Engine Marketing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEM-Bench: A Benchmark for Ad-Injected Response Generation within\n  Generative Engine Marketing"
                },
                "summary": "Generative Engine Marketing (GEM) is an emerging ecosystem for monetizing\ngenerative engines, such as LLM-based chatbots, by seamlessly integrating\nrelevant advertisements into their responses. At the core of GEM lies the\ngeneration and evaluation of ad-injected responses. However, existing\nbenchmarks are not specifically designed for this purpose, which limits future\nresearch. To address this gap, we propose GEM-Bench, the first comprehensive\nbenchmark for ad-injected response generation in GEM. GEM-Bench includes three\ncurated datasets covering both chatbot and search scenarios, a metric ontology\nthat captures multiple dimensions of user satisfaction and engagement, and\nseveral baseline solutions implemented within an extensible multi-agent\nframework. Our preliminary results indicate that, while simple prompt-based\nmethods achieve reasonable engagement such as click-through rate, they often\nreduce user satisfaction. In contrast, approaches that insert ads based on\npre-generated ad-free responses help mitigate this issue but introduce\nadditional overhead. These findings highlight the need for future research on\ndesigning more effective and efficient solutions for generating ad-injected\nresponses in GEM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Engine Marketing (GEM) is an emerging ecosystem for monetizing\ngenerative engines, such as LLM-based chatbots, by seamlessly integrating\nrelevant advertisements into their responses. At the core of GEM lies the\ngeneration and evaluation of ad-injected responses. However, existing\nbenchmarks are not specifically designed for this purpose, which limits future\nresearch. To address this gap, we propose GEM-Bench, the first comprehensive\nbenchmark for ad-injected response generation in GEM. GEM-Bench includes three\ncurated datasets covering both chatbot and search scenarios, a metric ontology\nthat captures multiple dimensions of user satisfaction and engagement, and\nseveral baseline solutions implemented within an extensible multi-agent\nframework. Our preliminary results indicate that, while simple prompt-based\nmethods achieve reasonable engagement such as click-through rate, they often\nreduce user satisfaction. In contrast, approaches that insert ads based on\npre-generated ad-free responses help mitigate this issue but introduce\nadditional overhead. These findings highlight the need for future research on\ndesigning more effective and efficient solutions for generating ad-injected\nresponses in GEM."
                },
                "authors": [
                    {
                        "name": "Silan Hu"
                    },
                    {
                        "name": "Shiqi Zhang"
                    },
                    {
                        "name": "Yimin Shi"
                    },
                    {
                        "name": "Xiaokui Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokui Xiao"
                },
                "author": "Xiaokui Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14218v1",
                "updated": "2025-09-17T17:51:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    51,
                    40,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T17:51:40Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    51,
                    40,
                    2,
                    260,
                    0
                ],
                "title": "Adaptive Off-Policy Inference for M-Estimators Under Model\n  Misspecification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Off-Policy Inference for M-Estimators Under Model\n  Misspecification"
                },
                "summary": "When data are collected adaptively, such as in bandit algorithms, classical\nstatistical approaches such as ordinary least squares and $M$-estimation will\noften fail to achieve asymptotic normality. Although recent lines of work have\nmodified the classical approaches to ensure valid inference on adaptively\ncollected data, most of these works assume that the model is correctly\nspecified. We propose a method that provides valid inference for M-estimators\nthat use adaptively collected bandit data with a (possibly) misspecified\nworking model. A key ingredient in our approach is the use of flexible machine\nlearning approaches to stabilize the variance induced by adaptive data\ncollection. A major novelty is that our procedure enables the construction of\nvalid confidence sets even in settings where treatment policies are unstable\nand non-converging, such as when there is no unique optimal arm and standard\nbandit algorithms are used. Empirical results on semi-synthetic datasets\nconstructed from the Osteoarthritis Initiative demonstrate that the method\nmaintains type I error control, while existing methods for inference in\nadaptive settings do not cover in the misspecified case.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When data are collected adaptively, such as in bandit algorithms, classical\nstatistical approaches such as ordinary least squares and $M$-estimation will\noften fail to achieve asymptotic normality. Although recent lines of work have\nmodified the classical approaches to ensure valid inference on adaptively\ncollected data, most of these works assume that the model is correctly\nspecified. We propose a method that provides valid inference for M-estimators\nthat use adaptively collected bandit data with a (possibly) misspecified\nworking model. A key ingredient in our approach is the use of flexible machine\nlearning approaches to stabilize the variance induced by adaptive data\ncollection. A major novelty is that our procedure enables the construction of\nvalid confidence sets even in settings where treatment policies are unstable\nand non-converging, such as when there is no unique optimal arm and standard\nbandit algorithms are used. Empirical results on semi-synthetic datasets\nconstructed from the Osteoarthritis Initiative demonstrate that the method\nmaintains type I error control, while existing methods for inference in\nadaptive settings do not cover in the misspecified case."
                },
                "authors": [
                    {
                        "name": "James Leiner"
                    },
                    {
                        "name": "Robin Dunn"
                    },
                    {
                        "name": "Aaditya Ramdas"
                    }
                ],
                "author_detail": {
                    "name": "Aaditya Ramdas"
                },
                "author": "Aaditya Ramdas",
                "arxiv_comment": "36 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14216v1",
                "updated": "2025-09-17T17:50:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    50,
                    59,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T17:50:59Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    50,
                    59,
                    2,
                    260,
                    0
                ],
                "title": "A Universal Banach--Bregman Framework for Stochastic Iterations:\n  Unifying Stochastic Mirror Descent, Learning and LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Universal Banach--Bregman Framework for Stochastic Iterations:\n  Unifying Stochastic Mirror Descent, Learning and LLM Training"
                },
                "summary": "Stochastic optimization powers the scalability of modern artificial\nintelligence, spanning machine learning, deep learning, reinforcement learning,\nand large language model training. Yet, existing theory remains largely\nconfined to Hilbert spaces, relying on inner-product frameworks and\northogonality. This paradigm fails to capture non-Euclidean settings, such as\nmirror descent on simplices, Bregman proximal methods for sparse learning,\nnatural gradient descent in information geometry, or\nKullback--Leibler-regularized language model training. Unlike Euclidean-based\nHilbert-space methods, this approach embraces general Banach spaces. This work\nintroduces a pioneering Banach--Bregman framework for stochastic iterations,\nestablishing Bregman geometry as a foundation for next-generation optimization.\nIt (i) provides a unified template via Bregman projections and Bregman--Fejer\nmonotonicity, encompassing stochastic approximation, mirror descent, natural\ngradient, adaptive methods, and mirror-prox; (ii) establishes super-relaxations\n($\\lambda > 2$) in non-Hilbert settings, enabling flexible geometries and\nelucidating their acceleration effect; and (iii) delivers convergence theorems\nspanning almost-sure boundedness to geometric rates, validated on synthetic and\nreal-world tasks. Empirical studies across machine learning (UCI benchmarks),\ndeep learning (e.g., Transformer training), reinforcement learning\n(actor--critic), and large language models (WikiText-2 with distilGPT-2) show\nup to 20% faster convergence, reduced variance, and enhanced accuracy over\nclassical baselines. These results position Banach--Bregman geometry as a\ncornerstone unifying optimization theory and practice across core AI paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic optimization powers the scalability of modern artificial\nintelligence, spanning machine learning, deep learning, reinforcement learning,\nand large language model training. Yet, existing theory remains largely\nconfined to Hilbert spaces, relying on inner-product frameworks and\northogonality. This paradigm fails to capture non-Euclidean settings, such as\nmirror descent on simplices, Bregman proximal methods for sparse learning,\nnatural gradient descent in information geometry, or\nKullback--Leibler-regularized language model training. Unlike Euclidean-based\nHilbert-space methods, this approach embraces general Banach spaces. This work\nintroduces a pioneering Banach--Bregman framework for stochastic iterations,\nestablishing Bregman geometry as a foundation for next-generation optimization.\nIt (i) provides a unified template via Bregman projections and Bregman--Fejer\nmonotonicity, encompassing stochastic approximation, mirror descent, natural\ngradient, adaptive methods, and mirror-prox; (ii) establishes super-relaxations\n($\\lambda > 2$) in non-Hilbert settings, enabling flexible geometries and\nelucidating their acceleration effect; and (iii) delivers convergence theorems\nspanning almost-sure boundedness to geometric rates, validated on synthetic and\nreal-world tasks. Empirical studies across machine learning (UCI benchmarks),\ndeep learning (e.g., Transformer training), reinforcement learning\n(actor--critic), and large language models (WikiText-2 with distilGPT-2) show\nup to 20% faster convergence, reduced variance, and enhanced accuracy over\nclassical baselines. These results position Banach--Bregman geometry as a\ncornerstone unifying optimization theory and practice across core AI paradigms."
                },
                "authors": [
                    {
                        "name": "Johnny R. Zhang"
                    },
                    {
                        "name": "Xiaomei Mi"
                    },
                    {
                        "name": "Gaoyuan Du"
                    },
                    {
                        "name": "Qianyi Sun"
                    },
                    {
                        "name": "Shiqi Wang"
                    },
                    {
                        "name": "Jiaxuan Li"
                    },
                    {
                        "name": "Wenhua Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wenhua Zhou"
                },
                "arxiv_affiliation": "Independent Researcher",
                "author": "Wenhua Zhou",
                "arxiv_comment": "69 pages, 10 figures. Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23804v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23804v2",
                "updated": "2025-09-17T17:39:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    39,
                    16,
                    2,
                    260,
                    0
                ],
                "published": "2025-05-27T01:01:55Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    1,
                    1,
                    55,
                    1,
                    147,
                    0
                ],
                "title": "Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause\n  Frequencies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause\n  Frequencies"
                },
                "summary": "While large language models (LLMs) achieve strong performance on text-to-SQL\nparsing, they sometimes exhibit unexpected failures in which they are\nconfidently incorrect. Building trustworthy text-to-SQL systems thus requires\neliciting reliable uncertainty measures from the LLM. In this paper, we study\nthe problem of providing a calibrated confidence score that conveys the\nlikelihood of an output query being correct. Our work is the first to establish\na benchmark for post-hoc calibration of LLM-based text-to-SQL parsing. In\nparticular, we show that Platt scaling, a canonical method for calibration,\nprovides substantial improvements over directly using raw model output\nprobabilities as confidence scores. Furthermore, we propose a method for\ntext-to-SQL calibration that leverages the structured nature of SQL queries to\nprovide more granular signals of correctness, named \"sub-clause frequency\"\n(SCF) scores. Using multivariate Platt scaling (MPS), our extension of the\ncanonical Platt scaling technique, we combine individual SCF scores into an\noverall accurate and calibrated score. Empirical evaluation on two popular\ntext-to-SQL datasets shows that our approach of combining MPS and SCF yields\nfurther improvements in calibration and the related task of error detection\nover traditional Platt scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) achieve strong performance on text-to-SQL\nparsing, they sometimes exhibit unexpected failures in which they are\nconfidently incorrect. Building trustworthy text-to-SQL systems thus requires\neliciting reliable uncertainty measures from the LLM. In this paper, we study\nthe problem of providing a calibrated confidence score that conveys the\nlikelihood of an output query being correct. Our work is the first to establish\na benchmark for post-hoc calibration of LLM-based text-to-SQL parsing. In\nparticular, we show that Platt scaling, a canonical method for calibration,\nprovides substantial improvements over directly using raw model output\nprobabilities as confidence scores. Furthermore, we propose a method for\ntext-to-SQL calibration that leverages the structured nature of SQL queries to\nprovide more granular signals of correctness, named \"sub-clause frequency\"\n(SCF) scores. Using multivariate Platt scaling (MPS), our extension of the\ncanonical Platt scaling technique, we combine individual SCF scores into an\noverall accurate and calibrated score. Empirical evaluation on two popular\ntext-to-SQL datasets shows that our approach of combining MPS and SCF yields\nfurther improvements in calibration and the related task of error detection\nover traditional Platt scaling."
                },
                "authors": [
                    {
                        "name": "Terrance Liu"
                    },
                    {
                        "name": "Shuyi Wang"
                    },
                    {
                        "name": "Daniel Preotiuc-Pietro"
                    },
                    {
                        "name": "Yash Chandarana"
                    },
                    {
                        "name": "Chirag Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Gupta"
                },
                "author": "Chirag Gupta",
                "arxiv_comment": "EMNLP 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23804v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23804v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14206v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14206v1",
                "updated": "2025-09-17T17:37:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    37,
                    25,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T17:37:25Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    37,
                    25,
                    2,
                    260,
                    0
                ],
                "title": "Looking into the faintEst WIth MUSE (LEWIS): Exploring the nature of\n  ultra-diffuse galaxies in the Hydra-I cluster IV. A study of the Globular\n  Cluster population in four UDGs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Looking into the faintEst WIth MUSE (LEWIS): Exploring the nature of\n  ultra-diffuse galaxies in the Hydra-I cluster IV. A study of the Globular\n  Cluster population in four UDGs"
                },
                "summary": "As old stellar systems, globular clusters (GCs) are key fossil tracers of\ngalaxy formation and interaction histories. This paper is part of the LEWIS\nproject, an integral-field spectroscopic survey of ultra-diffuse galaxies\n(UDGs) in the Hydra I cluster. We use MUSE spectroscopy and new VIRCAM $H$-band\nimaging data to study the GC populations and dark matter content in four dwarf\ngalaxies. We retrieved line-of-sight velocities for all sources in the observed\nMUSE fields. Since the spectroscopic measurements are limited to relatively\nbright sources, we developed a multi-band photometric procedure to identify\nadditional GC candidates too faint for spectroscopic confirmation. GC\ncandidates were selected using a combination of photometric properties and\nmorphometric criteria. Additionally, the $H$-band observations were used to\nconstrain the stellar masses of the studied galaxies. Based on the\nspectroscopic classification, we confirm one GC in UDG3, two in UDG7, and four\nin UDG11, while UDG9 has no spectroscopically confirmed bright GCs. We identify\nfour intra-cluster GCs in the vicinity of UDG3 and UDG11, and one ultra-compact\ndwarf with a radial velocity only $\\Delta v = -85 \\pm 10\\mathrm{km\\ s^{-1}}$\nrelative to UDG7, suggesting it may be bound to it. Considering completeness\ncorrections and accounting for possible contamination, from photometry we\nestimate that the number of GCs ranges between 0 and $\\sim40$ for the\ninvestigated UDGs. Their specific frequencies suggest that three out of four\nUDGs are either GC-rich, similar to those in the Coma cluster, or belong to an\nintermediate population as seen in the Perseus cluster. Dark matter content\nestimates, inferred from GC counts and stellar mass, indicate that these\ngalaxies are dark-matter dominated, with dynamical-to-stellar mass ratios of\n$M_{\\mathrm{dyn}} / M_\\star \\sim 10-1000$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As old stellar systems, globular clusters (GCs) are key fossil tracers of\ngalaxy formation and interaction histories. This paper is part of the LEWIS\nproject, an integral-field spectroscopic survey of ultra-diffuse galaxies\n(UDGs) in the Hydra I cluster. We use MUSE spectroscopy and new VIRCAM $H$-band\nimaging data to study the GC populations and dark matter content in four dwarf\ngalaxies. We retrieved line-of-sight velocities for all sources in the observed\nMUSE fields. Since the spectroscopic measurements are limited to relatively\nbright sources, we developed a multi-band photometric procedure to identify\nadditional GC candidates too faint for spectroscopic confirmation. GC\ncandidates were selected using a combination of photometric properties and\nmorphometric criteria. Additionally, the $H$-band observations were used to\nconstrain the stellar masses of the studied galaxies. Based on the\nspectroscopic classification, we confirm one GC in UDG3, two in UDG7, and four\nin UDG11, while UDG9 has no spectroscopically confirmed bright GCs. We identify\nfour intra-cluster GCs in the vicinity of UDG3 and UDG11, and one ultra-compact\ndwarf with a radial velocity only $\\Delta v = -85 \\pm 10\\mathrm{km\\ s^{-1}}$\nrelative to UDG7, suggesting it may be bound to it. Considering completeness\ncorrections and accounting for possible contamination, from photometry we\nestimate that the number of GCs ranges between 0 and $\\sim40$ for the\ninvestigated UDGs. Their specific frequencies suggest that three out of four\nUDGs are either GC-rich, similar to those in the Coma cluster, or belong to an\nintermediate population as seen in the Perseus cluster. Dark matter content\nestimates, inferred from GC counts and stellar mass, indicate that these\ngalaxies are dark-matter dominated, with dynamical-to-stellar mass ratios of\n$M_{\\mathrm{dyn}} / M_\\star \\sim 10-1000$."
                },
                "authors": [
                    {
                        "name": "Marco Mirabile"
                    },
                    {
                        "name": "Michele Cantiello"
                    },
                    {
                        "name": "Marina Rejkuba"
                    },
                    {
                        "name": "Steffen Mieske"
                    },
                    {
                        "name": "Enrichetta Iodice"
                    },
                    {
                        "name": "Chiara Buttitta"
                    },
                    {
                        "name": "Maria Luisa Buzzo"
                    },
                    {
                        "name": "Johanna Hartke"
                    },
                    {
                        "name": "Goran Doll"
                    },
                    {
                        "name": "Luca Rossi"
                    },
                    {
                        "name": "Magda Arnaboldi"
                    },
                    {
                        "name": "Marica Branchesi"
                    },
                    {
                        "name": "Giuseppe D'Ago"
                    },
                    {
                        "name": "Jesus Falcon-Barroso"
                    },
                    {
                        "name": "Katja Fahrion"
                    },
                    {
                        "name": "Duncan A. Forbes"
                    },
                    {
                        "name": "Marco Gullieuszik"
                    },
                    {
                        "name": "Michael Hilker"
                    },
                    {
                        "name": "Felipe S. Lohmann"
                    },
                    {
                        "name": "Maurizio Paolillo"
                    },
                    {
                        "name": "Gabriele Riccio"
                    },
                    {
                        "name": "Tom Richtler"
                    },
                    {
                        "name": "Marilena Spavone"
                    }
                ],
                "author_detail": {
                    "name": "Marilena Spavone"
                },
                "author": "Marilena Spavone",
                "arxiv_comment": "25 pages, 24 figures, 7 tables. Accepted for publication in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14206v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14204v1",
                "updated": "2025-09-17T17:36:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    36,
                    17,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T17:36:17Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    36,
                    17,
                    2,
                    260,
                    0
                ],
                "title": "Large deviations for probability graphons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large deviations for probability graphons"
                },
                "summary": "We establish a large deviation principle (LDP) for probability graphons,\nwhich are symmetric functions from the unit square into the space of\nprobability measures. This notion extends classical graphons and provides a\nflexible framework for studying the limit behavior of large dense weighted\ngraphs. In particular, our result generalizes the seminal work of Chatterjee\nand Varadhan (2011), who derived an LDP for Erd\\H{o}s-R\\'enyi random graphs via\ngraphon theory. We move beyond their binary (Bernoulli) setting to encompass\narbitrary edge-weight distributions. Specifically, we analyze the distribution\non probability graphons induced by random weighted graphs in which edges are\nsampled independently from a common reference probability measure supported on\na compact Polish space. We prove that this distribution satisfies an LDP with a\ngood rate function, expressed as an extension of the Kullback-Leibler\ndivergence between probability graphons and the reference measure. This theorem\ncan also be viewed as a Sanov-type result in the graphon setting. Our work\nprovides a rigorous foundation for analyzing rare events in weighted networks\nand supports statistical inference in structured random graph models under\ndistributional edge uncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We establish a large deviation principle (LDP) for probability graphons,\nwhich are symmetric functions from the unit square into the space of\nprobability measures. This notion extends classical graphons and provides a\nflexible framework for studying the limit behavior of large dense weighted\ngraphs. In particular, our result generalizes the seminal work of Chatterjee\nand Varadhan (2011), who derived an LDP for Erd\\H{o}s-R\\'enyi random graphs via\ngraphon theory. We move beyond their binary (Bernoulli) setting to encompass\narbitrary edge-weight distributions. Specifically, we analyze the distribution\non probability graphons induced by random weighted graphs in which edges are\nsampled independently from a common reference probability measure supported on\na compact Polish space. We prove that this distribution satisfies an LDP with a\ngood rate function, expressed as an extension of the Kullback-Leibler\ndivergence between probability graphons and the reference measure. This theorem\ncan also be viewed as a Sanov-type result in the graphon setting. Our work\nprovides a rigorous foundation for analyzing rare events in weighted networks\nand supports statistical inference in structured random graph models under\ndistributional edge uncertainty."
                },
                "authors": [
                    {
                        "name": "Pierfrancesco Dionigi"
                    },
                    {
                        "name": "Giulio Zucal"
                    }
                ],
                "author_detail": {
                    "name": "Giulio Zucal"
                },
                "author": "Giulio Zucal",
                "arxiv_comment": "Preprint, comments are very welcome. 55 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.FA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "05C80, 60F10 (Primary) 60B20, 60B10, 60C05, 28A33 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14201v1",
                "updated": "2025-09-17T17:35:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    35,
                    7,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T17:35:07Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    35,
                    7,
                    2,
                    260,
                    0
                ],
                "title": "Active Inference Framework for Closed-Loop Sensing, Communication, and\n  Control in UAV Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Inference Framework for Closed-Loop Sensing, Communication, and\n  Control in UAV Systems"
                },
                "summary": "Integrated sensing and communication (ISAC) is a core technology for 6G, and\nits application to closed-loop sensing, communication, and control (SCC)\nenables various services. Existing SCC solutions often treat sensing and\ncontrol separately, leading to suboptimal performance and resource usage. In\nthis work, we introduce the active inference framework (AIF) into SCC-enabled\nunmanned aerial vehicle (UAV) systems for joint state estimation, control, and\nsensing resource allocation. By formulating a unified generative model, the\nproblem reduces to minimizing variational free energy for inference and\nexpected free energy for action planning. Simulation results show that both\ncontrol cost and sensing cost are reduced relative to baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated sensing and communication (ISAC) is a core technology for 6G, and\nits application to closed-loop sensing, communication, and control (SCC)\nenables various services. Existing SCC solutions often treat sensing and\ncontrol separately, leading to suboptimal performance and resource usage. In\nthis work, we introduce the active inference framework (AIF) into SCC-enabled\nunmanned aerial vehicle (UAV) systems for joint state estimation, control, and\nsensing resource allocation. By formulating a unified generative model, the\nproblem reduces to minimizing variational free energy for inference and\nexpected free energy for action planning. Simulation results show that both\ncontrol cost and sensing cost are reduced relative to baselines."
                },
                "authors": [
                    {
                        "name": "Guangjin Pan"
                    },
                    {
                        "name": "Liping Bai"
                    },
                    {
                        "name": "Zhuojun Tian"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Mehdi Bennis"
                    },
                    {
                        "name": "Henk Wymeersch"
                    }
                ],
                "author_detail": {
                    "name": "Henk Wymeersch"
                },
                "author": "Henk Wymeersch",
                "arxiv_comment": "5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14197v1",
                "updated": "2025-09-17T17:31:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    31,
                    57,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T17:31:57Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    31,
                    57,
                    2,
                    260,
                    0
                ],
                "title": "Framing Migration: A Computational Analysis of UK Parliamentary\n  Discourse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Framing Migration: A Computational Analysis of UK Parliamentary\n  Discourse"
                },
                "summary": "We present a large-scale computational analysis of migration-related\ndiscourse in UK parliamentary debates spanning over 75 years and compare it\nwith US congressional discourse. Using open-weight LLMs, we annotate each\nstatement with high-level stances toward migrants and track the net tone toward\nmigrants across time and political parties. For the UK, we extend this with a\nsemi-automated framework for extracting fine-grained narrative frames to\ncapture nuances of migration discourse. Our findings show that, while US\ndiscourse has grown increasingly polarised, UK parliamentary attitudes remain\nrelatively aligned across parties, with a persistent ideological gap between\nLabour and the Conservatives, reaching its most negative level in 2025. The\nanalysis of narrative frames in the UK parliamentary statements reveals a shift\ntoward securitised narratives such as border control and illegal immigration,\nwhile longer-term integration-oriented frames such as social integration have\ndeclined. Moreover, discussions of national law about immigration have been\nreplaced over time by international law and human rights, revealing nuances in\ndiscourse trends. Taken together broadly, our findings demonstrate how LLMs can\nsupport scalable, fine-grained discourse analysis in political and historical\ncontexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a large-scale computational analysis of migration-related\ndiscourse in UK parliamentary debates spanning over 75 years and compare it\nwith US congressional discourse. Using open-weight LLMs, we annotate each\nstatement with high-level stances toward migrants and track the net tone toward\nmigrants across time and political parties. For the UK, we extend this with a\nsemi-automated framework for extracting fine-grained narrative frames to\ncapture nuances of migration discourse. Our findings show that, while US\ndiscourse has grown increasingly polarised, UK parliamentary attitudes remain\nrelatively aligned across parties, with a persistent ideological gap between\nLabour and the Conservatives, reaching its most negative level in 2025. The\nanalysis of narrative frames in the UK parliamentary statements reveals a shift\ntoward securitised narratives such as border control and illegal immigration,\nwhile longer-term integration-oriented frames such as social integration have\ndeclined. Moreover, discussions of national law about immigration have been\nreplaced over time by international law and human rights, revealing nuances in\ndiscourse trends. Taken together broadly, our findings demonstrate how LLMs can\nsupport scalable, fine-grained discourse analysis in political and historical\ncontexts."
                },
                "authors": [
                    {
                        "name": "Vahid Ghafouri"
                    },
                    {
                        "name": "Robert McNeil"
                    },
                    {
                        "name": "Teodor Yankov"
                    },
                    {
                        "name": "Madeleine Sumption"
                    },
                    {
                        "name": "Luc Rocher"
                    },
                    {
                        "name": "Scott A. Hale"
                    },
                    {
                        "name": "Adam Mahdi"
                    }
                ],
                "author_detail": {
                    "name": "Adam Mahdi"
                },
                "author": "Adam Mahdi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14189v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14189v2",
                "updated": "2025-09-18T01:04:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    1,
                    4,
                    39,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-17T17:27:12Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    27,
                    12,
                    2,
                    260,
                    0
                ],
                "title": "AI and the Future of Academic Peer Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI and the Future of Academic Peer Review"
                },
                "summary": "Peer review remains the central quality-control mechanism of science, yet its\nability to fulfill this role is increasingly strained. Empirical studies\ndocument serious shortcomings: long publication delays, escalating reviewer\nburden concentrated on a small minority of scholars, inconsistent quality and\nlow inter-reviewer agreement, and systematic biases by gender, language, and\ninstitutional prestige. Decades of human-centered reforms have yielded only\nmarginal improvements. Meanwhile, artificial intelligence, especially large\nlanguage models (LLMs), is being piloted across the peer-review pipeline by\njournals, funders, and individual reviewers. Early studies suggest that AI\nassistance can produce reviews comparable in quality to humans, accelerate\nreviewer selection and feedback, and reduce certain biases, but also raise\ndistinctive concerns about hallucination, confidentiality, gaming, novelty\nrecognition, and loss of trust. In this paper, we map the aims and persistent\nfailure modes of peer review to specific LLM applications and systematically\nanalyze the objections they raise alongside safeguards that could make their\nuse acceptable. Drawing on emerging evidence, we show that targeted, supervised\nLLM assistance can plausibly improve error detection, timeliness, and reviewer\nworkload without displacing human judgment. We highlight advanced\narchitectures, including fine-tuned, retrieval-augmented, and multi-agent\nsystems, that may enable more reliable, auditable, and interdisciplinary\nreview. We argue that ethical and practical considerations are not peripheral\nbut constitutive: the legitimacy of AI-assisted peer review depends on\ngovernance choices as much as technical capacity. The path forward is neither\nuncritical adoption nor reflexive rejection, but carefully scoped pilots with\nexplicit evaluation metrics, transparency, and accountability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peer review remains the central quality-control mechanism of science, yet its\nability to fulfill this role is increasingly strained. Empirical studies\ndocument serious shortcomings: long publication delays, escalating reviewer\nburden concentrated on a small minority of scholars, inconsistent quality and\nlow inter-reviewer agreement, and systematic biases by gender, language, and\ninstitutional prestige. Decades of human-centered reforms have yielded only\nmarginal improvements. Meanwhile, artificial intelligence, especially large\nlanguage models (LLMs), is being piloted across the peer-review pipeline by\njournals, funders, and individual reviewers. Early studies suggest that AI\nassistance can produce reviews comparable in quality to humans, accelerate\nreviewer selection and feedback, and reduce certain biases, but also raise\ndistinctive concerns about hallucination, confidentiality, gaming, novelty\nrecognition, and loss of trust. In this paper, we map the aims and persistent\nfailure modes of peer review to specific LLM applications and systematically\nanalyze the objections they raise alongside safeguards that could make their\nuse acceptable. Drawing on emerging evidence, we show that targeted, supervised\nLLM assistance can plausibly improve error detection, timeliness, and reviewer\nworkload without displacing human judgment. We highlight advanced\narchitectures, including fine-tuned, retrieval-augmented, and multi-agent\nsystems, that may enable more reliable, auditable, and interdisciplinary\nreview. We argue that ethical and practical considerations are not peripheral\nbut constitutive: the legitimacy of AI-assisted peer review depends on\ngovernance choices as much as technical capacity. The path forward is neither\nuncritical adoption nor reflexive rejection, but carefully scoped pilots with\nexplicit evaluation metrics, transparency, and accountability."
                },
                "authors": [
                    {
                        "name": "Sebastian Porsdam Mann"
                    },
                    {
                        "name": "Mateo Aboy"
                    },
                    {
                        "name": "Joel Jiehao Seah"
                    },
                    {
                        "name": "Zhicheng Lin"
                    },
                    {
                        "name": "Xufei Luo"
                    },
                    {
                        "name": "Daniel Rodger"
                    },
                    {
                        "name": "Hazem Zohny"
                    },
                    {
                        "name": "Timo Minssen"
                    },
                    {
                        "name": "Julian Savulescu"
                    },
                    {
                        "name": "Brian D. Earp"
                    }
                ],
                "author_detail": {
                    "name": "Brian D. Earp"
                },
                "author": "Brian D. Earp",
                "arxiv_comment": "34 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14189v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14189v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14187v1",
                "updated": "2025-09-17T17:26:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    26,
                    29,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T17:26:29Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    26,
                    29,
                    2,
                    260,
                    0
                ],
                "title": "Read to Hear: A Zero-Shot Pronunciation Assessment Using Textual\n  Descriptions and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read to Hear: A Zero-Shot Pronunciation Assessment Using Textual\n  Descriptions and LLMs"
                },
                "summary": "Automatic pronunciation assessment is typically performed by acoustic models\ntrained on audio-score pairs. Although effective, these systems provide only\nnumerical scores, without the information needed to help learners understand\ntheir errors. Meanwhile, large language models (LLMs) have proven effective in\nsupporting language learning, but their potential for assessing pronunciation\nremains unexplored. In this work, we introduce TextPA, a zero-shot, Textual\ndescription-based Pronunciation Assessment approach. TextPA utilizes\nhuman-readable representations of speech signals, which are fed into an LLM to\nassess pronunciation accuracy and fluency, while also providing reasoning\nbehind the assigned scores. Finally, a phoneme sequence match scoring method is\nused to refine the accuracy scores. Our work highlights a previously overlooked\ndirection for pronunciation assessment. Instead of relying on supervised\ntraining with audio-score examples, we exploit the rich pronunciation knowledge\nembedded in written text. Experimental results show that our approach is both\ncost-efficient and competitive in performance. Furthermore, TextPA\nsignificantly improves the performance of conventional audio-score-trained\nmodels on out-of-domain data by offering a complementary perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic pronunciation assessment is typically performed by acoustic models\ntrained on audio-score pairs. Although effective, these systems provide only\nnumerical scores, without the information needed to help learners understand\ntheir errors. Meanwhile, large language models (LLMs) have proven effective in\nsupporting language learning, but their potential for assessing pronunciation\nremains unexplored. In this work, we introduce TextPA, a zero-shot, Textual\ndescription-based Pronunciation Assessment approach. TextPA utilizes\nhuman-readable representations of speech signals, which are fed into an LLM to\nassess pronunciation accuracy and fluency, while also providing reasoning\nbehind the assigned scores. Finally, a phoneme sequence match scoring method is\nused to refine the accuracy scores. Our work highlights a previously overlooked\ndirection for pronunciation assessment. Instead of relying on supervised\ntraining with audio-score examples, we exploit the rich pronunciation knowledge\nembedded in written text. Experimental results show that our approach is both\ncost-efficient and competitive in performance. Furthermore, TextPA\nsignificantly improves the performance of conventional audio-score-trained\nmodels on out-of-domain data by offering a complementary perspective."
                },
                "authors": [
                    {
                        "name": "Yu-Wen Chen"
                    },
                    {
                        "name": "Melody Ma"
                    },
                    {
                        "name": "Julia Hirschberg"
                    }
                ],
                "author_detail": {
                    "name": "Julia Hirschberg"
                },
                "author": "Julia Hirschberg",
                "arxiv_comment": "EMNLP 2025 MainConference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06207v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06207v2",
                "updated": "2025-09-17T17:21:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    21,
                    24,
                    2,
                    260,
                    0
                ],
                "published": "2024-11-09T15:12:28Z",
                "published_parsed": [
                    2024,
                    11,
                    9,
                    15,
                    12,
                    28,
                    5,
                    314,
                    0
                ],
                "title": "KBM: Delineating Knowledge Boundary for Adaptive Retrieval in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KBM: Delineating Knowledge Boundary for Adaptive Retrieval in Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) often struggle with dynamically changing\nknowledge and handling unknown static information. Retrieval-Augmented\nGeneration (RAG) is employed to tackle these challenges and has a significant\nimpact on improving LLM performance. In fact, we find that not all questions\nneed to trigger RAG. By retrieving parts of knowledge unknown to the LLM and\nallowing the LLM to answer the rest, we can effectively reduce both time and\ncomputational costs. In our work, we propose a Knowledge Boundary Model (KBM)\nto express the known/unknown of a given question, and to determine whether a\nRAG needs to be triggered. Experiments conducted on 11 English and Chinese\ndatasets illustrate that the KBM effectively delineates the knowledge boundary,\nsignificantly decreasing the proportion of retrievals required for optimal\nend-to-end performance. Furthermore, we evaluate the effectiveness of KBM in\nthree complex scenarios: dynamic knowledge, long-tail static knowledge, and\nmulti-hop problems, as well as its functionality as an external LLM plug-in.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often struggle with dynamically changing\nknowledge and handling unknown static information. Retrieval-Augmented\nGeneration (RAG) is employed to tackle these challenges and has a significant\nimpact on improving LLM performance. In fact, we find that not all questions\nneed to trigger RAG. By retrieving parts of knowledge unknown to the LLM and\nallowing the LLM to answer the rest, we can effectively reduce both time and\ncomputational costs. In our work, we propose a Knowledge Boundary Model (KBM)\nto express the known/unknown of a given question, and to determine whether a\nRAG needs to be triggered. Experiments conducted on 11 English and Chinese\ndatasets illustrate that the KBM effectively delineates the knowledge boundary,\nsignificantly decreasing the proportion of retrievals required for optimal\nend-to-end performance. Furthermore, we evaluate the effectiveness of KBM in\nthree complex scenarios: dynamic knowledge, long-tail static knowledge, and\nmulti-hop problems, as well as its functionality as an external LLM plug-in."
                },
                "authors": [
                    {
                        "name": "Zhen Zhang"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Zile Qiao"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Guangyu Li"
                    },
                    {
                        "name": "Feiteng Mu"
                    },
                    {
                        "name": "Mengting Hu"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Huang"
                },
                "author": "Fei Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06207v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06207v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01153v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01153v4",
                "updated": "2025-09-17T17:16:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    16,
                    11,
                    2,
                    260,
                    0
                ],
                "published": "2025-04-01T19:36:14Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    36,
                    14,
                    1,
                    91,
                    0
                ],
                "title": "Catch Me if You Search: When Contextual Web Search Results Affect the\n  Detection of Hallucinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Catch Me if You Search: When Contextual Web Search Results Affect the\n  Detection of Hallucinations"
                },
                "summary": "While we increasingly rely on large language models (LLMs) for various tasks,\nthese models are known to produce inaccurate content or 'hallucinations' with\npotentially disastrous consequences. The recent integration of web search\nresults into LLMs prompts the question of whether people utilize them to verify\nthe generated content, thereby accurately detecting hallucinations. An online\nexperiment (N=560) investigated how the provision of search results, either\nstatic (i.e., fixed search results provided by LLM) or dynamic (i.e.,\nparticipant-led searches), affects participants' perceived accuracy of\nLLM-generated content (i.e., genuine, minor hallucination, major\nhallucination), self-confidence in accuracy ratings, as well as their overall\nevaluation of the LLM, as compared to the control condition (i.e., no search\nresults). Results showed that participants in both static and dynamic\nconditions (vs. control) rated hallucinated content to be less accurate and\nperceived the LLM more negatively. However, those in the dynamic condition\nrated genuine content as more accurate and demonstrated greater overall\nself-confidence in their assessments than those in the static search or control\nconditions. We highlighted practical implications of incorporating web search\nfunctionality into LLMs in real-world contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While we increasingly rely on large language models (LLMs) for various tasks,\nthese models are known to produce inaccurate content or 'hallucinations' with\npotentially disastrous consequences. The recent integration of web search\nresults into LLMs prompts the question of whether people utilize them to verify\nthe generated content, thereby accurately detecting hallucinations. An online\nexperiment (N=560) investigated how the provision of search results, either\nstatic (i.e., fixed search results provided by LLM) or dynamic (i.e.,\nparticipant-led searches), affects participants' perceived accuracy of\nLLM-generated content (i.e., genuine, minor hallucination, major\nhallucination), self-confidence in accuracy ratings, as well as their overall\nevaluation of the LLM, as compared to the control condition (i.e., no search\nresults). Results showed that participants in both static and dynamic\nconditions (vs. control) rated hallucinated content to be less accurate and\nperceived the LLM more negatively. However, those in the dynamic condition\nrated genuine content as more accurate and demonstrated greater overall\nself-confidence in their assessments than those in the static search or control\nconditions. We highlighted practical implications of incorporating web search\nfunctionality into LLMs in real-world contexts."
                },
                "authors": [
                    {
                        "name": "Mahjabin Nahar"
                    },
                    {
                        "name": "Eun-Ju Lee"
                    },
                    {
                        "name": "Jin Won Park"
                    },
                    {
                        "name": "Dongwon Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongwon Lee"
                },
                "author": "Dongwon Lee",
                "arxiv_comment": "Accepted to Computers in Human Behavior",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01153v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01153v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14180v1",
                "updated": "2025-09-17T17:12:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    12,
                    38,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T17:12:38Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    12,
                    38,
                    2,
                    260,
                    0
                ],
                "title": "Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation\n  Framework for Personal Finance LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation\n  Framework for Personal Finance LLMs"
                },
                "summary": "Personalized financial advice requires consideration of user goals,\nconstraints, risk tolerance, and jurisdiction. Prior LLM work has focused on\nsupport systems for investors and financial planners. Simultaneously, numerous\nrecent studies examine broader personal finance tasks, including budgeting,\ndebt management, retirement, and estate planning, through agentic pipelines\nthat incur high maintenance costs, yielding less than 25% of their expected\nfinancial returns. In this study, we introduce a novel and reproducible\nframework that integrates relevant financial context with behavioral finance\nstudies to construct supervision data for end-to-end advisors. Using this\nframework, we create a 19k sample reasoning dataset and conduct a comprehensive\nfine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test\nsplit and a blind LLM-jury study, we demonstrate that through careful data\ncuration and behavioral integration, our 8B model achieves performance\ncomparable to significantly larger baselines (14-32B parameters) across factual\naccuracy, fluency, and personalization metrics while incurring 80% lower costs\nthan the larger counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized financial advice requires consideration of user goals,\nconstraints, risk tolerance, and jurisdiction. Prior LLM work has focused on\nsupport systems for investors and financial planners. Simultaneously, numerous\nrecent studies examine broader personal finance tasks, including budgeting,\ndebt management, retirement, and estate planning, through agentic pipelines\nthat incur high maintenance costs, yielding less than 25% of their expected\nfinancial returns. In this study, we introduce a novel and reproducible\nframework that integrates relevant financial context with behavioral finance\nstudies to construct supervision data for end-to-end advisors. Using this\nframework, we create a 19k sample reasoning dataset and conduct a comprehensive\nfine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test\nsplit and a blind LLM-jury study, we demonstrate that through careful data\ncuration and behavioral integration, our 8B model achieves performance\ncomparable to significantly larger baselines (14-32B parameters) across factual\naccuracy, fluency, and personalization metrics while incurring 80% lower costs\nthan the larger counterparts."
                },
                "authors": [
                    {
                        "name": "Akhil Theerthala"
                    }
                ],
                "author_detail": {
                    "name": "Akhil Theerthala"
                },
                "author": "Akhil Theerthala",
                "arxiv_comment": "24 pages, 11 figures. The paper presents a novel framework for\n  generating a personal finance dataset. The resulting fine-tuned model and\n  dataset are publicly available",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20781v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20781v2",
                "updated": "2025-09-17T17:01:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    1,
                    5,
                    2,
                    260,
                    0
                ],
                "published": "2025-04-29T14:00:18Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    0,
                    18,
                    1,
                    119,
                    0
                ],
                "title": "Using LLMs in Generating Design Rationale for Software Architecture\n  Decisions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs in Generating Design Rationale for Software Architecture\n  Decisions"
                },
                "summary": "Design Rationale (DR) for software architecture decisions refers to the\nreasoning underlying architectural choices, which provides valuable insights\ninto the different phases of the architecting process throughout software\ndevelopment. However, in practice, DR is often inadequately documented due to a\nlack of motivation and effort from developers. With the recent advancements in\nLarge Language Models (LLMs), their capabilities in text comprehension,\nreasoning, and generation may enable the generation and recovery of DR for\narchitecture decisions. In this study, we evaluated the performance of LLMs in\ngenerating DR for architecture decisions. First, we collected 50 Stack Overflow\n(SO) posts, 25 GitHub issues, and 25 GitHub discussions related to architecture\ndecisions to construct a dataset of 100 architecture-related problems. Then, we\nselected five LLMs to generate DR for the architecture decisions with three\nprompting strategies, including zero-shot, chain of thought (CoT), and\nLLM-based agents. With the DR provided by human experts as ground truth, the\nPrecision of LLM-generated DR with the three prompting strategies ranges from\n0.267 to 0.278, Recall from 0.627 to 0.715, and F1-score from 0.351 to 0.389.\nAdditionally, 64.45% to 69.42% of the arguments of DR not mentioned by human\nexperts are also helpful, 4.12% to 4.87% of the arguments have uncertain\ncorrectness, and 1.59% to 3.24% of the arguments are potentially misleading. To\nfurther understand the trustworthiness and applicability of LLM-generated DR in\npractice, we conducted semi-structured interviews with six practitioners. Based\non the experimental and interview results, we discussed the pros and cons of\nthe three prompting strategies, the strengths and limitations of LLM-generated\nDR, and the implications for the practical use of LLM-generated DR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design Rationale (DR) for software architecture decisions refers to the\nreasoning underlying architectural choices, which provides valuable insights\ninto the different phases of the architecting process throughout software\ndevelopment. However, in practice, DR is often inadequately documented due to a\nlack of motivation and effort from developers. With the recent advancements in\nLarge Language Models (LLMs), their capabilities in text comprehension,\nreasoning, and generation may enable the generation and recovery of DR for\narchitecture decisions. In this study, we evaluated the performance of LLMs in\ngenerating DR for architecture decisions. First, we collected 50 Stack Overflow\n(SO) posts, 25 GitHub issues, and 25 GitHub discussions related to architecture\ndecisions to construct a dataset of 100 architecture-related problems. Then, we\nselected five LLMs to generate DR for the architecture decisions with three\nprompting strategies, including zero-shot, chain of thought (CoT), and\nLLM-based agents. With the DR provided by human experts as ground truth, the\nPrecision of LLM-generated DR with the three prompting strategies ranges from\n0.267 to 0.278, Recall from 0.627 to 0.715, and F1-score from 0.351 to 0.389.\nAdditionally, 64.45% to 69.42% of the arguments of DR not mentioned by human\nexperts are also helpful, 4.12% to 4.87% of the arguments have uncertain\ncorrectness, and 1.59% to 3.24% of the arguments are potentially misleading. To\nfurther understand the trustworthiness and applicability of LLM-generated DR in\npractice, we conducted semi-structured interviews with six practitioners. Based\non the experimental and interview results, we discussed the pros and cons of\nthe three prompting strategies, the strengths and limitations of LLM-generated\nDR, and the implications for the practical use of LLM-generated DR."
                },
                "authors": [
                    {
                        "name": "Xiyu Zhou"
                    },
                    {
                        "name": "Ruiyin Li"
                    },
                    {
                        "name": "Peng Liang"
                    },
                    {
                        "name": "Beiqi Zhang"
                    },
                    {
                        "name": "Mojtaba Shahin"
                    },
                    {
                        "name": "Zengyang Li"
                    },
                    {
                        "name": "Chen Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chen Yang"
                },
                "author": "Chen Yang",
                "arxiv_comment": "38 pages, 5 images, 9 tables, Manuscript revision submitted to a\n  journal (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20781v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20781v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10947v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10947v2",
                "updated": "2025-09-17T17:00:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    0,
                    13,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-13T19:01:38Z",
                "published_parsed": [
                    2025,
                    9,
                    13,
                    19,
                    1,
                    38,
                    5,
                    256,
                    0
                ],
                "title": "Hydrocarbon Hazes on Temperate sub-Neptune K2-18b supported by data from\n  the James Webb Space Telescope",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hydrocarbon Hazes on Temperate sub-Neptune K2-18b supported by data from\n  the James Webb Space Telescope"
                },
                "summary": "K2-18b, a sub-Neptune orbiting in the habitable zone of an M dwarf, has\nattracted significant interest following observations with the Hubble Space\nTelescope (HST) and, more recently, with the James Webb Space Telescope (JWST)\nthat reveal detectable atmospheric features. Previous studies have examined a\nwide range of possible compositions, focusing primarily in the near-infrared\n(0.8-5.2 $\\mu$m) or mid-infrared (5-12 $\\mu$m) wavelengths. We present a new\ninterpretation of K2-18b's JWST transit spectra, combining an independent\nreduction of MIRI LRS data with previously published NIRISS/NIRSpec\nobservations. We assess the impact of stellar parameter uncertainties on the\ninferred planetary properties and, using revised stellar parameters, derive a\nplanetary density of $\\rho_P = 3.34 \\pm 1.44$ g cm$^{-3}$. We consider\nscattering and absorption from laboratory-produced haze analogues and perform\nfree-chemistry Bayesian retrievals informed by equilibrium chemistry. Our\nresults are consistent with an H$_2$-dominated mini-Neptune atmosphere with a\nmean molecular weight of $\\mu \\sim$2.4 Daltons, and support the presence of\nhydrocarbon hazes across 0.85-12 $\\mu$m without requiring instrumental offsets.\nOur retrieved CH$_4$ and CO$_2$ abundances are broadly consistent between\nmodels but systematically lower than in haze-free studies, suggesting that haze\nreduces the need for high-$\\mu$ solutions. While our retrievals tend to favour\natmospheric temperatures $\\sim$100-200 K warmer than previously reported,\ncooler solutions ($\\sim$250 K) remain viable if the planetary mass is reduced\ntowards the lower end of its uncertainty. We emphasise the need for follow-up\nself-consistent photochemical and microphysical modelling, alongside further\nmid-infrared observations to constrain key hydrocarbon species.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "K2-18b, a sub-Neptune orbiting in the habitable zone of an M dwarf, has\nattracted significant interest following observations with the Hubble Space\nTelescope (HST) and, more recently, with the James Webb Space Telescope (JWST)\nthat reveal detectable atmospheric features. Previous studies have examined a\nwide range of possible compositions, focusing primarily in the near-infrared\n(0.8-5.2 $\\mu$m) or mid-infrared (5-12 $\\mu$m) wavelengths. We present a new\ninterpretation of K2-18b's JWST transit spectra, combining an independent\nreduction of MIRI LRS data with previously published NIRISS/NIRSpec\nobservations. We assess the impact of stellar parameter uncertainties on the\ninferred planetary properties and, using revised stellar parameters, derive a\nplanetary density of $\\rho_P = 3.34 \\pm 1.44$ g cm$^{-3}$. We consider\nscattering and absorption from laboratory-produced haze analogues and perform\nfree-chemistry Bayesian retrievals informed by equilibrium chemistry. Our\nresults are consistent with an H$_2$-dominated mini-Neptune atmosphere with a\nmean molecular weight of $\\mu \\sim$2.4 Daltons, and support the presence of\nhydrocarbon hazes across 0.85-12 $\\mu$m without requiring instrumental offsets.\nOur retrieved CH$_4$ and CO$_2$ abundances are broadly consistent between\nmodels but systematically lower than in haze-free studies, suggesting that haze\nreduces the need for high-$\\mu$ solutions. While our retrievals tend to favour\natmospheric temperatures $\\sim$100-200 K warmer than previously reported,\ncooler solutions ($\\sim$250 K) remain viable if the planetary mass is reduced\ntowards the lower end of its uncertainty. We emphasise the need for follow-up\nself-consistent photochemical and microphysical modelling, alongside further\nmid-infrared observations to constrain key hydrocarbon species."
                },
                "authors": [
                    {
                        "name": "Ruohan Liu"
                    },
                    {
                        "name": "Panayotis Lavvas"
                    },
                    {
                        "name": "Giovanna Tinetti"
                    },
                    {
                        "name": "Jesus Maldonado"
                    },
                    {
                        "name": "Sushuang Ma"
                    },
                    {
                        "name": "Arianna Saba"
                    }
                ],
                "author_detail": {
                    "name": "Arianna Saba"
                },
                "author": "Arianna Saba",
                "arxiv_comment": "Submitted to AAS Journals; 31 pages, 17 figures, and 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10947v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10947v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14169v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14169v1",
                "updated": "2025-09-17T16:52:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    52,
                    46,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T16:52:46Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    52,
                    46,
                    2,
                    260,
                    0
                ],
                "title": "TopoSizing: An LLM-aided Framework of Topology-based Understanding and\n  Sizing for AMS Circuits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TopoSizing: An LLM-aided Framework of Topology-based Understanding and\n  Sizing for AMS Circuits"
                },
                "summary": "Analog and mixed-signal circuit design remains challenging due to the\nshortage of high-quality data and the difficulty of embedding domain knowledge\ninto automated flows. Traditional black-box optimization achieves sampling\nefficiency but lacks circuit understanding, which often causes evaluations to\nbe wasted in low-value regions of the design space. In contrast, learning-based\nmethods embed structural knowledge but are case-specific and costly to retrain.\nRecent attempts with large language models show potential, yet they often rely\non manual intervention, limiting generality and transparency. We propose\nTopoSizing, an end-to-end framework that performs robust circuit understanding\ndirectly from raw netlists and translates this knowledge into optimization\ngains. Our approach first applies graph algorithms to organize circuits into a\nhierarchical device-module-stage representation. LLM agents then execute an\niterative hypothesis-verification-refinement loop with built-in consistency\nchecks, producing explicit annotations. Verified insights are integrated into\nBayesian optimization through LLM-guided initial sampling and\nstagnation-triggered trust-region updates, improving efficiency while\npreserving feasibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog and mixed-signal circuit design remains challenging due to the\nshortage of high-quality data and the difficulty of embedding domain knowledge\ninto automated flows. Traditional black-box optimization achieves sampling\nefficiency but lacks circuit understanding, which often causes evaluations to\nbe wasted in low-value regions of the design space. In contrast, learning-based\nmethods embed structural knowledge but are case-specific and costly to retrain.\nRecent attempts with large language models show potential, yet they often rely\non manual intervention, limiting generality and transparency. We propose\nTopoSizing, an end-to-end framework that performs robust circuit understanding\ndirectly from raw netlists and translates this knowledge into optimization\ngains. Our approach first applies graph algorithms to organize circuits into a\nhierarchical device-module-stage representation. LLM agents then execute an\niterative hypothesis-verification-refinement loop with built-in consistency\nchecks, producing explicit annotations. Verified insights are integrated into\nBayesian optimization through LLM-guided initial sampling and\nstagnation-triggered trust-region updates, improving efficiency while\npreserving feasibility."
                },
                "authors": [
                    {
                        "name": "Ziming Wei"
                    },
                    {
                        "name": "Zichen Kong"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "David Z. Pan"
                    },
                    {
                        "name": "Xiyuan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Xiyuan Tang"
                },
                "author": "Xiyuan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14169v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14166v1",
                "updated": "2025-09-17T16:49:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    49,
                    46,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T16:49:46Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    49,
                    46,
                    2,
                    260,
                    0
                ],
                "title": "Reaction-diffusion models of invasive tree pest spread: quantifying the\n  spread of oak processionary moth in the UK",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reaction-diffusion models of invasive tree pest spread: quantifying the\n  spread of oak processionary moth in the UK"
                },
                "summary": "UK woodlands, forests, and urban treescapes are under threat from invasive\nspecies, exacerbated by climate change, trade, and transport. Invasive tree\npests debilitate their host and disrupt forest ecosystems, thus it is\nimperative to quantitatively model and predict their spread. Addressing this,\nwe represent the spatial distribution of the pest as a population density field\nwhich evolves according to a spatiotemporal reaction-diffusion equation. We\nsolve this intractable system of equations numerically and, from the solution,\nwe determine first arrival times of the pest at locations in the field. The\nadopted model permits us to obtain the expansion rate of pest spread directly\nfrom the model parameters, which we infer in the Bayesian paradigm, using a\nMarkov chain Monte Carlo scheme. We apply our framework to the ongoing spread\nof oak processionary moth in the UK, an outbreak which continues to grow\ndespite management efforts. We demonstrate that our approach effectively\ncaptures the spread of the pest and that this has occurred at a non-constant\nexpansion rate. The proposed framework is a powerful tool for quantitatively\nmodelling the spread of an invasive tree pest and could underpin future\nprediction and management approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UK woodlands, forests, and urban treescapes are under threat from invasive\nspecies, exacerbated by climate change, trade, and transport. Invasive tree\npests debilitate their host and disrupt forest ecosystems, thus it is\nimperative to quantitatively model and predict their spread. Addressing this,\nwe represent the spatial distribution of the pest as a population density field\nwhich evolves according to a spatiotemporal reaction-diffusion equation. We\nsolve this intractable system of equations numerically and, from the solution,\nwe determine first arrival times of the pest at locations in the field. The\nadopted model permits us to obtain the expansion rate of pest spread directly\nfrom the model parameters, which we infer in the Bayesian paradigm, using a\nMarkov chain Monte Carlo scheme. We apply our framework to the ongoing spread\nof oak processionary moth in the UK, an outbreak which continues to grow\ndespite management efforts. We demonstrate that our approach effectively\ncaptures the spread of the pest and that this has occurred at a non-constant\nexpansion rate. The proposed framework is a powerful tool for quantitatively\nmodelling the spread of an invasive tree pest and could underpin future\nprediction and management approaches."
                },
                "authors": [
                    {
                        "name": "Jamie P. McKeown"
                    },
                    {
                        "name": "Laura E. Wadkin"
                    },
                    {
                        "name": "Nick G. Parker"
                    },
                    {
                        "name": "Andrew Golightly"
                    },
                    {
                        "name": "Andrew W. Baggaley"
                    }
                ],
                "author_detail": {
                    "name": "Andrew W. Baggaley"
                },
                "author": "Andrew W. Baggaley",
                "arxiv_comment": "19 pages, 8 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92D40, 92-10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14165v1",
                "updated": "2025-09-17T16:48:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    48,
                    0,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T16:48:00Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    48,
                    0,
                    2,
                    260,
                    0
                ],
                "title": "Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High\n  Resolutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High\n  Resolutions"
                },
                "summary": "Vision Transformers (ViTs) achieve state-of-the-art performance in semantic\nsegmentation but are hindered by high computational and memory costs. To\naddress this, we propose STEP (SuperToken and Early-Pruning), a hybrid\ntoken-reduction framework that combines dynamic patch merging and token pruning\nto enhance efficiency without significantly compromising accuracy. At the core\nof STEP is dCTS, a lightweight CNN-based policy network that enables flexible\nmerging into superpatches. Encoder blocks integrate also early-exits to remove\nhigh-confident supertokens, lowering computational load. We evaluate our method\non high-resolution semantic segmentation benchmarks, including images up to\n1024 x 1024, and show that when dCTS is applied alone, the token count can be\nreduced by a factor of 2.5 compared to the standard 16 x 16 pixel patching\nscheme. This yields a 2.6x reduction in computational cost and a 3.4x increase\nin throughput when using ViT-Large as the backbone. Applying the full STEP\nframework further improves efficiency, reaching up to a 4x reduction in\ncomputational complexity and a 1.7x gain in inference speed, with a maximum\naccuracy drop of no more than 2.0%. With the proposed STEP configurations, up\nto 40% of tokens can be confidently predicted and halted before reaching the\nfinal encoder layer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Transformers (ViTs) achieve state-of-the-art performance in semantic\nsegmentation but are hindered by high computational and memory costs. To\naddress this, we propose STEP (SuperToken and Early-Pruning), a hybrid\ntoken-reduction framework that combines dynamic patch merging and token pruning\nto enhance efficiency without significantly compromising accuracy. At the core\nof STEP is dCTS, a lightweight CNN-based policy network that enables flexible\nmerging into superpatches. Encoder blocks integrate also early-exits to remove\nhigh-confident supertokens, lowering computational load. We evaluate our method\non high-resolution semantic segmentation benchmarks, including images up to\n1024 x 1024, and show that when dCTS is applied alone, the token count can be\nreduced by a factor of 2.5 compared to the standard 16 x 16 pixel patching\nscheme. This yields a 2.6x reduction in computational cost and a 3.4x increase\nin throughput when using ViT-Large as the backbone. Applying the full STEP\nframework further improves efficiency, reaching up to a 4x reduction in\ncomputational complexity and a 1.7x gain in inference speed, with a maximum\naccuracy drop of no more than 2.0%. With the proposed STEP configurations, up\nto 40% of tokens can be confidently predicted and halted before reaching the\nfinal encoder layer."
                },
                "authors": [
                    {
                        "name": "Michal Szczepanski"
                    },
                    {
                        "name": "Martyna Poreba"
                    },
                    {
                        "name": "Karim Haroun"
                    }
                ],
                "author_detail": {
                    "name": "Karim Haroun"
                },
                "author": "Karim Haroun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18325v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18325v3",
                "updated": "2025-09-17T16:44:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    44,
                    58,
                    2,
                    260,
                    0
                ],
                "published": "2025-05-23T19:30:49Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    19,
                    30,
                    49,
                    4,
                    143,
                    0
                ],
                "title": "Understanding and Mitigating Overrefusal in LLMs from an Unveiling\n  Perspective of Safety Decision Boundary",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Mitigating Overrefusal in LLMs from an Unveiling\n  Perspective of Safety Decision Boundary"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet they often refuse to answer legitimate queries--a\nphenomenon known as overrefusal. Overrefusal typically stems from\nover-conservative safety alignment, causing models to treat many reasonable\nprompts as potentially risky. To systematically understand this issue, we probe\nand leverage the models' safety decision boundaries to analyze and mitigate\noverrefusal. Our findings reveal that overrefusal is closely tied to\nmisalignment at these boundary regions, where models struggle to distinguish\nsubtle differences between benign and harmful content. Building on these\ninsights, we present RASS, an automated framework for prompt generation and\nselection that strategically targets overrefusal prompts near the safety\nboundary. By harnessing steering vectors in the representation space, RASS\nefficiently identifies and curates boundary-aligned prompts, enabling more\neffective and targeted mitigation of overrefusal. This approach not only\nprovides a more precise and interpretable view of model safety decisions but\nalso seamlessly extends to multilingual scenarios. We have explored the safety\ndecision boundaries of various LLMs and construct the MORBench evaluation set\nto facilitate robust assessment of model safety and helpfulness across multiple\nlanguages. Code and datasets are available at\nhttps://github.com/Master-PLC/RASS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet they often refuse to answer legitimate queries--a\nphenomenon known as overrefusal. Overrefusal typically stems from\nover-conservative safety alignment, causing models to treat many reasonable\nprompts as potentially risky. To systematically understand this issue, we probe\nand leverage the models' safety decision boundaries to analyze and mitigate\noverrefusal. Our findings reveal that overrefusal is closely tied to\nmisalignment at these boundary regions, where models struggle to distinguish\nsubtle differences between benign and harmful content. Building on these\ninsights, we present RASS, an automated framework for prompt generation and\nselection that strategically targets overrefusal prompts near the safety\nboundary. By harnessing steering vectors in the representation space, RASS\nefficiently identifies and curates boundary-aligned prompts, enabling more\neffective and targeted mitigation of overrefusal. This approach not only\nprovides a more precise and interpretable view of model safety decisions but\nalso seamlessly extends to multilingual scenarios. We have explored the safety\ndecision boundaries of various LLMs and construct the MORBench evaluation set\nto facilitate robust assessment of model safety and helpfulness across multiple\nlanguages. Code and datasets are available at\nhttps://github.com/Master-PLC/RASS."
                },
                "authors": [
                    {
                        "name": "Licheng Pan"
                    },
                    {
                        "name": "Yongqi Tong"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Xiaolu Zhang"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Zhixuan Chu"
                    }
                ],
                "author_detail": {
                    "name": "Zhixuan Chu"
                },
                "author": "Zhixuan Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18325v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18325v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05606v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05606v2",
                "updated": "2025-09-17T16:44:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    44,
                    11,
                    2,
                    260,
                    0
                ],
                "published": "2025-08-07T17:45:17Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    45,
                    17,
                    3,
                    219,
                    0
                ],
                "title": "Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and\n  Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and\n  Vision"
                },
                "summary": "Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large\nLanguage Models (LLMs) by decomposing complex tasks into simpler, sequential\nsubtasks. However, extending CoT to vision-language reasoning tasks remains\nchallenging, as it often requires interpreting transitions of visual states to\nsupport reasoning. Existing methods often struggle with this due to limited\ncapacity of modeling visual state transitions or incoherent visual trajectories\ncaused by fragmented architectures.\n  To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought\nframework that enables coherent and grounded multimodal reasoning within a\nsingle unified model. The key idea is to leverage a model capable of both image\nunderstanding and generation to reason over visual content and model evolving\nvisual states. However, empowering a unified model to achieve that is\nnon-trivial, given the high computational cost and the burden of training. To\naddress this, Uni-CoT introduces a novel two-level reasoning paradigm: A\nMacro-Level CoT for high-level task planning and A Micro-Level CoT for subtask\nexecution. This design significantly reduces the computational overhead.\nFurthermore, we introduce a structured training paradigm that combines\ninterleaved image-text supervision for macro-level CoT with multi-task\nobjectives for micro-level CoT. Together, these innovations allow Uni-CoT to\nperform scalable and coherent multi-modal reasoning. Furthermore, thanks to our\ndesign, all experiments can be efficiently completed using only 8 A100 GPUs\nwith 80GB VRAM each. Experimental results on reasoning-driven image generation\nbenchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT\ndemonstrates SOTA performance and strong generalization, establishing Uni-CoT\nas a promising solution for multi-modal reasoning. Project Page and Code:\nhttps://sais-fuxi.github.io/projects/uni-cot/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large\nLanguage Models (LLMs) by decomposing complex tasks into simpler, sequential\nsubtasks. However, extending CoT to vision-language reasoning tasks remains\nchallenging, as it often requires interpreting transitions of visual states to\nsupport reasoning. Existing methods often struggle with this due to limited\ncapacity of modeling visual state transitions or incoherent visual trajectories\ncaused by fragmented architectures.\n  To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought\nframework that enables coherent and grounded multimodal reasoning within a\nsingle unified model. The key idea is to leverage a model capable of both image\nunderstanding and generation to reason over visual content and model evolving\nvisual states. However, empowering a unified model to achieve that is\nnon-trivial, given the high computational cost and the burden of training. To\naddress this, Uni-CoT introduces a novel two-level reasoning paradigm: A\nMacro-Level CoT for high-level task planning and A Micro-Level CoT for subtask\nexecution. This design significantly reduces the computational overhead.\nFurthermore, we introduce a structured training paradigm that combines\ninterleaved image-text supervision for macro-level CoT with multi-task\nobjectives for micro-level CoT. Together, these innovations allow Uni-CoT to\nperform scalable and coherent multi-modal reasoning. Furthermore, thanks to our\ndesign, all experiments can be efficiently completed using only 8 A100 GPUs\nwith 80GB VRAM each. Experimental results on reasoning-driven image generation\nbenchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT\ndemonstrates SOTA performance and strong generalization, establishing Uni-CoT\nas a promising solution for multi-modal reasoning. Project Page and Code:\nhttps://sais-fuxi.github.io/projects/uni-cot/"
                },
                "authors": [
                    {
                        "name": "Luozheng Qin"
                    },
                    {
                        "name": "Jia Gong"
                    },
                    {
                        "name": "Yuqing Sun"
                    },
                    {
                        "name": "Tianjiao Li"
                    },
                    {
                        "name": "Mengping Yang"
                    },
                    {
                        "name": "Xiaomeng Yang"
                    },
                    {
                        "name": "Chao Qu"
                    },
                    {
                        "name": "Zhiyu Tan"
                    },
                    {
                        "name": "Hao Li"
                    }
                ],
                "author_detail": {
                    "name": "Hao Li"
                },
                "author": "Hao Li",
                "arxiv_comment": "Project Page: https://sais-fuxi.github.io/projects/uni-cot/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05606v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05606v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19572v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19572v2",
                "updated": "2025-09-17T16:43:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    43,
                    55,
                    2,
                    260,
                    0
                ],
                "published": "2024-11-29T09:36:10Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    36,
                    10,
                    4,
                    334,
                    0
                ],
                "title": "Canonical correlation analysis of stochastic trends via functional\n  approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Canonical correlation analysis of stochastic trends via functional\n  approximation"
                },
                "summary": "This paper proposes a novel approach for semiparametric inference on the\nnumber $s$ of common trends and their loading matrix $\\psi$ in $I(1)/I(0)$\nsystems. It combines functional approximation of limits of random walks and\ncanonical correlations analysis, performed between the $p$ observed time series\nof length $T$ and the first $K$ discretized elements of an $L^2$ basis. Tests\nand selection criteria on $s$, and estimators and tests on $\\psi$ are proposed;\ntheir properties are discussed as $T$ and $K$ diverge sequentially for fixed\n$p$ and $s$. It is found that tests on $s$ are asymptotically pivotal,\nselection criteria of $s$ are consistent, estimators of $\\psi$ are\n$T$-consistent, mixed-Gaussian and efficient, so that Wald tests on $\\psi$ are\nasymptotically Normal or $\\chi^2$. The paper also discusses asymptotically\npivotal misspecification tests for checking model assumptions. The approach can\nbe coherently applied to subsets or aggregations of variables in a given panel.\nMonte Carlo simulations show that these tools have reasonable performance for\n$T\\geq 10 p$ and $p\\leq 300$. An empirical analysis of 20 exchange rates\nillustrates the methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a novel approach for semiparametric inference on the\nnumber $s$ of common trends and their loading matrix $\\psi$ in $I(1)/I(0)$\nsystems. It combines functional approximation of limits of random walks and\ncanonical correlations analysis, performed between the $p$ observed time series\nof length $T$ and the first $K$ discretized elements of an $L^2$ basis. Tests\nand selection criteria on $s$, and estimators and tests on $\\psi$ are proposed;\ntheir properties are discussed as $T$ and $K$ diverge sequentially for fixed\n$p$ and $s$. It is found that tests on $s$ are asymptotically pivotal,\nselection criteria of $s$ are consistent, estimators of $\\psi$ are\n$T$-consistent, mixed-Gaussian and efficient, so that Wald tests on $\\psi$ are\nasymptotically Normal or $\\chi^2$. The paper also discusses asymptotically\npivotal misspecification tests for checking model assumptions. The approach can\nbe coherently applied to subsets or aggregations of variables in a given panel.\nMonte Carlo simulations show that these tools have reasonable performance for\n$T\\geq 10 p$ and $p\\leq 300$. An empirical analysis of 20 exchange rates\nillustrates the methods."
                },
                "authors": [
                    {
                        "name": "Massimo Franchi"
                    },
                    {
                        "name": "Iliyan Georgiev"
                    },
                    {
                        "name": "Paolo Paruolo"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Paruolo"
                },
                "author": "Paolo Paruolo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19572v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19572v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13254v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13254v2",
                "updated": "2025-09-17T16:40:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    40,
                    55,
                    2,
                    260,
                    0
                ],
                "published": "2025-04-17T18:00:03Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    18,
                    0,
                    3,
                    3,
                    107,
                    0
                ],
                "title": "Reconstructing the epoch of reionisation with Planck PR4",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing the epoch of reionisation with Planck PR4"
                },
                "summary": "The epoch of reionisation is a key phase in cosmic history, characterised by\nthe ionisation of the intergalactic medium by the first luminous sources. In\nthis work, we constrain the reionisation history of the Universe using data\nfrom the cosmic microwave background, more specifically the latest Planck\nPublic Release 4 (PR4) dataset. We investigate a wide range of reionisation\nmodels, from simple parametric descriptions to more flexible non-parametric\napproaches, systematically evaluating their impact on the inferred constraints.\nSpecial attention is given to implicit priors introduced by each model and\ntheir influence on the derived reionisation optical depth, $\\tau$. To achieve\nthis, we employ both Bayesian and frequentist methods to derive robust\nconstraints. We obtain consistent estimates of $\\tau$ across models,\nhighlighting the robustness of the constraints on the integrated optical depth\nderived from the Planck PR4 data. Averaging across models, the posterior means\nand best-fit values, respectively, yield $\\tau = 0.0576 \\pm 0.0060$ and $\\tau =\n0.0581$, highlighting the presence of small volume effects. Based on our\nanalysis, we estimate that an additional uncertainty, associated with the\nmodelling of reionisation, contributes an error of approximately\n$\\sigma_\\tau\\!\\sim\\!0.0006$. Beyond the integrated optical depth, our analysis\nreveals that the inferred ionisation fraction as a function of redshift is\nhighly model-dependent. While current CMB data do not favour significant early\nionisation, they are consistent with a modest contribution from ionised gas at\nvery early times ($z>15$). Although indicative upper bounds can be placed on\nsuch contributions, these limits remain strongly dependent on the assumed\nmodel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The epoch of reionisation is a key phase in cosmic history, characterised by\nthe ionisation of the intergalactic medium by the first luminous sources. In\nthis work, we constrain the reionisation history of the Universe using data\nfrom the cosmic microwave background, more specifically the latest Planck\nPublic Release 4 (PR4) dataset. We investigate a wide range of reionisation\nmodels, from simple parametric descriptions to more flexible non-parametric\napproaches, systematically evaluating their impact on the inferred constraints.\nSpecial attention is given to implicit priors introduced by each model and\ntheir influence on the derived reionisation optical depth, $\\tau$. To achieve\nthis, we employ both Bayesian and frequentist methods to derive robust\nconstraints. We obtain consistent estimates of $\\tau$ across models,\nhighlighting the robustness of the constraints on the integrated optical depth\nderived from the Planck PR4 data. Averaging across models, the posterior means\nand best-fit values, respectively, yield $\\tau = 0.0576 \\pm 0.0060$ and $\\tau =\n0.0581$, highlighting the presence of small volume effects. Based on our\nanalysis, we estimate that an additional uncertainty, associated with the\nmodelling of reionisation, contributes an error of approximately\n$\\sigma_\\tau\\!\\sim\\!0.0006$. Beyond the integrated optical depth, our analysis\nreveals that the inferred ionisation fraction as a function of redshift is\nhighly model-dependent. While current CMB data do not favour significant early\nionisation, they are consistent with a modest contribution from ionised gas at\nvery early times ($z>15$). Although indicative upper bounds can be placed on\nsuch contributions, these limits remain strongly dependent on the assumed\nmodel."
                },
                "authors": [
                    {
                        "name": "S. Ilić"
                    },
                    {
                        "name": "M. Tristram"
                    },
                    {
                        "name": "M. Douspis"
                    },
                    {
                        "name": "A. Gorce"
                    },
                    {
                        "name": "S. Henrot-Versillé"
                    },
                    {
                        "name": "L. T. Hergt"
                    },
                    {
                        "name": "M. Langer"
                    },
                    {
                        "name": "L. McBride"
                    },
                    {
                        "name": "M. Muñoz-Echeverría"
                    },
                    {
                        "name": "E. Pointecouteau"
                    },
                    {
                        "name": "L. Salvati"
                    }
                ],
                "author_detail": {
                    "name": "L. Salvati"
                },
                "author": "L. Salvati",
                "arxiv_doi": "10.1051/0004-6361/202555196",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202555196",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.13254v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13254v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 12 figures, 2 tables. Updated to match version published in\n  A&A",
                "arxiv_journal_ref": "A&A 700, A26 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05733v2",
                "updated": "2025-09-17T16:30:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    30,
                    14,
                    2,
                    260,
                    0
                ],
                "published": "2025-08-07T18:00:00Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    18,
                    0,
                    0,
                    3,
                    219,
                    0
                ],
                "title": "Investigating the origin of the Milky Way streams. A revised look at\n  their orbital pole distribution in light of precession effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating the origin of the Milky Way streams. A revised look at\n  their orbital pole distribution in light of precession effects"
                },
                "summary": "Stellar streams around the Milky Way (MW) can provide valuable insights into\nits history and substructure formation. Previous studies have suggested that\nseveral MW streams could have an origin related to that of the disc of\nsatellite galaxies (DoS) and the young halo globular clusters of the MW, given\nthat many of these structures present a similar orbital pole orientation. In\nthis work we test the validity of this hypothesis by revising the orbital pole\ndistribution of the MW streams with the latest stream dataset (galstreams). For\na sample of 91 streams at Galactocentric distances of $d<100$ kpc we find that\nthe pole distribution has no preferred orbital direction. However, as we\nsubtract the streams closer to the Galactic centre, by imposing several lower\ndistance cuts, we find that the larger the Galactocentric distance of the\nstreams, the higher the fraction of stream poles pointing in a direction\nsimilar to the DoS. This trend could be explained if the stream pole\ndistribution were originally anisotropic, but precession effects displaced the\norbital poles of the streams closer to the Galactic centre. From the pole\ndistribution and the estimated precession rates of the streams in the sample,\nwe infer that the streams nearer the Galactic centre are indeed quite likely to\nbe affected by precession. Finally, we corroborate with hydrodynamical\nsimulations that, even in a scenario in which the MW substructures had a common\norigin, an overdensity in their orbital pole direction cannot be appreciated\nuntil the selected sample also includes material at $d \\gtrsim 150$ kpc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stellar streams around the Milky Way (MW) can provide valuable insights into\nits history and substructure formation. Previous studies have suggested that\nseveral MW streams could have an origin related to that of the disc of\nsatellite galaxies (DoS) and the young halo globular clusters of the MW, given\nthat many of these structures present a similar orbital pole orientation. In\nthis work we test the validity of this hypothesis by revising the orbital pole\ndistribution of the MW streams with the latest stream dataset (galstreams). For\na sample of 91 streams at Galactocentric distances of $d<100$ kpc we find that\nthe pole distribution has no preferred orbital direction. However, as we\nsubtract the streams closer to the Galactic centre, by imposing several lower\ndistance cuts, we find that the larger the Galactocentric distance of the\nstreams, the higher the fraction of stream poles pointing in a direction\nsimilar to the DoS. This trend could be explained if the stream pole\ndistribution were originally anisotropic, but precession effects displaced the\norbital poles of the streams closer to the Galactic centre. From the pole\ndistribution and the estimated precession rates of the streams in the sample,\nwe infer that the streams nearer the Galactic centre are indeed quite likely to\nbe affected by precession. Finally, we corroborate with hydrodynamical\nsimulations that, even in a scenario in which the MW substructures had a common\norigin, an overdensity in their orbital pole direction cannot be appreciated\nuntil the selected sample also includes material at $d \\gtrsim 150$ kpc."
                },
                "authors": [
                    {
                        "name": "Elena Asencio"
                    },
                    {
                        "name": "Pavel Kroupa"
                    },
                    {
                        "name": "Ingo Thies"
                    }
                ],
                "author_detail": {
                    "name": "Ingo Thies"
                },
                "author": "Ingo Thies",
                "arxiv_doi": "10.1051/0004-6361/202451179",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202451179",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.05733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 7 figures, 5 tables. Accepted for publication in A&A",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14142v1",
                "updated": "2025-09-17T16:21:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    21,
                    34,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T16:21:34Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    21,
                    34,
                    2,
                    260,
                    0
                ],
                "title": "MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods,\n  Results, Discussion, and Outlook",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods,\n  Results, Discussion, and Outlook"
                },
                "summary": "This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim\nto bring together different approaches in multimodal machine learning and LLMs\nvia a large benchmark. We hope it better allows researchers to follow the\nstate-of-the-art in this very dynamic area. Meanwhile, a growing number of\ntestbeds have boosted the evolution of general-purpose large language models.\nThus, this year's MARS2 focuses on real-world and specialized scenarios to\nbroaden the multimodal reasoning applications of MLLMs. Our organizing team\nreleased two tailored datasets Lens and AdsQA as test sets, which support\ngeneral reasoning in 12 daily scenarios and domain-specific reasoning in\nadvertisement videos, respectively. We evaluated 40+ baselines that include\nboth generalist MLLMs and task-specific models, and opened up three competition\ntracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question\nAnswering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative\nAdvertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and\nindustrial institutions have registered and 40+ valid submissions (out of\n1200+) have been included in our ranking lists. Our datasets, code sets (40+\nbaselines and 15+ participants' methods), and rankings are publicly available\non the MARS2 workshop website and our GitHub organization page\nhttps://github.com/mars2workshop/, where our updates and announcements of\nupcoming events will be continuously provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim\nto bring together different approaches in multimodal machine learning and LLMs\nvia a large benchmark. We hope it better allows researchers to follow the\nstate-of-the-art in this very dynamic area. Meanwhile, a growing number of\ntestbeds have boosted the evolution of general-purpose large language models.\nThus, this year's MARS2 focuses on real-world and specialized scenarios to\nbroaden the multimodal reasoning applications of MLLMs. Our organizing team\nreleased two tailored datasets Lens and AdsQA as test sets, which support\ngeneral reasoning in 12 daily scenarios and domain-specific reasoning in\nadvertisement videos, respectively. We evaluated 40+ baselines that include\nboth generalist MLLMs and task-specific models, and opened up three competition\ntracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question\nAnswering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative\nAdvertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and\nindustrial institutions have registered and 40+ valid submissions (out of\n1200+) have been included in our ranking lists. Our datasets, code sets (40+\nbaselines and 15+ participants' methods), and rankings are publicly available\non the MARS2 workshop website and our GitHub organization page\nhttps://github.com/mars2workshop/, where our updates and announcements of\nupcoming events will be continuously provided."
                },
                "authors": [
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Shengwu Xiong"
                    },
                    {
                        "name": "Jiajun Zhang"
                    },
                    {
                        "name": "Yaxiong Chen"
                    },
                    {
                        "name": "Bowen Zhou"
                    },
                    {
                        "name": "Chen Change Loy"
                    },
                    {
                        "name": "David A. Clifton"
                    },
                    {
                        "name": "Kyoung Mu Lee"
                    },
                    {
                        "name": "Luc Van Gool"
                    },
                    {
                        "name": "Ruiming He"
                    },
                    {
                        "name": "Ruilin Yao"
                    },
                    {
                        "name": "Xinwei Long"
                    },
                    {
                        "name": "Jirui Huang"
                    },
                    {
                        "name": "Kai Tian"
                    },
                    {
                        "name": "Sa Yang"
                    },
                    {
                        "name": "Yihua Shao"
                    },
                    {
                        "name": "Jin Feng"
                    },
                    {
                        "name": "Yue Zhong"
                    },
                    {
                        "name": "Jiakai Zhou"
                    },
                    {
                        "name": "Cheng Tang"
                    },
                    {
                        "name": "Tianyu Zou"
                    },
                    {
                        "name": "Yifang Zhang"
                    },
                    {
                        "name": "Junming Liang"
                    },
                    {
                        "name": "Guoyou Li"
                    },
                    {
                        "name": "Zhaoxiang Wang"
                    },
                    {
                        "name": "Qiang Zhou"
                    },
                    {
                        "name": "Yichen Zhao"
                    },
                    {
                        "name": "Shili Xiong"
                    },
                    {
                        "name": "Hyeongjin Nam"
                    },
                    {
                        "name": "Jaerin Lee"
                    },
                    {
                        "name": "Jaeyoung Chung"
                    },
                    {
                        "name": "JoonKyu Park"
                    },
                    {
                        "name": "Junghun Oh"
                    },
                    {
                        "name": "Kanggeon Lee"
                    },
                    {
                        "name": "Wooseok Lee"
                    },
                    {
                        "name": "Juneyoung Ro"
                    },
                    {
                        "name": "Turghun Osman"
                    },
                    {
                        "name": "Can Hu"
                    },
                    {
                        "name": "Chaoyang Liao"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Chengcheng Han"
                    },
                    {
                        "name": "Chenhao Qiu"
                    },
                    {
                        "name": "Chong Peng"
                    },
                    {
                        "name": "Cong Xu"
                    },
                    {
                        "name": "Dailin Li"
                    },
                    {
                        "name": "Feiyu Wang"
                    },
                    {
                        "name": "Feng Gao"
                    },
                    {
                        "name": "Guibo Zhu"
                    },
                    {
                        "name": "Guopeng Tang"
                    },
                    {
                        "name": "Haibo Lu"
                    },
                    {
                        "name": "Han Fang"
                    },
                    {
                        "name": "Han Qi"
                    },
                    {
                        "name": "Hanxiao Wu"
                    },
                    {
                        "name": "Haobo Cheng"
                    },
                    {
                        "name": "Hongbo Sun"
                    },
                    {
                        "name": "Hongyao Chen"
                    },
                    {
                        "name": "Huayong Hu"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Jiaheng Ma"
                    },
                    {
                        "name": "Jiang Yu"
                    },
                    {
                        "name": "Jianing Wang"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Jing He"
                    },
                    {
                        "name": "Jinglin Zhou"
                    },
                    {
                        "name": "Jingxuan Li"
                    },
                    {
                        "name": "Josef Kittler"
                    },
                    {
                        "name": "Lihao Zheng"
                    },
                    {
                        "name": "Linnan Zhao"
                    },
                    {
                        "name": "Mengxi Jia"
                    },
                    {
                        "name": "Muyang Yan"
                    },
                    {
                        "name": "Nguyen Thanh Thien"
                    },
                    {
                        "name": "Pu Luo"
                    },
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Shien Song"
                    },
                    {
                        "name": "Shijie Dong"
                    },
                    {
                        "name": "Shuai Shao"
                    },
                    {
                        "name": "Shutao Li"
                    },
                    {
                        "name": "Taofeng Xue"
                    },
                    {
                        "name": "Tianyang Xu"
                    },
                    {
                        "name": "Tianyi Gao"
                    },
                    {
                        "name": "Tingting Li"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Weiyang Su"
                    },
                    {
                        "name": "Xiaodong Dong"
                    },
                    {
                        "name": "Xiao-Jun Wu"
                    },
                    {
                        "name": "Xiaopeng Zhou"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Xin Wei"
                    },
                    {
                        "name": "Xinyi You"
                    },
                    {
                        "name": "Xudong Kang"
                    },
                    {
                        "name": "Xujie Zhou"
                    },
                    {
                        "name": "Xusheng Liu"
                    },
                    {
                        "name": "Yanan Wang"
                    },
                    {
                        "name": "Yanbin Huang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Yanglin Deng"
                    },
                    {
                        "name": "Yashu Kang"
                    },
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Yi Wen"
                    },
                    {
                        "name": "Yicen Tian"
                    },
                    {
                        "name": "Yilin Tao"
                    },
                    {
                        "name": "Yin Tang"
                    },
                    {
                        "name": "Yipeng Lin"
                    },
                    {
                        "name": "Yiqing Wang"
                    },
                    {
                        "name": "Yiting Xi"
                    },
                    {
                        "name": "Yongkang Yu"
                    },
                    {
                        "name": "Yumei Li"
                    },
                    {
                        "name": "Yuxin Qin"
                    },
                    {
                        "name": "Yuying Chen"
                    },
                    {
                        "name": "Yuzhe Cen"
                    },
                    {
                        "name": "Zhaofan Zou"
                    },
                    {
                        "name": "Zhaohong Liu"
                    },
                    {
                        "name": "Zhehao Shen"
                    },
                    {
                        "name": "Zhenglin Du"
                    },
                    {
                        "name": "Zhengyang Li"
                    },
                    {
                        "name": "Zhenni Huang"
                    },
                    {
                        "name": "Zhenwei Shao"
                    },
                    {
                        "name": "Zhilong Song"
                    },
                    {
                        "name": "Zhiyong Feng"
                    },
                    {
                        "name": "Zhiyu Wang"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Ziang Li"
                    },
                    {
                        "name": "Zihan Zhai"
                    },
                    {
                        "name": "Zijian Zhang"
                    },
                    {
                        "name": "Ziyang Peng"
                    },
                    {
                        "name": "Ziyun Xiao"
                    },
                    {
                        "name": "Zongshu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zongshu Li"
                },
                "author": "Zongshu Li",
                "arxiv_comment": "ICCV 2025 MARS2 Workshop and Challenge \"Multimodal Reasoning and Slow\n  Thinking in the Large Model Era: Towards System 2 and Beyond''",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14133v1",
                "updated": "2025-09-17T16:14:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    14,
                    9,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T16:14:09Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    14,
                    9,
                    2,
                    260,
                    0
                ],
                "title": "Room temperature reactive sputtering deposition of titanium nitride with\n  high sheet kinetic inductance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Room temperature reactive sputtering deposition of titanium nitride with\n  high sheet kinetic inductance"
                },
                "summary": "Superconducting thin films with high intrinsic kinetic inductance $L_{k}$ are\nimportant for high-sensitivity detectors, enabling strong coupling in hybrid\nquantum systems, and enhancing nonlinearities in quantum devices. We report the\nroom-temperature reactive sputtering of titanium nitride thin films with a\ncritical temperature $T_{c}$ of \\SI{3.8}{K} and a thickness of \\SI{27}{nm}.\nFabricated into resonators, these films exhibit a sheet kinetic inductance\n$L_{k, \\square}$ of 394~$\\textrm{pH}/\\square$, as inferred from resonant\nfrequency measurements. %from this film and measure quality factors of $4\\times\n10^{4}$; these quality factors are likely limited by the low resistivity wafer.\nX-ray diffraction analysis confirms the formation of stoichiometric TiN, with\nno residual unreacted titanium. The films also demonstrate a characteristic\nsheet resistivity of 475~$\\Omega/\\square$, yielding an impedance an order of\nmagnitude higher than conventional 50~$\\Omega$ resonators. This property could\nenhance microwave single\\textendash photon coupling strength by an order of\nmagnitude, offering transformative potential for hybrid quantum systems and\nquantum sensing. Furthermore, the high $L_{k}$ enables Kerr nonlinearities\ncomparable to state\\textendash of\\textendash the\\textendash art quantum\ndevices. Combined with its relatively high $T_{c}$, this thin film presents a\npromising platform for superconducting devices, including amplifiers and qubits\noperating at higher temperatures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superconducting thin films with high intrinsic kinetic inductance $L_{k}$ are\nimportant for high-sensitivity detectors, enabling strong coupling in hybrid\nquantum systems, and enhancing nonlinearities in quantum devices. We report the\nroom-temperature reactive sputtering of titanium nitride thin films with a\ncritical temperature $T_{c}$ of \\SI{3.8}{K} and a thickness of \\SI{27}{nm}.\nFabricated into resonators, these films exhibit a sheet kinetic inductance\n$L_{k, \\square}$ of 394~$\\textrm{pH}/\\square$, as inferred from resonant\nfrequency measurements. %from this film and measure quality factors of $4\\times\n10^{4}$; these quality factors are likely limited by the low resistivity wafer.\nX-ray diffraction analysis confirms the formation of stoichiometric TiN, with\nno residual unreacted titanium. The films also demonstrate a characteristic\nsheet resistivity of 475~$\\Omega/\\square$, yielding an impedance an order of\nmagnitude higher than conventional 50~$\\Omega$ resonators. This property could\nenhance microwave single\\textendash photon coupling strength by an order of\nmagnitude, offering transformative potential for hybrid quantum systems and\nquantum sensing. Furthermore, the high $L_{k}$ enables Kerr nonlinearities\ncomparable to state\\textendash of\\textendash the\\textendash art quantum\ndevices. Combined with its relatively high $T_{c}$, this thin film presents a\npromising platform for superconducting devices, including amplifiers and qubits\noperating at higher temperatures."
                },
                "authors": [
                    {
                        "name": "Juliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Juliang Li"
                },
                "author": "Juliang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14132v1",
                "updated": "2025-09-17T16:13:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    13,
                    37,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T16:13:37Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    13,
                    37,
                    2,
                    260,
                    0
                ],
                "title": "When Avatars Have Personality: Effects on Engagement and Communication\n  in Immersive Medical Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Avatars Have Personality: Effects on Engagement and Communication\n  in Immersive Medical Training"
                },
                "summary": "While virtual reality (VR) excels at simulating physical environments, its\neffectiveness for training complex interpersonal skills is limited by a lack of\npsychologically plausible virtual humans. This is a critical gap in high-stakes\ndomains like medical education, where communication is a core competency. This\npaper introduces a framework that integrates large language models (LLMs) into\nimmersive VR to create medically coherent virtual patients with distinct,\nconsistent personalities, built on a modular architecture that decouples\npersonality from clinical data. We evaluated our system in a mixed-method,\nwithin-subjects study with licensed physicians who engaged in simulated\nconsultations. Results demonstrate that the approach is not only feasible but\nis also perceived by physicians as a highly rewarding and effective training\nenhancement. Furthermore, our analysis uncovers critical design principles,\nincluding a ``realism-verbosity paradox\" where less communicative agents can\nseem more artificial, and the need for challenges to be perceived as authentic\nto be instructive. This work provides a validated framework and key insights\nfor developing the next generation of socially intelligent VR training\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While virtual reality (VR) excels at simulating physical environments, its\neffectiveness for training complex interpersonal skills is limited by a lack of\npsychologically plausible virtual humans. This is a critical gap in high-stakes\ndomains like medical education, where communication is a core competency. This\npaper introduces a framework that integrates large language models (LLMs) into\nimmersive VR to create medically coherent virtual patients with distinct,\nconsistent personalities, built on a modular architecture that decouples\npersonality from clinical data. We evaluated our system in a mixed-method,\nwithin-subjects study with licensed physicians who engaged in simulated\nconsultations. Results demonstrate that the approach is not only feasible but\nis also perceived by physicians as a highly rewarding and effective training\nenhancement. Furthermore, our analysis uncovers critical design principles,\nincluding a ``realism-verbosity paradox\" where less communicative agents can\nseem more artificial, and the need for challenges to be perceived as authentic\nto be instructive. This work provides a validated framework and key insights\nfor developing the next generation of socially intelligent VR training\nenvironments."
                },
                "authors": [
                    {
                        "name": "Julia S. Dollis"
                    },
                    {
                        "name": "Iago A. Brito"
                    },
                    {
                        "name": "Fernanda B. Färber"
                    },
                    {
                        "name": "Pedro S. F. B. Ribeiro"
                    },
                    {
                        "name": "Rafael T. Sousa"
                    },
                    {
                        "name": "Arlindo R. Galvão Filho"
                    }
                ],
                "author_detail": {
                    "name": "Arlindo R. Galvão Filho"
                },
                "author": "Arlindo R. Galvão Filho",
                "arxiv_comment": "8 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14128v1",
                "updated": "2025-09-17T16:08:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    8,
                    46,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T16:08:46Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    8,
                    46,
                    2,
                    260,
                    0
                ],
                "title": "Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance\n  Models for Multilingual ASR and AST",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance\n  Models for Multilingual ASR and AST"
                },
                "summary": "This report introduces Canary-1B-v2, a fast, robust multilingual model for\nAutomatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built\nwith a FastConformer encoder and Transformer decoder, it supports 25 languages\nprimarily European. The model was trained on 1.7M hours of total data samples,\nincluding Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce\nhallucinations for ASR and AST. We describe its two-stage pre-training and\nfine-tuning process with dynamic data balancing, as well as experiments with an\nnGPT encoder. Results show nGPT scales well with massive data, while\nFastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the\nNeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable\nsegment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2\noutperforms Whisper-large-v3 on English ASR while being 10x faster, and\ndelivers competitive multilingual ASR and AST performance against larger models\nlike Seamless-M4T-v2-large and LLM-based systems. We also release\nParakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the\nsame 25 languages with just 600M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report introduces Canary-1B-v2, a fast, robust multilingual model for\nAutomatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built\nwith a FastConformer encoder and Transformer decoder, it supports 25 languages\nprimarily European. The model was trained on 1.7M hours of total data samples,\nincluding Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce\nhallucinations for ASR and AST. We describe its two-stage pre-training and\nfine-tuning process with dynamic data balancing, as well as experiments with an\nnGPT encoder. Results show nGPT scales well with massive data, while\nFastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the\nNeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable\nsegment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2\noutperforms Whisper-large-v3 on English ASR while being 10x faster, and\ndelivers competitive multilingual ASR and AST performance against larger models\nlike Seamless-M4T-v2-large and LLM-based systems. We also release\nParakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the\nsame 25 languages with just 600M parameters."
                },
                "authors": [
                    {
                        "name": "Monica Sekoyan"
                    },
                    {
                        "name": "Nithin Rao Koluguri"
                    },
                    {
                        "name": "Nune Tadevosyan"
                    },
                    {
                        "name": "Piotr Zelasko"
                    },
                    {
                        "name": "Travis Bartley"
                    },
                    {
                        "name": "Nick Karpov"
                    },
                    {
                        "name": "Jagadeesh Balam"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Mini Version of it Submitted to ICASSP 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14117v1",
                "updated": "2025-09-17T15:57:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    57,
                    51,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T15:57:51Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    57,
                    51,
                    2,
                    260,
                    0
                ],
                "title": "GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model"
                },
                "summary": "Vision-Language-Action (VLA) models often fail to generalize to novel camera\nviewpoints, a limitation stemming from their difficulty in inferring robust 3D\ngeometry from 2D images. We introduce GeoAware-VLA, a simple yet effective\napproach that enhances viewpoint invariance by integrating strong geometric\npriors into the vision backbone. Instead of training a visual encoder or\nrelying on explicit 3D data, we leverage a frozen, pretrained geometric vision\nmodel as a feature extractor. A trainable projection layer then adapts these\ngeometrically-rich features for the policy decoder, relieving it of the burden\nof learning 3D consistency from scratch. Through extensive evaluations on\nLIBERO benchmark subsets, we show GeoAware-VLA achieves substantial\nimprovements in zero-shot generalization to novel camera poses, boosting\nsuccess rates by over 2x in simulation. Crucially, these benefits translate to\nthe physical world; our model shows a significant performance gain on a real\nrobot, especially when evaluated from unseen camera angles. Our approach proves\neffective across both continuous and discrete action spaces, highlighting that\nrobust geometric grounding is a key component for creating more generalizable\nrobotic agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models often fail to generalize to novel camera\nviewpoints, a limitation stemming from their difficulty in inferring robust 3D\ngeometry from 2D images. We introduce GeoAware-VLA, a simple yet effective\napproach that enhances viewpoint invariance by integrating strong geometric\npriors into the vision backbone. Instead of training a visual encoder or\nrelying on explicit 3D data, we leverage a frozen, pretrained geometric vision\nmodel as a feature extractor. A trainable projection layer then adapts these\ngeometrically-rich features for the policy decoder, relieving it of the burden\nof learning 3D consistency from scratch. Through extensive evaluations on\nLIBERO benchmark subsets, we show GeoAware-VLA achieves substantial\nimprovements in zero-shot generalization to novel camera poses, boosting\nsuccess rates by over 2x in simulation. Crucially, these benefits translate to\nthe physical world; our model shows a significant performance gain on a real\nrobot, especially when evaluated from unseen camera angles. Our approach proves\neffective across both continuous and discrete action spaces, highlighting that\nrobust geometric grounding is a key component for creating more generalizable\nrobotic agents."
                },
                "authors": [
                    {
                        "name": "Ali Abouzeid"
                    },
                    {
                        "name": "Malak Mansour"
                    },
                    {
                        "name": "Zezhou Sun"
                    },
                    {
                        "name": "Dezhen Song"
                    }
                ],
                "author_detail": {
                    "name": "Dezhen Song"
                },
                "author": "Dezhen Song",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24621v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24621v2",
                "updated": "2025-09-17T15:53:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    53,
                    19,
                    2,
                    260,
                    0
                ],
                "published": "2025-05-30T14:12:07Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    12,
                    7,
                    4,
                    150,
                    0
                ],
                "title": "Benchmarking Large Language Models for Cryptanalysis and Side-Channel\n  Vulnerabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Large Language Models for Cryptanalysis and Side-Channel\n  Vulnerabilities"
                },
                "summary": "Recent advancements in large language models (LLMs) have transformed natural\nlanguage understanding and generation, leading to extensive benchmarking across\ndiverse tasks. However, cryptanalysis - a critical area for data security and\nits connection to LLMs' generalization abilities - remains underexplored in LLM\nevaluations. To address this gap, we evaluate the cryptanalytic potential of\nstate-of-the-art LLMs on ciphertexts produced by a range of cryptographic\nalgorithms. We introduce a benchmark dataset of diverse plaintexts, spanning\nmultiple domains, lengths, writing styles, and topics, paired with their\nencrypted versions. Using zero-shot and few-shot settings along with\nchain-of-thought prompting, we assess LLMs' decryption success rate and discuss\ntheir comprehension abilities. Our findings reveal key insights into LLMs'\nstrengths and limitations in side-channel scenarios and raise concerns about\ntheir susceptibility to under-generalization-related attacks. This research\nhighlights the dual-use nature of LLMs in security contexts and contributes to\nthe ongoing discussion on AI safety and security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have transformed natural\nlanguage understanding and generation, leading to extensive benchmarking across\ndiverse tasks. However, cryptanalysis - a critical area for data security and\nits connection to LLMs' generalization abilities - remains underexplored in LLM\nevaluations. To address this gap, we evaluate the cryptanalytic potential of\nstate-of-the-art LLMs on ciphertexts produced by a range of cryptographic\nalgorithms. We introduce a benchmark dataset of diverse plaintexts, spanning\nmultiple domains, lengths, writing styles, and topics, paired with their\nencrypted versions. Using zero-shot and few-shot settings along with\nchain-of-thought prompting, we assess LLMs' decryption success rate and discuss\ntheir comprehension abilities. Our findings reveal key insights into LLMs'\nstrengths and limitations in side-channel scenarios and raise concerns about\ntheir susceptibility to under-generalization-related attacks. This research\nhighlights the dual-use nature of LLMs in security contexts and contributes to\nthe ongoing discussion on AI safety and security."
                },
                "authors": [
                    {
                        "name": "Utsav Maskey"
                    },
                    {
                        "name": "Chencheng Zhu"
                    },
                    {
                        "name": "Usman Naseem"
                    }
                ],
                "author_detail": {
                    "name": "Usman Naseem"
                },
                "author": "Usman Naseem",
                "arxiv_comment": "EMNLP'25 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24621v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24621v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14104v1",
                "updated": "2025-09-17T15:47:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    47,
                    18,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T15:47:18Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    47,
                    18,
                    2,
                    260,
                    0
                ],
                "title": "CSMoE: An Efficient Remote Sensing Foundation Model with Soft\n  Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSMoE: An Efficient Remote Sensing Foundation Model with Soft\n  Mixture-of-Experts"
                },
                "summary": "Self-supervised learning through masked autoencoders has attracted great\nattention for remote sensing (RS) foundation model (FM) development, enabling\nimproved representation learning across diverse sensors and downstream tasks.\nHowever, existing RS FMs often either suffer from substantial computational\ncomplexity during both training and inference or exhibit limited\nrepresentational capacity. These issues restrict their practical applicability\nin RS. To address this limitation, we propose an adaptation for enhancing the\nefficiency of RS FMs by integrating the Soft mixture-of-experts (MoE) mechanism\ninto the FM. The integration of Soft MoEs into the FM allows modality-specific\nexpert specialization alongside shared cross-sensor representation learning. To\ndemonstrate the effectiveness of our adaptation, we apply it on the\nCross-Sensor Masked Autoencoder (CSMAE) model, resulting in the Cross-Sensor\nMixture-of-Experts (CSMoE) model. In addition, we introduce a thematic-climatic\ndescriptor-driven sampling strategy for the construction of a representative\nand diverse training set to train our CSMoE model. Extensive experiments on\nscene classification, semantic segmentation, and content-based image retrieval\ndemonstrate that our adaptation yields a reduction in computational\nrequirements while maintaining or improving representational performance.\nCompared to state-of-the-art RS FMs, CSMoE achieves a superior trade-off\nbetween representational capacity, accuracy, and computational efficiency. On\naverage, CSMoE achieves more than twice the computational efficiency of\nexisting RS FMs, while maintaining competitive performance across all\nexperiments. These results show the effectiveness of the proposed adaptation\nfor creating computationally efficient RS FMs. The code for the model, the\ntraining set creation, and the model weights will be available at\nhttps://git.tu-berlin.de/rsim/csmoe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-supervised learning through masked autoencoders has attracted great\nattention for remote sensing (RS) foundation model (FM) development, enabling\nimproved representation learning across diverse sensors and downstream tasks.\nHowever, existing RS FMs often either suffer from substantial computational\ncomplexity during both training and inference or exhibit limited\nrepresentational capacity. These issues restrict their practical applicability\nin RS. To address this limitation, we propose an adaptation for enhancing the\nefficiency of RS FMs by integrating the Soft mixture-of-experts (MoE) mechanism\ninto the FM. The integration of Soft MoEs into the FM allows modality-specific\nexpert specialization alongside shared cross-sensor representation learning. To\ndemonstrate the effectiveness of our adaptation, we apply it on the\nCross-Sensor Masked Autoencoder (CSMAE) model, resulting in the Cross-Sensor\nMixture-of-Experts (CSMoE) model. In addition, we introduce a thematic-climatic\ndescriptor-driven sampling strategy for the construction of a representative\nand diverse training set to train our CSMoE model. Extensive experiments on\nscene classification, semantic segmentation, and content-based image retrieval\ndemonstrate that our adaptation yields a reduction in computational\nrequirements while maintaining or improving representational performance.\nCompared to state-of-the-art RS FMs, CSMoE achieves a superior trade-off\nbetween representational capacity, accuracy, and computational efficiency. On\naverage, CSMoE achieves more than twice the computational efficiency of\nexisting RS FMs, while maintaining competitive performance across all\nexperiments. These results show the effectiveness of the proposed adaptation\nfor creating computationally efficient RS FMs. The code for the model, the\ntraining set creation, and the model weights will be available at\nhttps://git.tu-berlin.de/rsim/csmoe."
                },
                "authors": [
                    {
                        "name": "Leonard Hackel"
                    },
                    {
                        "name": "Tom Burgert"
                    },
                    {
                        "name": "Begüm Demir"
                    }
                ],
                "author_detail": {
                    "name": "Begüm Demir"
                },
                "author": "Begüm Demir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14093v1",
                "updated": "2025-09-17T15:33:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    33,
                    44,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T15:33:44Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    33,
                    44,
                    2,
                    260,
                    0
                ],
                "title": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A\n  Self-Optimizing Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A\n  Self-Optimizing Framework"
                },
                "summary": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nprompting intermediate steps, improving accuracy and robustness in arithmetic,\nlogic, and commonsense tasks. However, this benefit comes with high\ncomputational costs: longer outputs increase latency, memory usage, and\nKV-cache demands. These issues are especially critical in software engineering\ntasks where concise and deterministic outputs are required. To investigate\nthese trade-offs, we conduct an empirical study based on code generation\nbenchmarks. The results reveal that longer CoT does not always help. Excessive\nreasoning often causes truncation, accuracy drops, and latency up to five times\nhigher, with failed outputs consistently longer than successful ones. These\nfindings challenge the assumption that longer reasoning is inherently better\nand highlight the need for adaptive CoT control. Motivated by this, we propose\nSEER (Self-Enhancing Efficient Reasoning), an adaptive framework that\ncompresses CoT while preserving accuracy. SEER combines Best-of-N sampling with\ntask-aware adaptive filtering, dynamically adjusting thresholds based on\npre-inference outputs to reduce verbosity and computational overhead. We then\nevaluate SEER on three software engineering tasks and one math task. On\naverage, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,\nand eliminates most infinite loops. These results demonstrate SEER as a\npractical method to make CoT-enhanced LLMs more efficient and robust, even\nunder resource constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nprompting intermediate steps, improving accuracy and robustness in arithmetic,\nlogic, and commonsense tasks. However, this benefit comes with high\ncomputational costs: longer outputs increase latency, memory usage, and\nKV-cache demands. These issues are especially critical in software engineering\ntasks where concise and deterministic outputs are required. To investigate\nthese trade-offs, we conduct an empirical study based on code generation\nbenchmarks. The results reveal that longer CoT does not always help. Excessive\nreasoning often causes truncation, accuracy drops, and latency up to five times\nhigher, with failed outputs consistently longer than successful ones. These\nfindings challenge the assumption that longer reasoning is inherently better\nand highlight the need for adaptive CoT control. Motivated by this, we propose\nSEER (Self-Enhancing Efficient Reasoning), an adaptive framework that\ncompresses CoT while preserving accuracy. SEER combines Best-of-N sampling with\ntask-aware adaptive filtering, dynamically adjusting thresholds based on\npre-inference outputs to reduce verbosity and computational overhead. We then\nevaluate SEER on three software engineering tasks and one math task. On\naverage, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,\nand eliminates most infinite loops. These results demonstrate SEER as a\npractical method to make CoT-enhanced LLMs more efficient and robust, even\nunder resource constraints."
                },
                "authors": [
                    {
                        "name": "Kerui Huang"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Tongtong Xu"
                    },
                    {
                        "name": "Lingfeng Bao"
                    },
                    {
                        "name": "Xin Xia"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xia"
                },
                "author": "Xin Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14092v1",
                "updated": "2025-09-17T15:33:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    33,
                    41,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T15:33:41Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    33,
                    41,
                    2,
                    260,
                    0
                ],
                "title": "Parallelizable Feynman-Kac Models for Universal Probabilistic\n  Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallelizable Feynman-Kac Models for Universal Probabilistic\n  Programming"
                },
                "summary": "We study provably correct and efficient instantiations of Sequential Monte\nCarlo (SMC) inference in the context of formal operational semantics of\nProbabilistic Programs (PPs). We focus on universal PPs featuring sampling from\narbitrary measures and conditioning/reweighting in unbounded loops. We first\nequip Probabilistic Program Graphs (PPGs), an automata-theoretic description\nformat of PPs, with an expectation-based semantics over infinite execution\ntraces, which also incorporates trace weights. We then prove a finite\napproximation theorem that provides bounds to this semantics based on\nexpectations taken over finite, fixed-length traces. This enables us to frame\nour semantics within a Feynman-Kac (FK) model, and ensures the consistency of\nthe Particle Filtering (PF) algorithm, an instance of SMC, with respect to our\nsemantics. Building on these results, we introduce VPF, a vectorized version of\nthe PF algorithm tailored to PPGs and our semantics. Experiments conducted with\na proof-of-concept implementation of VPF show very promising results compared\nto state-of-the-art PP inference tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study provably correct and efficient instantiations of Sequential Monte\nCarlo (SMC) inference in the context of formal operational semantics of\nProbabilistic Programs (PPs). We focus on universal PPs featuring sampling from\narbitrary measures and conditioning/reweighting in unbounded loops. We first\nequip Probabilistic Program Graphs (PPGs), an automata-theoretic description\nformat of PPs, with an expectation-based semantics over infinite execution\ntraces, which also incorporates trace weights. We then prove a finite\napproximation theorem that provides bounds to this semantics based on\nexpectations taken over finite, fixed-length traces. This enables us to frame\nour semantics within a Feynman-Kac (FK) model, and ensures the consistency of\nthe Particle Filtering (PF) algorithm, an instance of SMC, with respect to our\nsemantics. Building on these results, we introduce VPF, a vectorized version of\nthe PF algorithm tailored to PPGs and our semantics. Experiments conducted with\na proof-of-concept implementation of VPF show very promising results compared\nto state-of-the-art PP inference tools."
                },
                "authors": [
                    {
                        "name": "Michele Boreale"
                    },
                    {
                        "name": "Luisa Collodi"
                    }
                ],
                "author_detail": {
                    "name": "Luisa Collodi"
                },
                "arxiv_affiliation": "University of Florence",
                "author": "Luisa Collodi",
                "arxiv_doi": "10.4204/EPTCS.428.8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4204/EPTCS.428.8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.14092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings GandALF 2025, arXiv:2509.13258",
                "arxiv_journal_ref": "EPTCS 428, 2025, pp. 91-110",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12443v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12443v2",
                "updated": "2025-09-17T15:29:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    29,
                    42,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-15T20:50:15Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    20,
                    50,
                    15,
                    0,
                    258,
                    0
                ],
                "title": "From Legacy Fortran to Portable Kokkos: An Autonomous Agentic AI\n  Workflow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Legacy Fortran to Portable Kokkos: An Autonomous Agentic AI\n  Workflow"
                },
                "summary": "Scientific applications continue to rely on legacy Fortran codebases\noriginally developed for homogeneous, CPU-based systems. As High-Performance\nComputing (HPC) shifts toward heterogeneous GPU-accelerated architectures, many\naccelerators lack native Fortran bindings, creating an urgent need to modernize\nlegacy codes for portability. Frameworks like Kokkos provide performance\nportability and a single-source C++ abstraction, but manual Fortran-to-Kokkos\nporting demands significant expertise and time. Large language models (LLMs)\nhave shown promise in source-to-source code generation, yet their use in fully\nautonomous workflows for translating and optimizing parallel code remains\nlargely unexplored, especially for performance portability across diverse\nhardware. This paper presents an agentic AI workflow where specialized LLM\n\"agents\" collaborate to translate, validate, compile, run, test, debug, and\noptimize Fortran kernels into portable Kokkos C++ programs. Results show the\npipeline modernizes a range of benchmark kernels, producing\nperformance-portable Kokkos codes across hardware partitions. Paid OpenAI\nmodels such as GPT-5 and o4-mini-high executed the workflow for only a few U.S.\ndollars, generating optimized codes that surpassed Fortran baselines, whereas\nopen-source models like Llama4-Maverick often failed to yield functional codes.\nThis work demonstrates the feasibility of agentic AI for Fortran-to-Kokkos\ntransformation and offers a pathway for autonomously modernizing legacy\nscientific applications to run portably and efficiently on diverse\nsupercomputers. It further highlights the potential of LLM-driven agentic\nsystems to perform structured, domain-specific reasoning tasks in scientific\nand systems-oriented applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific applications continue to rely on legacy Fortran codebases\noriginally developed for homogeneous, CPU-based systems. As High-Performance\nComputing (HPC) shifts toward heterogeneous GPU-accelerated architectures, many\naccelerators lack native Fortran bindings, creating an urgent need to modernize\nlegacy codes for portability. Frameworks like Kokkos provide performance\nportability and a single-source C++ abstraction, but manual Fortran-to-Kokkos\nporting demands significant expertise and time. Large language models (LLMs)\nhave shown promise in source-to-source code generation, yet their use in fully\nautonomous workflows for translating and optimizing parallel code remains\nlargely unexplored, especially for performance portability across diverse\nhardware. This paper presents an agentic AI workflow where specialized LLM\n\"agents\" collaborate to translate, validate, compile, run, test, debug, and\noptimize Fortran kernels into portable Kokkos C++ programs. Results show the\npipeline modernizes a range of benchmark kernels, producing\nperformance-portable Kokkos codes across hardware partitions. Paid OpenAI\nmodels such as GPT-5 and o4-mini-high executed the workflow for only a few U.S.\ndollars, generating optimized codes that surpassed Fortran baselines, whereas\nopen-source models like Llama4-Maverick often failed to yield functional codes.\nThis work demonstrates the feasibility of agentic AI for Fortran-to-Kokkos\ntransformation and offers a pathway for autonomously modernizing legacy\nscientific applications to run portably and efficiently on diverse\nsupercomputers. It further highlights the potential of LLM-driven agentic\nsystems to perform structured, domain-specific reasoning tasks in scientific\nand systems-oriented applications."
                },
                "authors": [
                    {
                        "name": "Sparsh Gupta"
                    },
                    {
                        "name": "Kamalavasan Kamalakkannan"
                    },
                    {
                        "name": "Maxim Moraru"
                    },
                    {
                        "name": "Galen Shipman"
                    },
                    {
                        "name": "Patrick Diehl"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Diehl"
                },
                "author": "Patrick Diehl",
                "arxiv_comment": "11 pages, 6 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12443v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12443v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14066v1",
                "updated": "2025-09-17T15:15:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    15,
                    3,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T15:15:03Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    15,
                    3,
                    2,
                    260,
                    0
                ],
                "title": "A neuromorphic continuous soil monitoring system for precision\n  irrigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A neuromorphic continuous soil monitoring system for precision\n  irrigation"
                },
                "summary": "Sensory processing at the edge requires ultra-low power stand-alone computing\ntechnologies. This is particularly true for modern agriculture and precision\nirrigation systems which aim to optimize water usage by monitoring key\nenvironmental observables continuously using distributed efficient embedded\nprocessing elements. Neuromorphic processing systems are emerging as a\npromising technology for extreme edge-computing applications that need to run\non resource-constrained hardware. As such, they are a very good candidate for\nimplementing efficient water management systems based on data measured from\nsoil and plants, across large fields. In this work, we present a fully\nenergy-efficient neuromorphic irrigation control system that operates\nautonomously without any need for data transmission or remote processing.\nLeveraging the properties of a biologically realistic spiking neural network,\nour system performs computation, and decision-making locally. We validate this\napproach using real-world soil moisture data from apple and kiwi orchards\napplied to a mixed-signal neuromorphic processor, and show that the generated\nirrigation commands closely match those derived from conventional methods\nacross different soil depths. Our results show that local neuromorphic\ninference can maintain decision accuracy, paving the way for autonomous,\nsustainable irrigation solutions at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensory processing at the edge requires ultra-low power stand-alone computing\ntechnologies. This is particularly true for modern agriculture and precision\nirrigation systems which aim to optimize water usage by monitoring key\nenvironmental observables continuously using distributed efficient embedded\nprocessing elements. Neuromorphic processing systems are emerging as a\npromising technology for extreme edge-computing applications that need to run\non resource-constrained hardware. As such, they are a very good candidate for\nimplementing efficient water management systems based on data measured from\nsoil and plants, across large fields. In this work, we present a fully\nenergy-efficient neuromorphic irrigation control system that operates\nautonomously without any need for data transmission or remote processing.\nLeveraging the properties of a biologically realistic spiking neural network,\nour system performs computation, and decision-making locally. We validate this\napproach using real-world soil moisture data from apple and kiwi orchards\napplied to a mixed-signal neuromorphic processor, and show that the generated\nirrigation commands closely match those derived from conventional methods\nacross different soil depths. Our results show that local neuromorphic\ninference can maintain decision accuracy, paving the way for autonomous,\nsustainable irrigation solutions at scale."
                },
                "authors": [
                    {
                        "name": "Mirco Tincani"
                    },
                    {
                        "name": "Khaled Kerouch"
                    },
                    {
                        "name": "Umberto Garlando"
                    },
                    {
                        "name": "Mattia Barezzi"
                    },
                    {
                        "name": "Alessandro Sanginario"
                    },
                    {
                        "name": "Giacomo Indiveri"
                    },
                    {
                        "name": "Chiara De Luca"
                    }
                ],
                "author_detail": {
                    "name": "Chiara De Luca"
                },
                "author": "Chiara De Luca",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14065v1",
                "updated": "2025-09-17T15:14:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    14,
                    29,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T15:14:29Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    14,
                    29,
                    2,
                    260,
                    0
                ],
                "title": "Identifying Network Structure of Linear Dynamical Systems: Observability\n  and Edge Misclassification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Network Structure of Linear Dynamical Systems: Observability\n  and Edge Misclassification"
                },
                "summary": "This work studies the limitations of uniquely identifying a linear network's\ntopology from partial measurements of its nodes. We show that the set of\nnetworks that are consistent with the measurements are related through the\nnullspace of the observability matrix for the true network. In doing so, we\nillustrate how potentially many networks are fully consistent with the\nmeasurements despite having topologies that are structurally inconsistent with\neach other, an often neglected consideration in the design of topology\ninference methods. We then provide an aggregate characterization of the space\nof possible networks by analytically solving for the most structurally\ndissimilar network. We find that when observing over 6% of nodes in random\nnetwork models (e.g., Erd\\H{o}s-R\\'{e}nyi and Watts-Strogatz) the rate of edge\nmisclassification drops to ~1%. Extending this discussion, we construct a\nfamily of networks that keep measurements $\\epsilon$-\"close\" to each other, and\nconnect the identifiability of these networks to the spectral properties of an\naugmented observability Gramian.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work studies the limitations of uniquely identifying a linear network's\ntopology from partial measurements of its nodes. We show that the set of\nnetworks that are consistent with the measurements are related through the\nnullspace of the observability matrix for the true network. In doing so, we\nillustrate how potentially many networks are fully consistent with the\nmeasurements despite having topologies that are structurally inconsistent with\neach other, an often neglected consideration in the design of topology\ninference methods. We then provide an aggregate characterization of the space\nof possible networks by analytically solving for the most structurally\ndissimilar network. We find that when observing over 6% of nodes in random\nnetwork models (e.g., Erd\\H{o}s-R\\'{e}nyi and Watts-Strogatz) the rate of edge\nmisclassification drops to ~1%. Extending this discussion, we construct a\nfamily of networks that keep measurements $\\epsilon$-\"close\" to each other, and\nconnect the identifiability of these networks to the spectral properties of an\naugmented observability Gramian."
                },
                "authors": [
                    {
                        "name": "Jaidev Gill"
                    },
                    {
                        "name": "Jing Shuang Li"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shuang Li"
                },
                "author": "Jing Shuang Li",
                "arxiv_comment": "7 pages, 5 figures, in submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14061v1",
                "updated": "2025-09-17T15:05:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    5,
                    15,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T15:05:15Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    5,
                    15,
                    2,
                    260,
                    0
                ],
                "title": "Queen Detection in Beehives via Environmental Sensor Fusion for\n  Low-Power Edge Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Queen Detection in Beehives via Environmental Sensor Fusion for\n  Low-Power Edge Computing"
                },
                "summary": "Queen bee presence is essential for the health and stability of honeybee\ncolonies, yet current monitoring methods rely on manual inspections that are\nlabor-intensive, disruptive, and impractical for large-scale beekeeping. While\nrecent audio-based approaches have shown promise, they often require high power\nconsumption, complex preprocessing, and are susceptible to ambient noise. To\novercome these limitations, we propose a lightweight, multimodal system for\nqueen detection based on environmental sensor fusion-specifically, temperature,\nhumidity, and pressure differentials between the inside and outside of the\nhive. Our approach employs quantized decision tree inference on a commercial\nSTM32 microcontroller, enabling real-time, low-power edge computing without\ncompromising accuracy. We show that our system achieves over 99% queen\ndetection accuracy using only environmental inputs, with audio features\noffering no significant performance gain. This work presents a scalable and\nsustainable solution for non-invasive hive monitoring, paving the way for\nautonomous, precision beekeeping using off-the-shelf, energy-efficient\nhardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Queen bee presence is essential for the health and stability of honeybee\ncolonies, yet current monitoring methods rely on manual inspections that are\nlabor-intensive, disruptive, and impractical for large-scale beekeeping. While\nrecent audio-based approaches have shown promise, they often require high power\nconsumption, complex preprocessing, and are susceptible to ambient noise. To\novercome these limitations, we propose a lightweight, multimodal system for\nqueen detection based on environmental sensor fusion-specifically, temperature,\nhumidity, and pressure differentials between the inside and outside of the\nhive. Our approach employs quantized decision tree inference on a commercial\nSTM32 microcontroller, enabling real-time, low-power edge computing without\ncompromising accuracy. We show that our system achieves over 99% queen\ndetection accuracy using only environmental inputs, with audio features\noffering no significant performance gain. This work presents a scalable and\nsustainable solution for non-invasive hive monitoring, paving the way for\nautonomous, precision beekeeping using off-the-shelf, energy-efficient\nhardware."
                },
                "authors": [
                    {
                        "name": "Chiara De Luca"
                    },
                    {
                        "name": "Elisa Donati"
                    }
                ],
                "author_detail": {
                    "name": "Elisa Donati"
                },
                "author": "Elisa Donati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14054v1",
                "updated": "2025-09-17T14:56:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    56,
                    31,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T14:56:31Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    56,
                    31,
                    2,
                    260,
                    0
                ],
                "title": "Physics-based deep kernel learning for parameter estimation in high\n  dimensional PDEs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-based deep kernel learning for parameter estimation in high\n  dimensional PDEs"
                },
                "summary": "Inferring parameters of high-dimensional partial differential equations\n(PDEs) poses significant computational and inferential challenges, primarily\ndue to the curse of dimensionality and the inherent limitations of traditional\nnumerical methods. This paper introduces a novel two-stage Bayesian framework\nthat synergistically integrates training, physics-based deep kernel learning\n(DKL) with Hamiltonian Monte Carlo (HMC) to robustly infer unknown PDE\nparameters and quantify their uncertainties from sparse, exact observations.\nThe first stage leverages physics-based DKL to train a surrogate model, which\njointly yields an optimized neural network feature extractor and robust initial\nestimates for the PDE parameters. In the second stage, with the neural network\nweights fixed, HMC is employed within a full Bayesian framework to efficiently\nsample the joint posterior distribution of the kernel hyperparameters and the\nPDE parameters. Numerical experiments on canonical and high-dimensional inverse\nPDE problems demonstrate that our framework accurately estimates parameters,\nprovides reliable uncertainty estimates, and effectively addresses challenges\nof data sparsity and model complexity, offering a robust and scalable tool for\ndiverse scientific and engineering applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring parameters of high-dimensional partial differential equations\n(PDEs) poses significant computational and inferential challenges, primarily\ndue to the curse of dimensionality and the inherent limitations of traditional\nnumerical methods. This paper introduces a novel two-stage Bayesian framework\nthat synergistically integrates training, physics-based deep kernel learning\n(DKL) with Hamiltonian Monte Carlo (HMC) to robustly infer unknown PDE\nparameters and quantify their uncertainties from sparse, exact observations.\nThe first stage leverages physics-based DKL to train a surrogate model, which\njointly yields an optimized neural network feature extractor and robust initial\nestimates for the PDE parameters. In the second stage, with the neural network\nweights fixed, HMC is employed within a full Bayesian framework to efficiently\nsample the joint posterior distribution of the kernel hyperparameters and the\nPDE parameters. Numerical experiments on canonical and high-dimensional inverse\nPDE problems demonstrate that our framework accurately estimates parameters,\nprovides reliable uncertainty estimates, and effectively addresses challenges\nof data sparsity and model complexity, offering a robust and scalable tool for\ndiverse scientific and engineering applications."
                },
                "authors": [
                    {
                        "name": "Weihao Yan"
                    },
                    {
                        "name": "Christoph Brune"
                    },
                    {
                        "name": "Mengwu Guo"
                    }
                ],
                "author_detail": {
                    "name": "Mengwu Guo"
                },
                "author": "Mengwu Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14053v1",
                "updated": "2025-09-17T14:55:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    55,
                    54,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T14:55:54Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    55,
                    54,
                    2,
                    260,
                    0
                ],
                "title": "Network representations reveal structured uncertainty in music",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network representations reveal structured uncertainty in music"
                },
                "summary": "Music, as a structured yet perceptually rich experience, can be modeled as a\nnetwork to uncover how humans encode and process auditory information. While\nnetwork-based representations of music are increasingly common, the impact of\nfeature selection on structural properties and cognitive alignment remains\nunderexplored. In this study, we evaluated eight network models, each\nconstructed from symbolic representations of piano compositions using distinct\ncombinations of pitch, octave, duration, and interval, designed to be\nrepresentative of existing approaches in the literature. By comparing these\nmodels through topological metrics, entropy analysis, and divergence with\nrespect to inferred cognitive representations, we assessed both their\nstructural and perceptual efficiency. Our findings reveal that simpler,\nfeature-specific models better match human perception, whereas complex,\nmultidimensional representations introduce cognitive inefficiencies. These\nresults support the view that humans rely on modular, parallel cognitive\nnetworks--an architecture consistent with theories of predictive processing and\nfree energy minimization. Moreover, we find that musical networks are\nstructurally organized to guide attention toward transitions that are both\nuncertain and inferable. The resulting structure concentrates uncertainty in a\nfew frequently visited nodes, creating local entropy gradients that alternate\nbetween stable and unpredictable regions, thereby enabling the expressive\ndynamics of tension and release that define the musical experience. These\nfindings show that network structures make the organization of uncertainty in\nmusic observable, offering new insight into how patterned flows of expectation\nshape perception, and open new directions for studying how musical structures\nevolve across genres, cultures, and historical periods through the lens of\nnetwork science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Music, as a structured yet perceptually rich experience, can be modeled as a\nnetwork to uncover how humans encode and process auditory information. While\nnetwork-based representations of music are increasingly common, the impact of\nfeature selection on structural properties and cognitive alignment remains\nunderexplored. In this study, we evaluated eight network models, each\nconstructed from symbolic representations of piano compositions using distinct\ncombinations of pitch, octave, duration, and interval, designed to be\nrepresentative of existing approaches in the literature. By comparing these\nmodels through topological metrics, entropy analysis, and divergence with\nrespect to inferred cognitive representations, we assessed both their\nstructural and perceptual efficiency. Our findings reveal that simpler,\nfeature-specific models better match human perception, whereas complex,\nmultidimensional representations introduce cognitive inefficiencies. These\nresults support the view that humans rely on modular, parallel cognitive\nnetworks--an architecture consistent with theories of predictive processing and\nfree energy minimization. Moreover, we find that musical networks are\nstructurally organized to guide attention toward transitions that are both\nuncertain and inferable. The resulting structure concentrates uncertainty in a\nfew frequently visited nodes, creating local entropy gradients that alternate\nbetween stable and unpredictable regions, thereby enabling the expressive\ndynamics of tension and release that define the musical experience. These\nfindings show that network structures make the organization of uncertainty in\nmusic observable, offering new insight into how patterned flows of expectation\nshape perception, and open new directions for studying how musical structures\nevolve across genres, cultures, and historical periods through the lens of\nnetwork science."
                },
                "authors": [
                    {
                        "name": "Lluc Bono Rosselló"
                    },
                    {
                        "name": "Robert Jankowski"
                    },
                    {
                        "name": "Hugues Bersini"
                    },
                    {
                        "name": "Marián Boguñá"
                    },
                    {
                        "name": "M. Ángeles Serrano"
                    }
                ],
                "author_detail": {
                    "name": "M. Ángeles Serrano"
                },
                "author": "M. Ángeles Serrano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14049v1",
                "updated": "2025-09-17T14:53:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    53,
                    56,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T14:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    53,
                    56,
                    2,
                    260,
                    0
                ],
                "title": "Comprehensive Evaluation of CNN-Based Audio Tagging Models on\n  Resource-Constrained Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive Evaluation of CNN-Based Audio Tagging Models on\n  Resource-Constrained Devices"
                },
                "summary": "Convolutional Neural Networks (CNNs) have demonstrated exceptional\nperformance in audio tagging tasks. However, deploying these models on\nresource-constrained devices like the Raspberry Pi poses challenges related to\ncomputational efficiency and thermal management. In this paper, a comprehensive\nevaluation of multiple convolutional neural network (CNN) architectures for\naudio tagging on the Raspberry Pi is conducted, encompassing all 1D and 2D\nmodels from the Pretrained Audio Neural Networks (PANNs) framework, a\nConvNeXt-based model adapted for audio classification, as well as MobileNetV3\narchitectures. In addition, two PANNs-derived networks, CNN9 and CNN13,\nrecently proposed, are also evaluated. To enhance deployment efficiency and\nportability across diverse hardware platforms, all models are converted to the\nOpen Neural Network Exchange (ONNX) format. Unlike previous works that focus on\na single model, our analysis encompasses a broader range of architectures and\ninvolves continuous 24-hour inference sessions to assess performance stability.\nOur experiments reveal that, with appropriate model selection and optimization,\nit is possible to maintain consistent inference latency and manage thermal\nbehavior effectively over extended periods. These findings provide valuable\ninsights for deploying audio tagging models in real-world edge computing\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convolutional Neural Networks (CNNs) have demonstrated exceptional\nperformance in audio tagging tasks. However, deploying these models on\nresource-constrained devices like the Raspberry Pi poses challenges related to\ncomputational efficiency and thermal management. In this paper, a comprehensive\nevaluation of multiple convolutional neural network (CNN) architectures for\naudio tagging on the Raspberry Pi is conducted, encompassing all 1D and 2D\nmodels from the Pretrained Audio Neural Networks (PANNs) framework, a\nConvNeXt-based model adapted for audio classification, as well as MobileNetV3\narchitectures. In addition, two PANNs-derived networks, CNN9 and CNN13,\nrecently proposed, are also evaluated. To enhance deployment efficiency and\nportability across diverse hardware platforms, all models are converted to the\nOpen Neural Network Exchange (ONNX) format. Unlike previous works that focus on\na single model, our analysis encompasses a broader range of architectures and\ninvolves continuous 24-hour inference sessions to assess performance stability.\nOur experiments reveal that, with appropriate model selection and optimization,\nit is possible to maintain consistent inference latency and manage thermal\nbehavior effectively over extended periods. These findings provide valuable\ninsights for deploying audio tagging models in real-world edge computing\nscenarios."
                },
                "authors": [
                    {
                        "name": "Jordi Grau-Haro"
                    },
                    {
                        "name": "Ruben Ribes-Serrano"
                    },
                    {
                        "name": "Javier Naranjo-Alcazar"
                    },
                    {
                        "name": "Marta Garcia-Ballesteros"
                    },
                    {
                        "name": "Pedro Zuccarello"
                    }
                ],
                "author_detail": {
                    "name": "Pedro Zuccarello"
                },
                "author": "Pedro Zuccarello",
                "arxiv_comment": "Accepted at Computing Conference 2026, London, UK",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18916v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18916v2",
                "updated": "2025-09-17T14:42:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    42,
                    2,
                    2,
                    260,
                    0
                ],
                "published": "2025-05-25T00:50:43Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    0,
                    50,
                    43,
                    6,
                    145,
                    0
                ],
                "title": "SCRum-9: Multilingual Stance Classification over Rumours on Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCRum-9: Multilingual Stance Classification over Rumours on Social Media"
                },
                "summary": "We introduce SCRum-9, the largest multilingual Stance Classification dataset\nfor Rumour analysis in 9 languages, containing 7,516 tweets from X. SCRum-9\ngoes beyond existing stance classification datasets by covering more languages,\nlinking examples to more fact-checked claims (2.1k), and including\nconfidence-related annotations from multiple annotators to account for intra-\nand inter-annotator variability. Annotations were made by at least two native\nspeakers per language, totalling more than 405 hours of annotation and 8,150\ndollars in compensation. Further, SCRum-9 is used to benchmark five large\nlanguage models (LLMs) and two multilingual masked language models (MLMs) in\nIn-Context Learning (ICL) and fine-tuning setups. This paper also innovates by\nexploring the use of multilingual synthetic data for rumour stance\nclassification, showing that even LLMs with weak ICL performance can produce\nvaluable synthetic data for fine-tuning small MLMs, enabling them to achieve\nhigher performance than zero-shot ICL in LLMs. Finally, we examine the\nrelationship between model predictions and human uncertainty on ambiguous cases\nfinding that model predictions often match the second-choice labels assigned by\nannotators, rather than diverging entirely from human judgments. SCRum-9 is\npublicly released to the research community with potential to foster further\nresearch on multilingual analysis of misleading narratives on social media.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SCRum-9, the largest multilingual Stance Classification dataset\nfor Rumour analysis in 9 languages, containing 7,516 tweets from X. SCRum-9\ngoes beyond existing stance classification datasets by covering more languages,\nlinking examples to more fact-checked claims (2.1k), and including\nconfidence-related annotations from multiple annotators to account for intra-\nand inter-annotator variability. Annotations were made by at least two native\nspeakers per language, totalling more than 405 hours of annotation and 8,150\ndollars in compensation. Further, SCRum-9 is used to benchmark five large\nlanguage models (LLMs) and two multilingual masked language models (MLMs) in\nIn-Context Learning (ICL) and fine-tuning setups. This paper also innovates by\nexploring the use of multilingual synthetic data for rumour stance\nclassification, showing that even LLMs with weak ICL performance can produce\nvaluable synthetic data for fine-tuning small MLMs, enabling them to achieve\nhigher performance than zero-shot ICL in LLMs. Finally, we examine the\nrelationship between model predictions and human uncertainty on ambiguous cases\nfinding that model predictions often match the second-choice labels assigned by\nannotators, rather than diverging entirely from human judgments. SCRum-9 is\npublicly released to the research community with potential to foster further\nresearch on multilingual analysis of misleading narratives on social media."
                },
                "authors": [
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Jake Vasilakes"
                    },
                    {
                        "name": "Zhixue Zhao"
                    },
                    {
                        "name": "Carolina Scarton"
                    }
                ],
                "author_detail": {
                    "name": "Carolina Scarton"
                },
                "author": "Carolina Scarton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18916v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18916v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08627v2",
                "updated": "2025-09-17T14:38:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    38,
                    15,
                    2,
                    260,
                    0
                ],
                "published": "2025-07-11T14:29:21Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    29,
                    21,
                    4,
                    192,
                    0
                ],
                "title": "NL in the Middle: Code Translation with LLMs and Intermediate\n  Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NL in the Middle: Code Translation with LLMs and Intermediate\n  Representations"
                },
                "summary": "Studies show that large language models (LLMs) produce buggy code\ntranslations. One promising avenue to improve translation accuracy is through\nintermediate representations, which provide structured guidance for the\ntranslation process. We investigate whether LLM-based code translation can\nbenefit from intermediate representations, specifically in the form of natural\nlanguage (NL) summaries and abstract syntax trees (ASTs). Since prompt\nengineering greatly affects LLM performance, we consider several ways to\nintegrate these representations, from one-shot to chain-of-thought (CoT)\nprompting. Using Open GPT4 8X7B and specialized StarCoder and CodeGen models on\npopular code translation benchmarks (CodeNet and AVATAR), we find that CoT with\nan intermediate NL summary performs best, with an increase of 13.8% and 6.7%,\nrespectively, in successful translations for the best-performing model (Open\nGPT4 8X7B) compared to the zero-shot prompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Studies show that large language models (LLMs) produce buggy code\ntranslations. One promising avenue to improve translation accuracy is through\nintermediate representations, which provide structured guidance for the\ntranslation process. We investigate whether LLM-based code translation can\nbenefit from intermediate representations, specifically in the form of natural\nlanguage (NL) summaries and abstract syntax trees (ASTs). Since prompt\nengineering greatly affects LLM performance, we consider several ways to\nintegrate these representations, from one-shot to chain-of-thought (CoT)\nprompting. Using Open GPT4 8X7B and specialized StarCoder and CodeGen models on\npopular code translation benchmarks (CodeNet and AVATAR), we find that CoT with\nan intermediate NL summary performs best, with an increase of 13.8% and 6.7%,\nrespectively, in successful translations for the best-performing model (Open\nGPT4 8X7B) compared to the zero-shot prompt."
                },
                "authors": [
                    {
                        "name": "Chi-en Amy Tai"
                    },
                    {
                        "name": "Pengyu Nie"
                    },
                    {
                        "name": "Lukasz Golab"
                    },
                    {
                        "name": "Alexander Wong"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Wong"
                },
                "author": "Alexander Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14034v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14034v1",
                "updated": "2025-09-17T14:34:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    34,
                    27,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T14:34:27Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    34,
                    27,
                    2,
                    260,
                    0
                ],
                "title": "Enhancing Multi-Agent Debate System Performance via Confidence\n  Expression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Multi-Agent Debate System Performance via Confidence\n  Expression"
                },
                "summary": "Generative Large Language Models (LLMs) have demonstrated remarkable\nperformance across a wide range of tasks. Recent research has introduced\nMulti-Agent Debate (MAD) systems, which leverage multiple LLMs to simulate\nhuman debate and thereby improve task performance. However, while some LLMs may\npossess superior knowledge or reasoning capabilities for specific tasks, they\noften struggle to clearly communicate this advantage during debates, in part\ndue to a lack of confidence expression. Moreover, inappropriate confidence\nexpression can cause agents in MAD systems to either stubbornly maintain\nincorrect beliefs or converge prematurely on suboptimal answers, ultimately\nreducing debate effectiveness and overall system performance. To address these\nchallenges, we propose incorporating confidence expression into MAD systems to\nallow LLMs to explicitly communicate their confidence levels. To validate this\napproach, we develop ConfMAD, a MAD framework that integrates confidence\nexpression throughout the debate process. Experimental results demonstrate the\neffectiveness of our method, and we further analyze how confidence influences\ndebate dynamics, offering insights into the design of confidence-aware MAD\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Large Language Models (LLMs) have demonstrated remarkable\nperformance across a wide range of tasks. Recent research has introduced\nMulti-Agent Debate (MAD) systems, which leverage multiple LLMs to simulate\nhuman debate and thereby improve task performance. However, while some LLMs may\npossess superior knowledge or reasoning capabilities for specific tasks, they\noften struggle to clearly communicate this advantage during debates, in part\ndue to a lack of confidence expression. Moreover, inappropriate confidence\nexpression can cause agents in MAD systems to either stubbornly maintain\nincorrect beliefs or converge prematurely on suboptimal answers, ultimately\nreducing debate effectiveness and overall system performance. To address these\nchallenges, we propose incorporating confidence expression into MAD systems to\nallow LLMs to explicitly communicate their confidence levels. To validate this\napproach, we develop ConfMAD, a MAD framework that integrates confidence\nexpression throughout the debate process. Experimental results demonstrate the\neffectiveness of our method, and we further analyze how confidence influences\ndebate dynamics, offering insights into the design of confidence-aware MAD\nsystems."
                },
                "authors": [
                    {
                        "name": "Zijie Lin"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_comment": "EMNLP'25 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14034v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14034v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14033v1",
                "updated": "2025-09-17T14:34:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    34,
                    2,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T14:34:02Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    34,
                    2,
                    2,
                    260,
                    0
                ],
                "title": "SAIL-VL2 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAIL-VL2 Technical Report"
                },
                "summary": "We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)\nfor comprehensive multimodal understanding and reasoning. As the successor to\nSAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B\nparameter scales across diverse image and video benchmarks, demonstrating\nstrong capabilities from fine-grained perception to complex reasoning. Three\ncore innovations drive its effectiveness. First, a large-scale data curation\npipeline with scoring and filtering strategies enhances both quality and\ndistribution across captioning, OCR, QA, and video data, improving training\nefficiency. Second, a progressive training framework begins with a powerful\npre-trained vision encoder (SAIL-ViT), advances through multimodal\npre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that\nsystematically strengthens model capabilities. Third, architectural advances\nextend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.\nWith these contributions, SAIL-VL2 demonstrates competitive performance across\n106 datasets and achieves state-of-the-art results on challenging reasoning\nbenchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass\nleaderboard, SAIL-VL2-2B ranks first among officially released open-source\nmodels under the 4B parameter scale, while serving as an efficient and\nextensible foundation for the open-source multimodal community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)\nfor comprehensive multimodal understanding and reasoning. As the successor to\nSAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B\nparameter scales across diverse image and video benchmarks, demonstrating\nstrong capabilities from fine-grained perception to complex reasoning. Three\ncore innovations drive its effectiveness. First, a large-scale data curation\npipeline with scoring and filtering strategies enhances both quality and\ndistribution across captioning, OCR, QA, and video data, improving training\nefficiency. Second, a progressive training framework begins with a powerful\npre-trained vision encoder (SAIL-ViT), advances through multimodal\npre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that\nsystematically strengthens model capabilities. Third, architectural advances\nextend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.\nWith these contributions, SAIL-VL2 demonstrates competitive performance across\n106 datasets and achieves state-of-the-art results on challenging reasoning\nbenchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass\nleaderboard, SAIL-VL2-2B ranks first among officially released open-source\nmodels under the 4B parameter scale, while serving as an efficient and\nextensible foundation for the open-source multimodal community."
                },
                "authors": [
                    {
                        "name": "Weijie Yin"
                    },
                    {
                        "name": "Yongjie Ye"
                    },
                    {
                        "name": "Fangxun Shu"
                    },
                    {
                        "name": "Yue Liao"
                    },
                    {
                        "name": "Zijian Kang"
                    },
                    {
                        "name": "Hongyuan Dong"
                    },
                    {
                        "name": "Haiyang Yu"
                    },
                    {
                        "name": "Dingkang Yang"
                    },
                    {
                        "name": "Jiacong Wang"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Wenzhuo Liu"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Shuicheng Yan"
                    },
                    {
                        "name": "Chao Feng"
                    }
                ],
                "author_detail": {
                    "name": "Chao Feng"
                },
                "author": "Chao Feng",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18506v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18506v4",
                "updated": "2025-09-17T14:32:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    32,
                    24,
                    2,
                    260,
                    0
                ],
                "published": "2024-11-27T16:48:24Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    16,
                    48,
                    24,
                    2,
                    332,
                    0
                ],
                "title": "LLM-ABBA: Understanding time series via symbolic approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-ABBA: Understanding time series via symbolic approximation"
                },
                "summary": "The success of large language models (LLMs) for time series has been\ndemonstrated in previous work. Utilizing a symbolic time series representation,\none can efficiently bridge the gap between LLMs and time series. However, the\nremaining challenge is to exploit the semantic information hidden in time\nseries by using symbols or existing tokens of LLMs, while aligning the\nembedding space of LLMs according to the hidden information of time series. The\nsymbolic time series approximation (STSA) method called adaptive Brownian\nbridge-based symbolic aggregation (ABBA) shows outstanding efficacy in\npreserving salient time series features by modeling time series patterns in\nterms of amplitude and period while using existing tokens of LLMs.\n  In this paper, we introduce a method, called LLM-ABBA, that integrates ABBA\ninto large language models for various downstream time series tasks. By\nsymbolizing time series, LLM-ABBA compares favorably to the recent\nstate-of-the-art (SOTA) in UCR and three medical time series classification\ntasks. Meanwhile, a fixed-polygonal chain trick in ABBA is introduced to\n\\kc{avoid obvious drifting} during prediction tasks by significantly mitigating\nthe effects of cumulative error arising from misused symbols during the\ntransition from symbols to numerical values. In time series regression tasks,\nLLM-ABBA achieves the new SOTA on Time Series Extrinsic Regression (TSER)\nbenchmarks. LLM-ABBA also shows competitive prediction capability compared to\nrecent SOTA time series prediction results. We believe this framework can also\nseamlessly extend to other time series tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of large language models (LLMs) for time series has been\ndemonstrated in previous work. Utilizing a symbolic time series representation,\none can efficiently bridge the gap between LLMs and time series. However, the\nremaining challenge is to exploit the semantic information hidden in time\nseries by using symbols or existing tokens of LLMs, while aligning the\nembedding space of LLMs according to the hidden information of time series. The\nsymbolic time series approximation (STSA) method called adaptive Brownian\nbridge-based symbolic aggregation (ABBA) shows outstanding efficacy in\npreserving salient time series features by modeling time series patterns in\nterms of amplitude and period while using existing tokens of LLMs.\n  In this paper, we introduce a method, called LLM-ABBA, that integrates ABBA\ninto large language models for various downstream time series tasks. By\nsymbolizing time series, LLM-ABBA compares favorably to the recent\nstate-of-the-art (SOTA) in UCR and three medical time series classification\ntasks. Meanwhile, a fixed-polygonal chain trick in ABBA is introduced to\n\\kc{avoid obvious drifting} during prediction tasks by significantly mitigating\nthe effects of cumulative error arising from misused symbols during the\ntransition from symbols to numerical values. In time series regression tasks,\nLLM-ABBA achieves the new SOTA on Time Series Extrinsic Regression (TSER)\nbenchmarks. LLM-ABBA also shows competitive prediction capability compared to\nrecent SOTA time series prediction results. We believe this framework can also\nseamlessly extend to other time series tasks."
                },
                "authors": [
                    {
                        "name": "Erin Carson"
                    },
                    {
                        "name": "Xinye Chen"
                    },
                    {
                        "name": "Cheng Kang"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Kang"
                },
                "author": "Cheng Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18506v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18506v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14030v1",
                "updated": "2025-09-17T14:31:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    31,
                    18,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T14:31:18Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    31,
                    18,
                    2,
                    260,
                    0
                ],
                "title": "CrowdAgent: Multi-Agent Managed Multi-Source Annotation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CrowdAgent: Multi-Agent Managed Multi-Source Annotation System"
                },
                "summary": "High-quality annotated data is a cornerstone of modern Natural Language\nProcessing (NLP). While recent methods begin to leverage diverse annotation\nsources-including Large Language Models (LLMs), Small Language Models (SLMs),\nand human experts-they often focus narrowly on the labeling step itself. A\ncritical gap remains in the holistic process control required to manage these\nsources dynamically, addressing complex scheduling and quality-cost trade-offs\nin a unified manner. Inspired by real-world crowdsourcing companies, we\nintroduce CrowdAgent, a multi-agent system that provides end-to-end process\ncontrol by integrating task assignment, data annotation, and quality/cost\nmanagement. It implements a novel methodology that rationally assigns tasks,\nenabling LLMs, SLMs, and human experts to advance synergistically in a\ncollaborative annotation workflow. We demonstrate the effectiveness of\nCrowdAgent through extensive experiments on six diverse multimodal\nclassification tasks. The source code and video demo are available at\nhttps://github.com/QMMMS/CrowdAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-quality annotated data is a cornerstone of modern Natural Language\nProcessing (NLP). While recent methods begin to leverage diverse annotation\nsources-including Large Language Models (LLMs), Small Language Models (SLMs),\nand human experts-they often focus narrowly on the labeling step itself. A\ncritical gap remains in the holistic process control required to manage these\nsources dynamically, addressing complex scheduling and quality-cost trade-offs\nin a unified manner. Inspired by real-world crowdsourcing companies, we\nintroduce CrowdAgent, a multi-agent system that provides end-to-end process\ncontrol by integrating task assignment, data annotation, and quality/cost\nmanagement. It implements a novel methodology that rationally assigns tasks,\nenabling LLMs, SLMs, and human experts to advance synergistically in a\ncollaborative annotation workflow. We demonstrate the effectiveness of\nCrowdAgent through extensive experiments on six diverse multimodal\nclassification tasks. The source code and video demo are available at\nhttps://github.com/QMMMS/CrowdAgent."
                },
                "authors": [
                    {
                        "name": "Maosheng Qin"
                    },
                    {
                        "name": "Renyu Zhu"
                    },
                    {
                        "name": "Mingxuan Xia"
                    },
                    {
                        "name": "Chenkai Chen"
                    },
                    {
                        "name": "Zhen Zhu"
                    },
                    {
                        "name": "Minmin Lin"
                    },
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Lu Xu"
                    },
                    {
                        "name": "Changjie Fan"
                    },
                    {
                        "name": "Runze Wu"
                    },
                    {
                        "name": "Haobo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haobo Wang"
                },
                "author": "Haobo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01658v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01658v2",
                "updated": "2025-09-17T14:29:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    29,
                    1,
                    2,
                    260,
                    0
                ],
                "published": "2025-03-03T15:32:02Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    15,
                    32,
                    2,
                    0,
                    62,
                    0
                ],
                "title": "CoPL: Collaborative Preference Learning for Personalizing LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoPL: Collaborative Preference Learning for Personalizing LLMs"
                },
                "summary": "Personalizing large language models (LLMs) is important for aligning outputs\nwith diverse user preferences, yet existing methods struggle with flexibility\nand generalization. We propose CoPL (Collaborative Preference Learning), a\ngraph-based collaborative filtering framework that models user-response\nrelationships to enhance preference estimation, particularly in sparse\nannotation settings. By integrating a mixture of LoRA experts, CoPL efficiently\nfine-tunes LLMs while dynamically balancing shared and user-specific\npreferences. Additionally, an optimization-free adaptation strategy enables\ngeneralization to unseen users without fine-tuning. Experiments on\nUltraFeedback-P demonstrate that CoPL outperforms existing personalized reward\nmodels, effectively capturing both common and controversial preferences, making\nit a scalable solution for personalized LLM alignment. The code is available at\nhttps://github.com/ml-postech/CoPL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalizing large language models (LLMs) is important for aligning outputs\nwith diverse user preferences, yet existing methods struggle with flexibility\nand generalization. We propose CoPL (Collaborative Preference Learning), a\ngraph-based collaborative filtering framework that models user-response\nrelationships to enhance preference estimation, particularly in sparse\nannotation settings. By integrating a mixture of LoRA experts, CoPL efficiently\nfine-tunes LLMs while dynamically balancing shared and user-specific\npreferences. Additionally, an optimization-free adaptation strategy enables\ngeneralization to unseen users without fine-tuning. Experiments on\nUltraFeedback-P demonstrate that CoPL outperforms existing personalized reward\nmodels, effectively capturing both common and controversial preferences, making\nit a scalable solution for personalized LLM alignment. The code is available at\nhttps://github.com/ml-postech/CoPL."
                },
                "authors": [
                    {
                        "name": "Youngbin Choi"
                    },
                    {
                        "name": "Seunghyuk Cho"
                    },
                    {
                        "name": "Minjong Lee"
                    },
                    {
                        "name": "MoonJeong Park"
                    },
                    {
                        "name": "Yesong Ko"
                    },
                    {
                        "name": "Jungseul Ok"
                    },
                    {
                        "name": "Dongwoo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Dongwoo Kim"
                },
                "author": "Dongwoo Kim",
                "arxiv_comment": "19pages, 13 figures, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01658v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01658v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18216v2",
                "updated": "2025-09-17T14:25:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    25,
                    49,
                    2,
                    260,
                    0
                ],
                "published": "2024-11-27T10:48:37Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    48,
                    37,
                    2,
                    332,
                    0
                ],
                "title": "Evaluating and Improving the Robustness of Security Attack Detectors\n  Generated by LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating and Improving the Robustness of Security Attack Detectors\n  Generated by LLMs"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in software development to\ngenerate functions, such as attack detectors, that implement security\nrequirements. A key challenge is ensuring the LLMs have enough knowledge to\naddress specific security requirements, such as information about existing\nattacks. For this, we propose an approach integrating Retrieval Augmented\nGeneration (RAG) and Self-Ranking into the LLM pipeline. RAG enhances the\nrobustness of the output by incorporating external knowledge sources, while the\nSelf-Ranking technique, inspired by the concept of Self-Consistency, generates\nmultiple reasoning paths and creates ranks to select the most robust detector.\nOur extensive empirical study targets code generated by LLMs to detect two\nprevalent injection attacks in web security: Cross-Site Scripting (XSS) and SQL\ninjection (SQLi). Results show a significant improvement in detection\nperformance while employing RAG and Self-Ranking, with an increase of up to\n71%pt (on average 37%pt) and up to 43%pt (on average 6%pt) in the F2-Score for\nXSS and SQLi detection, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in software development to\ngenerate functions, such as attack detectors, that implement security\nrequirements. A key challenge is ensuring the LLMs have enough knowledge to\naddress specific security requirements, such as information about existing\nattacks. For this, we propose an approach integrating Retrieval Augmented\nGeneration (RAG) and Self-Ranking into the LLM pipeline. RAG enhances the\nrobustness of the output by incorporating external knowledge sources, while the\nSelf-Ranking technique, inspired by the concept of Self-Consistency, generates\nmultiple reasoning paths and creates ranks to select the most robust detector.\nOur extensive empirical study targets code generated by LLMs to detect two\nprevalent injection attacks in web security: Cross-Site Scripting (XSS) and SQL\ninjection (SQLi). Results show a significant improvement in detection\nperformance while employing RAG and Self-Ranking, with an increase of up to\n71%pt (on average 37%pt) and up to 43%pt (on average 6%pt) in the F2-Score for\nXSS and SQLi detection, respectively."
                },
                "authors": [
                    {
                        "name": "Samuele Pasini"
                    },
                    {
                        "name": "Jinhan Kim"
                    },
                    {
                        "name": "Tommaso Aiello"
                    },
                    {
                        "name": "Rocio Cabrera Lozoya"
                    },
                    {
                        "name": "Antonino Sabetta"
                    },
                    {
                        "name": "Paolo Tonella"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Tonella"
                },
                "author": "Paolo Tonella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14004v1",
                "updated": "2025-09-17T14:14:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    14,
                    5,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T14:14:05Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    14,
                    5,
                    2,
                    260,
                    0
                ],
                "title": "Early Stopping Chain-of-thoughts in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early Stopping Chain-of-thoughts in Large Language Models"
                },
                "summary": "Reasoning large language models (LLMs) have demonstrated superior capacities\nin solving complicated problems by generating long chain-of-thoughts (CoT), but\nsuch a lengthy CoT incurs high inference costs. In this study, we introduce\nES-CoT, an inference-time method that shortens CoT generation by detecting\nanswer convergence and stopping early with minimal performance loss. At the end\nof each reasoning step, we prompt the LLM to output its current final answer,\ndenoted as a step answer. We then track the run length of consecutive identical\nstep answers as a measure of answer convergence. Once the run length exhibits a\nsharp increase and exceeds a minimum threshold, the generation is terminated.\nWe provide both empirical and theoretical support for this heuristic: step\nanswers steadily converge to the final answer, and large run-length jumps\nreliably mark this convergence. Experiments on five reasoning datasets across\nthree LLMs show that ES-CoT reduces the number of inference tokens by about\n41\\% on average while maintaining accuracy comparable to standard CoT. Further,\nES-CoT integrates seamlessly with self-consistency prompting and remains robust\nacross hyperparameter choices, highlighting it as a practical and effective\napproach for efficient reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning large language models (LLMs) have demonstrated superior capacities\nin solving complicated problems by generating long chain-of-thoughts (CoT), but\nsuch a lengthy CoT incurs high inference costs. In this study, we introduce\nES-CoT, an inference-time method that shortens CoT generation by detecting\nanswer convergence and stopping early with minimal performance loss. At the end\nof each reasoning step, we prompt the LLM to output its current final answer,\ndenoted as a step answer. We then track the run length of consecutive identical\nstep answers as a measure of answer convergence. Once the run length exhibits a\nsharp increase and exceeds a minimum threshold, the generation is terminated.\nWe provide both empirical and theoretical support for this heuristic: step\nanswers steadily converge to the final answer, and large run-length jumps\nreliably mark this convergence. Experiments on five reasoning datasets across\nthree LLMs show that ES-CoT reduces the number of inference tokens by about\n41\\% on average while maintaining accuracy comparable to standard CoT. Further,\nES-CoT integrates seamlessly with self-consistency prompting and remains robust\nacross hyperparameter choices, highlighting it as a practical and effective\napproach for efficient reasoning."
                },
                "authors": [
                    {
                        "name": "Minjia Mao"
                    },
                    {
                        "name": "Bowen Yin"
                    },
                    {
                        "name": "Yu Zhu"
                    },
                    {
                        "name": "Xiao Fang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Fang"
                },
                "author": "Xiao Fang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14002v1",
                "updated": "2025-09-17T14:13:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    13,
                    29,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T14:13:29Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    13,
                    29,
                    2,
                    260,
                    0
                ],
                "title": "RepCaM++: Exploring Transparent Visual Prompt With Inference-Time\n  Re-Parameterization for Neural Video Delivery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepCaM++: Exploring Transparent Visual Prompt With Inference-Time\n  Re-Parameterization for Neural Video Delivery"
                },
                "summary": "Recently, content-aware methods have been employed to reduce bandwidth and\nenhance the quality of Internet video delivery. These methods involve training\ndistinct content-aware super-resolution (SR) models for each video chunk on the\nserver, subsequently streaming the low-resolution (LR) video chunks with the SR\nmodels to the client. Prior research has incorporated additional partial\nparameters to customize the models for individual video chunks. However, this\nleads to parameter accumulation and can fail to adapt appropriately as video\nlengths increase, resulting in increased delivery costs and reduced\nperformance. In this paper, we introduce RepCaM++, an innovative framework\nbased on a novel Re-parameterization Content-aware Modulation (RepCaM) module\nthat uniformly modulates video chunks. The RepCaM framework integrates extra\nparallel-cascade parameters during training to accommodate multiple chunks,\nsubsequently eliminating these additional parameters through\nre-parameterization during inference. Furthermore, to enhance RepCaM's\nperformance, we propose the Transparent Visual Prompt (TVP), which includes a\nminimal set of zero-initialized image-level parameters (e.g., less than 0.1%)\nto capture fine details within video chunks. We conduct extensive experiments\non the VSD4K dataset, encompassing six different video scenes, and achieve\nstate-of-the-art results in video restoration quality and delivery bandwidth\ncompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, content-aware methods have been employed to reduce bandwidth and\nenhance the quality of Internet video delivery. These methods involve training\ndistinct content-aware super-resolution (SR) models for each video chunk on the\nserver, subsequently streaming the low-resolution (LR) video chunks with the SR\nmodels to the client. Prior research has incorporated additional partial\nparameters to customize the models for individual video chunks. However, this\nleads to parameter accumulation and can fail to adapt appropriately as video\nlengths increase, resulting in increased delivery costs and reduced\nperformance. In this paper, we introduce RepCaM++, an innovative framework\nbased on a novel Re-parameterization Content-aware Modulation (RepCaM) module\nthat uniformly modulates video chunks. The RepCaM framework integrates extra\nparallel-cascade parameters during training to accommodate multiple chunks,\nsubsequently eliminating these additional parameters through\nre-parameterization during inference. Furthermore, to enhance RepCaM's\nperformance, we propose the Transparent Visual Prompt (TVP), which includes a\nminimal set of zero-initialized image-level parameters (e.g., less than 0.1%)\nto capture fine details within video chunks. We conduct extensive experiments\non the VSD4K dataset, encompassing six different video scenes, and achieve\nstate-of-the-art results in video restoration quality and delivery bandwidth\ncompression."
                },
                "authors": [
                    {
                        "name": "Rongyu Zhang"
                    },
                    {
                        "name": "Xize Duan"
                    },
                    {
                        "name": "Jiaming Liu"
                    },
                    {
                        "name": "Li Du"
                    },
                    {
                        "name": "Yuan Du"
                    },
                    {
                        "name": "Dan Wang"
                    },
                    {
                        "name": "Shanghang Zhang"
                    },
                    {
                        "name": "Fangxin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Fangxin Wang"
                },
                "author": "Fangxin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14001v1",
                "updated": "2025-09-17T14:13:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    13,
                    20,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T14:13:20Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    13,
                    20,
                    2,
                    260,
                    0
                ],
                "title": "MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment"
                },
                "summary": "We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment),\na knowledge distillation approach that transfers region-level multimodal\nsemantics from a large vision-language teacher (e.g., LLaVa) into a lightweight\nvision-only object detector student (e.g., YOLO). A translation module maps\nstudent features into a joint space, where the training of the student and\ntranslator is guided by a dual-objective loss that enforces both local\nalignment and global relational consistency. Unlike prior approaches focused on\ndense or global alignment, MOCHA operates at the object level, enabling\nefficient transfer of semantics without modifying the teacher or requiring\ntextual input at inference. We validate our method across four personalized\ndetection benchmarks under few-shot regimes. Results show consistent gains over\nbaselines, with a +10.1 average score improvement. Despite its compact\narchitecture, MOCHA reaches performance on par with larger multimodal models,\nproving its suitability for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment),\na knowledge distillation approach that transfers region-level multimodal\nsemantics from a large vision-language teacher (e.g., LLaVa) into a lightweight\nvision-only object detector student (e.g., YOLO). A translation module maps\nstudent features into a joint space, where the training of the student and\ntranslator is guided by a dual-objective loss that enforces both local\nalignment and global relational consistency. Unlike prior approaches focused on\ndense or global alignment, MOCHA operates at the object level, enabling\nefficient transfer of semantics without modifying the teacher or requiring\ntextual input at inference. We validate our method across four personalized\ndetection benchmarks under few-shot regimes. Results show consistent gains over\nbaselines, with a +10.1 average score improvement. Despite its compact\narchitecture, MOCHA reaches performance on par with larger multimodal models,\nproving its suitability for real-world deployment."
                },
                "authors": [
                    {
                        "name": "Elena Camuffo"
                    },
                    {
                        "name": "Francesco Barbato"
                    },
                    {
                        "name": "Mete Ozay"
                    },
                    {
                        "name": "Simone Milani"
                    },
                    {
                        "name": "Umberto Michieli"
                    }
                ],
                "author_detail": {
                    "name": "Umberto Michieli"
                },
                "author": "Umberto Michieli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13997v1",
                "updated": "2025-09-17T14:10:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    10,
                    44,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T14:10:44Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    10,
                    44,
                    2,
                    260,
                    0
                ],
                "title": "An RDMA-First Object Storage System with SmartNIC Offload",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An RDMA-First Object Storage System with SmartNIC Offload"
                },
                "summary": "AI training and inference impose sustained, fine-grain I/O that stresses\nhost-mediated, TCP-based storage paths. Motivated by kernel-bypass networking\nand user-space storage stacks, we revisit POSIX-compatible object storage for\nGPU-centric pipelines. We present ROS2, an RDMA-first object storage system\ndesign that offloads the DAOS client to an NVIDIA BlueField-3 SmartNIC while\nleaving the DAOS I/O engine unchanged on the storage server. ROS2 separates a\nlightweight control plane (gRPC for namespace and capability exchange) from a\nhigh-throughput data plane (UCX/libfabric over RDMA or TCP) and removes host\nmediation from the data path.\n  Using FIO/DFS across local and remote configurations, we find that on\nserver-grade CPUs RDMA consistently outperforms TCP for both large sequential\nand small random I/O. When the RDMA-driven DAOS client is offloaded to\nBlueField-3, end-to-end performance is comparable to the host, demonstrating\nthat SmartNIC offload preserves RDMA efficiency while enabling DPU-resident\nfeatures such as multi-tenant isolation and inline services (e.g.,\nencryption/decryption) close to the NIC. In contrast, TCP on the SmartNIC lags\nhost performance, underscoring the importance of RDMA for offloaded\ndeployments.\n  Overall, our results indicate that an RDMA-first, SmartNIC-offloaded\nobject-storage stack is a practical foundation for scaling data delivery in\nmodern LLM training environments; integrating optional GPU-direct placement for\nLLM tasks is left for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI training and inference impose sustained, fine-grain I/O that stresses\nhost-mediated, TCP-based storage paths. Motivated by kernel-bypass networking\nand user-space storage stacks, we revisit POSIX-compatible object storage for\nGPU-centric pipelines. We present ROS2, an RDMA-first object storage system\ndesign that offloads the DAOS client to an NVIDIA BlueField-3 SmartNIC while\nleaving the DAOS I/O engine unchanged on the storage server. ROS2 separates a\nlightweight control plane (gRPC for namespace and capability exchange) from a\nhigh-throughput data plane (UCX/libfabric over RDMA or TCP) and removes host\nmediation from the data path.\n  Using FIO/DFS across local and remote configurations, we find that on\nserver-grade CPUs RDMA consistently outperforms TCP for both large sequential\nand small random I/O. When the RDMA-driven DAOS client is offloaded to\nBlueField-3, end-to-end performance is comparable to the host, demonstrating\nthat SmartNIC offload preserves RDMA efficiency while enabling DPU-resident\nfeatures such as multi-tenant isolation and inline services (e.g.,\nencryption/decryption) close to the NIC. In contrast, TCP on the SmartNIC lags\nhost performance, underscoring the importance of RDMA for offloaded\ndeployments.\n  Overall, our results indicate that an RDMA-first, SmartNIC-offloaded\nobject-storage stack is a practical foundation for scaling data delivery in\nmodern LLM training environments; integrating optional GPU-direct placement for\nLLM tasks is left for future work."
                },
                "authors": [
                    {
                        "name": "Yu Zhu"
                    },
                    {
                        "name": "Aditya Dhakal"
                    },
                    {
                        "name": "Pedro Bruel"
                    },
                    {
                        "name": "Gourav Rattihalli"
                    },
                    {
                        "name": "Yunming Xiao"
                    },
                    {
                        "name": "Johann Lombardi"
                    },
                    {
                        "name": "Dejan Milojicic"
                    }
                ],
                "author_detail": {
                    "name": "Dejan Milojicic"
                },
                "author": "Dejan Milojicic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05582v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05582v2",
                "updated": "2025-09-17T14:08:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    8,
                    26,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-06T03:58:53Z",
                "published_parsed": [
                    2025,
                    9,
                    6,
                    3,
                    58,
                    53,
                    5,
                    249,
                    0
                ],
                "title": "Reconstruction and Reenactment Separated Method for Realistic Gaussian\n  Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstruction and Reenactment Separated Method for Realistic Gaussian\n  Head"
                },
                "summary": "In this paper, we explore a reconstruction and reenactment separated\nframework for 3D Gaussians head, which requires only a single portrait image as\ninput to generate controllable avatar. Specifically, we developed a large-scale\none-shot gaussian head generator built upon WebSSL and employed a two-stage\ntraining approach that significantly enhances the capabilities of\ngeneralization and high-frequency texture reconstruction. During inference, an\nultra-lightweight gaussian avatar driven by control signals enables high\nframe-rate rendering, achieving 90 FPS at a resolution of 512x512. We further\ndemonstrate that the proposed framework follows the scaling law, whereby\nincreasing the parameter scale of the reconstruction module leads to improved\nperformance. Moreover, thanks to the separation design, driving efficiency\nremains unaffected. Finally, extensive quantitative and qualitative experiments\nvalidate that our approach outperforms current state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we explore a reconstruction and reenactment separated\nframework for 3D Gaussians head, which requires only a single portrait image as\ninput to generate controllable avatar. Specifically, we developed a large-scale\none-shot gaussian head generator built upon WebSSL and employed a two-stage\ntraining approach that significantly enhances the capabilities of\ngeneralization and high-frequency texture reconstruction. During inference, an\nultra-lightweight gaussian avatar driven by control signals enables high\nframe-rate rendering, achieving 90 FPS at a resolution of 512x512. We further\ndemonstrate that the proposed framework follows the scaling law, whereby\nincreasing the parameter scale of the reconstruction module leads to improved\nperformance. Moreover, thanks to the separation design, driving efficiency\nremains unaffected. Finally, extensive quantitative and qualitative experiments\nvalidate that our approach outperforms current state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Zhiling Ye"
                    },
                    {
                        "name": "Cong Zhou"
                    },
                    {
                        "name": "Xiubao Zhang"
                    },
                    {
                        "name": "Haifeng Shen"
                    },
                    {
                        "name": "Weihong Deng"
                    },
                    {
                        "name": "Quan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Quan Lu"
                },
                "author": "Quan Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05582v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05582v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16150v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16150v3",
                "updated": "2025-09-17T14:07:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    7,
                    50,
                    2,
                    260,
                    0
                ],
                "published": "2025-08-22T07:19:33Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    19,
                    33,
                    4,
                    234,
                    0
                ],
                "title": "Evaluating the Defense Potential of Machine Unlearning against\n  Membership Inference Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Defense Potential of Machine Unlearning against\n  Membership Inference Attacks"
                },
                "summary": "Membership Inference Attacks (MIAs) pose a significant privacy risk, as they\nenable adversaries to determine whether a specific data point was included in\nthe training dataset of a model. While Machine Unlearning is primarily designed\nas a privacy mechanism to efficiently remove private data from a machine\nlearning model without the need for full retraining, its impact on the\nsusceptibility of models to MIA remains an open question. In this study, we\nsystematically assess the vulnerability of models to MIA after applying\nstate-of-art Machine Unlearning algorithms. Our analysis spans four diverse\ndatasets (two from the image domain and two in tabular format), exploring how\ndifferent unlearning approaches influence the exposure of models to membership\ninference. The findings highlight that while Machine Unlearning is not\ninherently a countermeasure against MIA, the unlearning algorithm and data\ncharacteristics can significantly affect a model's vulnerability. This work\nprovides essential insights into the interplay between Machine Unlearning and\nMIAs, offering guidance for the design of privacy-preserving machine learning\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership Inference Attacks (MIAs) pose a significant privacy risk, as they\nenable adversaries to determine whether a specific data point was included in\nthe training dataset of a model. While Machine Unlearning is primarily designed\nas a privacy mechanism to efficiently remove private data from a machine\nlearning model without the need for full retraining, its impact on the\nsusceptibility of models to MIA remains an open question. In this study, we\nsystematically assess the vulnerability of models to MIA after applying\nstate-of-art Machine Unlearning algorithms. Our analysis spans four diverse\ndatasets (two from the image domain and two in tabular format), exploring how\ndifferent unlearning approaches influence the exposure of models to membership\ninference. The findings highlight that while Machine Unlearning is not\ninherently a countermeasure against MIA, the unlearning algorithm and data\ncharacteristics can significantly affect a model's vulnerability. This work\nprovides essential insights into the interplay between Machine Unlearning and\nMIAs, offering guidance for the design of privacy-preserving machine learning\nsystems."
                },
                "authors": [
                    {
                        "name": "Aristeidis Sidiropoulos"
                    },
                    {
                        "name": "Christos Chrysanthos Nikolaidis"
                    },
                    {
                        "name": "Theodoros Tsiolakis"
                    },
                    {
                        "name": "Nikolaos Pavlidis"
                    },
                    {
                        "name": "Vasilis Perifanis"
                    },
                    {
                        "name": "Pavlos S. Efraimidis"
                    }
                ],
                "author_detail": {
                    "name": "Pavlos S. Efraimidis"
                },
                "author": "Pavlos S. Efraimidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16150v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16150v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13453v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13453v2",
                "updated": "2025-09-17T14:04:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    4,
                    22,
                    2,
                    260,
                    0
                ],
                "published": "2025-07-17T18:00:34Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    18,
                    0,
                    34,
                    3,
                    198,
                    0
                ],
                "title": "ASKAP J144834-685644: a newly discovered long period radio transient\n  detected from radio to X-rays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASKAP J144834-685644: a newly discovered long period radio transient\n  detected from radio to X-rays"
                },
                "summary": "Long-period radio transients (LPTs) are an emerging group of radio transients\nthat show periodic polarized radio bursts with periods varying from a few\nminutes to a few hours. Fewer than a dozen LPTs have been detected so far, and\ntheir origin (source and emission mechanism) remains unclear. Here, we report\nthe discovery of a 1.5 h LPT, ASKAP J144834-685644, adding to the current\nsample of sources. ASKAP J144834-685644 is one of the very few LPTs that has\nbeen detected from X-rays to radio. It shows a steep radio spectrum and\npolarized radio bursts, which resemble the radio emission in known LPTs. In\naddition, it also shows highly structured and periodic narrow-band radio\nemission. Multiwavelength properties suggest that the spectral energy\ndistribution (SED) peaks at near ultraviolet wavelengths, indicating the\npresence of a hot magnetic source. Combining multiwavelength information, we\ninfer that ASKAP J144834-685644 may be a near edge-on magnetic white dwarf\nbinary (MWD), although we cannot fully rule out ASKAP J144834-685644 being an\nisolated white dwarf pulsar or even a transitional millisecond pulsar (despite\nthe lack of radio pulsations). If ASKAP J144834-685644 is a MWD binary, the\nobserved broad-band SED can be explained by emission from an accretion disc.\nThis hints that some fraction of optically bright LPTs may be accreting\nbinaries with the radio period being the orbital period. It might further\nsuggest a connection between optically bright synchronized WD binaries, such as\npolars, and non-accreting asynchronous WD pulsars, such as AR Sco and\nJ1912-4410.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-period radio transients (LPTs) are an emerging group of radio transients\nthat show periodic polarized radio bursts with periods varying from a few\nminutes to a few hours. Fewer than a dozen LPTs have been detected so far, and\ntheir origin (source and emission mechanism) remains unclear. Here, we report\nthe discovery of a 1.5 h LPT, ASKAP J144834-685644, adding to the current\nsample of sources. ASKAP J144834-685644 is one of the very few LPTs that has\nbeen detected from X-rays to radio. It shows a steep radio spectrum and\npolarized radio bursts, which resemble the radio emission in known LPTs. In\naddition, it also shows highly structured and periodic narrow-band radio\nemission. Multiwavelength properties suggest that the spectral energy\ndistribution (SED) peaks at near ultraviolet wavelengths, indicating the\npresence of a hot magnetic source. Combining multiwavelength information, we\ninfer that ASKAP J144834-685644 may be a near edge-on magnetic white dwarf\nbinary (MWD), although we cannot fully rule out ASKAP J144834-685644 being an\nisolated white dwarf pulsar or even a transitional millisecond pulsar (despite\nthe lack of radio pulsations). If ASKAP J144834-685644 is a MWD binary, the\nobserved broad-band SED can be explained by emission from an accretion disc.\nThis hints that some fraction of optically bright LPTs may be accreting\nbinaries with the radio period being the orbital period. It might further\nsuggest a connection between optically bright synchronized WD binaries, such as\npolars, and non-accreting asynchronous WD pulsars, such as AR Sco and\nJ1912-4410."
                },
                "authors": [
                    {
                        "name": "Akash Anumarlapudi"
                    },
                    {
                        "name": "David L. Kaplan"
                    },
                    {
                        "name": "Nanda Rea"
                    },
                    {
                        "name": "Nicolas Erasmus"
                    },
                    {
                        "name": "Daniel Kelson"
                    },
                    {
                        "name": "Stella Koch Ocker"
                    },
                    {
                        "name": "Emil Lenc"
                    },
                    {
                        "name": "Dougal Dobie"
                    },
                    {
                        "name": "Natasha Hurley-Walker"
                    },
                    {
                        "name": "Gregory Sivakoff"
                    },
                    {
                        "name": "David A. H. Buckley"
                    },
                    {
                        "name": "Tara Murphy"
                    },
                    {
                        "name": "Joshua Pritchard"
                    },
                    {
                        "name": "Laura Driessen"
                    },
                    {
                        "name": "Kovi Rose"
                    },
                    {
                        "name": "Andrew Zic"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Zic"
                },
                "author": "Andrew Zic",
                "arxiv_comment": "Accepted for publication in the Monthly Notices of the Royal\n  Astronomical Society; minor change (additions to Acknowledgements) in v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13453v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13453v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13990v1",
                "updated": "2025-09-17T14:00:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    0,
                    51,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T14:00:51Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    0,
                    51,
                    2,
                    260,
                    0
                ],
                "title": "Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency"
                },
                "summary": "Recently, Test-Time Scaling (TTS) has gained increasing attention for\nimproving LLM reasoning performance at test time without retraining the model.\nA notable TTS technique is Self-Consistency (SC), which generates multiple\nreasoning chains in parallel and selects the final answer via majority voting.\nWhile effective, the order-of-magnitude computational overhead limits its broad\ndeployment. Prior attempts to accelerate SC mainly rely on model-based\nconfidence scores or heuristics with limited empirical support. For the first\ntime, we theoretically and empirically analyze the inefficiencies of SC and\nreveal actionable opportunities for improvement. Building on these insights, we\npropose Slim-SC, a step-wise pruning strategy that identifies and removes\nredundant chains using inter-chain similarity at the thought level. Experiments\non three STEM reasoning datasets and two recent LLM architectures show that\nSlim-SC reduces inference latency and KVC usage by up to 45% and 26%,\nrespectively, with R1-Distill, while maintaining or improving accuracy, thus\noffering a simple yet efficient TTS alternative for SC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Test-Time Scaling (TTS) has gained increasing attention for\nimproving LLM reasoning performance at test time without retraining the model.\nA notable TTS technique is Self-Consistency (SC), which generates multiple\nreasoning chains in parallel and selects the final answer via majority voting.\nWhile effective, the order-of-magnitude computational overhead limits its broad\ndeployment. Prior attempts to accelerate SC mainly rely on model-based\nconfidence scores or heuristics with limited empirical support. For the first\ntime, we theoretically and empirically analyze the inefficiencies of SC and\nreveal actionable opportunities for improvement. Building on these insights, we\npropose Slim-SC, a step-wise pruning strategy that identifies and removes\nredundant chains using inter-chain similarity at the thought level. Experiments\non three STEM reasoning datasets and two recent LLM architectures show that\nSlim-SC reduces inference latency and KVC usage by up to 45% and 26%,\nrespectively, with R1-Distill, while maintaining or improving accuracy, thus\noffering a simple yet efficient TTS alternative for SC."
                },
                "authors": [
                    {
                        "name": "Colin Hong"
                    },
                    {
                        "name": "Xu Guo"
                    },
                    {
                        "name": "Anand Chaanan Singh"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Dmitrii Ustiugov"
                    }
                ],
                "author_detail": {
                    "name": "Dmitrii Ustiugov"
                },
                "author": "Dmitrii Ustiugov",
                "arxiv_comment": "Accepted by EMNLP 2025 (Oral), 9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13987v1",
                "updated": "2025-09-17T13:59:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    59,
                    38,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T13:59:38Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    59,
                    38,
                    2,
                    260,
                    0
                ],
                "title": "Differential Privacy in Federated Learning: Mitigating Inference Attacks\n  with Randomized Response",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential Privacy in Federated Learning: Mitigating Inference Attacks\n  with Randomized Response"
                },
                "summary": "Machine learning models used for distributed architectures consisting of\nservers and clients require large amounts of data to achieve high accuracy.\nData obtained from clients are collected on a central server for model\ntraining. However, storing data on a central server raises concerns about\nsecurity and privacy. To address this issue, a federated learning architecture\nhas been proposed. In federated learning, each client trains a local model\nusing its own data. The trained models are periodically transmitted to the\ncentral server. The server then combines the received models using federated\naggregation algorithms to obtain a global model. This global model is\ndistributed back to the clients, and the process continues in a cyclical\nmanner. Although preventing data from leaving the clients enhances security,\ncertain concerns still remain. Attackers can perform inference attacks on the\nobtained models to approximate the training dataset, potentially causing data\nleakage. In this study, differential privacy was applied to address the\naforementioned security vulnerability, and a performance analysis was\nconducted. The Data-Unaware Classification Based on Association (duCBA)\nalgorithm was used as the federated aggregation method. Differential privacy\nwas implemented on the data using the Randomized Response technique, and the\ntrade-off between security and performance was examined under different epsilon\nvalues. As the epsilon value decreased, the model accuracy declined, and class\nprediction imbalances were observed. This indicates that higher levels of\nprivacy do not always lead to practical outcomes and that the balance between\nsecurity and performance must be carefully considered.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models used for distributed architectures consisting of\nservers and clients require large amounts of data to achieve high accuracy.\nData obtained from clients are collected on a central server for model\ntraining. However, storing data on a central server raises concerns about\nsecurity and privacy. To address this issue, a federated learning architecture\nhas been proposed. In federated learning, each client trains a local model\nusing its own data. The trained models are periodically transmitted to the\ncentral server. The server then combines the received models using federated\naggregation algorithms to obtain a global model. This global model is\ndistributed back to the clients, and the process continues in a cyclical\nmanner. Although preventing data from leaving the clients enhances security,\ncertain concerns still remain. Attackers can perform inference attacks on the\nobtained models to approximate the training dataset, potentially causing data\nleakage. In this study, differential privacy was applied to address the\naforementioned security vulnerability, and a performance analysis was\nconducted. The Data-Unaware Classification Based on Association (duCBA)\nalgorithm was used as the federated aggregation method. Differential privacy\nwas implemented on the data using the Randomized Response technique, and the\ntrade-off between security and performance was examined under different epsilon\nvalues. As the epsilon value decreased, the model accuracy declined, and class\nprediction imbalances were observed. This indicates that higher levels of\nprivacy do not always lead to practical outcomes and that the balance between\nsecurity and performance must be carefully considered."
                },
                "authors": [
                    {
                        "name": "Ozer Ozturk"
                    },
                    {
                        "name": "Busra Buyuktanir"
                    },
                    {
                        "name": "Gozde Karatas Baydogmus"
                    },
                    {
                        "name": "Kazim Yildiz"
                    }
                ],
                "author_detail": {
                    "name": "Kazim Yildiz"
                },
                "author": "Kazim Yildiz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13978v1",
                "updated": "2025-09-17T13:51:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    51,
                    29,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T13:51:29Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    51,
                    29,
                    2,
                    260,
                    0
                ],
                "title": "LLM Agents for Interactive Workflow Provenance: Reference Architecture\n  and Evaluation Methodology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Agents for Interactive Workflow Provenance: Reference Architecture\n  and Evaluation Methodology"
                },
                "summary": "Modern scientific discovery increasingly relies on workflows that process\ndata across the Edge, Cloud, and High Performance Computing (HPC) continuum.\nComprehensive and in-depth analyses of these data are critical for hypothesis\nvalidation, anomaly detection, reproducibility, and impactful findings.\nAlthough workflow provenance techniques support such analyses, at large scale,\nthe provenance data become complex and difficult to analyze. Existing systems\ndepend on custom scripts, structured queries, or static dashboards, limiting\ndata interaction. In this work, we introduce an evaluation methodology,\nreference architecture, and open-source implementation that leverages\ninteractive Large Language Model (LLM) agents for runtime data analysis. Our\napproach uses a lightweight, metadata-driven design that translates natural\nlanguage into structured provenance queries. Evaluations across LLaMA, GPT,\nGemini, and Claude, covering diverse query classes and a real-world chemistry\nworkflow, show that modular design, prompt tuning, and Retrieval-Augmented\nGeneration (RAG) enable accurate and insightful LLM agent responses beyond\nrecorded provenance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern scientific discovery increasingly relies on workflows that process\ndata across the Edge, Cloud, and High Performance Computing (HPC) continuum.\nComprehensive and in-depth analyses of these data are critical for hypothesis\nvalidation, anomaly detection, reproducibility, and impactful findings.\nAlthough workflow provenance techniques support such analyses, at large scale,\nthe provenance data become complex and difficult to analyze. Existing systems\ndepend on custom scripts, structured queries, or static dashboards, limiting\ndata interaction. In this work, we introduce an evaluation methodology,\nreference architecture, and open-source implementation that leverages\ninteractive Large Language Model (LLM) agents for runtime data analysis. Our\napproach uses a lightweight, metadata-driven design that translates natural\nlanguage into structured provenance queries. Evaluations across LLaMA, GPT,\nGemini, and Claude, covering diverse query classes and a real-world chemistry\nworkflow, show that modular design, prompt tuning, and Retrieval-Augmented\nGeneration (RAG) enable accurate and insightful LLM agent responses beyond\nrecorded provenance."
                },
                "authors": [
                    {
                        "name": "Renan Souza"
                    },
                    {
                        "name": "Timothy Poteet"
                    },
                    {
                        "name": "Brian Etz"
                    },
                    {
                        "name": "Daniel Rosendo"
                    },
                    {
                        "name": "Amal Gueroudji"
                    },
                    {
                        "name": "Woong Shin"
                    },
                    {
                        "name": "Prasanna Balaprakash"
                    },
                    {
                        "name": "Rafael Ferreira da Silva"
                    }
                ],
                "author_detail": {
                    "name": "Rafael Ferreira da Silva"
                },
                "author": "Rafael Ferreira da Silva",
                "arxiv_doi": "10.1145/3731599.3767582",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731599.3767582",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.13978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Paper accepted in the proceedings of the ACM/IEEE Supercomputing\n  Conference (SC). Cite it as Renan Souza, Timothy Poteet, Brian Etz, Daniel\n  Rosendo, Amal Gueroudji, Woong Shin, Prasanna Balaprakash, and Rafael\n  Ferreira da Silva. 2025. LLM Agents for Interactive Workflow Provenance:\n  Reference Architecture and Evaluation Methodology. In SC Workshops (WORKS)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M14, 68M20, 68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.4; D.1.3; I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04537v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04537v3",
                "updated": "2025-09-17T13:45:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    45,
                    52,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-04T08:09:42Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    9,
                    42,
                    3,
                    247,
                    0
                ],
                "title": "Emergent Social Dynamics of LLM Agents in the El Farol Bar Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent Social Dynamics of LLM Agents in the El Farol Bar Problem"
                },
                "summary": "We investigate the emergent social dynamics of Large Language Model (LLM)\nagents in a spatially extended El Farol Bar problem, observing how they\nautonomously navigate this classic social dilemma. As a result, the LLM agents\ngenerated a spontaneous motivation to go to the bar and changed their decision\nmaking by becoming a collective. We also observed that the LLM agents did not\nsolve the problem completely, but rather behaved more like humans. These\nfindings reveal a complex interplay between external incentives\n(prompt-specified constraints such as the 60% threshold) and internal\nincentives (culturally-encoded social preferences derived from pre-training),\ndemonstrating that LLM agents naturally balance formal game-theoretic\nrationality with social motivations that characterize human behavior. These\nfindings suggest that a new model of group decision making, which could not be\nhandled in the previous game-theoretic problem setting, can be realized by LLM\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the emergent social dynamics of Large Language Model (LLM)\nagents in a spatially extended El Farol Bar problem, observing how they\nautonomously navigate this classic social dilemma. As a result, the LLM agents\ngenerated a spontaneous motivation to go to the bar and changed their decision\nmaking by becoming a collective. We also observed that the LLM agents did not\nsolve the problem completely, but rather behaved more like humans. These\nfindings reveal a complex interplay between external incentives\n(prompt-specified constraints such as the 60% threshold) and internal\nincentives (culturally-encoded social preferences derived from pre-training),\ndemonstrating that LLM agents naturally balance formal game-theoretic\nrationality with social motivations that characterize human behavior. These\nfindings suggest that a new model of group decision making, which could not be\nhandled in the previous game-theoretic problem setting, can be realized by LLM\nagents."
                },
                "authors": [
                    {
                        "name": "Ryosuke Takata"
                    },
                    {
                        "name": "Atsushi Masumori"
                    },
                    {
                        "name": "Takashi Ikegami"
                    }
                ],
                "author_detail": {
                    "name": "Takashi Ikegami"
                },
                "author": "Takashi Ikegami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04537v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04537v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08364v2",
                "updated": "2025-09-17T13:35:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    35,
                    33,
                    2,
                    260,
                    0
                ],
                "published": "2025-05-13T09:10:48Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    10,
                    48,
                    1,
                    133,
                    0
                ],
                "title": "Learning Like Humans: Advancing LLM Reasoning Capabilities via Adaptive\n  Difficulty Curriculum Learning and Expert-Guided Self-Reformulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Like Humans: Advancing LLM Reasoning Capabilities via Adaptive\n  Difficulty Curriculum Learning and Expert-Guided Self-Reformulation"
                },
                "summary": "Despite impressive progress in areas like mathematical reasoning, large\nlanguage models still face significant challenges in consistently solving\ncomplex problems. Drawing inspiration from key human learning strategies, we\npropose two novel strategies to enhance the capability of large language models\nto solve these complex problems. First, Adaptive Difficulty Curriculum Learning\n(ADCL) is a novel curriculum learning strategy that tackles the Difficulty\nShift phenomenon (i.e., a model's perception of problem difficulty dynamically\nchanges during training) by periodically re-estimating difficulty within\nupcoming data batches to maintain alignment with the model's evolving\ncapabilities. Second, Expert-Guided Self-Reformulation (EGSR) is a novel\nreinforcement learning strategy that bridges the gap between imitation learning\nand pure exploration by guiding models to reformulate expert solutions within\ntheir own conceptual framework, rather than relying on direct imitation,\nfostering deeper understanding and knowledge assimilation. Extensive\nexperiments on challenging mathematical reasoning benchmarks, using Qwen2.5-7B\nas the base model, demonstrate that these human-inspired strategies\nsynergistically and significantly enhance performance. Notably, their combined\napplication improves performance over the standard Zero-RL baseline by 10% on\nthe AIME24 benchmark and 16.6% on AIME25.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite impressive progress in areas like mathematical reasoning, large\nlanguage models still face significant challenges in consistently solving\ncomplex problems. Drawing inspiration from key human learning strategies, we\npropose two novel strategies to enhance the capability of large language models\nto solve these complex problems. First, Adaptive Difficulty Curriculum Learning\n(ADCL) is a novel curriculum learning strategy that tackles the Difficulty\nShift phenomenon (i.e., a model's perception of problem difficulty dynamically\nchanges during training) by periodically re-estimating difficulty within\nupcoming data batches to maintain alignment with the model's evolving\ncapabilities. Second, Expert-Guided Self-Reformulation (EGSR) is a novel\nreinforcement learning strategy that bridges the gap between imitation learning\nand pure exploration by guiding models to reformulate expert solutions within\ntheir own conceptual framework, rather than relying on direct imitation,\nfostering deeper understanding and knowledge assimilation. Extensive\nexperiments on challenging mathematical reasoning benchmarks, using Qwen2.5-7B\nas the base model, demonstrate that these human-inspired strategies\nsynergistically and significantly enhance performance. Notably, their combined\napplication improves performance over the standard Zero-RL baseline by 10% on\nthe AIME24 benchmark and 16.6% on AIME25."
                },
                "authors": [
                    {
                        "name": "Enci Zhang"
                    },
                    {
                        "name": "Xingang Yan"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Tianxiang Zhang"
                    },
                    {
                        "name": "Qianchun Lu"
                    }
                ],
                "author_detail": {
                    "name": "Qianchun Lu"
                },
                "author": "Qianchun Lu",
                "arxiv_comment": "14 pages, 3 figs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13957v1",
                "updated": "2025-09-17T13:28:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    28,
                    46,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T13:28:46Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    28,
                    46,
                    2,
                    260,
                    0
                ],
                "title": "Enhancing Time Awareness in Generative Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Time Awareness in Generative Recommendation"
                },
                "summary": "Generative recommendation has emerged as a promising paradigm that formulates\nthe recommendations into a text-to-text generation task, harnessing the vast\nknowledge of large language models. However, existing studies focus on\nconsidering the sequential order of items and neglect to handle the temporal\ndynamics across items, which can imply evolving user preferences. To address\nthis limitation, we propose a novel model, Generative Recommender Using Time\nawareness (GRUT), effectively capturing hidden user preferences via various\ntemporal signals. We first introduce Time-aware Prompting, consisting of two\nkey contexts. The user-level temporal context models personalized temporal\npatterns across timestamps and time intervals, while the item-level transition\ncontext provides transition patterns across users. We also devise Trend-aware\nInference, a training-free method that enhances rankings by incorporating trend\ninformation about items with generation likelihood. Extensive experiments\ndemonstrate that GRUT outperforms state-of-the-art models, with gains of up to\n15.4% and 14.3% in Recall@5 and NDCG@5 across four benchmark datasets. The\nsource code is available at https://github.com/skleee/GRUT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative recommendation has emerged as a promising paradigm that formulates\nthe recommendations into a text-to-text generation task, harnessing the vast\nknowledge of large language models. However, existing studies focus on\nconsidering the sequential order of items and neglect to handle the temporal\ndynamics across items, which can imply evolving user preferences. To address\nthis limitation, we propose a novel model, Generative Recommender Using Time\nawareness (GRUT), effectively capturing hidden user preferences via various\ntemporal signals. We first introduce Time-aware Prompting, consisting of two\nkey contexts. The user-level temporal context models personalized temporal\npatterns across timestamps and time intervals, while the item-level transition\ncontext provides transition patterns across users. We also devise Trend-aware\nInference, a training-free method that enhances rankings by incorporating trend\ninformation about items with generation likelihood. Extensive experiments\ndemonstrate that GRUT outperforms state-of-the-art models, with gains of up to\n15.4% and 14.3% in Recall@5 and NDCG@5 across four benchmark datasets. The\nsource code is available at https://github.com/skleee/GRUT."
                },
                "authors": [
                    {
                        "name": "Sunkyung Lee"
                    },
                    {
                        "name": "Seongmin Park"
                    },
                    {
                        "name": "Jonghyo Kim"
                    },
                    {
                        "name": "Mincheol Yoon"
                    },
                    {
                        "name": "Jongwuk Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jongwuk Lee"
                },
                "author": "Jongwuk Lee",
                "arxiv_comment": "EMNLP 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09974v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09974v2",
                "updated": "2025-09-17T13:26:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    26,
                    12,
                    2,
                    260,
                    0
                ],
                "published": "2025-05-15T05:22:53Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    5,
                    22,
                    53,
                    3,
                    135,
                    0
                ],
                "title": "Analysing Safety Risks in LLMs Fine-Tuned with Pseudo-Malicious Cyber\n  Security Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysing Safety Risks in LLMs Fine-Tuned with Pseudo-Malicious Cyber\n  Security Data"
                },
                "summary": "Large language models (LLMs) have been used in many application domains,\nincluding cyber security. The application of LLMs in the cyber security domain\npresents significant opportunities, such as for enhancing threat analysis and\nmalware detection, but it can also introduce critical risks and safety\nconcerns, including potential personal data leakage and automated generation of\nnew malware. Building on recent findings that fine-tuning LLMs with\npseudo-malicious cyber security data significantly compromises their safety,\nthis paper presents a comprehensive validation and extension of these safety\nrisks using a different evaluation framework. We employ the garak red teaming\nframework with the OWASP Top 10 for LLM Applications to assess four open-source\nLLMs: Mistral 7B, Llama 3 8B, Gemma 2 9B, and DeepSeek R1 8B. Our evaluation\nconfirms and extends previous findings, showing that fine-tuning reduces safety\nresilience across all tested LLMs (e.g., the failure rate of Mistral 7B against\nprompt injection increases from 9.1% to 68.7%). We further propose and evaluate\na novel safety alignment approach that carefully rewords instruction-response\npairs to include explicit safety precautions and ethical considerations. This\nwork validates previous safety concerns through independent evaluation and\nintroduces new methods for mitigating these risks, contributing towards the\ndevelopment of secure, trustworthy, and ethically aligned LLMs. This approach\ndemonstrates that it is possible to maintain or even improve model safety while\npreserving technical utility, offering a practical path towards developing\nsafer fine-tuning methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been used in many application domains,\nincluding cyber security. The application of LLMs in the cyber security domain\npresents significant opportunities, such as for enhancing threat analysis and\nmalware detection, but it can also introduce critical risks and safety\nconcerns, including potential personal data leakage and automated generation of\nnew malware. Building on recent findings that fine-tuning LLMs with\npseudo-malicious cyber security data significantly compromises their safety,\nthis paper presents a comprehensive validation and extension of these safety\nrisks using a different evaluation framework. We employ the garak red teaming\nframework with the OWASP Top 10 for LLM Applications to assess four open-source\nLLMs: Mistral 7B, Llama 3 8B, Gemma 2 9B, and DeepSeek R1 8B. Our evaluation\nconfirms and extends previous findings, showing that fine-tuning reduces safety\nresilience across all tested LLMs (e.g., the failure rate of Mistral 7B against\nprompt injection increases from 9.1% to 68.7%). We further propose and evaluate\na novel safety alignment approach that carefully rewords instruction-response\npairs to include explicit safety precautions and ethical considerations. This\nwork validates previous safety concerns through independent evaluation and\nintroduces new methods for mitigating these risks, contributing towards the\ndevelopment of secure, trustworthy, and ethically aligned LLMs. This approach\ndemonstrates that it is possible to maintain or even improve model safety while\npreserving technical utility, offering a practical path towards developing\nsafer fine-tuning methodologies."
                },
                "authors": [
                    {
                        "name": "Adel ElZemity"
                    },
                    {
                        "name": "Budi Arief"
                    },
                    {
                        "name": "Shujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Shujun Li"
                },
                "author": "Shujun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09974v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09974v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21670v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21670v3",
                "updated": "2025-09-17T13:25:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    25,
                    13,
                    2,
                    260,
                    0
                ],
                "published": "2025-03-27T16:36:39Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    36,
                    39,
                    3,
                    86,
                    0
                ],
                "title": "COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in\n  Hindi-English Code-Mixing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in\n  Hindi-English Code-Mixing"
                },
                "summary": "We introduce COMI-LINGUA, the largest manually annotated Hindi-English\ncode-mixed dataset, comprising 125K+ high-quality instances across five core\nNLP tasks: Matrix Language Identification, Token-level Language Identification,\nPart-Of-Speech Tagging, Named Entity Recognition, and Machine Translation. Each\ninstance is annotated by three bilingual annotators, yielding over 376K expert\nannotations with strong inter-annotator agreement (Fleiss' Kappa $\\geq$ 0.81).\nThe rigorously preprocessed and filtered dataset covers both Devanagari and\nRoman scripts and spans diverse domains, ensuring real-world linguistic\ncoverage. Evaluation reveals that closed-source LLMs significantly outperform\ntraditional tools and open-source models in zero-shot settings. Notably,\none-shot prompting consistently boosts performance across tasks, especially in\nstructure-sensitive predictions like POS and NER. Fine-tuning state-of-the-art\nLLMs on COMI-LINGUA demonstrates substantial improvements, achieving up to\n95.25 F1 in NER, 98.77 F1 in MLI, and competitive MT performance, setting new\nbenchmarks for Hinglish code-mixed text. COMI-LINGUA is publicly available at\nthis URL: https://huggingface.co/datasets/LingoIITGN/COMI-LINGUA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce COMI-LINGUA, the largest manually annotated Hindi-English\ncode-mixed dataset, comprising 125K+ high-quality instances across five core\nNLP tasks: Matrix Language Identification, Token-level Language Identification,\nPart-Of-Speech Tagging, Named Entity Recognition, and Machine Translation. Each\ninstance is annotated by three bilingual annotators, yielding over 376K expert\nannotations with strong inter-annotator agreement (Fleiss' Kappa $\\geq$ 0.81).\nThe rigorously preprocessed and filtered dataset covers both Devanagari and\nRoman scripts and spans diverse domains, ensuring real-world linguistic\ncoverage. Evaluation reveals that closed-source LLMs significantly outperform\ntraditional tools and open-source models in zero-shot settings. Notably,\none-shot prompting consistently boosts performance across tasks, especially in\nstructure-sensitive predictions like POS and NER. Fine-tuning state-of-the-art\nLLMs on COMI-LINGUA demonstrates substantial improvements, achieving up to\n95.25 F1 in NER, 98.77 F1 in MLI, and competitive MT performance, setting new\nbenchmarks for Hinglish code-mixed text. COMI-LINGUA is publicly available at\nthis URL: https://huggingface.co/datasets/LingoIITGN/COMI-LINGUA."
                },
                "authors": [
                    {
                        "name": "Rajvee Sheth"
                    },
                    {
                        "name": "Himanshu Beniwal"
                    },
                    {
                        "name": "Mayank Singh"
                    }
                ],
                "author_detail": {
                    "name": "Mayank Singh"
                },
                "author": "Mayank Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21670v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21670v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09334v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09334v3",
                "updated": "2025-09-17T13:19:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    19,
                    14,
                    2,
                    260,
                    0
                ],
                "published": "2025-03-12T12:29:27Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    29,
                    27,
                    2,
                    71,
                    0
                ],
                "title": "CyberLLMInstruct: A Pseudo-malicious Dataset Revealing\n  Safety-performance Trade-offs in Cyber Security LLM Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CyberLLMInstruct: A Pseudo-malicious Dataset Revealing\n  Safety-performance Trade-offs in Cyber Security LLM Fine-tuning"
                },
                "summary": "The integration of large language models (LLMs) into cyber security\napplications presents both opportunities and critical safety risks. We\nintroduce CyberLLMInstruct, a dataset of 54,928 pseudo-malicious\ninstruction-response pairs spanning cyber security tasks including malware\nanalysis, phishing simulations, and zero-day vulnerabilities. Our comprehensive\nevaluation using seven open-source LLMs reveals a critical trade-off: while\nfine-tuning improves cyber security task performance (achieving up to 92.50%\naccuracy on CyberMetric), it severely compromises safety resilience across all\ntested models and attack vectors (e.g., Llama 3.1 8B's security score against\nprompt injection drops from 0.95 to 0.15). The dataset incorporates diverse\nsources including CTF challenges, academic papers, industry reports, and CVE\ndatabases to ensure comprehensive coverage of cyber security domains. Our\nfindings highlight the unique challenges of securing LLMs in adversarial\ndomains and establish the critical need for developing fine-tuning\nmethodologies that balance performance gains with safety preservation in\nsecurity-sensitive domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language models (LLMs) into cyber security\napplications presents both opportunities and critical safety risks. We\nintroduce CyberLLMInstruct, a dataset of 54,928 pseudo-malicious\ninstruction-response pairs spanning cyber security tasks including malware\nanalysis, phishing simulations, and zero-day vulnerabilities. Our comprehensive\nevaluation using seven open-source LLMs reveals a critical trade-off: while\nfine-tuning improves cyber security task performance (achieving up to 92.50%\naccuracy on CyberMetric), it severely compromises safety resilience across all\ntested models and attack vectors (e.g., Llama 3.1 8B's security score against\nprompt injection drops from 0.95 to 0.15). The dataset incorporates diverse\nsources including CTF challenges, academic papers, industry reports, and CVE\ndatabases to ensure comprehensive coverage of cyber security domains. Our\nfindings highlight the unique challenges of securing LLMs in adversarial\ndomains and establish the critical need for developing fine-tuning\nmethodologies that balance performance gains with safety preservation in\nsecurity-sensitive domains."
                },
                "authors": [
                    {
                        "name": "Adel ElZemity"
                    },
                    {
                        "name": "Budi Arief"
                    },
                    {
                        "name": "Shujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Shujun Li"
                },
                "author": "Shujun Li",
                "arxiv_doi": "10.1145/3733799.3762968",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3733799.3762968",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.09334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09334v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13942v1",
                "updated": "2025-09-17T13:11:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    11,
                    49,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T13:11:49Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    11,
                    49,
                    2,
                    260,
                    0
                ],
                "title": "Evaluating Classical Software Process Models as Coordination Mechanisms\n  for LLM-Based Software Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Classical Software Process Models as Coordination Mechanisms\n  for LLM-Based Software Generation"
                },
                "summary": "[Background] Large Language Model (LLM)-based multi-agent systems (MAS) are\ntransforming software development by enabling autonomous collaboration.\nClassical software processes such asWaterfall, V-Model, and Agile offer\nstructured coordination patterns that can be repurposed to guide these agent\ninteractions. [Aims] This study explores how traditional software development\nprocesses can be adapted as coordination scaffolds for LLM based MAS and\nexamines their impact on code quality, cost, and productivity. [Method] We\nexecuted 11 diverse software projects under three process models and four GPT\nvariants, totaling 132 runs. Each output was evaluated using standardized\nmetrics for size (files, LOC), cost (execution time, token usage), and quality\n(code smells, AI- and human detected bugs). [Results] Both process model and\nLLM choice significantly affected system performance. Waterfall was most\nefficient, V-Model produced the most verbose code, and Agile achieved the\nhighest code quality, albeit at higher computational cost. [Conclusions]\nClassical software processes can be effectively instantiated in LLM-based MAS,\nbut each entails trade-offs across quality, cost, and adaptability. Process\nselection should reflect project goals, whether prioritizing efficiency,\nrobustness, or structured validation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "[Background] Large Language Model (LLM)-based multi-agent systems (MAS) are\ntransforming software development by enabling autonomous collaboration.\nClassical software processes such asWaterfall, V-Model, and Agile offer\nstructured coordination patterns that can be repurposed to guide these agent\ninteractions. [Aims] This study explores how traditional software development\nprocesses can be adapted as coordination scaffolds for LLM based MAS and\nexamines their impact on code quality, cost, and productivity. [Method] We\nexecuted 11 diverse software projects under three process models and four GPT\nvariants, totaling 132 runs. Each output was evaluated using standardized\nmetrics for size (files, LOC), cost (execution time, token usage), and quality\n(code smells, AI- and human detected bugs). [Results] Both process model and\nLLM choice significantly affected system performance. Waterfall was most\nefficient, V-Model produced the most verbose code, and Agile achieved the\nhighest code quality, albeit at higher computational cost. [Conclusions]\nClassical software processes can be effectively instantiated in LLM-based MAS,\nbut each entails trade-offs across quality, cost, and adaptability. Process\nselection should reflect project goals, whether prioritizing efficiency,\nrobustness, or structured validation."
                },
                "authors": [
                    {
                        "name": "Duc Minh Ha"
                    },
                    {
                        "name": "Phu Trac Kien"
                    },
                    {
                        "name": "Tho Quan"
                    },
                    {
                        "name": "Anh Nguyen-Duc"
                    }
                ],
                "author_detail": {
                    "name": "Anh Nguyen-Duc"
                },
                "author": "Anh Nguyen-Duc",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13941v1",
                "updated": "2025-09-17T13:07:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    7,
                    52,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T13:07:52Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    7,
                    52,
                    2,
                    260,
                    0
                ],
                "title": "An Empirical Study on Failures in Automated Issue Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on Failures in Automated Issue Solving"
                },
                "summary": "Automated issue solving seeks to autonomously identify and repair defective\ncode snippets across an entire codebase. SWE-Bench has emerged as the most\nwidely adopted benchmark for evaluating progress in this area. While LLM-based\nagentic tools show great promise, they still fail on a substantial portion of\ntasks. Moreover, current evaluations primarily report aggregate issue-solving\nrates, which obscure the underlying causes of success and failure, making it\nchallenging to diagnose model weaknesses or guide targeted improvements. To\nbridge this gap, we first analyze the performance and efficiency of three SOTA\ntools, spanning both pipeline-based and agentic architectures, in automated\nissue solving tasks of SWE-Bench-Verified under varying task characteristics.\nFurthermore, to move from high-level performance metrics to underlying cause\nanalysis, we conducted a systematic manual analysis of 150 failed instances.\nFrom this analysis, we developed a comprehensive taxonomy of failure modes\ncomprising 3 primary phases, 9 main categories, and 25 fine-grained\nsubcategories. Then we systematically analyze the distribution of the\nidentified failure modes, the results reveal distinct failure fingerprints\nbetween the two architectural paradigms, with the majority of agentic failures\nstemming from flawed reasoning and cognitive deadlocks. Motivated by these\ninsights, we propose a collaborative Expert-Executor framework. It introduces a\nsupervisory Expert agent tasked with providing strategic oversight and\ncourse-correction for a primary Executor agent. This architecture is designed\nto correct flawed reasoning and break the cognitive deadlocks that frequently\nlead to failure. Experiments show that our framework solves 22.2% of previously\nintractable issues for a leading single agent. These findings pave the way for\nbuilding more robust agents through diagnostic evaluation and collaborative\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated issue solving seeks to autonomously identify and repair defective\ncode snippets across an entire codebase. SWE-Bench has emerged as the most\nwidely adopted benchmark for evaluating progress in this area. While LLM-based\nagentic tools show great promise, they still fail on a substantial portion of\ntasks. Moreover, current evaluations primarily report aggregate issue-solving\nrates, which obscure the underlying causes of success and failure, making it\nchallenging to diagnose model weaknesses or guide targeted improvements. To\nbridge this gap, we first analyze the performance and efficiency of three SOTA\ntools, spanning both pipeline-based and agentic architectures, in automated\nissue solving tasks of SWE-Bench-Verified under varying task characteristics.\nFurthermore, to move from high-level performance metrics to underlying cause\nanalysis, we conducted a systematic manual analysis of 150 failed instances.\nFrom this analysis, we developed a comprehensive taxonomy of failure modes\ncomprising 3 primary phases, 9 main categories, and 25 fine-grained\nsubcategories. Then we systematically analyze the distribution of the\nidentified failure modes, the results reveal distinct failure fingerprints\nbetween the two architectural paradigms, with the majority of agentic failures\nstemming from flawed reasoning and cognitive deadlocks. Motivated by these\ninsights, we propose a collaborative Expert-Executor framework. It introduces a\nsupervisory Expert agent tasked with providing strategic oversight and\ncourse-correction for a primary Executor agent. This architecture is designed\nto correct flawed reasoning and break the cognitive deadlocks that frequently\nlead to failure. Experiments show that our framework solves 22.2% of previously\nintractable issues for a leading single agent. These findings pave the way for\nbuilding more robust agents through diagnostic evaluation and collaborative\ndesign."
                },
                "authors": [
                    {
                        "name": "Simiao Liu"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Liehao Li"
                    },
                    {
                        "name": "Xin Tan"
                    },
                    {
                        "name": "Yinghao Zhu"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Li Zhang"
                },
                "author": "Li Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13934v1",
                "updated": "2025-09-17T13:05:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    5,
                    8,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T13:05:08Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    5,
                    8,
                    2,
                    260,
                    0
                ],
                "title": "Large Language Model-Empowered Decision Transformer for UAV-Enabled Data\n  Collection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Empowered Decision Transformer for UAV-Enabled Data\n  Collection"
                },
                "summary": "The deployment of unmanned aerial vehicles (UAVs) for reliable and\nenergy-efficient data collection from spatially distributed devices holds great\npromise in supporting diverse Internet of Things (IoT) applications.\nNevertheless, the limited endurance and communication range of UAVs necessitate\nintelligent trajectory planning. While reinforcement learning (RL) has been\nextensively explored for UAV trajectory optimization, its interactive nature\nentails high costs and risks in real-world environments. Offline RL mitigates\nthese issues but remains susceptible to unstable training and heavily rely on\nexpert-quality datasets. To address these challenges, we formulate a joint UAV\ntrajectory planning and resource allocation problem to maximize energy\nefficiency of data collection. The resource allocation subproblem is first\ntransformed into an equivalent linear programming formulation and solved\noptimally with polynomial-time complexity. Then, we propose a large language\nmodel (LLM)-empowered critic-regularized decision transformer (DT) framework,\ntermed LLM-CRDT, to learn effective UAV control policies. In LLM-CRDT, we\nincorporate critic networks to regularize the DT model training, thereby\nintegrating the sequence modeling capabilities of DT with critic-based value\nguidance to enable learning effective policies from suboptimal datasets.\nFurthermore, to mitigate the data-hungry nature of transformer models, we\nemploy a pre-trained LLM as the transformer backbone of the DT model and adopt\na parameter-efficient fine-tuning strategy, i.e., LoRA, enabling rapid\nadaptation to UAV control tasks with small-scale dataset and low computational\noverhead. Extensive simulations demonstrate that LLM-CRDT outperforms benchmark\nonline and offline RL methods, achieving up to 36.7\\% higher energy efficiency\nthan the current state-of-the-art DT approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of unmanned aerial vehicles (UAVs) for reliable and\nenergy-efficient data collection from spatially distributed devices holds great\npromise in supporting diverse Internet of Things (IoT) applications.\nNevertheless, the limited endurance and communication range of UAVs necessitate\nintelligent trajectory planning. While reinforcement learning (RL) has been\nextensively explored for UAV trajectory optimization, its interactive nature\nentails high costs and risks in real-world environments. Offline RL mitigates\nthese issues but remains susceptible to unstable training and heavily rely on\nexpert-quality datasets. To address these challenges, we formulate a joint UAV\ntrajectory planning and resource allocation problem to maximize energy\nefficiency of data collection. The resource allocation subproblem is first\ntransformed into an equivalent linear programming formulation and solved\noptimally with polynomial-time complexity. Then, we propose a large language\nmodel (LLM)-empowered critic-regularized decision transformer (DT) framework,\ntermed LLM-CRDT, to learn effective UAV control policies. In LLM-CRDT, we\nincorporate critic networks to regularize the DT model training, thereby\nintegrating the sequence modeling capabilities of DT with critic-based value\nguidance to enable learning effective policies from suboptimal datasets.\nFurthermore, to mitigate the data-hungry nature of transformer models, we\nemploy a pre-trained LLM as the transformer backbone of the DT model and adopt\na parameter-efficient fine-tuning strategy, i.e., LoRA, enabling rapid\nadaptation to UAV control tasks with small-scale dataset and low computational\noverhead. Extensive simulations demonstrate that LLM-CRDT outperforms benchmark\nonline and offline RL methods, achieving up to 36.7\\% higher energy efficiency\nthan the current state-of-the-art DT approaches."
                },
                "authors": [
                    {
                        "name": "Zhixion Chen"
                    },
                    {
                        "name": "Jiangzhou Wang"
                    },
                    {
                        "name": "and Hyundong Shin"
                    },
                    {
                        "name": "Arumugam Nallanathan"
                    }
                ],
                "author_detail": {
                    "name": "Arumugam Nallanathan"
                },
                "author": "Arumugam Nallanathan",
                "arxiv_comment": "14pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13932v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13932v1",
                "updated": "2025-09-17T13:01:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    1,
                    22,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T13:01:22Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    1,
                    22,
                    2,
                    260,
                    0
                ],
                "title": "Investigating aerosols as a reconciliation mechanism for K2-18 b JWST\n  MIRI and NIRISS/NIRSpec observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating aerosols as a reconciliation mechanism for K2-18 b JWST\n  MIRI and NIRISS/NIRSpec observations"
                },
                "summary": "Recent JWST observations of the temperate sub-Neptune K2-18 b with NIRISS\nSOSS/NIRSpec G395H and MIRI LRS have yielded apparently inconsistent results:\nthe MIRI spectra exhibit spectral features nearly twice as large as those seen\nat shorter wavelengths, challenging the high-metallicity, CH4-rich\nnon-equilibrium model that fits the NIRISS/NIRSpec data. We perform a suite of\natmospheric retrievals on both datasets, including free-chemistry,\nnon-equilibrium, and aerosol models, using laboratory-derived complex\nrefractive indices for a variety of photochemical haze analogues. Free\nretrievals systematically return lower metallicities than inferred by\nself-consistent chemical disequilibrium models, and the inclusion of absorbing\naerosols, especially CH4-dominated, nitrogen-poor tholins, can further reduce\nthe inferred metallicity by over an order of magnitude. These hazes reproduce\nthe observed NIRISS slope through scattering and match MIRI features via C-H\nbending absorption near 7 um, while yielding particle properties consistent\nwith photochemical production in H2-rich atmospheres. Although their inclusion\nimproves the joint fit and reduces tension between datasets, it also\nsignificantly lowers the retrieved CH4 abundance, highlighting degeneracies\nbetween metallicity, composition, and aerosol properties. Our results\nunderscore the importance of aerosol absorption in interpreting temperate\nsub-Neptune spectra, and motivate future JWST observations and laboratory work\nto break these degeneracies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent JWST observations of the temperate sub-Neptune K2-18 b with NIRISS\nSOSS/NIRSpec G395H and MIRI LRS have yielded apparently inconsistent results:\nthe MIRI spectra exhibit spectral features nearly twice as large as those seen\nat shorter wavelengths, challenging the high-metallicity, CH4-rich\nnon-equilibrium model that fits the NIRISS/NIRSpec data. We perform a suite of\natmospheric retrievals on both datasets, including free-chemistry,\nnon-equilibrium, and aerosol models, using laboratory-derived complex\nrefractive indices for a variety of photochemical haze analogues. Free\nretrievals systematically return lower metallicities than inferred by\nself-consistent chemical disequilibrium models, and the inclusion of absorbing\naerosols, especially CH4-dominated, nitrogen-poor tholins, can further reduce\nthe inferred metallicity by over an order of magnitude. These hazes reproduce\nthe observed NIRISS slope through scattering and match MIRI features via C-H\nbending absorption near 7 um, while yielding particle properties consistent\nwith photochemical production in H2-rich atmospheres. Although their inclusion\nimproves the joint fit and reduces tension between datasets, it also\nsignificantly lowers the retrieved CH4 abundance, highlighting degeneracies\nbetween metallicity, composition, and aerosol properties. Our results\nunderscore the importance of aerosol absorption in interpreting temperate\nsub-Neptune spectra, and motivate future JWST observations and laboratory work\nto break these degeneracies."
                },
                "authors": [
                    {
                        "name": "Adam Yassin Jaziri"
                    },
                    {
                        "name": "Thomas Drant"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Drant"
                },
                "author": "Thomas Drant",
                "arxiv_comment": "Revision version submitted in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13932v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13932v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18655v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18655v3",
                "updated": "2025-09-17T13:01:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    1,
                    21,
                    2,
                    260,
                    0
                ],
                "published": "2025-08-26T03:54:39Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    3,
                    54,
                    39,
                    1,
                    238,
                    0
                ],
                "title": "Empathy Omni: Enabling Empathetic Speech Response Generation through\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empathy Omni: Enabling Empathetic Speech Response Generation through\n  Large Language Models"
                },
                "summary": "With the development of speech large language models (speech LLMs), users can\nnow interact directly with assistants via speech. However, most existing models\nonly convert response content into speech without fully capturing the rich\nemotional cues in user queries, where the same sentence may convey different\nmeanings depending on the expression. Emotional understanding is thus essential\nfor improving human-machine interaction. Most empathetic speech LLMs rely on\nmassive datasets, demanding high computational cost. A key challenge is to\nbuild models that generate empathetic responses with limited data and without\nlarge-scale training. To this end, we propose Emotion Omni, a model that\nunderstands emotional content in user speech and generates empathetic\nresponses. We further developed a data pipeline to construct a 200k emotional\ndialogue dataset supporting empathetic speech assistants. Experiments show that\nEmotion Omni achieves comparable instruction-following ability without\nlarge-scale pretraining, while surpassing existing models in speech quality\n(UTMOS:4.41) and empathy (Emotion GPT Score: 3.97). These results confirm its\nimprovements in both speech fidelity and emotional expressiveness. Demos are\navailable at https://w311411.github.io/omni_demo/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of speech large language models (speech LLMs), users can\nnow interact directly with assistants via speech. However, most existing models\nonly convert response content into speech without fully capturing the rich\nemotional cues in user queries, where the same sentence may convey different\nmeanings depending on the expression. Emotional understanding is thus essential\nfor improving human-machine interaction. Most empathetic speech LLMs rely on\nmassive datasets, demanding high computational cost. A key challenge is to\nbuild models that generate empathetic responses with limited data and without\nlarge-scale training. To this end, we propose Emotion Omni, a model that\nunderstands emotional content in user speech and generates empathetic\nresponses. We further developed a data pipeline to construct a 200k emotional\ndialogue dataset supporting empathetic speech assistants. Experiments show that\nEmotion Omni achieves comparable instruction-following ability without\nlarge-scale pretraining, while surpassing existing models in speech quality\n(UTMOS:4.41) and empathy (Emotion GPT Score: 3.97). These results confirm its\nimprovements in both speech fidelity and emotional expressiveness. Demos are\navailable at https://w311411.github.io/omni_demo/."
                },
                "authors": [
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Guangyan Zhang"
                    },
                    {
                        "name": "Jiale Chen"
                    },
                    {
                        "name": "Jingyu Li"
                    },
                    {
                        "name": "Yuehai Wang"
                    },
                    {
                        "name": "Yiwen Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yiwen Guo"
                },
                "author": "Yiwen Guo",
                "arxiv_comment": "5 pages, 1 figure, submitted to ICASSP 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18655v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18655v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12918v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12918v2",
                "updated": "2025-09-17T12:59:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    12,
                    59,
                    25,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-16T10:11:59Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    11,
                    59,
                    1,
                    259,
                    0
                ],
                "title": "A Novel Compression Framework for YOLOv8: Achieving Real-Time Aerial\n  Object Detection on Edge Devices via Structured Pruning and Channel-Wise\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Compression Framework for YOLOv8: Achieving Real-Time Aerial\n  Object Detection on Edge Devices via Structured Pruning and Channel-Wise\n  Distillation"
                },
                "summary": "Efficient deployment of deep learning models for aerial object detection on\nresource-constrained devices requires significant compression without\ncom-promising performance. In this study, we propose a novel three-stage\ncompression pipeline for the YOLOv8 object detection model, integrating\nsparsity-aware training, structured channel pruning, and Channel-Wise Knowledge\nDistillation (CWD). First, sparsity-aware training introduces dynamic sparsity\nduring model optimization, effectively balancing parameter reduction and\ndetection accuracy. Second, we apply structured channel pruning by leveraging\nbatch normalization scaling factors to eliminate redundant channels,\nsignificantly reducing model size and computational complexity. Finally, to\nmitigate the accuracy drop caused by pruning, we employ CWD to transfer\nknowledge from the original model, using an adjustable temperature and loss\nweighting scheme tailored for small and medium object detection. Extensive\nexperiments on the VisDrone dataset demonstrate the effectiveness of our\napproach across multiple YOLOv8 variants. For YOLOv8m, our method reduces model\nparameters from 25.85M to 6.85M (a 73.51% reduction), FLOPs from 49.6G to\n13.3G, and MACs from 101G to 34.5G, while reducing AP50 by only 2.7%. The\nresulting compressed model achieves 47.9 AP50 and boosts inference speed from\n26 FPS (YOLOv8m baseline) to 45 FPS, enabling real-time deployment on edge\ndevices. We further apply TensorRT as a lightweight optimization step. While\nthis introduces a minor drop in AP50 (from 47.9 to 47.6), it significantly\nimproves inference speed from 45 to 68 FPS, demonstrating the practicality of\nour approach for high-throughput, re-source-constrained scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient deployment of deep learning models for aerial object detection on\nresource-constrained devices requires significant compression without\ncom-promising performance. In this study, we propose a novel three-stage\ncompression pipeline for the YOLOv8 object detection model, integrating\nsparsity-aware training, structured channel pruning, and Channel-Wise Knowledge\nDistillation (CWD). First, sparsity-aware training introduces dynamic sparsity\nduring model optimization, effectively balancing parameter reduction and\ndetection accuracy. Second, we apply structured channel pruning by leveraging\nbatch normalization scaling factors to eliminate redundant channels,\nsignificantly reducing model size and computational complexity. Finally, to\nmitigate the accuracy drop caused by pruning, we employ CWD to transfer\nknowledge from the original model, using an adjustable temperature and loss\nweighting scheme tailored for small and medium object detection. Extensive\nexperiments on the VisDrone dataset demonstrate the effectiveness of our\napproach across multiple YOLOv8 variants. For YOLOv8m, our method reduces model\nparameters from 25.85M to 6.85M (a 73.51% reduction), FLOPs from 49.6G to\n13.3G, and MACs from 101G to 34.5G, while reducing AP50 by only 2.7%. The\nresulting compressed model achieves 47.9 AP50 and boosts inference speed from\n26 FPS (YOLOv8m baseline) to 45 FPS, enabling real-time deployment on edge\ndevices. We further apply TensorRT as a lightweight optimization step. While\nthis introduces a minor drop in AP50 (from 47.9 to 47.6), it significantly\nimproves inference speed from 45 to 68 FPS, demonstrating the practicality of\nour approach for high-throughput, re-source-constrained scenarios."
                },
                "authors": [
                    {
                        "name": "Melika Sabaghian"
                    },
                    {
                        "name": "Mohammad Ali Keyvanrad"
                    },
                    {
                        "name": "Seyyedeh Mahila Moghadami"
                    }
                ],
                "author_detail": {
                    "name": "Seyyedeh Mahila Moghadami"
                },
                "author": "Seyyedeh Mahila Moghadami",
                "arxiv_comment": "28 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12918v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12918v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08716v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08716v3",
                "updated": "2025-09-17T12:56:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    12,
                    56,
                    34,
                    2,
                    260,
                    0
                ],
                "published": "2024-12-11T19:00:03Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    19,
                    0,
                    3,
                    2,
                    346,
                    0
                ],
                "title": "UFig v1: The ultra-fast image generator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UFig v1: The ultra-fast image generator"
                },
                "summary": "With the rise of simulation-based inference (SBI) methods, simulations need\nto be fast as well as realistic. $\\texttt{UFig v1}$ is a public Python package\nthat simulates astronomical images with exceptional speed, taking approximately\nthe same time as source extraction. This makes it particularly well-suited for\nSBI methods where computational efficiency is crucial. To render an image,\n$\\texttt{UFig}$ requires a galaxy catalog, and a description of the point\nspread function (PSF). It can also add background noise, sample stars using the\nBesan\\c{c}on model of the Milky Way, and run $\\texttt{SExtractor}$ to extract\nsources from the rendered image. The extracted sources can be matched to the\nintrinsic catalog, flagged based on $\\texttt{SExtractor}$ output and survey\nmasks, and emulators can be used to bypass the image simulation and extraction\nsteps. A first version of $\\texttt{UFig}$ was presented in Berg\\'e et al.\n(2013) and the software has since been used and further developed in a variety\nof forward modelling applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of simulation-based inference (SBI) methods, simulations need\nto be fast as well as realistic. $\\texttt{UFig v1}$ is a public Python package\nthat simulates astronomical images with exceptional speed, taking approximately\nthe same time as source extraction. This makes it particularly well-suited for\nSBI methods where computational efficiency is crucial. To render an image,\n$\\texttt{UFig}$ requires a galaxy catalog, and a description of the point\nspread function (PSF). It can also add background noise, sample stars using the\nBesan\\c{c}on model of the Milky Way, and run $\\texttt{SExtractor}$ to extract\nsources from the rendered image. The extracted sources can be matched to the\nintrinsic catalog, flagged based on $\\texttt{SExtractor}$ output and survey\nmasks, and emulators can be used to bypass the image simulation and extraction\nsteps. A first version of $\\texttt{UFig}$ was presented in Berg\\'e et al.\n(2013) and the software has since been used and further developed in a variety\nof forward modelling applications."
                },
                "authors": [
                    {
                        "name": "Silvan Fischbacher"
                    },
                    {
                        "name": "Beatrice Moser"
                    },
                    {
                        "name": "Tomasz Kacprzak"
                    },
                    {
                        "name": "Luca Tortorelli"
                    },
                    {
                        "name": "Joerg Herbel"
                    },
                    {
                        "name": "Claudio Bruderer"
                    },
                    {
                        "name": "Uwe Schmitt"
                    },
                    {
                        "name": "Alexandre Refregier"
                    },
                    {
                        "name": "Joel Berge"
                    },
                    {
                        "name": "Lukas Gamper"
                    },
                    {
                        "name": "Adam Amara"
                    }
                ],
                "author_detail": {
                    "name": "Adam Amara"
                },
                "author": "Adam Amara",
                "arxiv_doi": "10.21105/joss.08697",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.21105/joss.08697",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.08716v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08716v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 4 figures, accepted for publication in JOSS",
                "arxiv_journal_ref": "Journal of Open Source Software, 10(113), 8697 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06652v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06652v2",
                "updated": "2025-09-17T12:55:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    12,
                    55,
                    31,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-08T13:07:35Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    7,
                    35,
                    0,
                    251,
                    0
                ],
                "title": "IntrEx: A Dataset for Modeling Engagement in Educational Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IntrEx: A Dataset for Modeling Engagement in Educational Conversations"
                },
                "summary": "Engagement and motivation are crucial for second-language acquisition, yet\nmaintaining learner interest in educational conversations remains a challenge.\nWhile prior research has explored what makes educational texts interesting,\nstill little is known about the linguistic features that drive engagement in\nconversations. To address this gap, we introduce IntrEx, the first large\ndataset annotated for interestingness and expected interestingness in\nteacher-student interactions. Built upon the Teacher-Student Chatroom Corpus\n(TSCC), IntrEx extends prior work by incorporating sequence-level annotations,\nallowing for the study of engagement beyond isolated turns to capture how\ninterest evolves over extended dialogues. We employ a rigorous annotation\nprocess with over 100 second-language learners, using a comparison-based rating\napproach inspired by reinforcement learning from human feedback (RLHF) to\nimprove agreement. We investigate whether large language models (LLMs) can\npredict human interestingness judgments. We find that LLMs (7B/8B parameters)\nfine-tuned on interestingness ratings outperform larger proprietary models like\nGPT-4o, demonstrating the potential for specialised datasets to model\nengagement in educational settings. Finally, we analyze how linguistic and\ncognitive factors, such as concreteness, comprehensibility (readability), and\nuptake, influence engagement in educational dialogues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engagement and motivation are crucial for second-language acquisition, yet\nmaintaining learner interest in educational conversations remains a challenge.\nWhile prior research has explored what makes educational texts interesting,\nstill little is known about the linguistic features that drive engagement in\nconversations. To address this gap, we introduce IntrEx, the first large\ndataset annotated for interestingness and expected interestingness in\nteacher-student interactions. Built upon the Teacher-Student Chatroom Corpus\n(TSCC), IntrEx extends prior work by incorporating sequence-level annotations,\nallowing for the study of engagement beyond isolated turns to capture how\ninterest evolves over extended dialogues. We employ a rigorous annotation\nprocess with over 100 second-language learners, using a comparison-based rating\napproach inspired by reinforcement learning from human feedback (RLHF) to\nimprove agreement. We investigate whether large language models (LLMs) can\npredict human interestingness judgments. We find that LLMs (7B/8B parameters)\nfine-tuned on interestingness ratings outperform larger proprietary models like\nGPT-4o, demonstrating the potential for specialised datasets to model\nengagement in educational settings. Finally, we analyze how linguistic and\ncognitive factors, such as concreteness, comprehensibility (readability), and\nuptake, influence engagement in educational dialogues."
                },
                "authors": [
                    {
                        "name": "Xingwei Tan"
                    },
                    {
                        "name": "Mahathi Parvatham"
                    },
                    {
                        "name": "Chiara Gambi"
                    },
                    {
                        "name": "Gabriele Pergola"
                    }
                ],
                "author_detail": {
                    "name": "Gabriele Pergola"
                },
                "author": "Gabriele Pergola",
                "arxiv_comment": "EMNLP 2025 Findings camera-ready, 9+7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06652v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06652v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17827v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17827v4",
                "updated": "2025-09-17T11:36:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    11,
                    36,
                    56,
                    2,
                    260,
                    0
                ],
                "published": "2025-04-24T03:09:04Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    3,
                    9,
                    4,
                    3,
                    114,
                    0
                ],
                "title": "Evolution Meets Diffusion: Efficient Neural Architecture Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolution Meets Diffusion: Efficient Neural Architecture Generation"
                },
                "summary": "Neural Architecture Search (NAS) has gained widespread attention for its\ntransformative potential in deep learning model design. However, the vast and\ncomplex search space of NAS leads to significant computational and time costs.\nNeural Architecture Generation (NAG) addresses this by reframing NAS as a\ngeneration problem, enabling the precise generation of optimal architectures\nfor specific tasks. Despite its promise, mainstream methods like diffusion\nmodels face limitations in global search capabilities and are still hindered by\nhigh computational and time demands. To overcome these challenges, we propose\nEvolutionary Diffusion-based Neural Architecture Generation (EDNAG), a novel\napproach that achieves efficient and training-free architecture generation.\nEDNAG leverages evolutionary algorithms to simulate the denoising process in\ndiffusion models, using fitness to guide the transition from random Gaussian\ndistributions to optimal architecture distributions. This approach combines the\nstrengths of evolutionary strategies and diffusion models, enabling rapid and\neffective architecture generation. Extensive experiments demonstrate that EDNAG\nachieves state-of-the-art (SOTA) performance in architecture optimization, with\nan improvement in accuracy of up to 10.45%. Furthermore, it eliminates the need\nfor time-consuming training and boosts inference speed by an average of 50\ntimes, showcasing its exceptional efficiency and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Architecture Search (NAS) has gained widespread attention for its\ntransformative potential in deep learning model design. However, the vast and\ncomplex search space of NAS leads to significant computational and time costs.\nNeural Architecture Generation (NAG) addresses this by reframing NAS as a\ngeneration problem, enabling the precise generation of optimal architectures\nfor specific tasks. Despite its promise, mainstream methods like diffusion\nmodels face limitations in global search capabilities and are still hindered by\nhigh computational and time demands. To overcome these challenges, we propose\nEvolutionary Diffusion-based Neural Architecture Generation (EDNAG), a novel\napproach that achieves efficient and training-free architecture generation.\nEDNAG leverages evolutionary algorithms to simulate the denoising process in\ndiffusion models, using fitness to guide the transition from random Gaussian\ndistributions to optimal architecture distributions. This approach combines the\nstrengths of evolutionary strategies and diffusion models, enabling rapid and\neffective architecture generation. Extensive experiments demonstrate that EDNAG\nachieves state-of-the-art (SOTA) performance in architecture optimization, with\nan improvement in accuracy of up to 10.45%. Furthermore, it eliminates the need\nfor time-consuming training and boosts inference speed by an average of 50\ntimes, showcasing its exceptional efficiency and effectiveness."
                },
                "authors": [
                    {
                        "name": "Bingye Zhou"
                    },
                    {
                        "name": "Caiyang Yu"
                    }
                ],
                "author_detail": {
                    "name": "Caiyang Yu"
                },
                "author": "Caiyang Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17827v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17827v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10663v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10663v2",
                "updated": "2025-09-17T11:21:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    11,
                    21,
                    33,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-12T19:42:16Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    19,
                    42,
                    16,
                    4,
                    255,
                    0
                ],
                "title": "Context Copying Modulation: The Role of Entropy Neurons in Managing\n  Parametric and Contextual Knowledge Conflicts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Copying Modulation: The Role of Entropy Neurons in Managing\n  Parametric and Contextual Knowledge Conflicts"
                },
                "summary": "The behavior of Large Language Models (LLMs) when facing contextual\ninformation that conflicts with their internal parametric knowledge is\ninconsistent, with no generally accepted explanation for the expected outcome\ndistribution. Recent work has identified in autoregressive transformer models a\nclass of neurons -- called entropy neurons -- that produce a significant effect\non the model output entropy while having an overall moderate impact on the\nranking of the predicted tokens. In this paper, we investigate the preliminary\nclaim that these neurons are involved in inhibiting context copying behavior in\ntransformers by looking at their role in resolving conflicts between contextual\nand parametric information. We show that entropy neurons are responsible for\nsuppressing context copying across a range of LLMs, and that ablating them\nleads to a significant change in the generation process. These results enhance\nour understanding of the internal dynamics of LLMs when handling conflicting\ninformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The behavior of Large Language Models (LLMs) when facing contextual\ninformation that conflicts with their internal parametric knowledge is\ninconsistent, with no generally accepted explanation for the expected outcome\ndistribution. Recent work has identified in autoregressive transformer models a\nclass of neurons -- called entropy neurons -- that produce a significant effect\non the model output entropy while having an overall moderate impact on the\nranking of the predicted tokens. In this paper, we investigate the preliminary\nclaim that these neurons are involved in inhibiting context copying behavior in\ntransformers by looking at their role in resolving conflicts between contextual\nand parametric information. We show that entropy neurons are responsible for\nsuppressing context copying across a range of LLMs, and that ablating them\nleads to a significant change in the generation process. These results enhance\nour understanding of the internal dynamics of LLMs when handling conflicting\ninformation."
                },
                "authors": [
                    {
                        "name": "Zineddine Tighidet"
                    },
                    {
                        "name": "Andrea Mogini"
                    },
                    {
                        "name": "Hedi Ben-younes"
                    },
                    {
                        "name": "Jiali Mei"
                    },
                    {
                        "name": "Patrick Gallinari"
                    },
                    {
                        "name": "Benjamin Piwowarski"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Piwowarski"
                },
                "author": "Benjamin Piwowarski",
                "arxiv_comment": "Accepted at EMNLP 2025",
                "arxiv_journal_ref": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10663v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10663v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13911v1",
                "updated": "2025-09-17T11:15:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    11,
                    15,
                    42,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T11:15:42Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    11,
                    15,
                    42,
                    2,
                    260,
                    0
                ],
                "title": "Spatially resolved broad line region in a quasar at z=4: Dynamical black\n  hole mass and prominent outflow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatially resolved broad line region in a quasar at z=4: Dynamical black\n  hole mass and prominent outflow"
                },
                "summary": "We present the first near-infrared interferometric data of a QSO at z=4. The\nK-band observations were performed with GRAVITY+ on the VLTI using all 4 UTs,\ndetecting a differential phase signal that traces the spatially resolved\nkinematics for both the H$\\beta$ and H$\\gamma$ lines in the broad line region.\nWe fit the two lines simultaneously with an updated model that includes\ndistinct rotating and conical outflowing components. We find that more than\n80\\% of the HI line emission from the BLR originates in an outflow with a\nvelocity up to $10^4$ km s$^{-1}$. This is oriented so that our line of sight\nis along an edge of the conical structure, which produces the prominent blue\nwing on the line profile. A combination of anisotropic line emission and\nmid-plane opacity lead to the single-sided phase signal. The model is able to\nqualitatively match both the outflowing CIV line profile and the systemic OI\nfluorescent emission. The derived black hole mass of $8\\times10^8$ M$_\\odot$ is\nthe highest redshift black hole mass measurement to date obtained directly from\nBLR dynamics. It is an order of magnitude lower than that inferred from various\nsingle epoch scaling relations, and implies that the accretion is highly\nsuper-Eddington. With reference to recent simulations, the data suggest that\nthis QSO is emitting close to its radiative limit in a regime where strong\noutflows are expected around a polar conical region.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first near-infrared interferometric data of a QSO at z=4. The\nK-band observations were performed with GRAVITY+ on the VLTI using all 4 UTs,\ndetecting a differential phase signal that traces the spatially resolved\nkinematics for both the H$\\beta$ and H$\\gamma$ lines in the broad line region.\nWe fit the two lines simultaneously with an updated model that includes\ndistinct rotating and conical outflowing components. We find that more than\n80\\% of the HI line emission from the BLR originates in an outflow with a\nvelocity up to $10^4$ km s$^{-1}$. This is oriented so that our line of sight\nis along an edge of the conical structure, which produces the prominent blue\nwing on the line profile. A combination of anisotropic line emission and\nmid-plane opacity lead to the single-sided phase signal. The model is able to\nqualitatively match both the outflowing CIV line profile and the systemic OI\nfluorescent emission. The derived black hole mass of $8\\times10^8$ M$_\\odot$ is\nthe highest redshift black hole mass measurement to date obtained directly from\nBLR dynamics. It is an order of magnitude lower than that inferred from various\nsingle epoch scaling relations, and implies that the accretion is highly\nsuper-Eddington. With reference to recent simulations, the data suggest that\nthis QSO is emitting close to its radiative limit in a regime where strong\noutflows are expected around a polar conical region."
                },
                "authors": [
                    {
                        "name": "GRAVITY+ Collaboration"
                    },
                    {
                        "name": "K. Abd El Dayem"
                    },
                    {
                        "name": "N. Aimar"
                    },
                    {
                        "name": "A. Berdeu"
                    },
                    {
                        "name": "J. -P. Berger"
                    },
                    {
                        "name": "G. Bourdarot"
                    },
                    {
                        "name": "P. Bourget"
                    },
                    {
                        "name": "W. Brandner"
                    },
                    {
                        "name": "Y. Cao"
                    },
                    {
                        "name": "C. Correia"
                    },
                    {
                        "name": "S. Cuevas Cardona"
                    },
                    {
                        "name": "R. Davies"
                    },
                    {
                        "name": "D. Defrère"
                    },
                    {
                        "name": "A. Drescher"
                    },
                    {
                        "name": "A. Eckart"
                    },
                    {
                        "name": "F. Eisenhauer"
                    },
                    {
                        "name": "M. Fabricius"
                    },
                    {
                        "name": "A. Farah"
                    },
                    {
                        "name": "H. Feuchtgruber"
                    },
                    {
                        "name": "N. M. Förster Schreiber"
                    },
                    {
                        "name": "A. Foschi"
                    },
                    {
                        "name": "P. Garcia"
                    },
                    {
                        "name": "R. Garcia Lopez"
                    },
                    {
                        "name": "R. Genzel"
                    },
                    {
                        "name": "S. Gillessen"
                    },
                    {
                        "name": "T. Gomes"
                    },
                    {
                        "name": "F. Gonté"
                    },
                    {
                        "name": "V. Gopinath"
                    },
                    {
                        "name": "J. Graf"
                    },
                    {
                        "name": "M. Hartl"
                    },
                    {
                        "name": "X. Haubois"
                    },
                    {
                        "name": "F. Haußmann"
                    },
                    {
                        "name": "L. C. Ho"
                    },
                    {
                        "name": "S. Hönig"
                    },
                    {
                        "name": "M. Houllé"
                    },
                    {
                        "name": "S. Joharle"
                    },
                    {
                        "name": "C. Keiman"
                    },
                    {
                        "name": "P. Kervella"
                    },
                    {
                        "name": "J. Kolb"
                    },
                    {
                        "name": "L. Kreidberg"
                    },
                    {
                        "name": "A. Labdon"
                    },
                    {
                        "name": "S. Lacour"
                    },
                    {
                        "name": "O. Lai"
                    },
                    {
                        "name": "S. Lai"
                    },
                    {
                        "name": "R. Laugier"
                    },
                    {
                        "name": "J. -B. Le Bouquin"
                    },
                    {
                        "name": "J. Leftley"
                    },
                    {
                        "name": "R. Li"
                    },
                    {
                        "name": "B. Lopez"
                    },
                    {
                        "name": "D. Lutz"
                    },
                    {
                        "name": "F. Mang"
                    },
                    {
                        "name": "A. Mérand"
                    },
                    {
                        "name": "F. Millour"
                    },
                    {
                        "name": "M. Montargès"
                    },
                    {
                        "name": "N. More"
                    },
                    {
                        "name": "N. Morujão"
                    },
                    {
                        "name": "H. Nowacki"
                    },
                    {
                        "name": "M. Nowak"
                    },
                    {
                        "name": "S. Oberti"
                    },
                    {
                        "name": "C. Onken"
                    },
                    {
                        "name": "J. Osorno"
                    },
                    {
                        "name": "T. Ott"
                    },
                    {
                        "name": "T. Paumard"
                    },
                    {
                        "name": "K. Perraut"
                    },
                    {
                        "name": "G. Perrin"
                    },
                    {
                        "name": "R. Petrov"
                    },
                    {
                        "name": "P. -O. Petrucci"
                    },
                    {
                        "name": "N. Pourré"
                    },
                    {
                        "name": "S. Rabien"
                    },
                    {
                        "name": "C. Rau"
                    },
                    {
                        "name": "D. Ribeiro"
                    },
                    {
                        "name": "S. Robbe-Dubois"
                    },
                    {
                        "name": "M. Sadun Bordoni"
                    },
                    {
                        "name": "M. Salman"
                    },
                    {
                        "name": "J. Sanchez-Bermudez"
                    },
                    {
                        "name": "D. Santos"
                    },
                    {
                        "name": "J. Sauter"
                    },
                    {
                        "name": "M. Scialpi"
                    },
                    {
                        "name": "J. Scigliuto"
                    },
                    {
                        "name": "J. Shangguan"
                    },
                    {
                        "name": "P. Shchekaturov"
                    },
                    {
                        "name": "T. Shimizu"
                    },
                    {
                        "name": "F. Soulez"
                    },
                    {
                        "name": "C. Straubmeier"
                    },
                    {
                        "name": "E. Sturm"
                    },
                    {
                        "name": "M. Subroweit"
                    },
                    {
                        "name": "C. Sykes"
                    },
                    {
                        "name": "L. J. Tacconi"
                    },
                    {
                        "name": "H. Übler"
                    },
                    {
                        "name": "G. Ulbricht"
                    },
                    {
                        "name": "F. Vincent"
                    },
                    {
                        "name": "R. Webster"
                    },
                    {
                        "name": "E. Wieprecht"
                    },
                    {
                        "name": "J. Woillez"
                    },
                    {
                        "name": "C. Wolf"
                    }
                ],
                "author_detail": {
                    "name": "C. Wolf"
                },
                "author": "C. Wolf",
                "arxiv_comment": "submitted to A&A (15 pages, 10 figures)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13905v1",
                "updated": "2025-09-17T11:11:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    11,
                    11,
                    27,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T11:11:27Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    11,
                    11,
                    27,
                    2,
                    260,
                    0
                ],
                "title": "Do Large Language Models Understand Word Senses?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Understand Word Senses?"
                },
                "summary": "Understanding the meaning of words in context is a fundamental capability for\nLarge Language Models (LLMs). Despite extensive evaluation efforts, the extent\nto which LLMs show evidence that they truly grasp word senses remains\nunderexplored. In this paper, we address this gap by evaluating both i) the\nWord Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs,\ncomparing their performance to state-of-the-art systems specifically designed\nfor the task, and ii) the ability of two top-performing open- and closed-source\nLLMs to understand word senses in three generative settings: definition\ngeneration, free-form explanation, and example generation. Notably, we find\nthat, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve\nperformance on par with specialized WSD systems, while also demonstrating\ngreater robustness across domains and levels of difficulty. In the generation\ntasks, results reveal that LLMs can explain the meaning of words in context up\nto 98\\% accuracy, with the highest performance observed in the free-form\nexplanation task, which best aligns with their generative capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the meaning of words in context is a fundamental capability for\nLarge Language Models (LLMs). Despite extensive evaluation efforts, the extent\nto which LLMs show evidence that they truly grasp word senses remains\nunderexplored. In this paper, we address this gap by evaluating both i) the\nWord Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs,\ncomparing their performance to state-of-the-art systems specifically designed\nfor the task, and ii) the ability of two top-performing open- and closed-source\nLLMs to understand word senses in three generative settings: definition\ngeneration, free-form explanation, and example generation. Notably, we find\nthat, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve\nperformance on par with specialized WSD systems, while also demonstrating\ngreater robustness across domains and levels of difficulty. In the generation\ntasks, results reveal that LLMs can explain the meaning of words in context up\nto 98\\% accuracy, with the highest performance observed in the free-form\nexplanation task, which best aligns with their generative capabilities."
                },
                "authors": [
                    {
                        "name": "Domenico Meconi"
                    },
                    {
                        "name": "Simone Stirpe"
                    },
                    {
                        "name": "Federico Martelli"
                    },
                    {
                        "name": "Leonardo Lavalle"
                    },
                    {
                        "name": "Roberto Navigli"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Navigli"
                },
                "author": "Roberto Navigli",
                "arxiv_comment": "20 pages, to be published in EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11062v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11062v2",
                "updated": "2025-09-17T11:06:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    11,
                    6,
                    49,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-14T03:05:54Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    3,
                    5,
                    54,
                    6,
                    257,
                    0
                ],
                "title": "Auto-Slides: An Interactive Multi-Agent System for Creating and\n  Customizing Research Presentations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-Slides: An Interactive Multi-Agent System for Creating and\n  Customizing Research Presentations"
                },
                "summary": "The rapid progress of large language models (LLMs) has opened new\nopportunities for education. While learners can interact with academic papers\nthrough LLM-powered dialogue, limitations still exist: absence of structured\norganization and high text reliance can impede systematic understanding and\nengagement with complex concepts. To address these challenges, we propose\nAuto-Slides, an LLM-driven system that converts research papers into\npedagogically structured, multimodal slides (e.g., diagrams and tables).\nDrawing on cognitive science, it creates a presentation-oriented narrative and\nallows iterative refinement via an interactive editor, in order to match\nlearners' knowledge level and goals. Auto-Slides further incorporates\nverification and knowledge retrieval mechanisms to ensure accuracy and\ncontextual completeness. Through extensive user studies, Auto-Slides enhances\nlearners' comprehension and engagement compared to conventional LLM-based\nreading. Our contributions lie in designing a multi-agent framework for\ntransforming academic papers into pedagogically optimized slides and\nintroducing interactive customization for personalized learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress of large language models (LLMs) has opened new\nopportunities for education. While learners can interact with academic papers\nthrough LLM-powered dialogue, limitations still exist: absence of structured\norganization and high text reliance can impede systematic understanding and\nengagement with complex concepts. To address these challenges, we propose\nAuto-Slides, an LLM-driven system that converts research papers into\npedagogically structured, multimodal slides (e.g., diagrams and tables).\nDrawing on cognitive science, it creates a presentation-oriented narrative and\nallows iterative refinement via an interactive editor, in order to match\nlearners' knowledge level and goals. Auto-Slides further incorporates\nverification and knowledge retrieval mechanisms to ensure accuracy and\ncontextual completeness. Through extensive user studies, Auto-Slides enhances\nlearners' comprehension and engagement compared to conventional LLM-based\nreading. Our contributions lie in designing a multi-agent framework for\ntransforming academic papers into pedagogically optimized slides and\nintroducing interactive customization for personalized learning."
                },
                "authors": [
                    {
                        "name": "Yuheng Yang"
                    },
                    {
                        "name": "Wenjia Jiang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Chi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chi Zhang"
                },
                "author": "Chi Zhang",
                "arxiv_comment": "Project Homepage: https://auto-slides.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11062v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11062v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05170v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05170v2",
                "updated": "2025-09-17T10:56:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    10,
                    56,
                    50,
                    2,
                    260,
                    0
                ],
                "published": "2025-08-07T09:04:10Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    4,
                    10,
                    3,
                    219,
                    0
                ],
                "title": "Posterior-GRPO: Rewarding Reasoning Processes in Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Posterior-GRPO: Rewarding Reasoning Processes in Code Generation"
                },
                "summary": "Reinforcement learning (RL) has significantly advanced code generation for\nlarge language models (LLMs). However, current paradigms rely on outcome-based\nrewards from test cases, neglecting the quality of the intermediate reasoning\nprocess. While supervising the reasoning process directly is a promising\ndirection, it is highly susceptible to reward hacking, where the policy model\nlearns to exploit the reasoning reward signal without improving final outcomes.\nTo address this, we introduce a unified framework that can effectively\nincorporate the quality of the reasoning process during RL. First, to enable\nreasoning evaluation, we develop LCB-RB, a benchmark comprising preference\npairs of superior and inferior reasoning processes. Second, to accurately score\nreasoning quality, we introduce an Optimized-Degraded based (OD-based) method\nfor reward model training. This method generates high-quality preference pairs\nby systematically optimizing and degrading initial reasoning paths along\ncurated dimensions of reasoning quality, such as factual accuracy, logical\nrigor, and coherence. A 7B parameter reward model with this method achieves\nstate-of-the-art (SOTA) performance on LCB-RB and generalizes well to other\nbenchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method\nthat conditions process-based rewards on task success. By selectively applying\nrewards to the reasoning processes of only successful outcomes, P-GRPO\neffectively mitigates reward hacking and aligns the model's internal reasoning\nwith final code correctness. A 7B parameter model with P-GRPO achieves superior\nperformance across diverse code generation tasks, outperforming outcome-only\nbaselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further\ndemonstrate the generalizability of our approach by extending it to\nmathematical tasks. Our models, dataset, and code are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has significantly advanced code generation for\nlarge language models (LLMs). However, current paradigms rely on outcome-based\nrewards from test cases, neglecting the quality of the intermediate reasoning\nprocess. While supervising the reasoning process directly is a promising\ndirection, it is highly susceptible to reward hacking, where the policy model\nlearns to exploit the reasoning reward signal without improving final outcomes.\nTo address this, we introduce a unified framework that can effectively\nincorporate the quality of the reasoning process during RL. First, to enable\nreasoning evaluation, we develop LCB-RB, a benchmark comprising preference\npairs of superior and inferior reasoning processes. Second, to accurately score\nreasoning quality, we introduce an Optimized-Degraded based (OD-based) method\nfor reward model training. This method generates high-quality preference pairs\nby systematically optimizing and degrading initial reasoning paths along\ncurated dimensions of reasoning quality, such as factual accuracy, logical\nrigor, and coherence. A 7B parameter reward model with this method achieves\nstate-of-the-art (SOTA) performance on LCB-RB and generalizes well to other\nbenchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method\nthat conditions process-based rewards on task success. By selectively applying\nrewards to the reasoning processes of only successful outcomes, P-GRPO\neffectively mitigates reward hacking and aligns the model's internal reasoning\nwith final code correctness. A 7B parameter model with P-GRPO achieves superior\nperformance across diverse code generation tasks, outperforming outcome-only\nbaselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further\ndemonstrate the generalizability of our approach by extending it to\nmathematical tasks. Our models, dataset, and code are publicly available."
                },
                "authors": [
                    {
                        "name": "Lishui Fan"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Mouxiang Chen"
                    },
                    {
                        "name": "Zhongxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhongxin Liu"
                },
                "author": "Zhongxin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05170v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05170v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13899v1",
                "updated": "2025-09-17T10:54:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    10,
                    54,
                    17,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T10:54:17Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    10,
                    54,
                    17,
                    2,
                    260,
                    0
                ],
                "title": "AI as a teaching tool and learning partner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI as a teaching tool and learning partner"
                },
                "summary": "The arrival of AI tools and in particular Large Language Models (LLMs) has\nhad a transformative impact on teaching and learning and institutes are still\ntrying to determine how to integrate LLMs into education in constructive ways.\nHere, we explore the adoption of LLM-based tools into two teaching programmes,\none undergraduate and one postgraduate. We provided to our classes (1) a\nLLM-powered chatbot that had access to course materials by RAG and (2)\nAI-generated audio-only podcasts for each week$\\text{'}$s teaching material. At\nthe end of the semester, we surveyed the classes to gauge attitudes towards\nthese tools. The classes were small and from biological courses. The students\nfelt positive about AI generally and that AI tools made a positive impact on\nteaching. Students found the LLM-powered chatbot easy and enjoyable to use and\nfelt that it enhanced their learning. The podcasts were less popular and only a\nsmall proportion of the class listened weekly. The class as a whole was\nindifferent to whether the podcasts should be used more widely across courses,\nbut those who listened enjoyed them and were in favour.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The arrival of AI tools and in particular Large Language Models (LLMs) has\nhad a transformative impact on teaching and learning and institutes are still\ntrying to determine how to integrate LLMs into education in constructive ways.\nHere, we explore the adoption of LLM-based tools into two teaching programmes,\none undergraduate and one postgraduate. We provided to our classes (1) a\nLLM-powered chatbot that had access to course materials by RAG and (2)\nAI-generated audio-only podcasts for each week$\\text{'}$s teaching material. At\nthe end of the semester, we surveyed the classes to gauge attitudes towards\nthese tools. The classes were small and from biological courses. The students\nfelt positive about AI generally and that AI tools made a positive impact on\nteaching. Students found the LLM-powered chatbot easy and enjoyable to use and\nfelt that it enhanced their learning. The podcasts were less popular and only a\nsmall proportion of the class listened weekly. The class as a whole was\nindifferent to whether the podcasts should be used more widely across courses,\nbut those who listened enjoyed them and were in favour."
                },
                "authors": [
                    {
                        "name": "Steven Watterson"
                    },
                    {
                        "name": "Sarah Atkinson"
                    },
                    {
                        "name": "Elaine Murray"
                    },
                    {
                        "name": "Andrew McDowell"
                    }
                ],
                "author_detail": {
                    "name": "Andrew McDowell"
                },
                "author": "Andrew McDowell",
                "arxiv_comment": "6 Pages, 1 Figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13892v1",
                "updated": "2025-09-17T10:42:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    10,
                    42,
                    6,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T10:42:06Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    10,
                    42,
                    6,
                    2,
                    260,
                    0
                ],
                "title": "Synthetic Data Generation for Screen Time and App Usage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Data Generation for Screen Time and App Usage"
                },
                "summary": "Smartphone usage data can provide valuable insights for understanding\ninteraction with technology and human behavior. However, collecting\nlarge-scale, in-the-wild smartphone usage logs is challenging due to high\ncosts, privacy concerns, under representative user samples and biases like\nnon-response that can skew results. These challenges call for exploring\nalternative approaches to obtain smartphone usage datasets. In this context,\nlarge language models (LLMs) such as Open AI's ChatGPT present a novel approach\nfor synthetic smartphone usage data generation, addressing limitations of\nreal-world data collection. We describe a case study on how four prompt\nstrategies influenced the quality of generated smartphone usage data. We\ncontribute with insights on prompt design and measures of data quality,\nreporting a prompting strategy comparison combining two factors, prompt level\nof detail (describing a user persona, describing the expected results\ncharacteristics) and seed data inclusion (with versus without an initial real\nusage example). Our findings suggest that using LLMs to generate structured and\nbehaviorally plausible smartphone use datasets is feasible for some use cases,\nespecially when using detailed prompts. Challenges remain in capturing diverse\nnuances of human behavioral patterns in a single synthetic dataset, and\nevaluating tradeoffs between data fidelity and diversity, suggesting the need\nfor use-case-specific evaluation metrics and future research with more diverse\nseed data and different LLM models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smartphone usage data can provide valuable insights for understanding\ninteraction with technology and human behavior. However, collecting\nlarge-scale, in-the-wild smartphone usage logs is challenging due to high\ncosts, privacy concerns, under representative user samples and biases like\nnon-response that can skew results. These challenges call for exploring\nalternative approaches to obtain smartphone usage datasets. In this context,\nlarge language models (LLMs) such as Open AI's ChatGPT present a novel approach\nfor synthetic smartphone usage data generation, addressing limitations of\nreal-world data collection. We describe a case study on how four prompt\nstrategies influenced the quality of generated smartphone usage data. We\ncontribute with insights on prompt design and measures of data quality,\nreporting a prompting strategy comparison combining two factors, prompt level\nof detail (describing a user persona, describing the expected results\ncharacteristics) and seed data inclusion (with versus without an initial real\nusage example). Our findings suggest that using LLMs to generate structured and\nbehaviorally plausible smartphone use datasets is feasible for some use cases,\nespecially when using detailed prompts. Challenges remain in capturing diverse\nnuances of human behavioral patterns in a single synthetic dataset, and\nevaluating tradeoffs between data fidelity and diversity, suggesting the need\nfor use-case-specific evaluation metrics and future research with more diverse\nseed data and different LLM models."
                },
                "authors": [
                    {
                        "name": "Gustavo Kruger"
                    },
                    {
                        "name": "Nikhil Sachdeva"
                    },
                    {
                        "name": "Michael Sobolev"
                    }
                ],
                "author_detail": {
                    "name": "Michael Sobolev"
                },
                "author": "Michael Sobolev",
                "arxiv_comment": "14 pages",
                "arxiv_journal_ref": "International Conference on Computer-Human Interaction Research\n  and Applications (CHIRA) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13883v1",
                "updated": "2025-09-17T10:23:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    10,
                    23,
                    30,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T10:23:30Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    10,
                    23,
                    30,
                    2,
                    260,
                    0
                ],
                "title": "EvHand-FPV: Efficient Event-Based 3D Hand Tracking from First-Person\n  View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvHand-FPV: Efficient Event-Based 3D Hand Tracking from First-Person\n  View"
                },
                "summary": "Hand tracking holds great promise for intuitive interaction paradigms, but\nframe-based methods often struggle to meet the requirements of accuracy, low\nlatency, and energy efficiency, especially in resource-constrained settings\nsuch as Extended Reality (XR) devices. Event cameras provide $\\mu$s-level\ntemporal resolution at mW-level power by asynchronously sensing brightness\nchanges. In this work, we present EvHand-FPV, a lightweight framework for\negocentric First-Person-View 3D hand tracking from a single event camera. We\nconstruct an event-based FPV dataset that couples synthetic training data with\n3D labels and real event data with 2D labels for evaluation to address the\nscarcity of egocentric benchmarks. EvHand-FPV also introduces a wrist-based\nregion of interest (ROI) that localizes the hand region via geometric cues,\ncombined with an end-to-end mapping strategy that embeds ROI offsets into the\nnetwork to reduce computation without explicit reconstruction, and a multi-task\nlearning strategy with an auxiliary geometric feature head that improves\nrepresentations without test-time overhead. On our real FPV test set,\nEvHand-FPV improves 2D-AUCp from 0.77 to 0.85 while reducing parameters from\n11.2M to 1.2M by 89% and FLOPs per inference from 1.648G to 0.185G by 89%. It\nalso maintains a competitive 3D-AUCp of 0.84 on synthetic data. These results\ndemonstrate accurate and efficient egocentric event-based hand tracking\nsuitable for on-device XR applications. The dataset and code are available at\nhttps://github.com/zen5x5/EvHand-FPV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hand tracking holds great promise for intuitive interaction paradigms, but\nframe-based methods often struggle to meet the requirements of accuracy, low\nlatency, and energy efficiency, especially in resource-constrained settings\nsuch as Extended Reality (XR) devices. Event cameras provide $\\mu$s-level\ntemporal resolution at mW-level power by asynchronously sensing brightness\nchanges. In this work, we present EvHand-FPV, a lightweight framework for\negocentric First-Person-View 3D hand tracking from a single event camera. We\nconstruct an event-based FPV dataset that couples synthetic training data with\n3D labels and real event data with 2D labels for evaluation to address the\nscarcity of egocentric benchmarks. EvHand-FPV also introduces a wrist-based\nregion of interest (ROI) that localizes the hand region via geometric cues,\ncombined with an end-to-end mapping strategy that embeds ROI offsets into the\nnetwork to reduce computation without explicit reconstruction, and a multi-task\nlearning strategy with an auxiliary geometric feature head that improves\nrepresentations without test-time overhead. On our real FPV test set,\nEvHand-FPV improves 2D-AUCp from 0.77 to 0.85 while reducing parameters from\n11.2M to 1.2M by 89% and FLOPs per inference from 1.648G to 0.185G by 89%. It\nalso maintains a competitive 3D-AUCp of 0.84 on synthetic data. These results\ndemonstrate accurate and efficient egocentric event-based hand tracking\nsuitable for on-device XR applications. The dataset and code are available at\nhttps://github.com/zen5x5/EvHand-FPV."
                },
                "authors": [
                    {
                        "name": "Zhen Xu"
                    },
                    {
                        "name": "Guorui Lu"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Qinyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qinyu Chen"
                },
                "author": "Qinyu Chen",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23512v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23512v6",
                "updated": "2025-09-17T10:22:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    10,
                    22,
                    35,
                    2,
                    260,
                    0
                ],
                "published": "2025-03-30T16:48:27Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    16,
                    48,
                    27,
                    6,
                    89,
                    0
                ],
                "title": "SCORE: Story Coherence and Retrieval Enhancement for AI Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCORE: Story Coherence and Retrieval Enhancement for AI Narratives"
                },
                "summary": "Large Language Models (LLMs) can generate creative and engaging narratives\nfrom user-specified input, but maintaining coherence and emotional depth\nthroughout these AI-generated stories remains a challenge. In this work, we\npropose SCORE, a framework for Story Coherence and Retrieval Enhancement,\ndesigned to detect and resolve narrative inconsistencies. By tracking key item\nstatuses and generating episode summaries, SCORE uses a Retrieval-Augmented\nGeneration (RAG) approach to identify related episodes and enhance the overall\nstory structure. Experimental results from testing multiple LLM-generated\nstories demonstrate that SCORE significantly improves the consistency and\nstability of narrative coherence compared to baseline GPT models, providing a\nmore robust method for evaluating and refining AI-generated narratives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can generate creative and engaging narratives\nfrom user-specified input, but maintaining coherence and emotional depth\nthroughout these AI-generated stories remains a challenge. In this work, we\npropose SCORE, a framework for Story Coherence and Retrieval Enhancement,\ndesigned to detect and resolve narrative inconsistencies. By tracking key item\nstatuses and generating episode summaries, SCORE uses a Retrieval-Augmented\nGeneration (RAG) approach to identify related episodes and enhance the overall\nstory structure. Experimental results from testing multiple LLM-generated\nstories demonstrate that SCORE significantly improves the consistency and\nstability of narrative coherence compared to baseline GPT models, providing a\nmore robust method for evaluating and refining AI-generated narratives."
                },
                "authors": [
                    {
                        "name": "Qiang Yi"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Jianhui Wang"
                    },
                    {
                        "name": "Xinyuan Song"
                    },
                    {
                        "name": "ShiYao Qian"
                    },
                    {
                        "name": "Xinhang Yuan"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Yijin Wang"
                    },
                    {
                        "name": "Jingqun Tang"
                    },
                    {
                        "name": "Yuchen Li"
                    },
                    {
                        "name": "Junjiang Lin"
                    },
                    {
                        "name": "Hongyang He"
                    },
                    {
                        "name": "Zhen Tian"
                    },
                    {
                        "name": "Tianxiang Xu"
                    },
                    {
                        "name": "Keqin Li"
                    },
                    {
                        "name": "Kuan Lu"
                    },
                    {
                        "name": "Menghao Huo"
                    },
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Miao Zhang"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Jianyuan Ni"
                    }
                ],
                "author_detail": {
                    "name": "Jianyuan Ni"
                },
                "author": "Jianyuan Ni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23512v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23512v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12497v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12497v2",
                "updated": "2025-09-17T10:11:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    10,
                    11,
                    18,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-15T22:43:23Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    22,
                    43,
                    23,
                    0,
                    258,
                    0
                ],
                "title": "Prediction and Causality of functional MRI and synthetic signal using a\n  Zero-Shot Time-Series Foundation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prediction and Causality of functional MRI and synthetic signal using a\n  Zero-Shot Time-Series Foundation Model"
                },
                "summary": "Time-series forecasting and causal discovery are central in neuroscience, as\npredicting brain activity and identifying causal relationships between neural\npopulations and circuits can shed light on the mechanisms underlying cognition\nand disease. With the rise of foundation models, an open question is how they\ncompare to traditional methods for brain signal forecasting and causality\nanalysis, and whether they can be applied in a zero-shot setting. In this work,\nwe evaluate a foundation model against classical methods for inferring\ndirectional interactions from spontaneous brain activity measured with\nfunctional magnetic resonance imaging (fMRI) in humans. Traditional approaches\noften rely on Wiener-Granger causality. We tested the forecasting ability of\nthe foundation model in both zero-shot and fine-tuned settings, and assessed\ncausality by comparing Granger-like estimates from the model with standard\nGranger causality. We validated the approach using synthetic time series\ngenerated from ground-truth causal models, including logistic map coupling and\nOrnstein-Uhlenbeck processes. The foundation model achieved competitive\nzero-shot forecasting fMRI time series (mean absolute percentage error of 0.55\nin controls and 0.27 in patients). Although standard Granger causality did not\nshow clear quantitative differences between models, the foundation model\nprovided a more precise detection of causal interactions.\n  Overall, these findings suggest that foundation models offer versatility,\nstrong zero-shot performance, and potential utility for forecasting and causal\ndiscovery in time-series data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-series forecasting and causal discovery are central in neuroscience, as\npredicting brain activity and identifying causal relationships between neural\npopulations and circuits can shed light on the mechanisms underlying cognition\nand disease. With the rise of foundation models, an open question is how they\ncompare to traditional methods for brain signal forecasting and causality\nanalysis, and whether they can be applied in a zero-shot setting. In this work,\nwe evaluate a foundation model against classical methods for inferring\ndirectional interactions from spontaneous brain activity measured with\nfunctional magnetic resonance imaging (fMRI) in humans. Traditional approaches\noften rely on Wiener-Granger causality. We tested the forecasting ability of\nthe foundation model in both zero-shot and fine-tuned settings, and assessed\ncausality by comparing Granger-like estimates from the model with standard\nGranger causality. We validated the approach using synthetic time series\ngenerated from ground-truth causal models, including logistic map coupling and\nOrnstein-Uhlenbeck processes. The foundation model achieved competitive\nzero-shot forecasting fMRI time series (mean absolute percentage error of 0.55\nin controls and 0.27 in patients). Although standard Granger causality did not\nshow clear quantitative differences between models, the foundation model\nprovided a more precise detection of causal interactions.\n  Overall, these findings suggest that foundation models offer versatility,\nstrong zero-shot performance, and potential utility for forecasting and causal\ndiscovery in time-series data."
                },
                "authors": [
                    {
                        "name": "Alessandro Crimi"
                    },
                    {
                        "name": "Andrea Brovelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Brovelli"
                },
                "author": "Andrea Brovelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12497v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12497v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03561v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03561v2",
                "updated": "2025-09-17T10:07:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    10,
                    7,
                    17,
                    2,
                    260,
                    0
                ],
                "published": "2025-07-04T13:12:36Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    13,
                    12,
                    36,
                    4,
                    185,
                    0
                ],
                "title": "Modelling delta Scuti pulsations: A new grid of p, g, and f modes across\n  pre-main-sequence to post-main-sequence evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling delta Scuti pulsations: A new grid of p, g, and f modes across\n  pre-main-sequence to post-main-sequence evolution"
                },
                "summary": "Space-based photometry from missions such as TESS has revealed that many\nyoung delta Scuti stars exhibit regular high-frequency pulsation patterns.\nThese pulsations provide a powerful means of inferring stellar properties,\nparticularly ages, for pre-main-sequence and early main-sequence delta Scuti\nstars, for which traditional age-dating methods are poorly constrained.\nRealising this potential requires robust theoretical models that capture the\ncomplexities of stellar structure and evolution. We present a comprehensive\ngrid of 25 million stellar pulsation models, computed using the MESA stellar\nevolution code and the GYRE oscillation code, tailored specifically to delta\nScuti stars. The grid spans a wide range of masses, metallicities, and surface\nrotation velocities, and covers evolutionary phases from the early\npre-main-sequence through the main sequence and into the post-main-sequence\ncontraction phase. For each model, we compute hundreds of adiabatic pulsation\nfrequencies for degrees l = 0-3, capturing p-modes, g-modes, f-modes, and their\ninteractions through avoided crossings. Our analysis maps the behaviour of\nasteroseismic observables, including the large frequency separation (Delta nu)\nand the phase offset parameter (epsilon), across age, mass, and rotation. We\ninvestigate how these parameters change with evolutionary stage and revisit the\nscaling relations applicable to delta Scuti stars. This new model grid, which\nis publicly available, is designed to support the asteroseismology community in\ninterpreting delta Scuti pulsations and in probing the evolution and internal\nstructure of these stars. These improvements over previous model grids will\nallow for reliable age estimates and stronger constraints on stellar evolution\npathways and the timing of planet formation across A- and F-type stellar\npopulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Space-based photometry from missions such as TESS has revealed that many\nyoung delta Scuti stars exhibit regular high-frequency pulsation patterns.\nThese pulsations provide a powerful means of inferring stellar properties,\nparticularly ages, for pre-main-sequence and early main-sequence delta Scuti\nstars, for which traditional age-dating methods are poorly constrained.\nRealising this potential requires robust theoretical models that capture the\ncomplexities of stellar structure and evolution. We present a comprehensive\ngrid of 25 million stellar pulsation models, computed using the MESA stellar\nevolution code and the GYRE oscillation code, tailored specifically to delta\nScuti stars. The grid spans a wide range of masses, metallicities, and surface\nrotation velocities, and covers evolutionary phases from the early\npre-main-sequence through the main sequence and into the post-main-sequence\ncontraction phase. For each model, we compute hundreds of adiabatic pulsation\nfrequencies for degrees l = 0-3, capturing p-modes, g-modes, f-modes, and their\ninteractions through avoided crossings. Our analysis maps the behaviour of\nasteroseismic observables, including the large frequency separation (Delta nu)\nand the phase offset parameter (epsilon), across age, mass, and rotation. We\ninvestigate how these parameters change with evolutionary stage and revisit the\nscaling relations applicable to delta Scuti stars. This new model grid, which\nis publicly available, is designed to support the asteroseismology community in\ninterpreting delta Scuti pulsations and in probing the evolution and internal\nstructure of these stars. These improvements over previous model grids will\nallow for reliable age estimates and stronger constraints on stellar evolution\npathways and the timing of planet formation across A- and F-type stellar\npopulations."
                },
                "authors": [
                    {
                        "name": "Anuj Gautam"
                    },
                    {
                        "name": "Simon J. Murphy"
                    },
                    {
                        "name": "Timothy R. Bedding"
                    }
                ],
                "author_detail": {
                    "name": "Timothy R. Bedding"
                },
                "author": "Timothy R. Bedding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03561v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03561v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13869v1",
                "updated": "2025-09-17T09:58:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    58,
                    28,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T09:58:28Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    58,
                    28,
                    2,
                    260,
                    0
                ],
                "title": "Do LLMs Align Human Values Regarding Social Biases? Judging and\n  Explaining Social Biases with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Align Human Values Regarding Social Biases? Judging and\n  Explaining Social Biases with LLMs"
                },
                "summary": "Large language models (LLMs) can lead to undesired consequences when\nmisaligned with human values, especially in scenarios involving complex and\nsensitive social biases. Previous studies have revealed the misalignment of\nLLMs with human values using expert-designed or agent-based emulated bias\nscenarios. However, it remains unclear whether the alignment of LLMs with human\nvalues differs across different types of scenarios (e.g., scenarios containing\nnegative vs. non-negative questions). In this study, we investigate the\nalignment of LLMs with human values regarding social biases (HVSB) in different\ntypes of bias scenarios. Through extensive analysis of 12 LLMs from four model\nfamilies and four datasets, we demonstrate that LLMs with large model parameter\nscales do not necessarily have lower misalignment rate and attack success rate.\nMoreover, LLMs show a certain degree of alignment preference for specific types\nof scenarios and the LLMs from the same model family tend to have higher\njudgment consistency. In addition, we study the understanding capacity of LLMs\nwith their explanations of HVSB. We find no significant differences in the\nunderstanding of HVSB across LLMs. We also find LLMs prefer their own generated\nexplanations. Additionally, we endow smaller language models (LMs) with the\nability to explain HVSB. The generation results show that the explanations\ngenerated by the fine-tuned smaller LMs are more readable, but have a\nrelatively lower model agreeability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can lead to undesired consequences when\nmisaligned with human values, especially in scenarios involving complex and\nsensitive social biases. Previous studies have revealed the misalignment of\nLLMs with human values using expert-designed or agent-based emulated bias\nscenarios. However, it remains unclear whether the alignment of LLMs with human\nvalues differs across different types of scenarios (e.g., scenarios containing\nnegative vs. non-negative questions). In this study, we investigate the\nalignment of LLMs with human values regarding social biases (HVSB) in different\ntypes of bias scenarios. Through extensive analysis of 12 LLMs from four model\nfamilies and four datasets, we demonstrate that LLMs with large model parameter\nscales do not necessarily have lower misalignment rate and attack success rate.\nMoreover, LLMs show a certain degree of alignment preference for specific types\nof scenarios and the LLMs from the same model family tend to have higher\njudgment consistency. In addition, we study the understanding capacity of LLMs\nwith their explanations of HVSB. We find no significant differences in the\nunderstanding of HVSB across LLMs. We also find LLMs prefer their own generated\nexplanations. Additionally, we endow smaller language models (LMs) with the\nability to explain HVSB. The generation results show that the explanations\ngenerated by the fine-tuned smaller LMs are more readable, but have a\nrelatively lower model agreeability."
                },
                "authors": [
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Chenhui Chu"
                    }
                ],
                "author_detail": {
                    "name": "Chenhui Chu"
                },
                "author": "Chenhui Chu",
                "arxiv_comment": "38 pages, 31 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13868v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13868v1",
                "updated": "2025-09-17T09:58:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    58,
                    26,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T09:58:26Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    58,
                    26,
                    2,
                    260,
                    0
                ],
                "title": "Are Prompts All You Need? Evaluating Prompt-Based Large Language Models\n  (LLM)s for Software Requirements Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Prompts All You Need? Evaluating Prompt-Based Large Language Models\n  (LLM)s for Software Requirements Classification"
                },
                "summary": "Requirements classification assigns natural language requirements to\npredefined classes, such as functional and non functional. Accurate\nclassification reduces risk and improves software quality. Most existing models\nrely on supervised learning, which needs large labeled data that are costly,\nslow to create, and domain dependent; they also generalize poorly and often\nrequire retraining for each task. This study tests whether prompt based large\nlanguage models can reduce data needs. We benchmark several models and\nprompting styles (zero shot, few shot, persona, and chain of thought) across\nmultiple tasks on two English datasets, PROMISE and SecReq. For each task we\ncompare model prompt configurations and then compare the best LLM setups with a\nstrong fine tuned transformer baseline. Results show that prompt based LLMs,\nespecially with few shot prompts, can match or exceed the baseline. Adding a\npersona, or persona plus chain of thought, can yield further gains. We conclude\nthat prompt based LLMs are a practical and scalable option that reduces\ndependence on large annotations and can improve generalizability across tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Requirements classification assigns natural language requirements to\npredefined classes, such as functional and non functional. Accurate\nclassification reduces risk and improves software quality. Most existing models\nrely on supervised learning, which needs large labeled data that are costly,\nslow to create, and domain dependent; they also generalize poorly and often\nrequire retraining for each task. This study tests whether prompt based large\nlanguage models can reduce data needs. We benchmark several models and\nprompting styles (zero shot, few shot, persona, and chain of thought) across\nmultiple tasks on two English datasets, PROMISE and SecReq. For each task we\ncompare model prompt configurations and then compare the best LLM setups with a\nstrong fine tuned transformer baseline. Results show that prompt based LLMs,\nespecially with few shot prompts, can match or exceed the baseline. Adding a\npersona, or persona plus chain of thought, can yield further gains. We conclude\nthat prompt based LLMs are a practical and scalable option that reduces\ndependence on large annotations and can improve generalizability across tasks."
                },
                "authors": [
                    {
                        "name": "Manal Binkhonain"
                    },
                    {
                        "name": "Reem Alfayaz"
                    }
                ],
                "author_detail": {
                    "name": "Reem Alfayaz"
                },
                "author": "Reem Alfayaz",
                "arxiv_doi": "10.1007/s00766-025-00451-8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s00766-025-00451-8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.13868v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13868v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "33 pages, 12 figures",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09532v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09532v2",
                "updated": "2025-09-17T09:49:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    49,
                    4,
                    2,
                    260,
                    0
                ],
                "published": "2025-04-13T11:37:32Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    11,
                    37,
                    32,
                    6,
                    103,
                    0
                ],
                "title": "Humanoid Agent via Embodied Chain-of-Action Reasoning with Multimodal\n  Foundation Models for Zero-Shot Loco-Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humanoid Agent via Embodied Chain-of-Action Reasoning with Multimodal\n  Foundation Models for Zero-Shot Loco-Manipulation"
                },
                "summary": "Humanoid loco-manipulation, which integrates whole-body locomotion with\ndexterous manipulation, remains a fundamental challenge in robotics. Beyond\nwhole-body coordination and balance, a central difficulty lies in understanding\nhuman instructions and translating them into coherent sequences of embodied\nactions. Recent advances in foundation models provide transferable multimodal\nrepresentations and reasoning capabilities, yet existing efforts remain largely\nrestricted to either locomotion or manipulation in isolation, with limited\napplicability to humanoid settings. In this paper, we propose Humanoid-COA, the\nfirst humanoid agent framework that integrates foundation model reasoning with\nan Embodied Chain-of-Action (CoA) mechanism for zero-shot loco-manipulation.\nWithin the perception--reasoning--action paradigm, our key contribution lies in\nthe reasoning stage, where the proposed CoA mechanism decomposes high-level\nhuman instructions into structured sequences of locomotion and manipulation\nprimitives through affordance analysis, spatial inference, and whole-body\naction reasoning. Extensive experiments on two humanoid robots, Unitree H1-2\nand G1, in both an open test area and an apartment environment, demonstrate\nthat our framework substantially outperforms prior baselines across\nmanipulation, locomotion, and loco-manipulation tasks, achieving robust\ngeneralization to long-horizon and unstructured scenarios. Project page:\nhttps://humanoid-coa.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humanoid loco-manipulation, which integrates whole-body locomotion with\ndexterous manipulation, remains a fundamental challenge in robotics. Beyond\nwhole-body coordination and balance, a central difficulty lies in understanding\nhuman instructions and translating them into coherent sequences of embodied\nactions. Recent advances in foundation models provide transferable multimodal\nrepresentations and reasoning capabilities, yet existing efforts remain largely\nrestricted to either locomotion or manipulation in isolation, with limited\napplicability to humanoid settings. In this paper, we propose Humanoid-COA, the\nfirst humanoid agent framework that integrates foundation model reasoning with\nan Embodied Chain-of-Action (CoA) mechanism for zero-shot loco-manipulation.\nWithin the perception--reasoning--action paradigm, our key contribution lies in\nthe reasoning stage, where the proposed CoA mechanism decomposes high-level\nhuman instructions into structured sequences of locomotion and manipulation\nprimitives through affordance analysis, spatial inference, and whole-body\naction reasoning. Extensive experiments on two humanoid robots, Unitree H1-2\nand G1, in both an open test area and an apartment environment, demonstrate\nthat our framework substantially outperforms prior baselines across\nmanipulation, locomotion, and loco-manipulation tasks, achieving robust\ngeneralization to long-horizon and unstructured scenarios. Project page:\nhttps://humanoid-coa.github.io/"
                },
                "authors": [
                    {
                        "name": "Congcong Wen"
                    },
                    {
                        "name": "Geeta Chandra Raju Bethala"
                    },
                    {
                        "name": "Yu Hao"
                    },
                    {
                        "name": "Niraj Pudasaini"
                    },
                    {
                        "name": "Hao Huang"
                    },
                    {
                        "name": "Shuaihang Yuan"
                    },
                    {
                        "name": "Baoru Huang"
                    },
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Anthony Tzes"
                    },
                    {
                        "name": "Yi Fang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Fang"
                },
                "author": "Yi Fang",
                "arxiv_comment": "website link: https://humanoid-coa.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09532v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09532v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13858v1",
                "updated": "2025-09-17T09:48:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    48,
                    39,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T09:48:39Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    48,
                    39,
                    2,
                    260,
                    0
                ],
                "title": "EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics"
                },
                "summary": "Dataset distillation aims to synthesize a compact dataset from the original\nlarge-scale one, enabling highly efficient learning while preserving\ncompetitive model performance. However, traditional techniques primarily\ncapture low-level visual features, neglecting the high-level semantic and\nstructural information inherent in images. In this paper, we propose EDITS, a\nnovel framework that exploits the implicit textual semantics within the image\ndata to achieve enhanced distillation. First, external texts generated by a\nVision Language Model (VLM) are fused with image features through a Global\nSemantic Query module, forming the prior clustered buffer. Local Semantic\nAwareness then selects representative samples from the buffer to construct\nimage and text prototypes, with the latter produced by guiding a Large Language\nModel (LLM) with meticulously crafted prompt. Ultimately, Dual Prototype\nGuidance strategy generates the final synthetic dataset through a diffusion\nmodel. Extensive experiments confirm the effectiveness of our method.Source\ncode is available in: https://github.com/einsteinxia/EDITS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dataset distillation aims to synthesize a compact dataset from the original\nlarge-scale one, enabling highly efficient learning while preserving\ncompetitive model performance. However, traditional techniques primarily\ncapture low-level visual features, neglecting the high-level semantic and\nstructural information inherent in images. In this paper, we propose EDITS, a\nnovel framework that exploits the implicit textual semantics within the image\ndata to achieve enhanced distillation. First, external texts generated by a\nVision Language Model (VLM) are fused with image features through a Global\nSemantic Query module, forming the prior clustered buffer. Local Semantic\nAwareness then selects representative samples from the buffer to construct\nimage and text prototypes, with the latter produced by guiding a Large Language\nModel (LLM) with meticulously crafted prompt. Ultimately, Dual Prototype\nGuidance strategy generates the final synthetic dataset through a diffusion\nmodel. Extensive experiments confirm the effectiveness of our method.Source\ncode is available in: https://github.com/einsteinxia/EDITS."
                },
                "authors": [
                    {
                        "name": "Qianxin Xia"
                    },
                    {
                        "name": "Jiawei Du"
                    },
                    {
                        "name": "Guoming Lu"
                    },
                    {
                        "name": "Zhiyong Shu"
                    },
                    {
                        "name": "Jielei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jielei Wang"
                },
                "author": "Jielei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21710v2",
                "updated": "2025-09-17T09:36:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    36,
                    5,
                    2,
                    260,
                    0
                ],
                "published": "2025-03-27T17:21:47Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    21,
                    47,
                    3,
                    86,
                    0
                ],
                "title": "KGCompass: Knowledge Graph Enhanced Repository-Level Software Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KGCompass: Knowledge Graph Enhanced Repository-Level Software Repair"
                },
                "summary": "Repository-level software repair faces challenges in bridging semantic gaps\nbetween issue descriptions and code patches. Existing approaches, which\nprimarily rely on large language models (LLMs), are hindered by semantic\nambiguities, limited understanding of structural context, and insufficient\nreasoning capabilities. To address these limitations, we propose KGCompass with\ntwo innovations: (1) a novel repository-aware knowledge graph (KG) that\naccurately links repository artifacts (issues and pull requests) and codebase\nentities (files, classes, and functions), allowing us to effectively narrow\ndown the vast search space to only 20 most relevant functions with accurate\ncandidate fault locations and contextual information, and (2) a path-guided\nrepair mechanism that leverages KG-mined entity paths, tracing through which\nallows us to augment LLMs with relevant contextual information to generate\nprecise patches along with their explanations. Experimental results in the\nSWE-bench Lite demonstrate that KGCompass achieves state-of-the-art single-LLM\nrepair performance (58.3%) and function-level fault location accuracy (56.0%)\nacross open-source approaches with a single repair model, costing only $0.2 per\nrepair. Among the bugs that KGCompass successfully localizes, 89.7% lack\nexplicit location hints in the issue and are found only through multi-hop graph\ntraversal, where pure LLMs struggle to locate bugs accurately. Relative to\npure-LLM baselines, KGCompass lifts the resolved rate by 50.8% on Claude-4\nSonnet, 30.2% on Claude-3.5 Sonnet, 115.7% on DeepSeek-V3, and 156.4% on\nQwen2.5 Max. These consistent improvements demonstrate that this graph-guided\nrepair framework delivers model-agnostic, cost-efficient repair and sets a\nstrong new baseline for repository-level repair.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository-level software repair faces challenges in bridging semantic gaps\nbetween issue descriptions and code patches. Existing approaches, which\nprimarily rely on large language models (LLMs), are hindered by semantic\nambiguities, limited understanding of structural context, and insufficient\nreasoning capabilities. To address these limitations, we propose KGCompass with\ntwo innovations: (1) a novel repository-aware knowledge graph (KG) that\naccurately links repository artifacts (issues and pull requests) and codebase\nentities (files, classes, and functions), allowing us to effectively narrow\ndown the vast search space to only 20 most relevant functions with accurate\ncandidate fault locations and contextual information, and (2) a path-guided\nrepair mechanism that leverages KG-mined entity paths, tracing through which\nallows us to augment LLMs with relevant contextual information to generate\nprecise patches along with their explanations. Experimental results in the\nSWE-bench Lite demonstrate that KGCompass achieves state-of-the-art single-LLM\nrepair performance (58.3%) and function-level fault location accuracy (56.0%)\nacross open-source approaches with a single repair model, costing only $0.2 per\nrepair. Among the bugs that KGCompass successfully localizes, 89.7% lack\nexplicit location hints in the issue and are found only through multi-hop graph\ntraversal, where pure LLMs struggle to locate bugs accurately. Relative to\npure-LLM baselines, KGCompass lifts the resolved rate by 50.8% on Claude-4\nSonnet, 30.2% on Claude-3.5 Sonnet, 115.7% on DeepSeek-V3, and 156.4% on\nQwen2.5 Max. These consistent improvements demonstrate that this graph-guided\nrepair framework delivers model-agnostic, cost-efficient repair and sets a\nstrong new baseline for repository-level repair."
                },
                "authors": [
                    {
                        "name": "Boyang Yang"
                    },
                    {
                        "name": "Jiadong Ren"
                    },
                    {
                        "name": "Shunfu Jin"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Bach Le"
                    },
                    {
                        "name": "Haoye Tian"
                    }
                ],
                "author_detail": {
                    "name": "Haoye Tian"
                },
                "author": "Haoye Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15214v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15214v2",
                "updated": "2025-09-17T09:34:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    34,
                    53,
                    2,
                    260,
                    0
                ],
                "published": "2025-08-21T03:56:38Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    56,
                    38,
                    3,
                    233,
                    0
                ],
                "title": "Self-Guided Function Calling in Large Language Models via Stepwise\n  Experience Recall",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Guided Function Calling in Large Language Models via Stepwise\n  Experience Recall"
                },
                "summary": "Function calling enables large language models (LLMs) to interact with\nexternal systems by leveraging tools and APIs. When faced with multi-step tool\nusage, LLMs still struggle with tool selection, parameter generation, and\ntool-chain planning. Existing methods typically rely on manually designing\ntask-specific demonstrations, or retrieving from a curated library. These\napproaches demand substantial expert effort and prompt engineering becomes\nincreasingly complex and inefficient as tool diversity and task difficulty\nscale. To address these challenges, we propose a self-guided method, Stepwise\nExperience Recall (SEER), which performs fine-grained, stepwise retrieval from\na continually updated experience pool. Instead of relying on static or manually\ncurated library, SEER incrementally augments the experience pool with past\nsuccessful trajectories, enabling continuous expansion of the pool and improved\nmodel performance over time. Evaluated on the ToolQA benchmark, SEER achieves\nan average improvement of 6.1% on easy and 4.7% on hard questions. We further\ntest SEER on $\\tau$-bench, which includes two real-world domains. Powered by\nQwen2.5-7B and Qwen2.5-72B models, SEER demonstrates substantial accuracy gains\nof 7.44% and 23.38%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Function calling enables large language models (LLMs) to interact with\nexternal systems by leveraging tools and APIs. When faced with multi-step tool\nusage, LLMs still struggle with tool selection, parameter generation, and\ntool-chain planning. Existing methods typically rely on manually designing\ntask-specific demonstrations, or retrieving from a curated library. These\napproaches demand substantial expert effort and prompt engineering becomes\nincreasingly complex and inefficient as tool diversity and task difficulty\nscale. To address these challenges, we propose a self-guided method, Stepwise\nExperience Recall (SEER), which performs fine-grained, stepwise retrieval from\na continually updated experience pool. Instead of relying on static or manually\ncurated library, SEER incrementally augments the experience pool with past\nsuccessful trajectories, enabling continuous expansion of the pool and improved\nmodel performance over time. Evaluated on the ToolQA benchmark, SEER achieves\nan average improvement of 6.1% on easy and 4.7% on hard questions. We further\ntest SEER on $\\tau$-bench, which includes two real-world domains. Powered by\nQwen2.5-7B and Qwen2.5-72B models, SEER demonstrates substantial accuracy gains\nof 7.44% and 23.38%, respectively."
                },
                "authors": [
                    {
                        "name": "Sijia Cui"
                    },
                    {
                        "name": "Aiyao He"
                    },
                    {
                        "name": "Shuai Xu"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Yanna Wang"
                    },
                    {
                        "name": "Qingyang Zhang"
                    },
                    {
                        "name": "Yajing Wang"
                    },
                    {
                        "name": "Bo Xu"
                    }
                ],
                "author_detail": {
                    "name": "Bo Xu"
                },
                "author": "Bo Xu",
                "arxiv_comment": "Accepted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15214v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15214v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20923v2",
                "updated": "2025-09-17T09:33:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    33,
                    21,
                    2,
                    260,
                    0
                ],
                "published": "2025-07-28T15:26:43Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    15,
                    26,
                    43,
                    0,
                    209,
                    0
                ],
                "title": "Pareto-Grid-Guided Large Language Models for Fast and High-Quality\n  Heuristics Design in Multi-Objective Combinatorial Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pareto-Grid-Guided Large Language Models for Fast and High-Quality\n  Heuristics Design in Multi-Objective Combinatorial Optimization"
                },
                "summary": "Multi-objective combinatorial optimization problems (MOCOP) frequently arise\nin practical applications that require the simultaneous optimization of\nconflicting objectives. Although traditional evolutionary algorithms can be\neffective, they typically depend on domain knowledge and repeated parameter\ntuning, limiting flexibility when applied to unseen MOCOP instances. Recently,\nintegration of Large Language Models (LLMs) into evolutionary computation has\nopened new avenues for automatic heuristic generation, using their advanced\nlanguage understanding and code synthesis capabilities. Nevertheless, most\nexisting approaches predominantly focus on single-objective tasks, often\nneglecting key considerations such as runtime efficiency and heuristic\ndiversity in multi-objective settings. To bridge this gap, we introduce\nMulti-heuristics for MOCOP via Pareto-Grid-guided Evolution of LLMs (MPaGE), a\nnovel enhancement of the Simple Evolutionary Multiobjective Optimization (SEMO)\nframework that leverages LLMs and Pareto Front Grid (PFG) technique. By\npartitioning the objective space into grids and retaining top-performing\ncandidates to guide heuristic generation, MPaGE utilizes LLMs to prioritize\nheuristics with semantically distinct logical structures during variation, thus\npromoting diversity and mitigating redundancy within the population. Through\nextensive evaluations, MPaGE demonstrates superior performance over existing\nLLM-based frameworks, and achieves competitive results to traditional\nMulti-objective evolutionary algorithms (MOEAs), with significantly faster\nruntime. Our code is available at: https://github.com/langkhachhoha/MPaGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-objective combinatorial optimization problems (MOCOP) frequently arise\nin practical applications that require the simultaneous optimization of\nconflicting objectives. Although traditional evolutionary algorithms can be\neffective, they typically depend on domain knowledge and repeated parameter\ntuning, limiting flexibility when applied to unseen MOCOP instances. Recently,\nintegration of Large Language Models (LLMs) into evolutionary computation has\nopened new avenues for automatic heuristic generation, using their advanced\nlanguage understanding and code synthesis capabilities. Nevertheless, most\nexisting approaches predominantly focus on single-objective tasks, often\nneglecting key considerations such as runtime efficiency and heuristic\ndiversity in multi-objective settings. To bridge this gap, we introduce\nMulti-heuristics for MOCOP via Pareto-Grid-guided Evolution of LLMs (MPaGE), a\nnovel enhancement of the Simple Evolutionary Multiobjective Optimization (SEMO)\nframework that leverages LLMs and Pareto Front Grid (PFG) technique. By\npartitioning the objective space into grids and retaining top-performing\ncandidates to guide heuristic generation, MPaGE utilizes LLMs to prioritize\nheuristics with semantically distinct logical structures during variation, thus\npromoting diversity and mitigating redundancy within the population. Through\nextensive evaluations, MPaGE demonstrates superior performance over existing\nLLM-based frameworks, and achieves competitive results to traditional\nMulti-objective evolutionary algorithms (MOEAs), with significantly faster\nruntime. Our code is available at: https://github.com/langkhachhoha/MPaGE."
                },
                "authors": [
                    {
                        "name": "Minh Hieu Ha"
                    },
                    {
                        "name": "Hung Phan"
                    },
                    {
                        "name": "Tung Duy Doan"
                    },
                    {
                        "name": "Tung Dao"
                    },
                    {
                        "name": "Dao Tran"
                    },
                    {
                        "name": "Huynh Thi Thanh Binh"
                    }
                ],
                "author_detail": {
                    "name": "Huynh Thi Thanh Binh"
                },
                "author": "Huynh Thi Thanh Binh",
                "arxiv_comment": "36 pages, 20 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10408v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10408v3",
                "updated": "2025-09-17T09:28:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    28,
                    46,
                    2,
                    260,
                    0
                ],
                "published": "2025-03-13T14:32:30Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    14,
                    32,
                    30,
                    3,
                    72,
                    0
                ],
                "title": "Out-of-Context Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-Context Reasoning in Large Language Models"
                },
                "summary": "We study how large language models (LLMs) reason about memorized knowledge\nthrough simple binary relations such as equality ($=$), inequality ($<$), and\ninclusion ($\\subset$). Unlike in-context reasoning, the axioms (e.g., $a < b, b\n< c$) are only seen during training and not provided in the task prompt (e.g.,\nevaluating $a < c$). The tasks require one or more reasoning steps, and data\naggregation from one or more sources, showing performance change with task\ncomplexity. We introduce a lightweight technique, out-of-context representation\nlearning, which trains only new token embeddings on axioms and evaluates them\non unseen tasks. Across reflexivity, symmetry, and transitivity tests, LLMs\nmostly perform statistically significant better than chance, making the correct\nanswer extractable when testing multiple phrasing variations, but still fall\nshort of consistent reasoning on every single query. Analysis shows that the\nlearned embeddings are organized in structured ways, suggesting real relational\nunderstanding. Surprisingly, it also indicates that the core reasoning happens\nduring the training, not inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study how large language models (LLMs) reason about memorized knowledge\nthrough simple binary relations such as equality ($=$), inequality ($<$), and\ninclusion ($\\subset$). Unlike in-context reasoning, the axioms (e.g., $a < b, b\n< c$) are only seen during training and not provided in the task prompt (e.g.,\nevaluating $a < c$). The tasks require one or more reasoning steps, and data\naggregation from one or more sources, showing performance change with task\ncomplexity. We introduce a lightweight technique, out-of-context representation\nlearning, which trains only new token embeddings on axioms and evaluates them\non unseen tasks. Across reflexivity, symmetry, and transitivity tests, LLMs\nmostly perform statistically significant better than chance, making the correct\nanswer extractable when testing multiple phrasing variations, but still fall\nshort of consistent reasoning on every single query. Analysis shows that the\nlearned embeddings are organized in structured ways, suggesting real relational\nunderstanding. Surprisingly, it also indicates that the core reasoning happens\nduring the training, not inference."
                },
                "authors": [
                    {
                        "name": "Jonathan Shaki"
                    },
                    {
                        "name": "Emanuele La Malfa"
                    },
                    {
                        "name": "Michael Wooldridge"
                    },
                    {
                        "name": "Sarit Kraus"
                    }
                ],
                "author_detail": {
                    "name": "Sarit Kraus"
                },
                "author": "Sarit Kraus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10408v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10408v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08321v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08321v3",
                "updated": "2025-09-17T09:25:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    25,
                    53,
                    2,
                    260,
                    0
                ],
                "published": "2025-01-14T18:55:08Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    55,
                    8,
                    1,
                    14,
                    0
                ],
                "title": "Computation of FCC-ee Sensitivity to Heavy New Physics with Interactions\n  of Any Flavor Structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computation of FCC-ee Sensitivity to Heavy New Physics with Interactions\n  of Any Flavor Structure"
                },
                "summary": "We present a tool to compute the sensitivity of the Future Circular\nElectron--Positron Collider (FCC-ee) to the interactions of new, heavy\nparticles via publicly available extensions to the smelli and flavio computer\nprograms. We parameterize new particles' effects without any flavor assumptions\nand take into account the projected experimental and correlated theoretical\nuncertainties of various electroweak and Higgs observables at the proposed\ncollider. We illustrate a use of the tool by estimating the sensitivity of the\nFCC-ee to a $Z^\\prime$ model with flavor-specific couplings which explains\nanomalies inferred from present-day measurements and Standard Model predictions\nof observables that involve the $b \\rightarrow s \\ell^+ \\ell^-$ transition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a tool to compute the sensitivity of the Future Circular\nElectron--Positron Collider (FCC-ee) to the interactions of new, heavy\nparticles via publicly available extensions to the smelli and flavio computer\nprograms. We parameterize new particles' effects without any flavor assumptions\nand take into account the projected experimental and correlated theoretical\nuncertainties of various electroweak and Higgs observables at the proposed\ncollider. We illustrate a use of the tool by estimating the sensitivity of the\nFCC-ee to a $Z^\\prime$ model with flavor-specific couplings which explains\nanomalies inferred from present-day measurements and Standard Model predictions\nof observables that involve the $b \\rightarrow s \\ell^+ \\ell^-$ transition."
                },
                "authors": [
                    {
                        "name": "Ben Allanach"
                    },
                    {
                        "name": "Eetu Loisa"
                    }
                ],
                "author_detail": {
                    "name": "Eetu Loisa"
                },
                "author": "Eetu Loisa",
                "arxiv_comment": "7 pages, 1 figure. To access the new tools, see\n  https://github.com/eetuloisa/smelli_fcc and\n  https://github.com/eetuloisa/flavio_fcc v2 has added observables and a\n  considerable rewrite of the text in order to change the focus, v3 has added\n  material about the FCC-ee luminosity measurement",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08321v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08321v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13848v1",
                "updated": "2025-09-17T09:24:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    24,
                    40,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T09:24:40Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    24,
                    40,
                    2,
                    260,
                    0
                ],
                "title": "SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation"
                },
                "summary": "Feature caching has recently emerged as a promising method for diffusion\nmodel acceleration. It effectively alleviates the inefficiency problem caused\nby high computational requirements by caching similar features in the inference\nprocess of the diffusion model. In this paper, we analyze existing feature\ncaching methods from the perspective of information utilization, and point out\nthat relying solely on historical information will lead to constrained accuracy\nand speed performance. And we propose a novel paradigm that introduces future\ninformation via self-speculation based on the information similarity at the\nsame time step across different iteration times. Based on this paradigm, we\npresent \\textit{SpecDiff}, a training-free multi-level feature caching strategy\nincluding a cached feature selection algorithm and a multi-level feature\nclassification algorithm. (1) Feature selection algorithm based on\nself-speculative information. \\textit{SpecDiff} determines a dynamic importance\nscore for each token based on self-speculative information and historical\ninformation, and performs cached feature selection through the importance\nscore. (2) Multi-level feature classification algorithm based on feature\nimportance scores. \\textit{SpecDiff} classifies tokens by leveraging the\ndifferences in feature importance scores and introduces a multi-level feature\ncalculation strategy. Extensive experiments show that \\textit{SpecDiff}\nachieves average 2.80 \\times, 2.74 \\times , and 3.17\\times speedup with\nnegligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow\non NVIDIA A800-80GB GPU. By merging speculative and historical information,\n\\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing\nthe Pareto frontier of speedup and accuracy in the efficient diffusion model\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching has recently emerged as a promising method for diffusion\nmodel acceleration. It effectively alleviates the inefficiency problem caused\nby high computational requirements by caching similar features in the inference\nprocess of the diffusion model. In this paper, we analyze existing feature\ncaching methods from the perspective of information utilization, and point out\nthat relying solely on historical information will lead to constrained accuracy\nand speed performance. And we propose a novel paradigm that introduces future\ninformation via self-speculation based on the information similarity at the\nsame time step across different iteration times. Based on this paradigm, we\npresent \\textit{SpecDiff}, a training-free multi-level feature caching strategy\nincluding a cached feature selection algorithm and a multi-level feature\nclassification algorithm. (1) Feature selection algorithm based on\nself-speculative information. \\textit{SpecDiff} determines a dynamic importance\nscore for each token based on self-speculative information and historical\ninformation, and performs cached feature selection through the importance\nscore. (2) Multi-level feature classification algorithm based on feature\nimportance scores. \\textit{SpecDiff} classifies tokens by leveraging the\ndifferences in feature importance scores and introduces a multi-level feature\ncalculation strategy. Extensive experiments show that \\textit{SpecDiff}\nachieves average 2.80 \\times, 2.74 \\times , and 3.17\\times speedup with\nnegligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow\non NVIDIA A800-80GB GPU. By merging speculative and historical information,\n\\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing\nthe Pareto frontier of speedup and accuracy in the efficient diffusion model\ninference."
                },
                "authors": [
                    {
                        "name": "Jiayi Pan"
                    },
                    {
                        "name": "Jiaming Xu"
                    },
                    {
                        "name": "Yongkang Zhou"
                    },
                    {
                        "name": "Guohao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Guohao Dai"
                },
                "author": "Guohao Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13842v1",
                "updated": "2025-09-17T09:17:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    17,
                    42,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T09:17:42Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    17,
                    42,
                    2,
                    260,
                    0
                ],
                "title": "Simulation-based Inference of Massive Black Hole Binaries using\n  Sequential Neural Likelihood",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based Inference of Massive Black Hole Binaries using\n  Sequential Neural Likelihood"
                },
                "summary": "We propose a machine learning-based approach for parameter estimation of\nMassive Black Hole Binaries (MBHBs), leveraging normalizing flows to\napproximate the likelihood function. By training these flows on simulated data,\nwe can generate posterior samples via Markov Chain Monte Carlo with a\nrelatively reduced computational cost. Our method enables iterative refinement\nof smaller models targeting specific MBHB events, with significantly fewer\nwaveform template evaluations. However, dimensionality reduction is crucial to\nmake the method computationally feasible: it dictates both the quality and time\nefficiency of the method. We present initial results for a single MBHB with\nGaussian noise and aim to extend our work to increasingly realistic scenarios,\nincluding waveforms with higher modes, non-stationary noise, glitches, and data\ngaps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a machine learning-based approach for parameter estimation of\nMassive Black Hole Binaries (MBHBs), leveraging normalizing flows to\napproximate the likelihood function. By training these flows on simulated data,\nwe can generate posterior samples via Markov Chain Monte Carlo with a\nrelatively reduced computational cost. Our method enables iterative refinement\nof smaller models targeting specific MBHB events, with significantly fewer\nwaveform template evaluations. However, dimensionality reduction is crucial to\nmake the method computationally feasible: it dictates both the quality and time\nefficiency of the method. We present initial results for a single MBHB with\nGaussian noise and aim to extend our work to increasingly realistic scenarios,\nincluding waveforms with higher modes, non-stationary noise, glitches, and data\ngaps."
                },
                "authors": [
                    {
                        "name": "Iván Martín Vílchez"
                    },
                    {
                        "name": "Carlos F. Sopuerta"
                    }
                ],
                "author_detail": {
                    "name": "Carlos F. Sopuerta"
                },
                "author": "Carlos F. Sopuerta",
                "arxiv_comment": "Submitted to the Proceedings of the 24th International Conference on\n  General Relativity and Gravitation & 16th Edoardo Amaldi Conference on\n  Gravitational Waves. 5 pages, 1 figure, 3 plots",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.14234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14234v1",
                "updated": "2025-09-17T17:59:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    59,
                    42,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T17:59:42Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    59,
                    42,
                    2,
                    260,
                    0
                ],
                "title": "Compute as Teacher: Turning Inference Compute Into Reference-Free\n  Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute as Teacher: Turning Inference Compute Into Reference-Free\n  Supervision"
                },
                "summary": "Where do learning signals come from when there is no ground truth in\npost-training? We propose turning exploration into supervision through Compute\nas Teacher (CaT), which converts the model's own exploration at inference-time\ninto reference-free supervision by synthesizing a single reference from a group\nof parallel rollouts and then optimizing toward it. Concretely, the current\npolicy produces a group of rollouts; a frozen anchor (the initial policy)\nreconciles omissions and contradictions to estimate a reference, turning extra\ninference-time compute into a teacher signal. We turn this into rewards in two\nregimes: (i) verifiable tasks use programmatic equivalence on final answers;\n(ii) non-verifiable tasks use self-proposed rubrics-binary, auditable criteria\nscored by an independent LLM judge, with reward given by the fraction\nsatisfied. Unlike selection methods (best-of-N, majority, perplexity, or judge\nscores), synthesis may disagree with the majority and be correct even when all\nrollouts are wrong; performance scales with the number of rollouts. As a\ntest-time procedure, CaT improves Gemma 3 4B, Qwen 3 4B, and Llama 3.1 8B (up\nto +27% on MATH-500; +12% on HealthBench). With reinforcement learning\n(CaT-RL), we obtain further gains (up to +33% and +30%), with the trained\npolicy surpassing the initial teacher signal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where do learning signals come from when there is no ground truth in\npost-training? We propose turning exploration into supervision through Compute\nas Teacher (CaT), which converts the model's own exploration at inference-time\ninto reference-free supervision by synthesizing a single reference from a group\nof parallel rollouts and then optimizing toward it. Concretely, the current\npolicy produces a group of rollouts; a frozen anchor (the initial policy)\nreconciles omissions and contradictions to estimate a reference, turning extra\ninference-time compute into a teacher signal. We turn this into rewards in two\nregimes: (i) verifiable tasks use programmatic equivalence on final answers;\n(ii) non-verifiable tasks use self-proposed rubrics-binary, auditable criteria\nscored by an independent LLM judge, with reward given by the fraction\nsatisfied. Unlike selection methods (best-of-N, majority, perplexity, or judge\nscores), synthesis may disagree with the majority and be correct even when all\nrollouts are wrong; performance scales with the number of rollouts. As a\ntest-time procedure, CaT improves Gemma 3 4B, Qwen 3 4B, and Llama 3.1 8B (up\nto +27% on MATH-500; +12% on HealthBench). With reinforcement learning\n(CaT-RL), we obtain further gains (up to +33% and +30%), with the trained\npolicy surpassing the initial teacher signal."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "Shashwat Goel"
                    },
                    {
                        "name": "Thomas Foster"
                    },
                    {
                        "name": "Parag Jain"
                    },
                    {
                        "name": "Suchin Gururangan"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Anirudh Goyal"
                    },
                    {
                        "name": "Alan Schelten"
                    }
                ],
                "author_detail": {
                    "name": "Alan Schelten"
                },
                "author": "Alan Schelten",
                "arxiv_comment": "22 pages, 8 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14233v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14233v1",
                "updated": "2025-09-17T17:59:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    59,
                    21,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T17:59:21Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    59,
                    21,
                    2,
                    260,
                    0
                ],
                "title": "Apertus: Democratizing Open and Compliant LLMs for Global Language\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apertus: Democratizing Open and Compliant LLMs for Global Language\n  Environments"
                },
                "summary": "We present Apertus, a fully open suite of large language models (LLMs)\ndesigned to address two systemic shortcomings in today's open model ecosystem:\ndata compliance and multilingual representation. Unlike many prior models that\nrelease weights without reproducible data pipelines or regard for content-owner\nrights, Apertus models are pretrained exclusively on openly available data,\nretroactively respecting robots.txt exclusions and filtering for\nnon-permissive, toxic, and personally identifiable content. To mitigate risks\nof memorization, we adopt the Goldfish objective during pretraining, strongly\nsuppressing verbatim recall of data while retaining downstream task\nperformance. The Apertus models also expand multilingual coverage, training on\n15T tokens from over 1800 languages, with ~40% of pretraining data allocated to\nnon-English content. Released at 8B and 70B scales, Apertus approaches\nstate-of-the-art results among fully open models on multilingual benchmarks,\nrivalling or surpassing open-weight counterparts. Beyond model weights, we\nrelease all scientific artifacts from our development cycle with a permissive\nlicense, including data preparation scripts, checkpoints, evaluation suites,\nand training code, enabling transparent audit and extension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Apertus, a fully open suite of large language models (LLMs)\ndesigned to address two systemic shortcomings in today's open model ecosystem:\ndata compliance and multilingual representation. Unlike many prior models that\nrelease weights without reproducible data pipelines or regard for content-owner\nrights, Apertus models are pretrained exclusively on openly available data,\nretroactively respecting robots.txt exclusions and filtering for\nnon-permissive, toxic, and personally identifiable content. To mitigate risks\nof memorization, we adopt the Goldfish objective during pretraining, strongly\nsuppressing verbatim recall of data while retaining downstream task\nperformance. The Apertus models also expand multilingual coverage, training on\n15T tokens from over 1800 languages, with ~40% of pretraining data allocated to\nnon-English content. Released at 8B and 70B scales, Apertus approaches\nstate-of-the-art results among fully open models on multilingual benchmarks,\nrivalling or surpassing open-weight counterparts. Beyond model weights, we\nrelease all scientific artifacts from our development cycle with a permissive\nlicense, including data preparation scripts, checkpoints, evaluation suites,\nand training code, enabling transparent audit and extension."
                },
                "authors": [
                    {
                        "name": "Alejandro Hernández-Cano"
                    },
                    {
                        "name": "Alexander Hägele"
                    },
                    {
                        "name": "Allen Hao Huang"
                    },
                    {
                        "name": "Angelika Romanou"
                    },
                    {
                        "name": "Antoni-Joan Solergibert"
                    },
                    {
                        "name": "Barna Pasztor"
                    },
                    {
                        "name": "Bettina Messmer"
                    },
                    {
                        "name": "Dhia Garbaya"
                    },
                    {
                        "name": "Eduard Frank Ďurech"
                    },
                    {
                        "name": "Ido Hakimi"
                    },
                    {
                        "name": "Juan García Giraldo"
                    },
                    {
                        "name": "Mete Ismayilzada"
                    },
                    {
                        "name": "Negar Foroutan"
                    },
                    {
                        "name": "Skander Moalla"
                    },
                    {
                        "name": "Tiancheng Chen"
                    },
                    {
                        "name": "Vinko Sabolčec"
                    },
                    {
                        "name": "Yixuan Xu"
                    },
                    {
                        "name": "Michael Aerni"
                    },
                    {
                        "name": "Badr AlKhamissi"
                    },
                    {
                        "name": "Ines Altemir Marinas"
                    },
                    {
                        "name": "Mohammad Hossein Amani"
                    },
                    {
                        "name": "Matin Ansaripour"
                    },
                    {
                        "name": "Ilia Badanin"
                    },
                    {
                        "name": "Harold Benoit"
                    },
                    {
                        "name": "Emanuela Boros"
                    },
                    {
                        "name": "Nicholas Browning"
                    },
                    {
                        "name": "Fabian Bösch"
                    },
                    {
                        "name": "Maximilian Böther"
                    },
                    {
                        "name": "Niklas Canova"
                    },
                    {
                        "name": "Camille Challier"
                    },
                    {
                        "name": "Clement Charmillot"
                    },
                    {
                        "name": "Jonathan Coles"
                    },
                    {
                        "name": "Jan Deriu"
                    },
                    {
                        "name": "Arnout Devos"
                    },
                    {
                        "name": "Lukas Drescher"
                    },
                    {
                        "name": "Daniil Dzenhaliou"
                    },
                    {
                        "name": "Maud Ehrmann"
                    },
                    {
                        "name": "Dongyang Fan"
                    },
                    {
                        "name": "Simin Fan"
                    },
                    {
                        "name": "Silin Gao"
                    },
                    {
                        "name": "Miguel Gila"
                    },
                    {
                        "name": "María Grandury"
                    },
                    {
                        "name": "Diba Hashemi"
                    },
                    {
                        "name": "Alexander Hoyle"
                    },
                    {
                        "name": "Jiaming Jiang"
                    },
                    {
                        "name": "Mark Klein"
                    },
                    {
                        "name": "Andrei Kucharavy"
                    },
                    {
                        "name": "Anastasiia Kucherenko"
                    },
                    {
                        "name": "Frederike Lübeck"
                    },
                    {
                        "name": "Roman Machacek"
                    },
                    {
                        "name": "Theofilos Manitaras"
                    },
                    {
                        "name": "Andreas Marfurt"
                    },
                    {
                        "name": "Kyle Matoba"
                    },
                    {
                        "name": "Simon Matrenok"
                    },
                    {
                        "name": "Henrique Mendoncça"
                    },
                    {
                        "name": "Fawzi Roberto Mohamed"
                    },
                    {
                        "name": "Syrielle Montariol"
                    },
                    {
                        "name": "Luca Mouchel"
                    },
                    {
                        "name": "Sven Najem-Meyer"
                    },
                    {
                        "name": "Jingwei Ni"
                    },
                    {
                        "name": "Gennaro Oliva"
                    },
                    {
                        "name": "Matteo Pagliardini"
                    },
                    {
                        "name": "Elia Palme"
                    },
                    {
                        "name": "Andrei Panferov"
                    },
                    {
                        "name": "Léo Paoletti"
                    },
                    {
                        "name": "Marco Passerini"
                    },
                    {
                        "name": "Ivan Pavlov"
                    },
                    {
                        "name": "Auguste Poiroux"
                    },
                    {
                        "name": "Kaustubh Ponkshe"
                    },
                    {
                        "name": "Nathan Ranchin"
                    },
                    {
                        "name": "Javi Rando"
                    },
                    {
                        "name": "Mathieu Sauser"
                    },
                    {
                        "name": "Jakhongir Saydaliev"
                    },
                    {
                        "name": "Muhammad Ali Sayfiddinov"
                    },
                    {
                        "name": "Marian Schneider"
                    },
                    {
                        "name": "Stefano Schuppli"
                    },
                    {
                        "name": "Marco Scialanga"
                    },
                    {
                        "name": "Andrei Semenov"
                    },
                    {
                        "name": "Kumar Shridhar"
                    },
                    {
                        "name": "Raghav Singhal"
                    },
                    {
                        "name": "Anna Sotnikova"
                    },
                    {
                        "name": "Alexander Sternfeld"
                    },
                    {
                        "name": "Ayush Kumar Tarun"
                    },
                    {
                        "name": "Paul Teiletche"
                    },
                    {
                        "name": "Jannis Vamvas"
                    },
                    {
                        "name": "Xiaozhe Yao"
                    },
                    {
                        "name": "Hao Zhao Alexander Ilic"
                    },
                    {
                        "name": "Ana Klimovic"
                    },
                    {
                        "name": "Andreas Krause"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    },
                    {
                        "name": "David Rosenthal"
                    },
                    {
                        "name": "Elliott Ash"
                    },
                    {
                        "name": "Florian Tramèr"
                    },
                    {
                        "name": "Joost VandeVondele"
                    },
                    {
                        "name": "Livio Veraldi"
                    },
                    {
                        "name": "Martin Rajman"
                    },
                    {
                        "name": "Thomas Schulthess"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "Antoine Bosselut"
                    },
                    {
                        "name": "Martin Jaggi"
                    },
                    {
                        "name": "Imanol Schlag"
                    }
                ],
                "author_detail": {
                    "name": "Imanol Schlag"
                },
                "author": "Imanol Schlag",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14233v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14230v1",
                "updated": "2025-09-17T17:59:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    59,
                    0,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T17:59:00Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    59,
                    0,
                    2,
                    260,
                    0
                ],
                "title": "NIRVANA: Structured pruning reimagined for large language models\n  compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NIRVANA: Structured pruning reimagined for large language models\n  compression"
                },
                "summary": "Structured pruning of large language models (LLMs) offers substantial\nefficiency improvements by removing entire hidden units, yet current approaches\noften suffer from significant performance degradation, particularly in\nzero-shot settings, and necessitate costly recovery techniques such as\nsupervised fine-tuning (SFT) or adapter insertion. To address these critical\nshortcomings, we introduce NIRVANA, a novel pruning method explicitly designed\nto balance immediate zero-shot accuracy preservation with robust fine-tuning\ncapability. Leveraging a first-order saliency criterion derived from the Neural\nTangent Kernel under Adam optimization dynamics, NIRVANA provides a\ntheoretically grounded pruning strategy that respects essential model training\nbehaviors. To further address the unique challenges posed by structured\npruning, NIRVANA incorporates an adaptive sparsity allocation mechanism across\nlayers and modules (attention vs. MLP), which adjusts pruning intensity between\nmodules in a globally balanced manner. Additionally, to mitigate the high\nsensitivity of pruning decisions to calibration data quality, we propose a\nsimple yet effective KL divergence-based calibration data selection strategy,\nensuring more reliable and task-agnostic pruning outcomes. Comprehensive\nexperiments conducted on Llama3, Qwen, and T5 models demonstrate that NIRVANA\noutperforms existing structured pruning methods under equivalent sparsity\nconstraints, providing a theoretically sound and practical approach to LLM\ncompression. The code is available at\nhttps://github.com/iDEA-iSAIL-Lab-UIUC/NIRVANA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured pruning of large language models (LLMs) offers substantial\nefficiency improvements by removing entire hidden units, yet current approaches\noften suffer from significant performance degradation, particularly in\nzero-shot settings, and necessitate costly recovery techniques such as\nsupervised fine-tuning (SFT) or adapter insertion. To address these critical\nshortcomings, we introduce NIRVANA, a novel pruning method explicitly designed\nto balance immediate zero-shot accuracy preservation with robust fine-tuning\ncapability. Leveraging a first-order saliency criterion derived from the Neural\nTangent Kernel under Adam optimization dynamics, NIRVANA provides a\ntheoretically grounded pruning strategy that respects essential model training\nbehaviors. To further address the unique challenges posed by structured\npruning, NIRVANA incorporates an adaptive sparsity allocation mechanism across\nlayers and modules (attention vs. MLP), which adjusts pruning intensity between\nmodules in a globally balanced manner. Additionally, to mitigate the high\nsensitivity of pruning decisions to calibration data quality, we propose a\nsimple yet effective KL divergence-based calibration data selection strategy,\nensuring more reliable and task-agnostic pruning outcomes. Comprehensive\nexperiments conducted on Llama3, Qwen, and T5 models demonstrate that NIRVANA\noutperforms existing structured pruning methods under equivalent sparsity\nconstraints, providing a theoretically sound and practical approach to LLM\ncompression. The code is available at\nhttps://github.com/iDEA-iSAIL-Lab-UIUC/NIRVANA."
                },
                "authors": [
                    {
                        "name": "Mengting Ai"
                    },
                    {
                        "name": "Tianxin Wei"
                    },
                    {
                        "name": "Sirui Chen"
                    },
                    {
                        "name": "Jingrui He"
                    }
                ],
                "author_detail": {
                    "name": "Jingrui He"
                },
                "author": "Jingrui He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18614v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18614v4",
                "updated": "2025-09-18T08:19:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    8,
                    19,
                    20,
                    3,
                    261,
                    0
                ],
                "published": "2025-05-24T09:28:09Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    9,
                    28,
                    9,
                    5,
                    144,
                    0
                ],
                "title": "MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song\n  Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song\n  Translation"
                },
                "summary": "Lyrics translation requires both accurate semantic transfer and preservation\nof musical rhythm, syllabic structure, and poetic style. In animated musicals,\nthe challenge intensifies due to alignment with visual and auditory cues. We\nintroduce Multilingual Audio-Video Lyrics Benchmark for Animated Song\nTranslation (MAVL), the first multilingual, multimodal benchmark for singable\nlyrics translation. By integrating text, audio, and video, MAVL enables richer\nand more expressive translations than text-only approaches. Building on this,\nwe propose Syllable-Constrained Audio-Video LLM with Chain-of-Thought\nSylAVL-CoT, which leverages audio-video cues and enforces syllabic constraints\nto produce natural-sounding lyrics. Experimental results demonstrate that\nSylAVL-CoT significantly outperforms text-based models in singability and\ncontextual accuracy, emphasizing the value of multimodal, multilingual\napproaches for lyrics translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lyrics translation requires both accurate semantic transfer and preservation\nof musical rhythm, syllabic structure, and poetic style. In animated musicals,\nthe challenge intensifies due to alignment with visual and auditory cues. We\nintroduce Multilingual Audio-Video Lyrics Benchmark for Animated Song\nTranslation (MAVL), the first multilingual, multimodal benchmark for singable\nlyrics translation. By integrating text, audio, and video, MAVL enables richer\nand more expressive translations than text-only approaches. Building on this,\nwe propose Syllable-Constrained Audio-Video LLM with Chain-of-Thought\nSylAVL-CoT, which leverages audio-video cues and enforces syllabic constraints\nto produce natural-sounding lyrics. Experimental results demonstrate that\nSylAVL-CoT significantly outperforms text-based models in singability and\ncontextual accuracy, emphasizing the value of multimodal, multilingual\napproaches for lyrics translation."
                },
                "authors": [
                    {
                        "name": "Woohyun Cho"
                    },
                    {
                        "name": "Youngmin Kim"
                    },
                    {
                        "name": "Sunghyun Lee"
                    },
                    {
                        "name": "Youngjae Yu"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Yu"
                },
                "author": "Youngjae Yu",
                "arxiv_comment": "Accepted to EMNLP 2025, Project Page:\n  https://k1064190.github.io/papers/paper1.html, our codes and datasets are\n  available at https://github.com/k1064190/MAVL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18614v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18614v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14221v1",
                "updated": "2025-09-17T17:53:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    53,
                    43,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T17:53:43Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    53,
                    43,
                    2,
                    260,
                    0
                ],
                "title": "GEM-Bench: A Benchmark for Ad-Injected Response Generation within\n  Generative Engine Marketing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEM-Bench: A Benchmark for Ad-Injected Response Generation within\n  Generative Engine Marketing"
                },
                "summary": "Generative Engine Marketing (GEM) is an emerging ecosystem for monetizing\ngenerative engines, such as LLM-based chatbots, by seamlessly integrating\nrelevant advertisements into their responses. At the core of GEM lies the\ngeneration and evaluation of ad-injected responses. However, existing\nbenchmarks are not specifically designed for this purpose, which limits future\nresearch. To address this gap, we propose GEM-Bench, the first comprehensive\nbenchmark for ad-injected response generation in GEM. GEM-Bench includes three\ncurated datasets covering both chatbot and search scenarios, a metric ontology\nthat captures multiple dimensions of user satisfaction and engagement, and\nseveral baseline solutions implemented within an extensible multi-agent\nframework. Our preliminary results indicate that, while simple prompt-based\nmethods achieve reasonable engagement such as click-through rate, they often\nreduce user satisfaction. In contrast, approaches that insert ads based on\npre-generated ad-free responses help mitigate this issue but introduce\nadditional overhead. These findings highlight the need for future research on\ndesigning more effective and efficient solutions for generating ad-injected\nresponses in GEM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Engine Marketing (GEM) is an emerging ecosystem for monetizing\ngenerative engines, such as LLM-based chatbots, by seamlessly integrating\nrelevant advertisements into their responses. At the core of GEM lies the\ngeneration and evaluation of ad-injected responses. However, existing\nbenchmarks are not specifically designed for this purpose, which limits future\nresearch. To address this gap, we propose GEM-Bench, the first comprehensive\nbenchmark for ad-injected response generation in GEM. GEM-Bench includes three\ncurated datasets covering both chatbot and search scenarios, a metric ontology\nthat captures multiple dimensions of user satisfaction and engagement, and\nseveral baseline solutions implemented within an extensible multi-agent\nframework. Our preliminary results indicate that, while simple prompt-based\nmethods achieve reasonable engagement such as click-through rate, they often\nreduce user satisfaction. In contrast, approaches that insert ads based on\npre-generated ad-free responses help mitigate this issue but introduce\nadditional overhead. These findings highlight the need for future research on\ndesigning more effective and efficient solutions for generating ad-injected\nresponses in GEM."
                },
                "authors": [
                    {
                        "name": "Silan Hu"
                    },
                    {
                        "name": "Shiqi Zhang"
                    },
                    {
                        "name": "Yimin Shi"
                    },
                    {
                        "name": "Xiaokui Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokui Xiao"
                },
                "author": "Xiaokui Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14216v1",
                "updated": "2025-09-17T17:50:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    50,
                    59,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T17:50:59Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    50,
                    59,
                    2,
                    260,
                    0
                ],
                "title": "A Universal Banach--Bregman Framework for Stochastic Iterations:\n  Unifying Stochastic Mirror Descent, Learning and LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Universal Banach--Bregman Framework for Stochastic Iterations:\n  Unifying Stochastic Mirror Descent, Learning and LLM Training"
                },
                "summary": "Stochastic optimization powers the scalability of modern artificial\nintelligence, spanning machine learning, deep learning, reinforcement learning,\nand large language model training. Yet, existing theory remains largely\nconfined to Hilbert spaces, relying on inner-product frameworks and\northogonality. This paradigm fails to capture non-Euclidean settings, such as\nmirror descent on simplices, Bregman proximal methods for sparse learning,\nnatural gradient descent in information geometry, or\nKullback--Leibler-regularized language model training. Unlike Euclidean-based\nHilbert-space methods, this approach embraces general Banach spaces. This work\nintroduces a pioneering Banach--Bregman framework for stochastic iterations,\nestablishing Bregman geometry as a foundation for next-generation optimization.\nIt (i) provides a unified template via Bregman projections and Bregman--Fejer\nmonotonicity, encompassing stochastic approximation, mirror descent, natural\ngradient, adaptive methods, and mirror-prox; (ii) establishes super-relaxations\n($\\lambda > 2$) in non-Hilbert settings, enabling flexible geometries and\nelucidating their acceleration effect; and (iii) delivers convergence theorems\nspanning almost-sure boundedness to geometric rates, validated on synthetic and\nreal-world tasks. Empirical studies across machine learning (UCI benchmarks),\ndeep learning (e.g., Transformer training), reinforcement learning\n(actor--critic), and large language models (WikiText-2 with distilGPT-2) show\nup to 20% faster convergence, reduced variance, and enhanced accuracy over\nclassical baselines. These results position Banach--Bregman geometry as a\ncornerstone unifying optimization theory and practice across core AI paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic optimization powers the scalability of modern artificial\nintelligence, spanning machine learning, deep learning, reinforcement learning,\nand large language model training. Yet, existing theory remains largely\nconfined to Hilbert spaces, relying on inner-product frameworks and\northogonality. This paradigm fails to capture non-Euclidean settings, such as\nmirror descent on simplices, Bregman proximal methods for sparse learning,\nnatural gradient descent in information geometry, or\nKullback--Leibler-regularized language model training. Unlike Euclidean-based\nHilbert-space methods, this approach embraces general Banach spaces. This work\nintroduces a pioneering Banach--Bregman framework for stochastic iterations,\nestablishing Bregman geometry as a foundation for next-generation optimization.\nIt (i) provides a unified template via Bregman projections and Bregman--Fejer\nmonotonicity, encompassing stochastic approximation, mirror descent, natural\ngradient, adaptive methods, and mirror-prox; (ii) establishes super-relaxations\n($\\lambda > 2$) in non-Hilbert settings, enabling flexible geometries and\nelucidating their acceleration effect; and (iii) delivers convergence theorems\nspanning almost-sure boundedness to geometric rates, validated on synthetic and\nreal-world tasks. Empirical studies across machine learning (UCI benchmarks),\ndeep learning (e.g., Transformer training), reinforcement learning\n(actor--critic), and large language models (WikiText-2 with distilGPT-2) show\nup to 20% faster convergence, reduced variance, and enhanced accuracy over\nclassical baselines. These results position Banach--Bregman geometry as a\ncornerstone unifying optimization theory and practice across core AI paradigms."
                },
                "authors": [
                    {
                        "name": "Johnny R. Zhang"
                    },
                    {
                        "name": "Xiaomei Mi"
                    },
                    {
                        "name": "Gaoyuan Du"
                    },
                    {
                        "name": "Qianyi Sun"
                    },
                    {
                        "name": "Shiqi Wang"
                    },
                    {
                        "name": "Jiaxuan Li"
                    },
                    {
                        "name": "Wenhua Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wenhua Zhou"
                },
                "arxiv_affiliation": "Independent Researcher",
                "author": "Wenhua Zhou",
                "arxiv_comment": "69 pages, 10 figures. Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23804v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23804v2",
                "updated": "2025-09-17T17:39:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    39,
                    16,
                    2,
                    260,
                    0
                ],
                "published": "2025-05-27T01:01:55Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    1,
                    1,
                    55,
                    1,
                    147,
                    0
                ],
                "title": "Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause\n  Frequencies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause\n  Frequencies"
                },
                "summary": "While large language models (LLMs) achieve strong performance on text-to-SQL\nparsing, they sometimes exhibit unexpected failures in which they are\nconfidently incorrect. Building trustworthy text-to-SQL systems thus requires\neliciting reliable uncertainty measures from the LLM. In this paper, we study\nthe problem of providing a calibrated confidence score that conveys the\nlikelihood of an output query being correct. Our work is the first to establish\na benchmark for post-hoc calibration of LLM-based text-to-SQL parsing. In\nparticular, we show that Platt scaling, a canonical method for calibration,\nprovides substantial improvements over directly using raw model output\nprobabilities as confidence scores. Furthermore, we propose a method for\ntext-to-SQL calibration that leverages the structured nature of SQL queries to\nprovide more granular signals of correctness, named \"sub-clause frequency\"\n(SCF) scores. Using multivariate Platt scaling (MPS), our extension of the\ncanonical Platt scaling technique, we combine individual SCF scores into an\noverall accurate and calibrated score. Empirical evaluation on two popular\ntext-to-SQL datasets shows that our approach of combining MPS and SCF yields\nfurther improvements in calibration and the related task of error detection\nover traditional Platt scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) achieve strong performance on text-to-SQL\nparsing, they sometimes exhibit unexpected failures in which they are\nconfidently incorrect. Building trustworthy text-to-SQL systems thus requires\neliciting reliable uncertainty measures from the LLM. In this paper, we study\nthe problem of providing a calibrated confidence score that conveys the\nlikelihood of an output query being correct. Our work is the first to establish\na benchmark for post-hoc calibration of LLM-based text-to-SQL parsing. In\nparticular, we show that Platt scaling, a canonical method for calibration,\nprovides substantial improvements over directly using raw model output\nprobabilities as confidence scores. Furthermore, we propose a method for\ntext-to-SQL calibration that leverages the structured nature of SQL queries to\nprovide more granular signals of correctness, named \"sub-clause frequency\"\n(SCF) scores. Using multivariate Platt scaling (MPS), our extension of the\ncanonical Platt scaling technique, we combine individual SCF scores into an\noverall accurate and calibrated score. Empirical evaluation on two popular\ntext-to-SQL datasets shows that our approach of combining MPS and SCF yields\nfurther improvements in calibration and the related task of error detection\nover traditional Platt scaling."
                },
                "authors": [
                    {
                        "name": "Terrance Liu"
                    },
                    {
                        "name": "Shuyi Wang"
                    },
                    {
                        "name": "Daniel Preotiuc-Pietro"
                    },
                    {
                        "name": "Yash Chandarana"
                    },
                    {
                        "name": "Chirag Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Gupta"
                },
                "author": "Chirag Gupta",
                "arxiv_comment": "EMNLP 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23804v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23804v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14197v1",
                "updated": "2025-09-17T17:31:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    31,
                    57,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T17:31:57Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    31,
                    57,
                    2,
                    260,
                    0
                ],
                "title": "Framing Migration: A Computational Analysis of UK Parliamentary\n  Discourse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Framing Migration: A Computational Analysis of UK Parliamentary\n  Discourse"
                },
                "summary": "We present a large-scale computational analysis of migration-related\ndiscourse in UK parliamentary debates spanning over 75 years and compare it\nwith US congressional discourse. Using open-weight LLMs, we annotate each\nstatement with high-level stances toward migrants and track the net tone toward\nmigrants across time and political parties. For the UK, we extend this with a\nsemi-automated framework for extracting fine-grained narrative frames to\ncapture nuances of migration discourse. Our findings show that, while US\ndiscourse has grown increasingly polarised, UK parliamentary attitudes remain\nrelatively aligned across parties, with a persistent ideological gap between\nLabour and the Conservatives, reaching its most negative level in 2025. The\nanalysis of narrative frames in the UK parliamentary statements reveals a shift\ntoward securitised narratives such as border control and illegal immigration,\nwhile longer-term integration-oriented frames such as social integration have\ndeclined. Moreover, discussions of national law about immigration have been\nreplaced over time by international law and human rights, revealing nuances in\ndiscourse trends. Taken together broadly, our findings demonstrate how LLMs can\nsupport scalable, fine-grained discourse analysis in political and historical\ncontexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a large-scale computational analysis of migration-related\ndiscourse in UK parliamentary debates spanning over 75 years and compare it\nwith US congressional discourse. Using open-weight LLMs, we annotate each\nstatement with high-level stances toward migrants and track the net tone toward\nmigrants across time and political parties. For the UK, we extend this with a\nsemi-automated framework for extracting fine-grained narrative frames to\ncapture nuances of migration discourse. Our findings show that, while US\ndiscourse has grown increasingly polarised, UK parliamentary attitudes remain\nrelatively aligned across parties, with a persistent ideological gap between\nLabour and the Conservatives, reaching its most negative level in 2025. The\nanalysis of narrative frames in the UK parliamentary statements reveals a shift\ntoward securitised narratives such as border control and illegal immigration,\nwhile longer-term integration-oriented frames such as social integration have\ndeclined. Moreover, discussions of national law about immigration have been\nreplaced over time by international law and human rights, revealing nuances in\ndiscourse trends. Taken together broadly, our findings demonstrate how LLMs can\nsupport scalable, fine-grained discourse analysis in political and historical\ncontexts."
                },
                "authors": [
                    {
                        "name": "Vahid Ghafouri"
                    },
                    {
                        "name": "Robert McNeil"
                    },
                    {
                        "name": "Teodor Yankov"
                    },
                    {
                        "name": "Madeleine Sumption"
                    },
                    {
                        "name": "Luc Rocher"
                    },
                    {
                        "name": "Scott A. Hale"
                    },
                    {
                        "name": "Adam Mahdi"
                    }
                ],
                "author_detail": {
                    "name": "Adam Mahdi"
                },
                "author": "Adam Mahdi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14189v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14189v2",
                "updated": "2025-09-18T01:04:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    1,
                    4,
                    39,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-17T17:27:12Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    27,
                    12,
                    2,
                    260,
                    0
                ],
                "title": "AI and the Future of Academic Peer Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI and the Future of Academic Peer Review"
                },
                "summary": "Peer review remains the central quality-control mechanism of science, yet its\nability to fulfill this role is increasingly strained. Empirical studies\ndocument serious shortcomings: long publication delays, escalating reviewer\nburden concentrated on a small minority of scholars, inconsistent quality and\nlow inter-reviewer agreement, and systematic biases by gender, language, and\ninstitutional prestige. Decades of human-centered reforms have yielded only\nmarginal improvements. Meanwhile, artificial intelligence, especially large\nlanguage models (LLMs), is being piloted across the peer-review pipeline by\njournals, funders, and individual reviewers. Early studies suggest that AI\nassistance can produce reviews comparable in quality to humans, accelerate\nreviewer selection and feedback, and reduce certain biases, but also raise\ndistinctive concerns about hallucination, confidentiality, gaming, novelty\nrecognition, and loss of trust. In this paper, we map the aims and persistent\nfailure modes of peer review to specific LLM applications and systematically\nanalyze the objections they raise alongside safeguards that could make their\nuse acceptable. Drawing on emerging evidence, we show that targeted, supervised\nLLM assistance can plausibly improve error detection, timeliness, and reviewer\nworkload without displacing human judgment. We highlight advanced\narchitectures, including fine-tuned, retrieval-augmented, and multi-agent\nsystems, that may enable more reliable, auditable, and interdisciplinary\nreview. We argue that ethical and practical considerations are not peripheral\nbut constitutive: the legitimacy of AI-assisted peer review depends on\ngovernance choices as much as technical capacity. The path forward is neither\nuncritical adoption nor reflexive rejection, but carefully scoped pilots with\nexplicit evaluation metrics, transparency, and accountability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peer review remains the central quality-control mechanism of science, yet its\nability to fulfill this role is increasingly strained. Empirical studies\ndocument serious shortcomings: long publication delays, escalating reviewer\nburden concentrated on a small minority of scholars, inconsistent quality and\nlow inter-reviewer agreement, and systematic biases by gender, language, and\ninstitutional prestige. Decades of human-centered reforms have yielded only\nmarginal improvements. Meanwhile, artificial intelligence, especially large\nlanguage models (LLMs), is being piloted across the peer-review pipeline by\njournals, funders, and individual reviewers. Early studies suggest that AI\nassistance can produce reviews comparable in quality to humans, accelerate\nreviewer selection and feedback, and reduce certain biases, but also raise\ndistinctive concerns about hallucination, confidentiality, gaming, novelty\nrecognition, and loss of trust. In this paper, we map the aims and persistent\nfailure modes of peer review to specific LLM applications and systematically\nanalyze the objections they raise alongside safeguards that could make their\nuse acceptable. Drawing on emerging evidence, we show that targeted, supervised\nLLM assistance can plausibly improve error detection, timeliness, and reviewer\nworkload without displacing human judgment. We highlight advanced\narchitectures, including fine-tuned, retrieval-augmented, and multi-agent\nsystems, that may enable more reliable, auditable, and interdisciplinary\nreview. We argue that ethical and practical considerations are not peripheral\nbut constitutive: the legitimacy of AI-assisted peer review depends on\ngovernance choices as much as technical capacity. The path forward is neither\nuncritical adoption nor reflexive rejection, but carefully scoped pilots with\nexplicit evaluation metrics, transparency, and accountability."
                },
                "authors": [
                    {
                        "name": "Sebastian Porsdam Mann"
                    },
                    {
                        "name": "Mateo Aboy"
                    },
                    {
                        "name": "Joel Jiehao Seah"
                    },
                    {
                        "name": "Zhicheng Lin"
                    },
                    {
                        "name": "Xufei Luo"
                    },
                    {
                        "name": "Daniel Rodger"
                    },
                    {
                        "name": "Hazem Zohny"
                    },
                    {
                        "name": "Timo Minssen"
                    },
                    {
                        "name": "Julian Savulescu"
                    },
                    {
                        "name": "Brian D. Earp"
                    }
                ],
                "author_detail": {
                    "name": "Brian D. Earp"
                },
                "author": "Brian D. Earp",
                "arxiv_comment": "34 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14189v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14189v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14187v1",
                "updated": "2025-09-17T17:26:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    26,
                    29,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T17:26:29Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    26,
                    29,
                    2,
                    260,
                    0
                ],
                "title": "Read to Hear: A Zero-Shot Pronunciation Assessment Using Textual\n  Descriptions and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read to Hear: A Zero-Shot Pronunciation Assessment Using Textual\n  Descriptions and LLMs"
                },
                "summary": "Automatic pronunciation assessment is typically performed by acoustic models\ntrained on audio-score pairs. Although effective, these systems provide only\nnumerical scores, without the information needed to help learners understand\ntheir errors. Meanwhile, large language models (LLMs) have proven effective in\nsupporting language learning, but their potential for assessing pronunciation\nremains unexplored. In this work, we introduce TextPA, a zero-shot, Textual\ndescription-based Pronunciation Assessment approach. TextPA utilizes\nhuman-readable representations of speech signals, which are fed into an LLM to\nassess pronunciation accuracy and fluency, while also providing reasoning\nbehind the assigned scores. Finally, a phoneme sequence match scoring method is\nused to refine the accuracy scores. Our work highlights a previously overlooked\ndirection for pronunciation assessment. Instead of relying on supervised\ntraining with audio-score examples, we exploit the rich pronunciation knowledge\nembedded in written text. Experimental results show that our approach is both\ncost-efficient and competitive in performance. Furthermore, TextPA\nsignificantly improves the performance of conventional audio-score-trained\nmodels on out-of-domain data by offering a complementary perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic pronunciation assessment is typically performed by acoustic models\ntrained on audio-score pairs. Although effective, these systems provide only\nnumerical scores, without the information needed to help learners understand\ntheir errors. Meanwhile, large language models (LLMs) have proven effective in\nsupporting language learning, but their potential for assessing pronunciation\nremains unexplored. In this work, we introduce TextPA, a zero-shot, Textual\ndescription-based Pronunciation Assessment approach. TextPA utilizes\nhuman-readable representations of speech signals, which are fed into an LLM to\nassess pronunciation accuracy and fluency, while also providing reasoning\nbehind the assigned scores. Finally, a phoneme sequence match scoring method is\nused to refine the accuracy scores. Our work highlights a previously overlooked\ndirection for pronunciation assessment. Instead of relying on supervised\ntraining with audio-score examples, we exploit the rich pronunciation knowledge\nembedded in written text. Experimental results show that our approach is both\ncost-efficient and competitive in performance. Furthermore, TextPA\nsignificantly improves the performance of conventional audio-score-trained\nmodels on out-of-domain data by offering a complementary perspective."
                },
                "authors": [
                    {
                        "name": "Yu-Wen Chen"
                    },
                    {
                        "name": "Melody Ma"
                    },
                    {
                        "name": "Julia Hirschberg"
                    }
                ],
                "author_detail": {
                    "name": "Julia Hirschberg"
                },
                "author": "Julia Hirschberg",
                "arxiv_comment": "EMNLP 2025 MainConference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06207v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06207v2",
                "updated": "2025-09-17T17:21:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    21,
                    24,
                    2,
                    260,
                    0
                ],
                "published": "2024-11-09T15:12:28Z",
                "published_parsed": [
                    2024,
                    11,
                    9,
                    15,
                    12,
                    28,
                    5,
                    314,
                    0
                ],
                "title": "KBM: Delineating Knowledge Boundary for Adaptive Retrieval in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KBM: Delineating Knowledge Boundary for Adaptive Retrieval in Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) often struggle with dynamically changing\nknowledge and handling unknown static information. Retrieval-Augmented\nGeneration (RAG) is employed to tackle these challenges and has a significant\nimpact on improving LLM performance. In fact, we find that not all questions\nneed to trigger RAG. By retrieving parts of knowledge unknown to the LLM and\nallowing the LLM to answer the rest, we can effectively reduce both time and\ncomputational costs. In our work, we propose a Knowledge Boundary Model (KBM)\nto express the known/unknown of a given question, and to determine whether a\nRAG needs to be triggered. Experiments conducted on 11 English and Chinese\ndatasets illustrate that the KBM effectively delineates the knowledge boundary,\nsignificantly decreasing the proportion of retrievals required for optimal\nend-to-end performance. Furthermore, we evaluate the effectiveness of KBM in\nthree complex scenarios: dynamic knowledge, long-tail static knowledge, and\nmulti-hop problems, as well as its functionality as an external LLM plug-in.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often struggle with dynamically changing\nknowledge and handling unknown static information. Retrieval-Augmented\nGeneration (RAG) is employed to tackle these challenges and has a significant\nimpact on improving LLM performance. In fact, we find that not all questions\nneed to trigger RAG. By retrieving parts of knowledge unknown to the LLM and\nallowing the LLM to answer the rest, we can effectively reduce both time and\ncomputational costs. In our work, we propose a Knowledge Boundary Model (KBM)\nto express the known/unknown of a given question, and to determine whether a\nRAG needs to be triggered. Experiments conducted on 11 English and Chinese\ndatasets illustrate that the KBM effectively delineates the knowledge boundary,\nsignificantly decreasing the proportion of retrievals required for optimal\nend-to-end performance. Furthermore, we evaluate the effectiveness of KBM in\nthree complex scenarios: dynamic knowledge, long-tail static knowledge, and\nmulti-hop problems, as well as its functionality as an external LLM plug-in."
                },
                "authors": [
                    {
                        "name": "Zhen Zhang"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Zile Qiao"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Guangyu Li"
                    },
                    {
                        "name": "Feiteng Mu"
                    },
                    {
                        "name": "Mengting Hu"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Huang"
                },
                "author": "Fei Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06207v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06207v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01153v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01153v4",
                "updated": "2025-09-17T17:16:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    16,
                    11,
                    2,
                    260,
                    0
                ],
                "published": "2025-04-01T19:36:14Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    36,
                    14,
                    1,
                    91,
                    0
                ],
                "title": "Catch Me if You Search: When Contextual Web Search Results Affect the\n  Detection of Hallucinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Catch Me if You Search: When Contextual Web Search Results Affect the\n  Detection of Hallucinations"
                },
                "summary": "While we increasingly rely on large language models (LLMs) for various tasks,\nthese models are known to produce inaccurate content or 'hallucinations' with\npotentially disastrous consequences. The recent integration of web search\nresults into LLMs prompts the question of whether people utilize them to verify\nthe generated content, thereby accurately detecting hallucinations. An online\nexperiment (N=560) investigated how the provision of search results, either\nstatic (i.e., fixed search results provided by LLM) or dynamic (i.e.,\nparticipant-led searches), affects participants' perceived accuracy of\nLLM-generated content (i.e., genuine, minor hallucination, major\nhallucination), self-confidence in accuracy ratings, as well as their overall\nevaluation of the LLM, as compared to the control condition (i.e., no search\nresults). Results showed that participants in both static and dynamic\nconditions (vs. control) rated hallucinated content to be less accurate and\nperceived the LLM more negatively. However, those in the dynamic condition\nrated genuine content as more accurate and demonstrated greater overall\nself-confidence in their assessments than those in the static search or control\nconditions. We highlighted practical implications of incorporating web search\nfunctionality into LLMs in real-world contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While we increasingly rely on large language models (LLMs) for various tasks,\nthese models are known to produce inaccurate content or 'hallucinations' with\npotentially disastrous consequences. The recent integration of web search\nresults into LLMs prompts the question of whether people utilize them to verify\nthe generated content, thereby accurately detecting hallucinations. An online\nexperiment (N=560) investigated how the provision of search results, either\nstatic (i.e., fixed search results provided by LLM) or dynamic (i.e.,\nparticipant-led searches), affects participants' perceived accuracy of\nLLM-generated content (i.e., genuine, minor hallucination, major\nhallucination), self-confidence in accuracy ratings, as well as their overall\nevaluation of the LLM, as compared to the control condition (i.e., no search\nresults). Results showed that participants in both static and dynamic\nconditions (vs. control) rated hallucinated content to be less accurate and\nperceived the LLM more negatively. However, those in the dynamic condition\nrated genuine content as more accurate and demonstrated greater overall\nself-confidence in their assessments than those in the static search or control\nconditions. We highlighted practical implications of incorporating web search\nfunctionality into LLMs in real-world contexts."
                },
                "authors": [
                    {
                        "name": "Mahjabin Nahar"
                    },
                    {
                        "name": "Eun-Ju Lee"
                    },
                    {
                        "name": "Jin Won Park"
                    },
                    {
                        "name": "Dongwon Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongwon Lee"
                },
                "author": "Dongwon Lee",
                "arxiv_comment": "Accepted to Computers in Human Behavior",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01153v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01153v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14180v1",
                "updated": "2025-09-17T17:12:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    12,
                    38,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T17:12:38Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    12,
                    38,
                    2,
                    260,
                    0
                ],
                "title": "Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation\n  Framework for Personal Finance LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation\n  Framework for Personal Finance LLMs"
                },
                "summary": "Personalized financial advice requires consideration of user goals,\nconstraints, risk tolerance, and jurisdiction. Prior LLM work has focused on\nsupport systems for investors and financial planners. Simultaneously, numerous\nrecent studies examine broader personal finance tasks, including budgeting,\ndebt management, retirement, and estate planning, through agentic pipelines\nthat incur high maintenance costs, yielding less than 25% of their expected\nfinancial returns. In this study, we introduce a novel and reproducible\nframework that integrates relevant financial context with behavioral finance\nstudies to construct supervision data for end-to-end advisors. Using this\nframework, we create a 19k sample reasoning dataset and conduct a comprehensive\nfine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test\nsplit and a blind LLM-jury study, we demonstrate that through careful data\ncuration and behavioral integration, our 8B model achieves performance\ncomparable to significantly larger baselines (14-32B parameters) across factual\naccuracy, fluency, and personalization metrics while incurring 80% lower costs\nthan the larger counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized financial advice requires consideration of user goals,\nconstraints, risk tolerance, and jurisdiction. Prior LLM work has focused on\nsupport systems for investors and financial planners. Simultaneously, numerous\nrecent studies examine broader personal finance tasks, including budgeting,\ndebt management, retirement, and estate planning, through agentic pipelines\nthat incur high maintenance costs, yielding less than 25% of their expected\nfinancial returns. In this study, we introduce a novel and reproducible\nframework that integrates relevant financial context with behavioral finance\nstudies to construct supervision data for end-to-end advisors. Using this\nframework, we create a 19k sample reasoning dataset and conduct a comprehensive\nfine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test\nsplit and a blind LLM-jury study, we demonstrate that through careful data\ncuration and behavioral integration, our 8B model achieves performance\ncomparable to significantly larger baselines (14-32B parameters) across factual\naccuracy, fluency, and personalization metrics while incurring 80% lower costs\nthan the larger counterparts."
                },
                "authors": [
                    {
                        "name": "Akhil Theerthala"
                    }
                ],
                "author_detail": {
                    "name": "Akhil Theerthala"
                },
                "author": "Akhil Theerthala",
                "arxiv_comment": "24 pages, 11 figures. The paper presents a novel framework for\n  generating a personal finance dataset. The resulting fine-tuned model and\n  dataset are publicly available",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20781v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20781v2",
                "updated": "2025-09-17T17:01:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    17,
                    1,
                    5,
                    2,
                    260,
                    0
                ],
                "published": "2025-04-29T14:00:18Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    0,
                    18,
                    1,
                    119,
                    0
                ],
                "title": "Using LLMs in Generating Design Rationale for Software Architecture\n  Decisions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs in Generating Design Rationale for Software Architecture\n  Decisions"
                },
                "summary": "Design Rationale (DR) for software architecture decisions refers to the\nreasoning underlying architectural choices, which provides valuable insights\ninto the different phases of the architecting process throughout software\ndevelopment. However, in practice, DR is often inadequately documented due to a\nlack of motivation and effort from developers. With the recent advancements in\nLarge Language Models (LLMs), their capabilities in text comprehension,\nreasoning, and generation may enable the generation and recovery of DR for\narchitecture decisions. In this study, we evaluated the performance of LLMs in\ngenerating DR for architecture decisions. First, we collected 50 Stack Overflow\n(SO) posts, 25 GitHub issues, and 25 GitHub discussions related to architecture\ndecisions to construct a dataset of 100 architecture-related problems. Then, we\nselected five LLMs to generate DR for the architecture decisions with three\nprompting strategies, including zero-shot, chain of thought (CoT), and\nLLM-based agents. With the DR provided by human experts as ground truth, the\nPrecision of LLM-generated DR with the three prompting strategies ranges from\n0.267 to 0.278, Recall from 0.627 to 0.715, and F1-score from 0.351 to 0.389.\nAdditionally, 64.45% to 69.42% of the arguments of DR not mentioned by human\nexperts are also helpful, 4.12% to 4.87% of the arguments have uncertain\ncorrectness, and 1.59% to 3.24% of the arguments are potentially misleading. To\nfurther understand the trustworthiness and applicability of LLM-generated DR in\npractice, we conducted semi-structured interviews with six practitioners. Based\non the experimental and interview results, we discussed the pros and cons of\nthe three prompting strategies, the strengths and limitations of LLM-generated\nDR, and the implications for the practical use of LLM-generated DR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design Rationale (DR) for software architecture decisions refers to the\nreasoning underlying architectural choices, which provides valuable insights\ninto the different phases of the architecting process throughout software\ndevelopment. However, in practice, DR is often inadequately documented due to a\nlack of motivation and effort from developers. With the recent advancements in\nLarge Language Models (LLMs), their capabilities in text comprehension,\nreasoning, and generation may enable the generation and recovery of DR for\narchitecture decisions. In this study, we evaluated the performance of LLMs in\ngenerating DR for architecture decisions. First, we collected 50 Stack Overflow\n(SO) posts, 25 GitHub issues, and 25 GitHub discussions related to architecture\ndecisions to construct a dataset of 100 architecture-related problems. Then, we\nselected five LLMs to generate DR for the architecture decisions with three\nprompting strategies, including zero-shot, chain of thought (CoT), and\nLLM-based agents. With the DR provided by human experts as ground truth, the\nPrecision of LLM-generated DR with the three prompting strategies ranges from\n0.267 to 0.278, Recall from 0.627 to 0.715, and F1-score from 0.351 to 0.389.\nAdditionally, 64.45% to 69.42% of the arguments of DR not mentioned by human\nexperts are also helpful, 4.12% to 4.87% of the arguments have uncertain\ncorrectness, and 1.59% to 3.24% of the arguments are potentially misleading. To\nfurther understand the trustworthiness and applicability of LLM-generated DR in\npractice, we conducted semi-structured interviews with six practitioners. Based\non the experimental and interview results, we discussed the pros and cons of\nthe three prompting strategies, the strengths and limitations of LLM-generated\nDR, and the implications for the practical use of LLM-generated DR."
                },
                "authors": [
                    {
                        "name": "Xiyu Zhou"
                    },
                    {
                        "name": "Ruiyin Li"
                    },
                    {
                        "name": "Peng Liang"
                    },
                    {
                        "name": "Beiqi Zhang"
                    },
                    {
                        "name": "Mojtaba Shahin"
                    },
                    {
                        "name": "Zengyang Li"
                    },
                    {
                        "name": "Chen Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chen Yang"
                },
                "author": "Chen Yang",
                "arxiv_comment": "38 pages, 5 images, 9 tables, Manuscript revision submitted to a\n  journal (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20781v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20781v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14169v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14169v1",
                "updated": "2025-09-17T16:52:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    52,
                    46,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T16:52:46Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    52,
                    46,
                    2,
                    260,
                    0
                ],
                "title": "TopoSizing: An LLM-aided Framework of Topology-based Understanding and\n  Sizing for AMS Circuits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TopoSizing: An LLM-aided Framework of Topology-based Understanding and\n  Sizing for AMS Circuits"
                },
                "summary": "Analog and mixed-signal circuit design remains challenging due to the\nshortage of high-quality data and the difficulty of embedding domain knowledge\ninto automated flows. Traditional black-box optimization achieves sampling\nefficiency but lacks circuit understanding, which often causes evaluations to\nbe wasted in low-value regions of the design space. In contrast, learning-based\nmethods embed structural knowledge but are case-specific and costly to retrain.\nRecent attempts with large language models show potential, yet they often rely\non manual intervention, limiting generality and transparency. We propose\nTopoSizing, an end-to-end framework that performs robust circuit understanding\ndirectly from raw netlists and translates this knowledge into optimization\ngains. Our approach first applies graph algorithms to organize circuits into a\nhierarchical device-module-stage representation. LLM agents then execute an\niterative hypothesis-verification-refinement loop with built-in consistency\nchecks, producing explicit annotations. Verified insights are integrated into\nBayesian optimization through LLM-guided initial sampling and\nstagnation-triggered trust-region updates, improving efficiency while\npreserving feasibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog and mixed-signal circuit design remains challenging due to the\nshortage of high-quality data and the difficulty of embedding domain knowledge\ninto automated flows. Traditional black-box optimization achieves sampling\nefficiency but lacks circuit understanding, which often causes evaluations to\nbe wasted in low-value regions of the design space. In contrast, learning-based\nmethods embed structural knowledge but are case-specific and costly to retrain.\nRecent attempts with large language models show potential, yet they often rely\non manual intervention, limiting generality and transparency. We propose\nTopoSizing, an end-to-end framework that performs robust circuit understanding\ndirectly from raw netlists and translates this knowledge into optimization\ngains. Our approach first applies graph algorithms to organize circuits into a\nhierarchical device-module-stage representation. LLM agents then execute an\niterative hypothesis-verification-refinement loop with built-in consistency\nchecks, producing explicit annotations. Verified insights are integrated into\nBayesian optimization through LLM-guided initial sampling and\nstagnation-triggered trust-region updates, improving efficiency while\npreserving feasibility."
                },
                "authors": [
                    {
                        "name": "Ziming Wei"
                    },
                    {
                        "name": "Zichen Kong"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "David Z. Pan"
                    },
                    {
                        "name": "Xiyuan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Xiyuan Tang"
                },
                "author": "Xiyuan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14169v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18325v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18325v3",
                "updated": "2025-09-17T16:44:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    44,
                    58,
                    2,
                    260,
                    0
                ],
                "published": "2025-05-23T19:30:49Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    19,
                    30,
                    49,
                    4,
                    143,
                    0
                ],
                "title": "Understanding and Mitigating Overrefusal in LLMs from an Unveiling\n  Perspective of Safety Decision Boundary",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Mitigating Overrefusal in LLMs from an Unveiling\n  Perspective of Safety Decision Boundary"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet they often refuse to answer legitimate queries--a\nphenomenon known as overrefusal. Overrefusal typically stems from\nover-conservative safety alignment, causing models to treat many reasonable\nprompts as potentially risky. To systematically understand this issue, we probe\nand leverage the models' safety decision boundaries to analyze and mitigate\noverrefusal. Our findings reveal that overrefusal is closely tied to\nmisalignment at these boundary regions, where models struggle to distinguish\nsubtle differences between benign and harmful content. Building on these\ninsights, we present RASS, an automated framework for prompt generation and\nselection that strategically targets overrefusal prompts near the safety\nboundary. By harnessing steering vectors in the representation space, RASS\nefficiently identifies and curates boundary-aligned prompts, enabling more\neffective and targeted mitigation of overrefusal. This approach not only\nprovides a more precise and interpretable view of model safety decisions but\nalso seamlessly extends to multilingual scenarios. We have explored the safety\ndecision boundaries of various LLMs and construct the MORBench evaluation set\nto facilitate robust assessment of model safety and helpfulness across multiple\nlanguages. Code and datasets are available at\nhttps://github.com/Master-PLC/RASS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet they often refuse to answer legitimate queries--a\nphenomenon known as overrefusal. Overrefusal typically stems from\nover-conservative safety alignment, causing models to treat many reasonable\nprompts as potentially risky. To systematically understand this issue, we probe\nand leverage the models' safety decision boundaries to analyze and mitigate\noverrefusal. Our findings reveal that overrefusal is closely tied to\nmisalignment at these boundary regions, where models struggle to distinguish\nsubtle differences between benign and harmful content. Building on these\ninsights, we present RASS, an automated framework for prompt generation and\nselection that strategically targets overrefusal prompts near the safety\nboundary. By harnessing steering vectors in the representation space, RASS\nefficiently identifies and curates boundary-aligned prompts, enabling more\neffective and targeted mitigation of overrefusal. This approach not only\nprovides a more precise and interpretable view of model safety decisions but\nalso seamlessly extends to multilingual scenarios. We have explored the safety\ndecision boundaries of various LLMs and construct the MORBench evaluation set\nto facilitate robust assessment of model safety and helpfulness across multiple\nlanguages. Code and datasets are available at\nhttps://github.com/Master-PLC/RASS."
                },
                "authors": [
                    {
                        "name": "Licheng Pan"
                    },
                    {
                        "name": "Yongqi Tong"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Xiaolu Zhang"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Zhixuan Chu"
                    }
                ],
                "author_detail": {
                    "name": "Zhixuan Chu"
                },
                "author": "Zhixuan Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18325v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18325v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05606v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05606v2",
                "updated": "2025-09-17T16:44:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    44,
                    11,
                    2,
                    260,
                    0
                ],
                "published": "2025-08-07T17:45:17Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    45,
                    17,
                    3,
                    219,
                    0
                ],
                "title": "Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and\n  Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and\n  Vision"
                },
                "summary": "Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large\nLanguage Models (LLMs) by decomposing complex tasks into simpler, sequential\nsubtasks. However, extending CoT to vision-language reasoning tasks remains\nchallenging, as it often requires interpreting transitions of visual states to\nsupport reasoning. Existing methods often struggle with this due to limited\ncapacity of modeling visual state transitions or incoherent visual trajectories\ncaused by fragmented architectures.\n  To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought\nframework that enables coherent and grounded multimodal reasoning within a\nsingle unified model. The key idea is to leverage a model capable of both image\nunderstanding and generation to reason over visual content and model evolving\nvisual states. However, empowering a unified model to achieve that is\nnon-trivial, given the high computational cost and the burden of training. To\naddress this, Uni-CoT introduces a novel two-level reasoning paradigm: A\nMacro-Level CoT for high-level task planning and A Micro-Level CoT for subtask\nexecution. This design significantly reduces the computational overhead.\nFurthermore, we introduce a structured training paradigm that combines\ninterleaved image-text supervision for macro-level CoT with multi-task\nobjectives for micro-level CoT. Together, these innovations allow Uni-CoT to\nperform scalable and coherent multi-modal reasoning. Furthermore, thanks to our\ndesign, all experiments can be efficiently completed using only 8 A100 GPUs\nwith 80GB VRAM each. Experimental results on reasoning-driven image generation\nbenchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT\ndemonstrates SOTA performance and strong generalization, establishing Uni-CoT\nas a promising solution for multi-modal reasoning. Project Page and Code:\nhttps://sais-fuxi.github.io/projects/uni-cot/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large\nLanguage Models (LLMs) by decomposing complex tasks into simpler, sequential\nsubtasks. However, extending CoT to vision-language reasoning tasks remains\nchallenging, as it often requires interpreting transitions of visual states to\nsupport reasoning. Existing methods often struggle with this due to limited\ncapacity of modeling visual state transitions or incoherent visual trajectories\ncaused by fragmented architectures.\n  To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought\nframework that enables coherent and grounded multimodal reasoning within a\nsingle unified model. The key idea is to leverage a model capable of both image\nunderstanding and generation to reason over visual content and model evolving\nvisual states. However, empowering a unified model to achieve that is\nnon-trivial, given the high computational cost and the burden of training. To\naddress this, Uni-CoT introduces a novel two-level reasoning paradigm: A\nMacro-Level CoT for high-level task planning and A Micro-Level CoT for subtask\nexecution. This design significantly reduces the computational overhead.\nFurthermore, we introduce a structured training paradigm that combines\ninterleaved image-text supervision for macro-level CoT with multi-task\nobjectives for micro-level CoT. Together, these innovations allow Uni-CoT to\nperform scalable and coherent multi-modal reasoning. Furthermore, thanks to our\ndesign, all experiments can be efficiently completed using only 8 A100 GPUs\nwith 80GB VRAM each. Experimental results on reasoning-driven image generation\nbenchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT\ndemonstrates SOTA performance and strong generalization, establishing Uni-CoT\nas a promising solution for multi-modal reasoning. Project Page and Code:\nhttps://sais-fuxi.github.io/projects/uni-cot/"
                },
                "authors": [
                    {
                        "name": "Luozheng Qin"
                    },
                    {
                        "name": "Jia Gong"
                    },
                    {
                        "name": "Yuqing Sun"
                    },
                    {
                        "name": "Tianjiao Li"
                    },
                    {
                        "name": "Mengping Yang"
                    },
                    {
                        "name": "Xiaomeng Yang"
                    },
                    {
                        "name": "Chao Qu"
                    },
                    {
                        "name": "Zhiyu Tan"
                    },
                    {
                        "name": "Hao Li"
                    }
                ],
                "author_detail": {
                    "name": "Hao Li"
                },
                "author": "Hao Li",
                "arxiv_comment": "Project Page: https://sais-fuxi.github.io/projects/uni-cot/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05606v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05606v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14142v1",
                "updated": "2025-09-17T16:21:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    21,
                    34,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T16:21:34Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    21,
                    34,
                    2,
                    260,
                    0
                ],
                "title": "MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods,\n  Results, Discussion, and Outlook",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods,\n  Results, Discussion, and Outlook"
                },
                "summary": "This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim\nto bring together different approaches in multimodal machine learning and LLMs\nvia a large benchmark. We hope it better allows researchers to follow the\nstate-of-the-art in this very dynamic area. Meanwhile, a growing number of\ntestbeds have boosted the evolution of general-purpose large language models.\nThus, this year's MARS2 focuses on real-world and specialized scenarios to\nbroaden the multimodal reasoning applications of MLLMs. Our organizing team\nreleased two tailored datasets Lens and AdsQA as test sets, which support\ngeneral reasoning in 12 daily scenarios and domain-specific reasoning in\nadvertisement videos, respectively. We evaluated 40+ baselines that include\nboth generalist MLLMs and task-specific models, and opened up three competition\ntracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question\nAnswering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative\nAdvertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and\nindustrial institutions have registered and 40+ valid submissions (out of\n1200+) have been included in our ranking lists. Our datasets, code sets (40+\nbaselines and 15+ participants' methods), and rankings are publicly available\non the MARS2 workshop website and our GitHub organization page\nhttps://github.com/mars2workshop/, where our updates and announcements of\nupcoming events will be continuously provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim\nto bring together different approaches in multimodal machine learning and LLMs\nvia a large benchmark. We hope it better allows researchers to follow the\nstate-of-the-art in this very dynamic area. Meanwhile, a growing number of\ntestbeds have boosted the evolution of general-purpose large language models.\nThus, this year's MARS2 focuses on real-world and specialized scenarios to\nbroaden the multimodal reasoning applications of MLLMs. Our organizing team\nreleased two tailored datasets Lens and AdsQA as test sets, which support\ngeneral reasoning in 12 daily scenarios and domain-specific reasoning in\nadvertisement videos, respectively. We evaluated 40+ baselines that include\nboth generalist MLLMs and task-specific models, and opened up three competition\ntracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question\nAnswering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative\nAdvertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and\nindustrial institutions have registered and 40+ valid submissions (out of\n1200+) have been included in our ranking lists. Our datasets, code sets (40+\nbaselines and 15+ participants' methods), and rankings are publicly available\non the MARS2 workshop website and our GitHub organization page\nhttps://github.com/mars2workshop/, where our updates and announcements of\nupcoming events will be continuously provided."
                },
                "authors": [
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Shengwu Xiong"
                    },
                    {
                        "name": "Jiajun Zhang"
                    },
                    {
                        "name": "Yaxiong Chen"
                    },
                    {
                        "name": "Bowen Zhou"
                    },
                    {
                        "name": "Chen Change Loy"
                    },
                    {
                        "name": "David A. Clifton"
                    },
                    {
                        "name": "Kyoung Mu Lee"
                    },
                    {
                        "name": "Luc Van Gool"
                    },
                    {
                        "name": "Ruiming He"
                    },
                    {
                        "name": "Ruilin Yao"
                    },
                    {
                        "name": "Xinwei Long"
                    },
                    {
                        "name": "Jirui Huang"
                    },
                    {
                        "name": "Kai Tian"
                    },
                    {
                        "name": "Sa Yang"
                    },
                    {
                        "name": "Yihua Shao"
                    },
                    {
                        "name": "Jin Feng"
                    },
                    {
                        "name": "Yue Zhong"
                    },
                    {
                        "name": "Jiakai Zhou"
                    },
                    {
                        "name": "Cheng Tang"
                    },
                    {
                        "name": "Tianyu Zou"
                    },
                    {
                        "name": "Yifang Zhang"
                    },
                    {
                        "name": "Junming Liang"
                    },
                    {
                        "name": "Guoyou Li"
                    },
                    {
                        "name": "Zhaoxiang Wang"
                    },
                    {
                        "name": "Qiang Zhou"
                    },
                    {
                        "name": "Yichen Zhao"
                    },
                    {
                        "name": "Shili Xiong"
                    },
                    {
                        "name": "Hyeongjin Nam"
                    },
                    {
                        "name": "Jaerin Lee"
                    },
                    {
                        "name": "Jaeyoung Chung"
                    },
                    {
                        "name": "JoonKyu Park"
                    },
                    {
                        "name": "Junghun Oh"
                    },
                    {
                        "name": "Kanggeon Lee"
                    },
                    {
                        "name": "Wooseok Lee"
                    },
                    {
                        "name": "Juneyoung Ro"
                    },
                    {
                        "name": "Turghun Osman"
                    },
                    {
                        "name": "Can Hu"
                    },
                    {
                        "name": "Chaoyang Liao"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Chengcheng Han"
                    },
                    {
                        "name": "Chenhao Qiu"
                    },
                    {
                        "name": "Chong Peng"
                    },
                    {
                        "name": "Cong Xu"
                    },
                    {
                        "name": "Dailin Li"
                    },
                    {
                        "name": "Feiyu Wang"
                    },
                    {
                        "name": "Feng Gao"
                    },
                    {
                        "name": "Guibo Zhu"
                    },
                    {
                        "name": "Guopeng Tang"
                    },
                    {
                        "name": "Haibo Lu"
                    },
                    {
                        "name": "Han Fang"
                    },
                    {
                        "name": "Han Qi"
                    },
                    {
                        "name": "Hanxiao Wu"
                    },
                    {
                        "name": "Haobo Cheng"
                    },
                    {
                        "name": "Hongbo Sun"
                    },
                    {
                        "name": "Hongyao Chen"
                    },
                    {
                        "name": "Huayong Hu"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Jiaheng Ma"
                    },
                    {
                        "name": "Jiang Yu"
                    },
                    {
                        "name": "Jianing Wang"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Jing He"
                    },
                    {
                        "name": "Jinglin Zhou"
                    },
                    {
                        "name": "Jingxuan Li"
                    },
                    {
                        "name": "Josef Kittler"
                    },
                    {
                        "name": "Lihao Zheng"
                    },
                    {
                        "name": "Linnan Zhao"
                    },
                    {
                        "name": "Mengxi Jia"
                    },
                    {
                        "name": "Muyang Yan"
                    },
                    {
                        "name": "Nguyen Thanh Thien"
                    },
                    {
                        "name": "Pu Luo"
                    },
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Shien Song"
                    },
                    {
                        "name": "Shijie Dong"
                    },
                    {
                        "name": "Shuai Shao"
                    },
                    {
                        "name": "Shutao Li"
                    },
                    {
                        "name": "Taofeng Xue"
                    },
                    {
                        "name": "Tianyang Xu"
                    },
                    {
                        "name": "Tianyi Gao"
                    },
                    {
                        "name": "Tingting Li"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Weiyang Su"
                    },
                    {
                        "name": "Xiaodong Dong"
                    },
                    {
                        "name": "Xiao-Jun Wu"
                    },
                    {
                        "name": "Xiaopeng Zhou"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Xin Wei"
                    },
                    {
                        "name": "Xinyi You"
                    },
                    {
                        "name": "Xudong Kang"
                    },
                    {
                        "name": "Xujie Zhou"
                    },
                    {
                        "name": "Xusheng Liu"
                    },
                    {
                        "name": "Yanan Wang"
                    },
                    {
                        "name": "Yanbin Huang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Yanglin Deng"
                    },
                    {
                        "name": "Yashu Kang"
                    },
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Yi Wen"
                    },
                    {
                        "name": "Yicen Tian"
                    },
                    {
                        "name": "Yilin Tao"
                    },
                    {
                        "name": "Yin Tang"
                    },
                    {
                        "name": "Yipeng Lin"
                    },
                    {
                        "name": "Yiqing Wang"
                    },
                    {
                        "name": "Yiting Xi"
                    },
                    {
                        "name": "Yongkang Yu"
                    },
                    {
                        "name": "Yumei Li"
                    },
                    {
                        "name": "Yuxin Qin"
                    },
                    {
                        "name": "Yuying Chen"
                    },
                    {
                        "name": "Yuzhe Cen"
                    },
                    {
                        "name": "Zhaofan Zou"
                    },
                    {
                        "name": "Zhaohong Liu"
                    },
                    {
                        "name": "Zhehao Shen"
                    },
                    {
                        "name": "Zhenglin Du"
                    },
                    {
                        "name": "Zhengyang Li"
                    },
                    {
                        "name": "Zhenni Huang"
                    },
                    {
                        "name": "Zhenwei Shao"
                    },
                    {
                        "name": "Zhilong Song"
                    },
                    {
                        "name": "Zhiyong Feng"
                    },
                    {
                        "name": "Zhiyu Wang"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Ziang Li"
                    },
                    {
                        "name": "Zihan Zhai"
                    },
                    {
                        "name": "Zijian Zhang"
                    },
                    {
                        "name": "Ziyang Peng"
                    },
                    {
                        "name": "Ziyun Xiao"
                    },
                    {
                        "name": "Zongshu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zongshu Li"
                },
                "author": "Zongshu Li",
                "arxiv_comment": "ICCV 2025 MARS2 Workshop and Challenge \"Multimodal Reasoning and Slow\n  Thinking in the Large Model Era: Towards System 2 and Beyond''",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14136v1",
                "updated": "2025-09-17T16:16:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    16,
                    30,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T16:16:30Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    16,
                    30,
                    2,
                    260,
                    0
                ],
                "title": "SV-Mixer: Replacing the Transformer Encoder with Lightweight MLPs for\n  Self-Supervised Model Compression in Speaker Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SV-Mixer: Replacing the Transformer Encoder with Lightweight MLPs for\n  Self-Supervised Model Compression in Speaker Verification"
                },
                "summary": "Self-supervised learning (SSL) has pushed speaker verification accuracy close\nto state-of-the-art levels, but the Transformer backbones used in most SSL\nencoders hinder on-device and real-time deployment. Prior compression work\ntrims layer depth or width yet still inherits the quadratic cost of\nself-attention. We propose SV-Mixer, the first fully MLP-based student encoder\nfor SSL distillation. SV-Mixer replaces Transformer with three lightweight\nmodules: Multi-Scale Mixing for multi-resolution temporal features,\nLocal-Global Mixing for frame-to-utterance context, and Group Channel Mixing\nfor spectral subspaces. Distilled from WavLM, SV-Mixer outperforms a\nTransformer student by 14.6% while cutting parameters and GMACs by over half,\nand at 75% compression, it closely matches the teacher's performance. Our\nresults show that attention-free SSL students can deliver teacher-level\naccuracy with hardware-friendly footprints, opening the door to robust\non-device speaker verification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-supervised learning (SSL) has pushed speaker verification accuracy close\nto state-of-the-art levels, but the Transformer backbones used in most SSL\nencoders hinder on-device and real-time deployment. Prior compression work\ntrims layer depth or width yet still inherits the quadratic cost of\nself-attention. We propose SV-Mixer, the first fully MLP-based student encoder\nfor SSL distillation. SV-Mixer replaces Transformer with three lightweight\nmodules: Multi-Scale Mixing for multi-resolution temporal features,\nLocal-Global Mixing for frame-to-utterance context, and Group Channel Mixing\nfor spectral subspaces. Distilled from WavLM, SV-Mixer outperforms a\nTransformer student by 14.6% while cutting parameters and GMACs by over half,\nand at 75% compression, it closely matches the teacher's performance. Our\nresults show that attention-free SSL students can deliver teacher-level\naccuracy with hardware-friendly footprints, opening the door to robust\non-device speaker verification."
                },
                "authors": [
                    {
                        "name": "Jungwoo Heo"
                    },
                    {
                        "name": "Hyun-seo Shin"
                    },
                    {
                        "name": "Chan-yeong Lim"
                    },
                    {
                        "name": "Kyo-won Koo"
                    },
                    {
                        "name": "Seung-bin Kim"
                    },
                    {
                        "name": "Jisoo Son"
                    },
                    {
                        "name": "Ha-Jin Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ha-Jin Yu"
                },
                "author": "Ha-Jin Yu",
                "arxiv_comment": "8 pages, 5 figures, accepted at IEEE ASRU 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14132v1",
                "updated": "2025-09-17T16:13:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    13,
                    37,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T16:13:37Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    13,
                    37,
                    2,
                    260,
                    0
                ],
                "title": "When Avatars Have Personality: Effects on Engagement and Communication\n  in Immersive Medical Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Avatars Have Personality: Effects on Engagement and Communication\n  in Immersive Medical Training"
                },
                "summary": "While virtual reality (VR) excels at simulating physical environments, its\neffectiveness for training complex interpersonal skills is limited by a lack of\npsychologically plausible virtual humans. This is a critical gap in high-stakes\ndomains like medical education, where communication is a core competency. This\npaper introduces a framework that integrates large language models (LLMs) into\nimmersive VR to create medically coherent virtual patients with distinct,\nconsistent personalities, built on a modular architecture that decouples\npersonality from clinical data. We evaluated our system in a mixed-method,\nwithin-subjects study with licensed physicians who engaged in simulated\nconsultations. Results demonstrate that the approach is not only feasible but\nis also perceived by physicians as a highly rewarding and effective training\nenhancement. Furthermore, our analysis uncovers critical design principles,\nincluding a ``realism-verbosity paradox\" where less communicative agents can\nseem more artificial, and the need for challenges to be perceived as authentic\nto be instructive. This work provides a validated framework and key insights\nfor developing the next generation of socially intelligent VR training\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While virtual reality (VR) excels at simulating physical environments, its\neffectiveness for training complex interpersonal skills is limited by a lack of\npsychologically plausible virtual humans. This is a critical gap in high-stakes\ndomains like medical education, where communication is a core competency. This\npaper introduces a framework that integrates large language models (LLMs) into\nimmersive VR to create medically coherent virtual patients with distinct,\nconsistent personalities, built on a modular architecture that decouples\npersonality from clinical data. We evaluated our system in a mixed-method,\nwithin-subjects study with licensed physicians who engaged in simulated\nconsultations. Results demonstrate that the approach is not only feasible but\nis also perceived by physicians as a highly rewarding and effective training\nenhancement. Furthermore, our analysis uncovers critical design principles,\nincluding a ``realism-verbosity paradox\" where less communicative agents can\nseem more artificial, and the need for challenges to be perceived as authentic\nto be instructive. This work provides a validated framework and key insights\nfor developing the next generation of socially intelligent VR training\nenvironments."
                },
                "authors": [
                    {
                        "name": "Julia S. Dollis"
                    },
                    {
                        "name": "Iago A. Brito"
                    },
                    {
                        "name": "Fernanda B. Färber"
                    },
                    {
                        "name": "Pedro S. F. B. Ribeiro"
                    },
                    {
                        "name": "Rafael T. Sousa"
                    },
                    {
                        "name": "Arlindo R. Galvão Filho"
                    }
                ],
                "author_detail": {
                    "name": "Arlindo R. Galvão Filho"
                },
                "author": "Arlindo R. Galvão Filho",
                "arxiv_comment": "8 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14128v1",
                "updated": "2025-09-17T16:08:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    8,
                    46,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T16:08:46Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    8,
                    46,
                    2,
                    260,
                    0
                ],
                "title": "Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance\n  Models for Multilingual ASR and AST",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance\n  Models for Multilingual ASR and AST"
                },
                "summary": "This report introduces Canary-1B-v2, a fast, robust multilingual model for\nAutomatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built\nwith a FastConformer encoder and Transformer decoder, it supports 25 languages\nprimarily European. The model was trained on 1.7M hours of total data samples,\nincluding Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce\nhallucinations for ASR and AST. We describe its two-stage pre-training and\nfine-tuning process with dynamic data balancing, as well as experiments with an\nnGPT encoder. Results show nGPT scales well with massive data, while\nFastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the\nNeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable\nsegment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2\noutperforms Whisper-large-v3 on English ASR while being 10x faster, and\ndelivers competitive multilingual ASR and AST performance against larger models\nlike Seamless-M4T-v2-large and LLM-based systems. We also release\nParakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the\nsame 25 languages with just 600M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report introduces Canary-1B-v2, a fast, robust multilingual model for\nAutomatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built\nwith a FastConformer encoder and Transformer decoder, it supports 25 languages\nprimarily European. The model was trained on 1.7M hours of total data samples,\nincluding Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce\nhallucinations for ASR and AST. We describe its two-stage pre-training and\nfine-tuning process with dynamic data balancing, as well as experiments with an\nnGPT encoder. Results show nGPT scales well with massive data, while\nFastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the\nNeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable\nsegment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2\noutperforms Whisper-large-v3 on English ASR while being 10x faster, and\ndelivers competitive multilingual ASR and AST performance against larger models\nlike Seamless-M4T-v2-large and LLM-based systems. We also release\nParakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the\nsame 25 languages with just 600M parameters."
                },
                "authors": [
                    {
                        "name": "Monica Sekoyan"
                    },
                    {
                        "name": "Nithin Rao Koluguri"
                    },
                    {
                        "name": "Nune Tadevosyan"
                    },
                    {
                        "name": "Piotr Zelasko"
                    },
                    {
                        "name": "Travis Bartley"
                    },
                    {
                        "name": "Nick Karpov"
                    },
                    {
                        "name": "Jagadeesh Balam"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Mini Version of it Submitted to ICASSP 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24621v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24621v2",
                "updated": "2025-09-17T15:53:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    53,
                    19,
                    2,
                    260,
                    0
                ],
                "published": "2025-05-30T14:12:07Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    12,
                    7,
                    4,
                    150,
                    0
                ],
                "title": "Benchmarking Large Language Models for Cryptanalysis and Side-Channel\n  Vulnerabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Large Language Models for Cryptanalysis and Side-Channel\n  Vulnerabilities"
                },
                "summary": "Recent advancements in large language models (LLMs) have transformed natural\nlanguage understanding and generation, leading to extensive benchmarking across\ndiverse tasks. However, cryptanalysis - a critical area for data security and\nits connection to LLMs' generalization abilities - remains underexplored in LLM\nevaluations. To address this gap, we evaluate the cryptanalytic potential of\nstate-of-the-art LLMs on ciphertexts produced by a range of cryptographic\nalgorithms. We introduce a benchmark dataset of diverse plaintexts, spanning\nmultiple domains, lengths, writing styles, and topics, paired with their\nencrypted versions. Using zero-shot and few-shot settings along with\nchain-of-thought prompting, we assess LLMs' decryption success rate and discuss\ntheir comprehension abilities. Our findings reveal key insights into LLMs'\nstrengths and limitations in side-channel scenarios and raise concerns about\ntheir susceptibility to under-generalization-related attacks. This research\nhighlights the dual-use nature of LLMs in security contexts and contributes to\nthe ongoing discussion on AI safety and security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have transformed natural\nlanguage understanding and generation, leading to extensive benchmarking across\ndiverse tasks. However, cryptanalysis - a critical area for data security and\nits connection to LLMs' generalization abilities - remains underexplored in LLM\nevaluations. To address this gap, we evaluate the cryptanalytic potential of\nstate-of-the-art LLMs on ciphertexts produced by a range of cryptographic\nalgorithms. We introduce a benchmark dataset of diverse plaintexts, spanning\nmultiple domains, lengths, writing styles, and topics, paired with their\nencrypted versions. Using zero-shot and few-shot settings along with\nchain-of-thought prompting, we assess LLMs' decryption success rate and discuss\ntheir comprehension abilities. Our findings reveal key insights into LLMs'\nstrengths and limitations in side-channel scenarios and raise concerns about\ntheir susceptibility to under-generalization-related attacks. This research\nhighlights the dual-use nature of LLMs in security contexts and contributes to\nthe ongoing discussion on AI safety and security."
                },
                "authors": [
                    {
                        "name": "Utsav Maskey"
                    },
                    {
                        "name": "Chencheng Zhu"
                    },
                    {
                        "name": "Usman Naseem"
                    }
                ],
                "author_detail": {
                    "name": "Usman Naseem"
                },
                "author": "Usman Naseem",
                "arxiv_comment": "EMNLP'25 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24621v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24621v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14093v1",
                "updated": "2025-09-17T15:33:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    33,
                    44,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T15:33:44Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    33,
                    44,
                    2,
                    260,
                    0
                ],
                "title": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A\n  Self-Optimizing Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A\n  Self-Optimizing Framework"
                },
                "summary": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nprompting intermediate steps, improving accuracy and robustness in arithmetic,\nlogic, and commonsense tasks. However, this benefit comes with high\ncomputational costs: longer outputs increase latency, memory usage, and\nKV-cache demands. These issues are especially critical in software engineering\ntasks where concise and deterministic outputs are required. To investigate\nthese trade-offs, we conduct an empirical study based on code generation\nbenchmarks. The results reveal that longer CoT does not always help. Excessive\nreasoning often causes truncation, accuracy drops, and latency up to five times\nhigher, with failed outputs consistently longer than successful ones. These\nfindings challenge the assumption that longer reasoning is inherently better\nand highlight the need for adaptive CoT control. Motivated by this, we propose\nSEER (Self-Enhancing Efficient Reasoning), an adaptive framework that\ncompresses CoT while preserving accuracy. SEER combines Best-of-N sampling with\ntask-aware adaptive filtering, dynamically adjusting thresholds based on\npre-inference outputs to reduce verbosity and computational overhead. We then\nevaluate SEER on three software engineering tasks and one math task. On\naverage, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,\nand eliminates most infinite loops. These results demonstrate SEER as a\npractical method to make CoT-enhanced LLMs more efficient and robust, even\nunder resource constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nprompting intermediate steps, improving accuracy and robustness in arithmetic,\nlogic, and commonsense tasks. However, this benefit comes with high\ncomputational costs: longer outputs increase latency, memory usage, and\nKV-cache demands. These issues are especially critical in software engineering\ntasks where concise and deterministic outputs are required. To investigate\nthese trade-offs, we conduct an empirical study based on code generation\nbenchmarks. The results reveal that longer CoT does not always help. Excessive\nreasoning often causes truncation, accuracy drops, and latency up to five times\nhigher, with failed outputs consistently longer than successful ones. These\nfindings challenge the assumption that longer reasoning is inherently better\nand highlight the need for adaptive CoT control. Motivated by this, we propose\nSEER (Self-Enhancing Efficient Reasoning), an adaptive framework that\ncompresses CoT while preserving accuracy. SEER combines Best-of-N sampling with\ntask-aware adaptive filtering, dynamically adjusting thresholds based on\npre-inference outputs to reduce verbosity and computational overhead. We then\nevaluate SEER on three software engineering tasks and one math task. On\naverage, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,\nand eliminates most infinite loops. These results demonstrate SEER as a\npractical method to make CoT-enhanced LLMs more efficient and robust, even\nunder resource constraints."
                },
                "authors": [
                    {
                        "name": "Kerui Huang"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Tongtong Xu"
                    },
                    {
                        "name": "Lingfeng Bao"
                    },
                    {
                        "name": "Xin Xia"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xia"
                },
                "author": "Xin Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12443v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12443v2",
                "updated": "2025-09-17T15:29:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    29,
                    42,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-15T20:50:15Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    20,
                    50,
                    15,
                    0,
                    258,
                    0
                ],
                "title": "From Legacy Fortran to Portable Kokkos: An Autonomous Agentic AI\n  Workflow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Legacy Fortran to Portable Kokkos: An Autonomous Agentic AI\n  Workflow"
                },
                "summary": "Scientific applications continue to rely on legacy Fortran codebases\noriginally developed for homogeneous, CPU-based systems. As High-Performance\nComputing (HPC) shifts toward heterogeneous GPU-accelerated architectures, many\naccelerators lack native Fortran bindings, creating an urgent need to modernize\nlegacy codes for portability. Frameworks like Kokkos provide performance\nportability and a single-source C++ abstraction, but manual Fortran-to-Kokkos\nporting demands significant expertise and time. Large language models (LLMs)\nhave shown promise in source-to-source code generation, yet their use in fully\nautonomous workflows for translating and optimizing parallel code remains\nlargely unexplored, especially for performance portability across diverse\nhardware. This paper presents an agentic AI workflow where specialized LLM\n\"agents\" collaborate to translate, validate, compile, run, test, debug, and\noptimize Fortran kernels into portable Kokkos C++ programs. Results show the\npipeline modernizes a range of benchmark kernels, producing\nperformance-portable Kokkos codes across hardware partitions. Paid OpenAI\nmodels such as GPT-5 and o4-mini-high executed the workflow for only a few U.S.\ndollars, generating optimized codes that surpassed Fortran baselines, whereas\nopen-source models like Llama4-Maverick often failed to yield functional codes.\nThis work demonstrates the feasibility of agentic AI for Fortran-to-Kokkos\ntransformation and offers a pathway for autonomously modernizing legacy\nscientific applications to run portably and efficiently on diverse\nsupercomputers. It further highlights the potential of LLM-driven agentic\nsystems to perform structured, domain-specific reasoning tasks in scientific\nand systems-oriented applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific applications continue to rely on legacy Fortran codebases\noriginally developed for homogeneous, CPU-based systems. As High-Performance\nComputing (HPC) shifts toward heterogeneous GPU-accelerated architectures, many\naccelerators lack native Fortran bindings, creating an urgent need to modernize\nlegacy codes for portability. Frameworks like Kokkos provide performance\nportability and a single-source C++ abstraction, but manual Fortran-to-Kokkos\nporting demands significant expertise and time. Large language models (LLMs)\nhave shown promise in source-to-source code generation, yet their use in fully\nautonomous workflows for translating and optimizing parallel code remains\nlargely unexplored, especially for performance portability across diverse\nhardware. This paper presents an agentic AI workflow where specialized LLM\n\"agents\" collaborate to translate, validate, compile, run, test, debug, and\noptimize Fortran kernels into portable Kokkos C++ programs. Results show the\npipeline modernizes a range of benchmark kernels, producing\nperformance-portable Kokkos codes across hardware partitions. Paid OpenAI\nmodels such as GPT-5 and o4-mini-high executed the workflow for only a few U.S.\ndollars, generating optimized codes that surpassed Fortran baselines, whereas\nopen-source models like Llama4-Maverick often failed to yield functional codes.\nThis work demonstrates the feasibility of agentic AI for Fortran-to-Kokkos\ntransformation and offers a pathway for autonomously modernizing legacy\nscientific applications to run portably and efficiently on diverse\nsupercomputers. It further highlights the potential of LLM-driven agentic\nsystems to perform structured, domain-specific reasoning tasks in scientific\nand systems-oriented applications."
                },
                "authors": [
                    {
                        "name": "Sparsh Gupta"
                    },
                    {
                        "name": "Kamalavasan Kamalakkannan"
                    },
                    {
                        "name": "Maxim Moraru"
                    },
                    {
                        "name": "Galen Shipman"
                    },
                    {
                        "name": "Patrick Diehl"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Diehl"
                },
                "author": "Patrick Diehl",
                "arxiv_comment": "11 pages, 6 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12443v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12443v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17534v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17534v4",
                "updated": "2025-09-17T15:02:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    2,
                    41,
                    2,
                    260,
                    0
                ],
                "published": "2025-03-21T20:31:47Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    20,
                    31,
                    47,
                    4,
                    80,
                    0
                ],
                "title": "MetaSel: A Test Selection Approach for Fine-tuned DNN Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaSel: A Test Selection Approach for Fine-tuned DNN Models"
                },
                "summary": "Deep Neural Networks (DNNs) face challenges during deployment due to\ncovariate shift, i.e., data distribution shifts between development and\ndeployment contexts. Fine-tuning adapts pre-trained models to new contexts\nrequiring smaller labeled sets. However, testing fine-tuned models under\nconstrained labeling budgets remains a critical challenge. This paper\nintroduces MetaSel, a new approach tailored for DNN models that have been\nfine-tuned to address covariate shift, to select tests from unlabeled inputs.\nMetaSel assumes that fine-tuned and pre-trained models share related data\ndistributions and exhibit similar behaviors for many inputs. However, their\nbehaviors diverge within the input subspace where fine-tuning alters decision\nboundaries, making those inputs more prone to misclassification. Unlike general\napproaches that rely solely on the DNN model and its input set, MetaSel\nleverages information from both the fine-tuned and pre-trained models and their\nbehavioral differences to estimate misclassification probability for unlabeled\ntest inputs, enabling more effective test selection. Our extensive empirical\nevaluation, comparing MetaSel against 11 state-of-the-art approaches and\ninvolving 68 fine-tuned models across weak, medium, and strong distribution\nshifts, demonstrates that MetaSel consistently delivers significant\nimprovements in Test Relative Coverage (TRC) over existing baselines,\nparticularly under highly constrained labeling budgets. MetaSel shows average\nTRC improvements of 28.46% to 56.18% over the most frequent second-best\nbaselines while maintaining a high TRC median and low variability. Our results\nconfirm MetaSel's practicality, robustness, and cost-effectiveness for test\nselection in the context of fine-tuned models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks (DNNs) face challenges during deployment due to\ncovariate shift, i.e., data distribution shifts between development and\ndeployment contexts. Fine-tuning adapts pre-trained models to new contexts\nrequiring smaller labeled sets. However, testing fine-tuned models under\nconstrained labeling budgets remains a critical challenge. This paper\nintroduces MetaSel, a new approach tailored for DNN models that have been\nfine-tuned to address covariate shift, to select tests from unlabeled inputs.\nMetaSel assumes that fine-tuned and pre-trained models share related data\ndistributions and exhibit similar behaviors for many inputs. However, their\nbehaviors diverge within the input subspace where fine-tuning alters decision\nboundaries, making those inputs more prone to misclassification. Unlike general\napproaches that rely solely on the DNN model and its input set, MetaSel\nleverages information from both the fine-tuned and pre-trained models and their\nbehavioral differences to estimate misclassification probability for unlabeled\ntest inputs, enabling more effective test selection. Our extensive empirical\nevaluation, comparing MetaSel against 11 state-of-the-art approaches and\ninvolving 68 fine-tuned models across weak, medium, and strong distribution\nshifts, demonstrates that MetaSel consistently delivers significant\nimprovements in Test Relative Coverage (TRC) over existing baselines,\nparticularly under highly constrained labeling budgets. MetaSel shows average\nTRC improvements of 28.46% to 56.18% over the most frequent second-best\nbaselines while maintaining a high TRC median and low variability. Our results\nconfirm MetaSel's practicality, robustness, and cost-effectiveness for test\nselection in the context of fine-tuned models."
                },
                "authors": [
                    {
                        "name": "Amin Abbasishahkoo"
                    },
                    {
                        "name": "Mahboubeh Dadkhah"
                    },
                    {
                        "name": "Lionel Briand"
                    },
                    {
                        "name": "Dayi Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dayi Lin"
                },
                "author": "Dayi Lin",
                "arxiv_doi": "10.1109/TSE.2025.3612253",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TSE.2025.3612253",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.17534v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17534v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "EEE Transactions on Software Engineering (TSE). (2025)",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14049v1",
                "updated": "2025-09-17T14:53:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    53,
                    56,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T14:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    53,
                    56,
                    2,
                    260,
                    0
                ],
                "title": "Comprehensive Evaluation of CNN-Based Audio Tagging Models on\n  Resource-Constrained Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive Evaluation of CNN-Based Audio Tagging Models on\n  Resource-Constrained Devices"
                },
                "summary": "Convolutional Neural Networks (CNNs) have demonstrated exceptional\nperformance in audio tagging tasks. However, deploying these models on\nresource-constrained devices like the Raspberry Pi poses challenges related to\ncomputational efficiency and thermal management. In this paper, a comprehensive\nevaluation of multiple convolutional neural network (CNN) architectures for\naudio tagging on the Raspberry Pi is conducted, encompassing all 1D and 2D\nmodels from the Pretrained Audio Neural Networks (PANNs) framework, a\nConvNeXt-based model adapted for audio classification, as well as MobileNetV3\narchitectures. In addition, two PANNs-derived networks, CNN9 and CNN13,\nrecently proposed, are also evaluated. To enhance deployment efficiency and\nportability across diverse hardware platforms, all models are converted to the\nOpen Neural Network Exchange (ONNX) format. Unlike previous works that focus on\na single model, our analysis encompasses a broader range of architectures and\ninvolves continuous 24-hour inference sessions to assess performance stability.\nOur experiments reveal that, with appropriate model selection and optimization,\nit is possible to maintain consistent inference latency and manage thermal\nbehavior effectively over extended periods. These findings provide valuable\ninsights for deploying audio tagging models in real-world edge computing\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convolutional Neural Networks (CNNs) have demonstrated exceptional\nperformance in audio tagging tasks. However, deploying these models on\nresource-constrained devices like the Raspberry Pi poses challenges related to\ncomputational efficiency and thermal management. In this paper, a comprehensive\nevaluation of multiple convolutional neural network (CNN) architectures for\naudio tagging on the Raspberry Pi is conducted, encompassing all 1D and 2D\nmodels from the Pretrained Audio Neural Networks (PANNs) framework, a\nConvNeXt-based model adapted for audio classification, as well as MobileNetV3\narchitectures. In addition, two PANNs-derived networks, CNN9 and CNN13,\nrecently proposed, are also evaluated. To enhance deployment efficiency and\nportability across diverse hardware platforms, all models are converted to the\nOpen Neural Network Exchange (ONNX) format. Unlike previous works that focus on\na single model, our analysis encompasses a broader range of architectures and\ninvolves continuous 24-hour inference sessions to assess performance stability.\nOur experiments reveal that, with appropriate model selection and optimization,\nit is possible to maintain consistent inference latency and manage thermal\nbehavior effectively over extended periods. These findings provide valuable\ninsights for deploying audio tagging models in real-world edge computing\nscenarios."
                },
                "authors": [
                    {
                        "name": "Jordi Grau-Haro"
                    },
                    {
                        "name": "Ruben Ribes-Serrano"
                    },
                    {
                        "name": "Javier Naranjo-Alcazar"
                    },
                    {
                        "name": "Marta Garcia-Ballesteros"
                    },
                    {
                        "name": "Pedro Zuccarello"
                    }
                ],
                "author_detail": {
                    "name": "Pedro Zuccarello"
                },
                "author": "Pedro Zuccarello",
                "arxiv_comment": "Accepted at Computing Conference 2026, London, UK",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13668v2",
                "updated": "2025-09-17T14:47:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    47,
                    17,
                    2,
                    260,
                    0
                ],
                "published": "2025-05-19T19:16:37Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    19,
                    16,
                    37,
                    0,
                    139,
                    0
                ],
                "title": "MAFA: A multi-agent framework for annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAFA: A multi-agent framework for annotation"
                },
                "summary": "Modern consumer banking applications require accurate and efficient retrieval\nof information in response to user queries. Mapping user utterances to the most\nrelevant Frequently Asked Questions (FAQs) is a crucial component of these\nsystems. Traditional approaches often rely on a single model or technique,\nwhich may not capture the nuances of diverse user inquiries. In this paper, we\nintroduce a multi-agent framework for FAQ annotation that combines multiple\nspecialized agents with different approaches and a judge agent that reranks\ncandidates to produce optimal results. Our agents utilize a structured\nreasoning approach inspired by Attentive Reasoning Queries (ARQs), which guides\nthem through systematic reasoning steps using targeted, task-specific JSON\nqueries. Our framework features a few-shot example strategy, where each agent\nreceives different few-shots, enhancing ensemble diversity and coverage of the\nquery space. We evaluate our framework on a real-world major bank dataset as\nwell as public benchmark datasets (LCQMC and FiQA), demonstrating significant\nimprovements over single-agent approaches across multiple metrics, including a\n14% increase in Top-1 accuracy, an 18% increase in Top-5 accuracy, and a 12%\nimprovement in Mean Reciprocal Rank on our dataset, and similar gains on public\nbenchmarks when compared with traditional and single-agent annotation\ntechniques. Our framework is particularly effective at handling ambiguous\nqueries, making it well-suited for deployment in production banking\napplications while showing strong generalization capabilities across different\ndomains and languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern consumer banking applications require accurate and efficient retrieval\nof information in response to user queries. Mapping user utterances to the most\nrelevant Frequently Asked Questions (FAQs) is a crucial component of these\nsystems. Traditional approaches often rely on a single model or technique,\nwhich may not capture the nuances of diverse user inquiries. In this paper, we\nintroduce a multi-agent framework for FAQ annotation that combines multiple\nspecialized agents with different approaches and a judge agent that reranks\ncandidates to produce optimal results. Our agents utilize a structured\nreasoning approach inspired by Attentive Reasoning Queries (ARQs), which guides\nthem through systematic reasoning steps using targeted, task-specific JSON\nqueries. Our framework features a few-shot example strategy, where each agent\nreceives different few-shots, enhancing ensemble diversity and coverage of the\nquery space. We evaluate our framework on a real-world major bank dataset as\nwell as public benchmark datasets (LCQMC and FiQA), demonstrating significant\nimprovements over single-agent approaches across multiple metrics, including a\n14% increase in Top-1 accuracy, an 18% increase in Top-5 accuracy, and a 12%\nimprovement in Mean Reciprocal Rank on our dataset, and similar gains on public\nbenchmarks when compared with traditional and single-agent annotation\ntechniques. Our framework is particularly effective at handling ambiguous\nqueries, making it well-suited for deployment in production banking\napplications while showing strong generalization capabilities across different\ndomains and languages."
                },
                "authors": [
                    {
                        "name": "Mahmood Hegazy"
                    },
                    {
                        "name": "Aaron Rodrigues"
                    },
                    {
                        "name": "Azzam Naeem"
                    }
                ],
                "author_detail": {
                    "name": "Azzam Naeem"
                },
                "author": "Azzam Naeem",
                "arxiv_journal_ref": "ECAI 2025 Workshop on AI in Finance",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18916v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18916v2",
                "updated": "2025-09-17T14:42:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    42,
                    2,
                    2,
                    260,
                    0
                ],
                "published": "2025-05-25T00:50:43Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    0,
                    50,
                    43,
                    6,
                    145,
                    0
                ],
                "title": "SCRum-9: Multilingual Stance Classification over Rumours on Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCRum-9: Multilingual Stance Classification over Rumours on Social Media"
                },
                "summary": "We introduce SCRum-9, the largest multilingual Stance Classification dataset\nfor Rumour analysis in 9 languages, containing 7,516 tweets from X. SCRum-9\ngoes beyond existing stance classification datasets by covering more languages,\nlinking examples to more fact-checked claims (2.1k), and including\nconfidence-related annotations from multiple annotators to account for intra-\nand inter-annotator variability. Annotations were made by at least two native\nspeakers per language, totalling more than 405 hours of annotation and 8,150\ndollars in compensation. Further, SCRum-9 is used to benchmark five large\nlanguage models (LLMs) and two multilingual masked language models (MLMs) in\nIn-Context Learning (ICL) and fine-tuning setups. This paper also innovates by\nexploring the use of multilingual synthetic data for rumour stance\nclassification, showing that even LLMs with weak ICL performance can produce\nvaluable synthetic data for fine-tuning small MLMs, enabling them to achieve\nhigher performance than zero-shot ICL in LLMs. Finally, we examine the\nrelationship between model predictions and human uncertainty on ambiguous cases\nfinding that model predictions often match the second-choice labels assigned by\nannotators, rather than diverging entirely from human judgments. SCRum-9 is\npublicly released to the research community with potential to foster further\nresearch on multilingual analysis of misleading narratives on social media.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SCRum-9, the largest multilingual Stance Classification dataset\nfor Rumour analysis in 9 languages, containing 7,516 tweets from X. SCRum-9\ngoes beyond existing stance classification datasets by covering more languages,\nlinking examples to more fact-checked claims (2.1k), and including\nconfidence-related annotations from multiple annotators to account for intra-\nand inter-annotator variability. Annotations were made by at least two native\nspeakers per language, totalling more than 405 hours of annotation and 8,150\ndollars in compensation. Further, SCRum-9 is used to benchmark five large\nlanguage models (LLMs) and two multilingual masked language models (MLMs) in\nIn-Context Learning (ICL) and fine-tuning setups. This paper also innovates by\nexploring the use of multilingual synthetic data for rumour stance\nclassification, showing that even LLMs with weak ICL performance can produce\nvaluable synthetic data for fine-tuning small MLMs, enabling them to achieve\nhigher performance than zero-shot ICL in LLMs. Finally, we examine the\nrelationship between model predictions and human uncertainty on ambiguous cases\nfinding that model predictions often match the second-choice labels assigned by\nannotators, rather than diverging entirely from human judgments. SCRum-9 is\npublicly released to the research community with potential to foster further\nresearch on multilingual analysis of misleading narratives on social media."
                },
                "authors": [
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Jake Vasilakes"
                    },
                    {
                        "name": "Zhixue Zhao"
                    },
                    {
                        "name": "Carolina Scarton"
                    }
                ],
                "author_detail": {
                    "name": "Carolina Scarton"
                },
                "author": "Carolina Scarton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18916v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18916v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08627v2",
                "updated": "2025-09-17T14:38:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    38,
                    15,
                    2,
                    260,
                    0
                ],
                "published": "2025-07-11T14:29:21Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    29,
                    21,
                    4,
                    192,
                    0
                ],
                "title": "NL in the Middle: Code Translation with LLMs and Intermediate\n  Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NL in the Middle: Code Translation with LLMs and Intermediate\n  Representations"
                },
                "summary": "Studies show that large language models (LLMs) produce buggy code\ntranslations. One promising avenue to improve translation accuracy is through\nintermediate representations, which provide structured guidance for the\ntranslation process. We investigate whether LLM-based code translation can\nbenefit from intermediate representations, specifically in the form of natural\nlanguage (NL) summaries and abstract syntax trees (ASTs). Since prompt\nengineering greatly affects LLM performance, we consider several ways to\nintegrate these representations, from one-shot to chain-of-thought (CoT)\nprompting. Using Open GPT4 8X7B and specialized StarCoder and CodeGen models on\npopular code translation benchmarks (CodeNet and AVATAR), we find that CoT with\nan intermediate NL summary performs best, with an increase of 13.8% and 6.7%,\nrespectively, in successful translations for the best-performing model (Open\nGPT4 8X7B) compared to the zero-shot prompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Studies show that large language models (LLMs) produce buggy code\ntranslations. One promising avenue to improve translation accuracy is through\nintermediate representations, which provide structured guidance for the\ntranslation process. We investigate whether LLM-based code translation can\nbenefit from intermediate representations, specifically in the form of natural\nlanguage (NL) summaries and abstract syntax trees (ASTs). Since prompt\nengineering greatly affects LLM performance, we consider several ways to\nintegrate these representations, from one-shot to chain-of-thought (CoT)\nprompting. Using Open GPT4 8X7B and specialized StarCoder and CodeGen models on\npopular code translation benchmarks (CodeNet and AVATAR), we find that CoT with\nan intermediate NL summary performs best, with an increase of 13.8% and 6.7%,\nrespectively, in successful translations for the best-performing model (Open\nGPT4 8X7B) compared to the zero-shot prompt."
                },
                "authors": [
                    {
                        "name": "Chi-en Amy Tai"
                    },
                    {
                        "name": "Pengyu Nie"
                    },
                    {
                        "name": "Lukasz Golab"
                    },
                    {
                        "name": "Alexander Wong"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Wong"
                },
                "author": "Alexander Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14034v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14034v1",
                "updated": "2025-09-17T14:34:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    34,
                    27,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T14:34:27Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    34,
                    27,
                    2,
                    260,
                    0
                ],
                "title": "Enhancing Multi-Agent Debate System Performance via Confidence\n  Expression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Multi-Agent Debate System Performance via Confidence\n  Expression"
                },
                "summary": "Generative Large Language Models (LLMs) have demonstrated remarkable\nperformance across a wide range of tasks. Recent research has introduced\nMulti-Agent Debate (MAD) systems, which leverage multiple LLMs to simulate\nhuman debate and thereby improve task performance. However, while some LLMs may\npossess superior knowledge or reasoning capabilities for specific tasks, they\noften struggle to clearly communicate this advantage during debates, in part\ndue to a lack of confidence expression. Moreover, inappropriate confidence\nexpression can cause agents in MAD systems to either stubbornly maintain\nincorrect beliefs or converge prematurely on suboptimal answers, ultimately\nreducing debate effectiveness and overall system performance. To address these\nchallenges, we propose incorporating confidence expression into MAD systems to\nallow LLMs to explicitly communicate their confidence levels. To validate this\napproach, we develop ConfMAD, a MAD framework that integrates confidence\nexpression throughout the debate process. Experimental results demonstrate the\neffectiveness of our method, and we further analyze how confidence influences\ndebate dynamics, offering insights into the design of confidence-aware MAD\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Large Language Models (LLMs) have demonstrated remarkable\nperformance across a wide range of tasks. Recent research has introduced\nMulti-Agent Debate (MAD) systems, which leverage multiple LLMs to simulate\nhuman debate and thereby improve task performance. However, while some LLMs may\npossess superior knowledge or reasoning capabilities for specific tasks, they\noften struggle to clearly communicate this advantage during debates, in part\ndue to a lack of confidence expression. Moreover, inappropriate confidence\nexpression can cause agents in MAD systems to either stubbornly maintain\nincorrect beliefs or converge prematurely on suboptimal answers, ultimately\nreducing debate effectiveness and overall system performance. To address these\nchallenges, we propose incorporating confidence expression into MAD systems to\nallow LLMs to explicitly communicate their confidence levels. To validate this\napproach, we develop ConfMAD, a MAD framework that integrates confidence\nexpression throughout the debate process. Experimental results demonstrate the\neffectiveness of our method, and we further analyze how confidence influences\ndebate dynamics, offering insights into the design of confidence-aware MAD\nsystems."
                },
                "authors": [
                    {
                        "name": "Zijie Lin"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_comment": "EMNLP'25 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14034v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14034v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14033v1",
                "updated": "2025-09-17T14:34:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    34,
                    2,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T14:34:02Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    34,
                    2,
                    2,
                    260,
                    0
                ],
                "title": "SAIL-VL2 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAIL-VL2 Technical Report"
                },
                "summary": "We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)\nfor comprehensive multimodal understanding and reasoning. As the successor to\nSAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B\nparameter scales across diverse image and video benchmarks, demonstrating\nstrong capabilities from fine-grained perception to complex reasoning. Three\ncore innovations drive its effectiveness. First, a large-scale data curation\npipeline with scoring and filtering strategies enhances both quality and\ndistribution across captioning, OCR, QA, and video data, improving training\nefficiency. Second, a progressive training framework begins with a powerful\npre-trained vision encoder (SAIL-ViT), advances through multimodal\npre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that\nsystematically strengthens model capabilities. Third, architectural advances\nextend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.\nWith these contributions, SAIL-VL2 demonstrates competitive performance across\n106 datasets and achieves state-of-the-art results on challenging reasoning\nbenchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass\nleaderboard, SAIL-VL2-2B ranks first among officially released open-source\nmodels under the 4B parameter scale, while serving as an efficient and\nextensible foundation for the open-source multimodal community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)\nfor comprehensive multimodal understanding and reasoning. As the successor to\nSAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B\nparameter scales across diverse image and video benchmarks, demonstrating\nstrong capabilities from fine-grained perception to complex reasoning. Three\ncore innovations drive its effectiveness. First, a large-scale data curation\npipeline with scoring and filtering strategies enhances both quality and\ndistribution across captioning, OCR, QA, and video data, improving training\nefficiency. Second, a progressive training framework begins with a powerful\npre-trained vision encoder (SAIL-ViT), advances through multimodal\npre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that\nsystematically strengthens model capabilities. Third, architectural advances\nextend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.\nWith these contributions, SAIL-VL2 demonstrates competitive performance across\n106 datasets and achieves state-of-the-art results on challenging reasoning\nbenchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass\nleaderboard, SAIL-VL2-2B ranks first among officially released open-source\nmodels under the 4B parameter scale, while serving as an efficient and\nextensible foundation for the open-source multimodal community."
                },
                "authors": [
                    {
                        "name": "Weijie Yin"
                    },
                    {
                        "name": "Yongjie Ye"
                    },
                    {
                        "name": "Fangxun Shu"
                    },
                    {
                        "name": "Yue Liao"
                    },
                    {
                        "name": "Zijian Kang"
                    },
                    {
                        "name": "Hongyuan Dong"
                    },
                    {
                        "name": "Haiyang Yu"
                    },
                    {
                        "name": "Dingkang Yang"
                    },
                    {
                        "name": "Jiacong Wang"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Wenzhuo Liu"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Shuicheng Yan"
                    },
                    {
                        "name": "Chao Feng"
                    }
                ],
                "author_detail": {
                    "name": "Chao Feng"
                },
                "author": "Chao Feng",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18506v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18506v4",
                "updated": "2025-09-17T14:32:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    32,
                    24,
                    2,
                    260,
                    0
                ],
                "published": "2024-11-27T16:48:24Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    16,
                    48,
                    24,
                    2,
                    332,
                    0
                ],
                "title": "LLM-ABBA: Understanding time series via symbolic approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-ABBA: Understanding time series via symbolic approximation"
                },
                "summary": "The success of large language models (LLMs) for time series has been\ndemonstrated in previous work. Utilizing a symbolic time series representation,\none can efficiently bridge the gap between LLMs and time series. However, the\nremaining challenge is to exploit the semantic information hidden in time\nseries by using symbols or existing tokens of LLMs, while aligning the\nembedding space of LLMs according to the hidden information of time series. The\nsymbolic time series approximation (STSA) method called adaptive Brownian\nbridge-based symbolic aggregation (ABBA) shows outstanding efficacy in\npreserving salient time series features by modeling time series patterns in\nterms of amplitude and period while using existing tokens of LLMs.\n  In this paper, we introduce a method, called LLM-ABBA, that integrates ABBA\ninto large language models for various downstream time series tasks. By\nsymbolizing time series, LLM-ABBA compares favorably to the recent\nstate-of-the-art (SOTA) in UCR and three medical time series classification\ntasks. Meanwhile, a fixed-polygonal chain trick in ABBA is introduced to\n\\kc{avoid obvious drifting} during prediction tasks by significantly mitigating\nthe effects of cumulative error arising from misused symbols during the\ntransition from symbols to numerical values. In time series regression tasks,\nLLM-ABBA achieves the new SOTA on Time Series Extrinsic Regression (TSER)\nbenchmarks. LLM-ABBA also shows competitive prediction capability compared to\nrecent SOTA time series prediction results. We believe this framework can also\nseamlessly extend to other time series tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of large language models (LLMs) for time series has been\ndemonstrated in previous work. Utilizing a symbolic time series representation,\none can efficiently bridge the gap between LLMs and time series. However, the\nremaining challenge is to exploit the semantic information hidden in time\nseries by using symbols or existing tokens of LLMs, while aligning the\nembedding space of LLMs according to the hidden information of time series. The\nsymbolic time series approximation (STSA) method called adaptive Brownian\nbridge-based symbolic aggregation (ABBA) shows outstanding efficacy in\npreserving salient time series features by modeling time series patterns in\nterms of amplitude and period while using existing tokens of LLMs.\n  In this paper, we introduce a method, called LLM-ABBA, that integrates ABBA\ninto large language models for various downstream time series tasks. By\nsymbolizing time series, LLM-ABBA compares favorably to the recent\nstate-of-the-art (SOTA) in UCR and three medical time series classification\ntasks. Meanwhile, a fixed-polygonal chain trick in ABBA is introduced to\n\\kc{avoid obvious drifting} during prediction tasks by significantly mitigating\nthe effects of cumulative error arising from misused symbols during the\ntransition from symbols to numerical values. In time series regression tasks,\nLLM-ABBA achieves the new SOTA on Time Series Extrinsic Regression (TSER)\nbenchmarks. LLM-ABBA also shows competitive prediction capability compared to\nrecent SOTA time series prediction results. We believe this framework can also\nseamlessly extend to other time series tasks."
                },
                "authors": [
                    {
                        "name": "Erin Carson"
                    },
                    {
                        "name": "Xinye Chen"
                    },
                    {
                        "name": "Cheng Kang"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Kang"
                },
                "author": "Cheng Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18506v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18506v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14030v1",
                "updated": "2025-09-17T14:31:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    31,
                    18,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T14:31:18Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    31,
                    18,
                    2,
                    260,
                    0
                ],
                "title": "CrowdAgent: Multi-Agent Managed Multi-Source Annotation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CrowdAgent: Multi-Agent Managed Multi-Source Annotation System"
                },
                "summary": "High-quality annotated data is a cornerstone of modern Natural Language\nProcessing (NLP). While recent methods begin to leverage diverse annotation\nsources-including Large Language Models (LLMs), Small Language Models (SLMs),\nand human experts-they often focus narrowly on the labeling step itself. A\ncritical gap remains in the holistic process control required to manage these\nsources dynamically, addressing complex scheduling and quality-cost trade-offs\nin a unified manner. Inspired by real-world crowdsourcing companies, we\nintroduce CrowdAgent, a multi-agent system that provides end-to-end process\ncontrol by integrating task assignment, data annotation, and quality/cost\nmanagement. It implements a novel methodology that rationally assigns tasks,\nenabling LLMs, SLMs, and human experts to advance synergistically in a\ncollaborative annotation workflow. We demonstrate the effectiveness of\nCrowdAgent through extensive experiments on six diverse multimodal\nclassification tasks. The source code and video demo are available at\nhttps://github.com/QMMMS/CrowdAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-quality annotated data is a cornerstone of modern Natural Language\nProcessing (NLP). While recent methods begin to leverage diverse annotation\nsources-including Large Language Models (LLMs), Small Language Models (SLMs),\nand human experts-they often focus narrowly on the labeling step itself. A\ncritical gap remains in the holistic process control required to manage these\nsources dynamically, addressing complex scheduling and quality-cost trade-offs\nin a unified manner. Inspired by real-world crowdsourcing companies, we\nintroduce CrowdAgent, a multi-agent system that provides end-to-end process\ncontrol by integrating task assignment, data annotation, and quality/cost\nmanagement. It implements a novel methodology that rationally assigns tasks,\nenabling LLMs, SLMs, and human experts to advance synergistically in a\ncollaborative annotation workflow. We demonstrate the effectiveness of\nCrowdAgent through extensive experiments on six diverse multimodal\nclassification tasks. The source code and video demo are available at\nhttps://github.com/QMMMS/CrowdAgent."
                },
                "authors": [
                    {
                        "name": "Maosheng Qin"
                    },
                    {
                        "name": "Renyu Zhu"
                    },
                    {
                        "name": "Mingxuan Xia"
                    },
                    {
                        "name": "Chenkai Chen"
                    },
                    {
                        "name": "Zhen Zhu"
                    },
                    {
                        "name": "Minmin Lin"
                    },
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Lu Xu"
                    },
                    {
                        "name": "Changjie Fan"
                    },
                    {
                        "name": "Runze Wu"
                    },
                    {
                        "name": "Haobo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haobo Wang"
                },
                "author": "Haobo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01658v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01658v2",
                "updated": "2025-09-17T14:29:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    29,
                    1,
                    2,
                    260,
                    0
                ],
                "published": "2025-03-03T15:32:02Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    15,
                    32,
                    2,
                    0,
                    62,
                    0
                ],
                "title": "CoPL: Collaborative Preference Learning for Personalizing LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoPL: Collaborative Preference Learning for Personalizing LLMs"
                },
                "summary": "Personalizing large language models (LLMs) is important for aligning outputs\nwith diverse user preferences, yet existing methods struggle with flexibility\nand generalization. We propose CoPL (Collaborative Preference Learning), a\ngraph-based collaborative filtering framework that models user-response\nrelationships to enhance preference estimation, particularly in sparse\nannotation settings. By integrating a mixture of LoRA experts, CoPL efficiently\nfine-tunes LLMs while dynamically balancing shared and user-specific\npreferences. Additionally, an optimization-free adaptation strategy enables\ngeneralization to unseen users without fine-tuning. Experiments on\nUltraFeedback-P demonstrate that CoPL outperforms existing personalized reward\nmodels, effectively capturing both common and controversial preferences, making\nit a scalable solution for personalized LLM alignment. The code is available at\nhttps://github.com/ml-postech/CoPL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalizing large language models (LLMs) is important for aligning outputs\nwith diverse user preferences, yet existing methods struggle with flexibility\nand generalization. We propose CoPL (Collaborative Preference Learning), a\ngraph-based collaborative filtering framework that models user-response\nrelationships to enhance preference estimation, particularly in sparse\nannotation settings. By integrating a mixture of LoRA experts, CoPL efficiently\nfine-tunes LLMs while dynamically balancing shared and user-specific\npreferences. Additionally, an optimization-free adaptation strategy enables\ngeneralization to unseen users without fine-tuning. Experiments on\nUltraFeedback-P demonstrate that CoPL outperforms existing personalized reward\nmodels, effectively capturing both common and controversial preferences, making\nit a scalable solution for personalized LLM alignment. The code is available at\nhttps://github.com/ml-postech/CoPL."
                },
                "authors": [
                    {
                        "name": "Youngbin Choi"
                    },
                    {
                        "name": "Seunghyuk Cho"
                    },
                    {
                        "name": "Minjong Lee"
                    },
                    {
                        "name": "MoonJeong Park"
                    },
                    {
                        "name": "Yesong Ko"
                    },
                    {
                        "name": "Jungseul Ok"
                    },
                    {
                        "name": "Dongwoo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Dongwoo Kim"
                },
                "author": "Dongwoo Kim",
                "arxiv_comment": "19pages, 13 figures, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01658v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01658v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18216v2",
                "updated": "2025-09-17T14:25:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    25,
                    49,
                    2,
                    260,
                    0
                ],
                "published": "2024-11-27T10:48:37Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    48,
                    37,
                    2,
                    332,
                    0
                ],
                "title": "Evaluating and Improving the Robustness of Security Attack Detectors\n  Generated by LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating and Improving the Robustness of Security Attack Detectors\n  Generated by LLMs"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in software development to\ngenerate functions, such as attack detectors, that implement security\nrequirements. A key challenge is ensuring the LLMs have enough knowledge to\naddress specific security requirements, such as information about existing\nattacks. For this, we propose an approach integrating Retrieval Augmented\nGeneration (RAG) and Self-Ranking into the LLM pipeline. RAG enhances the\nrobustness of the output by incorporating external knowledge sources, while the\nSelf-Ranking technique, inspired by the concept of Self-Consistency, generates\nmultiple reasoning paths and creates ranks to select the most robust detector.\nOur extensive empirical study targets code generated by LLMs to detect two\nprevalent injection attacks in web security: Cross-Site Scripting (XSS) and SQL\ninjection (SQLi). Results show a significant improvement in detection\nperformance while employing RAG and Self-Ranking, with an increase of up to\n71%pt (on average 37%pt) and up to 43%pt (on average 6%pt) in the F2-Score for\nXSS and SQLi detection, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in software development to\ngenerate functions, such as attack detectors, that implement security\nrequirements. A key challenge is ensuring the LLMs have enough knowledge to\naddress specific security requirements, such as information about existing\nattacks. For this, we propose an approach integrating Retrieval Augmented\nGeneration (RAG) and Self-Ranking into the LLM pipeline. RAG enhances the\nrobustness of the output by incorporating external knowledge sources, while the\nSelf-Ranking technique, inspired by the concept of Self-Consistency, generates\nmultiple reasoning paths and creates ranks to select the most robust detector.\nOur extensive empirical study targets code generated by LLMs to detect two\nprevalent injection attacks in web security: Cross-Site Scripting (XSS) and SQL\ninjection (SQLi). Results show a significant improvement in detection\nperformance while employing RAG and Self-Ranking, with an increase of up to\n71%pt (on average 37%pt) and up to 43%pt (on average 6%pt) in the F2-Score for\nXSS and SQLi detection, respectively."
                },
                "authors": [
                    {
                        "name": "Samuele Pasini"
                    },
                    {
                        "name": "Jinhan Kim"
                    },
                    {
                        "name": "Tommaso Aiello"
                    },
                    {
                        "name": "Rocio Cabrera Lozoya"
                    },
                    {
                        "name": "Antonino Sabetta"
                    },
                    {
                        "name": "Paolo Tonella"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Tonella"
                },
                "author": "Paolo Tonella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03936v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03936v2",
                "updated": "2025-09-17T14:23:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    23,
                    2,
                    2,
                    260,
                    0
                ],
                "published": "2025-04-04T21:05:51Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    21,
                    5,
                    51,
                    4,
                    94,
                    0
                ],
                "title": "Commit-Reveal$^2$: Securing Randomness Beacons with Randomized Reveal\n  Order in Smart Contracts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commit-Reveal$^2$: Securing Randomness Beacons with Randomized Reveal\n  Order in Smart Contracts"
                },
                "summary": "Simple commit-reveal beacons are vulnerable to last-revealer strategies, and\nexisting descriptions often leave accountability and recovery mechanisms\nunspecified for practical deployments. We present Commit-Reveal$^2$, a layered\ndesign for blockchain deployments that cryptographically randomizes the final\nreveal order, together with a concrete accountability and fallback mechanism\nthat we implement as smart-contract logic. The protocol is architected as a\nhybrid system, where routine coordination runs off chain for efficiency and the\nblockchain acts as the trust anchor for commitments and the final arbiter for\ndisputes. Our implementation covers leader coordination, on-chain verification,\nslashing for non-cooperation, and an explicit on-chain recovery path that\nmaintains progress when off-chain coordination fails. We formally define two\nsecurity goals for distributed randomness beacons, unpredictability and\nbit-wise bias resistance, and we show that Commit-Reveal$^2$ meets these\nnotions under standard hash assumptions in the random-oracle model. In\nmeasurements with small to moderate operator sets, the hybrid design reduces\non-chain gas by more than 80% compared to a fully on-chain baseline. We release\na publicly verifiable prototype and evaluation artifacts to support replication\nand adoption in blockchain applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple commit-reveal beacons are vulnerable to last-revealer strategies, and\nexisting descriptions often leave accountability and recovery mechanisms\nunspecified for practical deployments. We present Commit-Reveal$^2$, a layered\ndesign for blockchain deployments that cryptographically randomizes the final\nreveal order, together with a concrete accountability and fallback mechanism\nthat we implement as smart-contract logic. The protocol is architected as a\nhybrid system, where routine coordination runs off chain for efficiency and the\nblockchain acts as the trust anchor for commitments and the final arbiter for\ndisputes. Our implementation covers leader coordination, on-chain verification,\nslashing for non-cooperation, and an explicit on-chain recovery path that\nmaintains progress when off-chain coordination fails. We formally define two\nsecurity goals for distributed randomness beacons, unpredictability and\nbit-wise bias resistance, and we show that Commit-Reveal$^2$ meets these\nnotions under standard hash assumptions in the random-oracle model. In\nmeasurements with small to moderate operator sets, the hybrid design reduces\non-chain gas by more than 80% compared to a fully on-chain baseline. We release\na publicly verifiable prototype and evaluation artifacts to support replication\nand adoption in blockchain applications."
                },
                "authors": [
                    {
                        "name": "Suhyeon Lee"
                    },
                    {
                        "name": "Euisin Gee"
                    },
                    {
                        "name": "Najmeh Soroush"
                    },
                    {
                        "name": "Muhammed Ali Bingol"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "arxiv_doi": "10.1109/ICBC64466.2025.11114691",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICBC64466.2025.11114691",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.03936v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03936v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Extended version of ICBC 2025 paper:S. Lee and E. Gee,\n  \"Commit-Reveal2: Randomized Reveal Order Mitigates Last-Revealer Attacks in\n  Commit-Reveal,\" 2025 IEEE International Conference on Blockchain and\n  Cryptocurrency (ICBC), Pisa, Italy, 2025, pp. 1-5, doi:\n  10.1109/ICBC64466.2025.11114691. This version added formal security proofs,\n  fallback/accountability, and expanded cost analysis",
                "arxiv_journal_ref": "2025 IEEE International Conference on Blockchain and\n  Cryptocurrency (ICBC), Pisa, Italy, 2025, pp. 1-5",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14004v1",
                "updated": "2025-09-17T14:14:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    14,
                    5,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T14:14:05Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    14,
                    5,
                    2,
                    260,
                    0
                ],
                "title": "Early Stopping Chain-of-thoughts in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early Stopping Chain-of-thoughts in Large Language Models"
                },
                "summary": "Reasoning large language models (LLMs) have demonstrated superior capacities\nin solving complicated problems by generating long chain-of-thoughts (CoT), but\nsuch a lengthy CoT incurs high inference costs. In this study, we introduce\nES-CoT, an inference-time method that shortens CoT generation by detecting\nanswer convergence and stopping early with minimal performance loss. At the end\nof each reasoning step, we prompt the LLM to output its current final answer,\ndenoted as a step answer. We then track the run length of consecutive identical\nstep answers as a measure of answer convergence. Once the run length exhibits a\nsharp increase and exceeds a minimum threshold, the generation is terminated.\nWe provide both empirical and theoretical support for this heuristic: step\nanswers steadily converge to the final answer, and large run-length jumps\nreliably mark this convergence. Experiments on five reasoning datasets across\nthree LLMs show that ES-CoT reduces the number of inference tokens by about\n41\\% on average while maintaining accuracy comparable to standard CoT. Further,\nES-CoT integrates seamlessly with self-consistency prompting and remains robust\nacross hyperparameter choices, highlighting it as a practical and effective\napproach for efficient reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning large language models (LLMs) have demonstrated superior capacities\nin solving complicated problems by generating long chain-of-thoughts (CoT), but\nsuch a lengthy CoT incurs high inference costs. In this study, we introduce\nES-CoT, an inference-time method that shortens CoT generation by detecting\nanswer convergence and stopping early with minimal performance loss. At the end\nof each reasoning step, we prompt the LLM to output its current final answer,\ndenoted as a step answer. We then track the run length of consecutive identical\nstep answers as a measure of answer convergence. Once the run length exhibits a\nsharp increase and exceeds a minimum threshold, the generation is terminated.\nWe provide both empirical and theoretical support for this heuristic: step\nanswers steadily converge to the final answer, and large run-length jumps\nreliably mark this convergence. Experiments on five reasoning datasets across\nthree LLMs show that ES-CoT reduces the number of inference tokens by about\n41\\% on average while maintaining accuracy comparable to standard CoT. Further,\nES-CoT integrates seamlessly with self-consistency prompting and remains robust\nacross hyperparameter choices, highlighting it as a practical and effective\napproach for efficient reasoning."
                },
                "authors": [
                    {
                        "name": "Minjia Mao"
                    },
                    {
                        "name": "Bowen Yin"
                    },
                    {
                        "name": "Yu Zhu"
                    },
                    {
                        "name": "Xiao Fang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Fang"
                },
                "author": "Xiao Fang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02849v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02849v2",
                "updated": "2025-09-17T14:13:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    13,
                    37,
                    2,
                    260,
                    0
                ],
                "published": "2024-09-04T16:19:55Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    19,
                    55,
                    2,
                    248,
                    0
                ],
                "title": "Anomaly Detection in Offshore Open Radio Access Network Using Long\n  Short-Term Memory Models on a Novel Artificial Intelligence-Driven\n  Cloud-Native Data Platform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anomaly Detection in Offshore Open Radio Access Network Using Long\n  Short-Term Memory Models on a Novel Artificial Intelligence-Driven\n  Cloud-Native Data Platform"
                },
                "summary": "The Radio Access Network (RAN) is a critical component of modern\ntelecommunications infrastructure, currently evolving towards disaggregated and\nopen architectures. These advancements are pivotal for integrating intelligent,\ndata-driven applications aimed at enhancing network reliability and operational\nautonomy through the introduction of cognitive capabilities, as exemplified by\nthe emerging Open Radio Access Network (O-RAN) standards. Despite its\npotential, the nascent nature of O-RAN technology presents challenges,\nprimarily due to the absence of mature operational standards. This complicates\nthe management of data and intelligent applications, particularly when\nintegrating with traditional network management and operational support\nsystems. Divergent vendor-specific design approaches further hinder migration\nand limit solution reusability. These challenges are compounded by a skills gap\nin telecommunications business-oriented engineering, which remains a key\nbarrier to effective O-RAN deployment and intelligent application development.\nTo address these challenges, Boldyn Networks developed a novel cloud-native\ndata analytics platform, specifically designed to support scalable AI\nintegration within O-RAN deployments. This platform underwent rigorous testing\nin real-world scenarios, and applied advanced Artificial Intelligence (AI)\ntechniques to improve operational efficiency and customer experience.\nImplementation involved adopting Development Operations (DevOps) practices,\nleveraging data lakehouse architectures tailored for AI applications, and\nemploying sophisticated data engineering strategies. The platform successfully\naddresses connectivity challenges inherent in real-world offshore windfarm\ndeployments using Long Short-Term Memory (LSTM) models for anomaly detection in\nnetwork connectivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Radio Access Network (RAN) is a critical component of modern\ntelecommunications infrastructure, currently evolving towards disaggregated and\nopen architectures. These advancements are pivotal for integrating intelligent,\ndata-driven applications aimed at enhancing network reliability and operational\nautonomy through the introduction of cognitive capabilities, as exemplified by\nthe emerging Open Radio Access Network (O-RAN) standards. Despite its\npotential, the nascent nature of O-RAN technology presents challenges,\nprimarily due to the absence of mature operational standards. This complicates\nthe management of data and intelligent applications, particularly when\nintegrating with traditional network management and operational support\nsystems. Divergent vendor-specific design approaches further hinder migration\nand limit solution reusability. These challenges are compounded by a skills gap\nin telecommunications business-oriented engineering, which remains a key\nbarrier to effective O-RAN deployment and intelligent application development.\nTo address these challenges, Boldyn Networks developed a novel cloud-native\ndata analytics platform, specifically designed to support scalable AI\nintegration within O-RAN deployments. This platform underwent rigorous testing\nin real-world scenarios, and applied advanced Artificial Intelligence (AI)\ntechniques to improve operational efficiency and customer experience.\nImplementation involved adopting Development Operations (DevOps) practices,\nleveraging data lakehouse architectures tailored for AI applications, and\nemploying sophisticated data engineering strategies. The platform successfully\naddresses connectivity challenges inherent in real-world offshore windfarm\ndeployments using Long Short-Term Memory (LSTM) models for anomaly detection in\nnetwork connectivity."
                },
                "authors": [
                    {
                        "name": "Abdelrahim Ahmad"
                    },
                    {
                        "name": "Peizheng Li"
                    },
                    {
                        "name": "Robert Piechocki"
                    },
                    {
                        "name": "Rui Inacio"
                    }
                ],
                "author_detail": {
                    "name": "Rui Inacio"
                },
                "author": "Rui Inacio",
                "arxiv_doi": "10.1016/j.engappai.2025.112274",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.engappai.2025.112274",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02849v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02849v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 14 figures. This work has been published in Engineering\n  Applications of Artificial Intelligence}, Volume 161, Part C, 12 December\n  2025, 112274. https://doi.org/10.1016/j.engappai.2025.112274",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14001v1",
                "updated": "2025-09-17T14:13:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    13,
                    20,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T14:13:20Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    13,
                    20,
                    2,
                    260,
                    0
                ],
                "title": "MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment"
                },
                "summary": "We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment),\na knowledge distillation approach that transfers region-level multimodal\nsemantics from a large vision-language teacher (e.g., LLaVa) into a lightweight\nvision-only object detector student (e.g., YOLO). A translation module maps\nstudent features into a joint space, where the training of the student and\ntranslator is guided by a dual-objective loss that enforces both local\nalignment and global relational consistency. Unlike prior approaches focused on\ndense or global alignment, MOCHA operates at the object level, enabling\nefficient transfer of semantics without modifying the teacher or requiring\ntextual input at inference. We validate our method across four personalized\ndetection benchmarks under few-shot regimes. Results show consistent gains over\nbaselines, with a +10.1 average score improvement. Despite its compact\narchitecture, MOCHA reaches performance on par with larger multimodal models,\nproving its suitability for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment),\na knowledge distillation approach that transfers region-level multimodal\nsemantics from a large vision-language teacher (e.g., LLaVa) into a lightweight\nvision-only object detector student (e.g., YOLO). A translation module maps\nstudent features into a joint space, where the training of the student and\ntranslator is guided by a dual-objective loss that enforces both local\nalignment and global relational consistency. Unlike prior approaches focused on\ndense or global alignment, MOCHA operates at the object level, enabling\nefficient transfer of semantics without modifying the teacher or requiring\ntextual input at inference. We validate our method across four personalized\ndetection benchmarks under few-shot regimes. Results show consistent gains over\nbaselines, with a +10.1 average score improvement. Despite its compact\narchitecture, MOCHA reaches performance on par with larger multimodal models,\nproving its suitability for real-world deployment."
                },
                "authors": [
                    {
                        "name": "Elena Camuffo"
                    },
                    {
                        "name": "Francesco Barbato"
                    },
                    {
                        "name": "Mete Ozay"
                    },
                    {
                        "name": "Simone Milani"
                    },
                    {
                        "name": "Umberto Michieli"
                    }
                ],
                "author_detail": {
                    "name": "Umberto Michieli"
                },
                "author": "Umberto Michieli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13997v1",
                "updated": "2025-09-17T14:10:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    10,
                    44,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T14:10:44Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    10,
                    44,
                    2,
                    260,
                    0
                ],
                "title": "An RDMA-First Object Storage System with SmartNIC Offload",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An RDMA-First Object Storage System with SmartNIC Offload"
                },
                "summary": "AI training and inference impose sustained, fine-grain I/O that stresses\nhost-mediated, TCP-based storage paths. Motivated by kernel-bypass networking\nand user-space storage stacks, we revisit POSIX-compatible object storage for\nGPU-centric pipelines. We present ROS2, an RDMA-first object storage system\ndesign that offloads the DAOS client to an NVIDIA BlueField-3 SmartNIC while\nleaving the DAOS I/O engine unchanged on the storage server. ROS2 separates a\nlightweight control plane (gRPC for namespace and capability exchange) from a\nhigh-throughput data plane (UCX/libfabric over RDMA or TCP) and removes host\nmediation from the data path.\n  Using FIO/DFS across local and remote configurations, we find that on\nserver-grade CPUs RDMA consistently outperforms TCP for both large sequential\nand small random I/O. When the RDMA-driven DAOS client is offloaded to\nBlueField-3, end-to-end performance is comparable to the host, demonstrating\nthat SmartNIC offload preserves RDMA efficiency while enabling DPU-resident\nfeatures such as multi-tenant isolation and inline services (e.g.,\nencryption/decryption) close to the NIC. In contrast, TCP on the SmartNIC lags\nhost performance, underscoring the importance of RDMA for offloaded\ndeployments.\n  Overall, our results indicate that an RDMA-first, SmartNIC-offloaded\nobject-storage stack is a practical foundation for scaling data delivery in\nmodern LLM training environments; integrating optional GPU-direct placement for\nLLM tasks is left for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI training and inference impose sustained, fine-grain I/O that stresses\nhost-mediated, TCP-based storage paths. Motivated by kernel-bypass networking\nand user-space storage stacks, we revisit POSIX-compatible object storage for\nGPU-centric pipelines. We present ROS2, an RDMA-first object storage system\ndesign that offloads the DAOS client to an NVIDIA BlueField-3 SmartNIC while\nleaving the DAOS I/O engine unchanged on the storage server. ROS2 separates a\nlightweight control plane (gRPC for namespace and capability exchange) from a\nhigh-throughput data plane (UCX/libfabric over RDMA or TCP) and removes host\nmediation from the data path.\n  Using FIO/DFS across local and remote configurations, we find that on\nserver-grade CPUs RDMA consistently outperforms TCP for both large sequential\nand small random I/O. When the RDMA-driven DAOS client is offloaded to\nBlueField-3, end-to-end performance is comparable to the host, demonstrating\nthat SmartNIC offload preserves RDMA efficiency while enabling DPU-resident\nfeatures such as multi-tenant isolation and inline services (e.g.,\nencryption/decryption) close to the NIC. In contrast, TCP on the SmartNIC lags\nhost performance, underscoring the importance of RDMA for offloaded\ndeployments.\n  Overall, our results indicate that an RDMA-first, SmartNIC-offloaded\nobject-storage stack is a practical foundation for scaling data delivery in\nmodern LLM training environments; integrating optional GPU-direct placement for\nLLM tasks is left for future work."
                },
                "authors": [
                    {
                        "name": "Yu Zhu"
                    },
                    {
                        "name": "Aditya Dhakal"
                    },
                    {
                        "name": "Pedro Bruel"
                    },
                    {
                        "name": "Gourav Rattihalli"
                    },
                    {
                        "name": "Yunming Xiao"
                    },
                    {
                        "name": "Johann Lombardi"
                    },
                    {
                        "name": "Dejan Milojicic"
                    }
                ],
                "author_detail": {
                    "name": "Dejan Milojicic"
                },
                "author": "Dejan Milojicic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13990v1",
                "updated": "2025-09-17T14:00:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    0,
                    51,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T14:00:51Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    0,
                    51,
                    2,
                    260,
                    0
                ],
                "title": "Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency"
                },
                "summary": "Recently, Test-Time Scaling (TTS) has gained increasing attention for\nimproving LLM reasoning performance at test time without retraining the model.\nA notable TTS technique is Self-Consistency (SC), which generates multiple\nreasoning chains in parallel and selects the final answer via majority voting.\nWhile effective, the order-of-magnitude computational overhead limits its broad\ndeployment. Prior attempts to accelerate SC mainly rely on model-based\nconfidence scores or heuristics with limited empirical support. For the first\ntime, we theoretically and empirically analyze the inefficiencies of SC and\nreveal actionable opportunities for improvement. Building on these insights, we\npropose Slim-SC, a step-wise pruning strategy that identifies and removes\nredundant chains using inter-chain similarity at the thought level. Experiments\non three STEM reasoning datasets and two recent LLM architectures show that\nSlim-SC reduces inference latency and KVC usage by up to 45% and 26%,\nrespectively, with R1-Distill, while maintaining or improving accuracy, thus\noffering a simple yet efficient TTS alternative for SC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Test-Time Scaling (TTS) has gained increasing attention for\nimproving LLM reasoning performance at test time without retraining the model.\nA notable TTS technique is Self-Consistency (SC), which generates multiple\nreasoning chains in parallel and selects the final answer via majority voting.\nWhile effective, the order-of-magnitude computational overhead limits its broad\ndeployment. Prior attempts to accelerate SC mainly rely on model-based\nconfidence scores or heuristics with limited empirical support. For the first\ntime, we theoretically and empirically analyze the inefficiencies of SC and\nreveal actionable opportunities for improvement. Building on these insights, we\npropose Slim-SC, a step-wise pruning strategy that identifies and removes\nredundant chains using inter-chain similarity at the thought level. Experiments\non three STEM reasoning datasets and two recent LLM architectures show that\nSlim-SC reduces inference latency and KVC usage by up to 45% and 26%,\nrespectively, with R1-Distill, while maintaining or improving accuracy, thus\noffering a simple yet efficient TTS alternative for SC."
                },
                "authors": [
                    {
                        "name": "Colin Hong"
                    },
                    {
                        "name": "Xu Guo"
                    },
                    {
                        "name": "Anand Chaanan Singh"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Dmitrii Ustiugov"
                    }
                ],
                "author_detail": {
                    "name": "Dmitrii Ustiugov"
                },
                "author": "Dmitrii Ustiugov",
                "arxiv_comment": "Accepted by EMNLP 2025 (Oral), 9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13978v1",
                "updated": "2025-09-17T13:51:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    51,
                    29,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T13:51:29Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    51,
                    29,
                    2,
                    260,
                    0
                ],
                "title": "LLM Agents for Interactive Workflow Provenance: Reference Architecture\n  and Evaluation Methodology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Agents for Interactive Workflow Provenance: Reference Architecture\n  and Evaluation Methodology"
                },
                "summary": "Modern scientific discovery increasingly relies on workflows that process\ndata across the Edge, Cloud, and High Performance Computing (HPC) continuum.\nComprehensive and in-depth analyses of these data are critical for hypothesis\nvalidation, anomaly detection, reproducibility, and impactful findings.\nAlthough workflow provenance techniques support such analyses, at large scale,\nthe provenance data become complex and difficult to analyze. Existing systems\ndepend on custom scripts, structured queries, or static dashboards, limiting\ndata interaction. In this work, we introduce an evaluation methodology,\nreference architecture, and open-source implementation that leverages\ninteractive Large Language Model (LLM) agents for runtime data analysis. Our\napproach uses a lightweight, metadata-driven design that translates natural\nlanguage into structured provenance queries. Evaluations across LLaMA, GPT,\nGemini, and Claude, covering diverse query classes and a real-world chemistry\nworkflow, show that modular design, prompt tuning, and Retrieval-Augmented\nGeneration (RAG) enable accurate and insightful LLM agent responses beyond\nrecorded provenance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern scientific discovery increasingly relies on workflows that process\ndata across the Edge, Cloud, and High Performance Computing (HPC) continuum.\nComprehensive and in-depth analyses of these data are critical for hypothesis\nvalidation, anomaly detection, reproducibility, and impactful findings.\nAlthough workflow provenance techniques support such analyses, at large scale,\nthe provenance data become complex and difficult to analyze. Existing systems\ndepend on custom scripts, structured queries, or static dashboards, limiting\ndata interaction. In this work, we introduce an evaluation methodology,\nreference architecture, and open-source implementation that leverages\ninteractive Large Language Model (LLM) agents for runtime data analysis. Our\napproach uses a lightweight, metadata-driven design that translates natural\nlanguage into structured provenance queries. Evaluations across LLaMA, GPT,\nGemini, and Claude, covering diverse query classes and a real-world chemistry\nworkflow, show that modular design, prompt tuning, and Retrieval-Augmented\nGeneration (RAG) enable accurate and insightful LLM agent responses beyond\nrecorded provenance."
                },
                "authors": [
                    {
                        "name": "Renan Souza"
                    },
                    {
                        "name": "Timothy Poteet"
                    },
                    {
                        "name": "Brian Etz"
                    },
                    {
                        "name": "Daniel Rosendo"
                    },
                    {
                        "name": "Amal Gueroudji"
                    },
                    {
                        "name": "Woong Shin"
                    },
                    {
                        "name": "Prasanna Balaprakash"
                    },
                    {
                        "name": "Rafael Ferreira da Silva"
                    }
                ],
                "author_detail": {
                    "name": "Rafael Ferreira da Silva"
                },
                "author": "Rafael Ferreira da Silva",
                "arxiv_doi": "10.1145/3731599.3767582",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731599.3767582",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.13978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Paper accepted in the proceedings of the ACM/IEEE Supercomputing\n  Conference (SC). Cite it as Renan Souza, Timothy Poteet, Brian Etz, Daniel\n  Rosendo, Amal Gueroudji, Woong Shin, Prasanna Balaprakash, and Rafael\n  Ferreira da Silva. 2025. LLM Agents for Interactive Workflow Provenance:\n  Reference Architecture and Evaluation Methodology. In SC Workshops (WORKS)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M14, 68M20, 68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.4; D.1.3; I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13974v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13974v1",
                "updated": "2025-09-17T13:47:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    47,
                    45,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T13:47:45Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    47,
                    45,
                    2,
                    260,
                    0
                ],
                "title": "Personalization on a Budget: Minimally-Labeled Continual Learning for\n  Resource-Efficient Seizure Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalization on a Budget: Minimally-Labeled Continual Learning for\n  Resource-Efficient Seizure Detection"
                },
                "summary": "Objective: Epilepsy, a prevalent neurological disease, demands careful\ndiagnosis and continuous care. Seizure detection remains challenging, as\ncurrent clinical practice relies on expert analysis of electroencephalography,\nwhich is a time-consuming process and requires specialized knowledge.\nAddressing this challenge, this paper explores automated epileptic seizure\ndetection using deep learning, focusing on personalized continual learning\nmodels that adapt to each patient's unique electroencephalography signal\nfeatures, which evolve over time. Methods: In this context, our approach\naddresses the challenge of integrating new data into existing models without\ncatastrophic forgetting, a common issue in static deep learning models. We\npropose EpiSMART, a continual learning framework for seizure detection that\nuses a size-constrained replay buffer and an informed sample selection strategy\nto incrementally adapt to patient-specific electroencephalography signals. By\nselectively retaining high-entropy and seizure-predicted samples, our method\npreserves critical past information while maintaining high performance with\nminimal memory and computational requirements. Results: Validation on the\nCHB-MIT dataset, shows that EpiSMART achieves a 21% improvement in the F1 score\nover a trained baseline without updates in all other patients. On average,\nEpiSMART requires only 6.46 minutes of labeled data and 6.28 updates per day,\nmaking it suitable for real-time deployment in wearable systems.\nConclusion:EpiSMART enables robust and personalized seizure detection under\nrealistic and resource-constrained conditions by effectively integrating new\ndata into existing models without degrading past knowledge. Significance: This\nframework advances automated seizure detection by providing a continual\nlearning approach that supports patient-specific adaptation and practical\ndeployment in wearable healthcare systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objective: Epilepsy, a prevalent neurological disease, demands careful\ndiagnosis and continuous care. Seizure detection remains challenging, as\ncurrent clinical practice relies on expert analysis of electroencephalography,\nwhich is a time-consuming process and requires specialized knowledge.\nAddressing this challenge, this paper explores automated epileptic seizure\ndetection using deep learning, focusing on personalized continual learning\nmodels that adapt to each patient's unique electroencephalography signal\nfeatures, which evolve over time. Methods: In this context, our approach\naddresses the challenge of integrating new data into existing models without\ncatastrophic forgetting, a common issue in static deep learning models. We\npropose EpiSMART, a continual learning framework for seizure detection that\nuses a size-constrained replay buffer and an informed sample selection strategy\nto incrementally adapt to patient-specific electroencephalography signals. By\nselectively retaining high-entropy and seizure-predicted samples, our method\npreserves critical past information while maintaining high performance with\nminimal memory and computational requirements. Results: Validation on the\nCHB-MIT dataset, shows that EpiSMART achieves a 21% improvement in the F1 score\nover a trained baseline without updates in all other patients. On average,\nEpiSMART requires only 6.46 minutes of labeled data and 6.28 updates per day,\nmaking it suitable for real-time deployment in wearable systems.\nConclusion:EpiSMART enables robust and personalized seizure detection under\nrealistic and resource-constrained conditions by effectively integrating new\ndata into existing models without degrading past knowledge. Significance: This\nframework advances automated seizure detection by providing a continual\nlearning approach that supports patient-specific adaptation and practical\ndeployment in wearable healthcare systems."
                },
                "authors": [
                    {
                        "name": "Amirhossein Shahbazinia"
                    },
                    {
                        "name": "Jonathan Dan"
                    },
                    {
                        "name": "Jose A. Miranda"
                    },
                    {
                        "name": "Giovanni Ansaloni"
                    },
                    {
                        "name": "David Atienza"
                    }
                ],
                "author_detail": {
                    "name": "David Atienza"
                },
                "author": "David Atienza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13974v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13974v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04537v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04537v3",
                "updated": "2025-09-17T13:45:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    45,
                    52,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-04T08:09:42Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    9,
                    42,
                    3,
                    247,
                    0
                ],
                "title": "Emergent Social Dynamics of LLM Agents in the El Farol Bar Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent Social Dynamics of LLM Agents in the El Farol Bar Problem"
                },
                "summary": "We investigate the emergent social dynamics of Large Language Model (LLM)\nagents in a spatially extended El Farol Bar problem, observing how they\nautonomously navigate this classic social dilemma. As a result, the LLM agents\ngenerated a spontaneous motivation to go to the bar and changed their decision\nmaking by becoming a collective. We also observed that the LLM agents did not\nsolve the problem completely, but rather behaved more like humans. These\nfindings reveal a complex interplay between external incentives\n(prompt-specified constraints such as the 60% threshold) and internal\nincentives (culturally-encoded social preferences derived from pre-training),\ndemonstrating that LLM agents naturally balance formal game-theoretic\nrationality with social motivations that characterize human behavior. These\nfindings suggest that a new model of group decision making, which could not be\nhandled in the previous game-theoretic problem setting, can be realized by LLM\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the emergent social dynamics of Large Language Model (LLM)\nagents in a spatially extended El Farol Bar problem, observing how they\nautonomously navigate this classic social dilemma. As a result, the LLM agents\ngenerated a spontaneous motivation to go to the bar and changed their decision\nmaking by becoming a collective. We also observed that the LLM agents did not\nsolve the problem completely, but rather behaved more like humans. These\nfindings reveal a complex interplay between external incentives\n(prompt-specified constraints such as the 60% threshold) and internal\nincentives (culturally-encoded social preferences derived from pre-training),\ndemonstrating that LLM agents naturally balance formal game-theoretic\nrationality with social motivations that characterize human behavior. These\nfindings suggest that a new model of group decision making, which could not be\nhandled in the previous game-theoretic problem setting, can be realized by LLM\nagents."
                },
                "authors": [
                    {
                        "name": "Ryosuke Takata"
                    },
                    {
                        "name": "Atsushi Masumori"
                    },
                    {
                        "name": "Takashi Ikegami"
                    }
                ],
                "author_detail": {
                    "name": "Takashi Ikegami"
                },
                "author": "Takashi Ikegami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04537v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04537v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08364v2",
                "updated": "2025-09-17T13:35:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    35,
                    33,
                    2,
                    260,
                    0
                ],
                "published": "2025-05-13T09:10:48Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    10,
                    48,
                    1,
                    133,
                    0
                ],
                "title": "Learning Like Humans: Advancing LLM Reasoning Capabilities via Adaptive\n  Difficulty Curriculum Learning and Expert-Guided Self-Reformulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Like Humans: Advancing LLM Reasoning Capabilities via Adaptive\n  Difficulty Curriculum Learning and Expert-Guided Self-Reformulation"
                },
                "summary": "Despite impressive progress in areas like mathematical reasoning, large\nlanguage models still face significant challenges in consistently solving\ncomplex problems. Drawing inspiration from key human learning strategies, we\npropose two novel strategies to enhance the capability of large language models\nto solve these complex problems. First, Adaptive Difficulty Curriculum Learning\n(ADCL) is a novel curriculum learning strategy that tackles the Difficulty\nShift phenomenon (i.e., a model's perception of problem difficulty dynamically\nchanges during training) by periodically re-estimating difficulty within\nupcoming data batches to maintain alignment with the model's evolving\ncapabilities. Second, Expert-Guided Self-Reformulation (EGSR) is a novel\nreinforcement learning strategy that bridges the gap between imitation learning\nand pure exploration by guiding models to reformulate expert solutions within\ntheir own conceptual framework, rather than relying on direct imitation,\nfostering deeper understanding and knowledge assimilation. Extensive\nexperiments on challenging mathematical reasoning benchmarks, using Qwen2.5-7B\nas the base model, demonstrate that these human-inspired strategies\nsynergistically and significantly enhance performance. Notably, their combined\napplication improves performance over the standard Zero-RL baseline by 10% on\nthe AIME24 benchmark and 16.6% on AIME25.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite impressive progress in areas like mathematical reasoning, large\nlanguage models still face significant challenges in consistently solving\ncomplex problems. Drawing inspiration from key human learning strategies, we\npropose two novel strategies to enhance the capability of large language models\nto solve these complex problems. First, Adaptive Difficulty Curriculum Learning\n(ADCL) is a novel curriculum learning strategy that tackles the Difficulty\nShift phenomenon (i.e., a model's perception of problem difficulty dynamically\nchanges during training) by periodically re-estimating difficulty within\nupcoming data batches to maintain alignment with the model's evolving\ncapabilities. Second, Expert-Guided Self-Reformulation (EGSR) is a novel\nreinforcement learning strategy that bridges the gap between imitation learning\nand pure exploration by guiding models to reformulate expert solutions within\ntheir own conceptual framework, rather than relying on direct imitation,\nfostering deeper understanding and knowledge assimilation. Extensive\nexperiments on challenging mathematical reasoning benchmarks, using Qwen2.5-7B\nas the base model, demonstrate that these human-inspired strategies\nsynergistically and significantly enhance performance. Notably, their combined\napplication improves performance over the standard Zero-RL baseline by 10% on\nthe AIME24 benchmark and 16.6% on AIME25."
                },
                "authors": [
                    {
                        "name": "Enci Zhang"
                    },
                    {
                        "name": "Xingang Yan"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Tianxiang Zhang"
                    },
                    {
                        "name": "Qianchun Lu"
                    }
                ],
                "author_detail": {
                    "name": "Qianchun Lu"
                },
                "author": "Qianchun Lu",
                "arxiv_comment": "14 pages, 3 figs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09974v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09974v2",
                "updated": "2025-09-17T13:26:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    26,
                    12,
                    2,
                    260,
                    0
                ],
                "published": "2025-05-15T05:22:53Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    5,
                    22,
                    53,
                    3,
                    135,
                    0
                ],
                "title": "Analysing Safety Risks in LLMs Fine-Tuned with Pseudo-Malicious Cyber\n  Security Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysing Safety Risks in LLMs Fine-Tuned with Pseudo-Malicious Cyber\n  Security Data"
                },
                "summary": "Large language models (LLMs) have been used in many application domains,\nincluding cyber security. The application of LLMs in the cyber security domain\npresents significant opportunities, such as for enhancing threat analysis and\nmalware detection, but it can also introduce critical risks and safety\nconcerns, including potential personal data leakage and automated generation of\nnew malware. Building on recent findings that fine-tuning LLMs with\npseudo-malicious cyber security data significantly compromises their safety,\nthis paper presents a comprehensive validation and extension of these safety\nrisks using a different evaluation framework. We employ the garak red teaming\nframework with the OWASP Top 10 for LLM Applications to assess four open-source\nLLMs: Mistral 7B, Llama 3 8B, Gemma 2 9B, and DeepSeek R1 8B. Our evaluation\nconfirms and extends previous findings, showing that fine-tuning reduces safety\nresilience across all tested LLMs (e.g., the failure rate of Mistral 7B against\nprompt injection increases from 9.1% to 68.7%). We further propose and evaluate\na novel safety alignment approach that carefully rewords instruction-response\npairs to include explicit safety precautions and ethical considerations. This\nwork validates previous safety concerns through independent evaluation and\nintroduces new methods for mitigating these risks, contributing towards the\ndevelopment of secure, trustworthy, and ethically aligned LLMs. This approach\ndemonstrates that it is possible to maintain or even improve model safety while\npreserving technical utility, offering a practical path towards developing\nsafer fine-tuning methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been used in many application domains,\nincluding cyber security. The application of LLMs in the cyber security domain\npresents significant opportunities, such as for enhancing threat analysis and\nmalware detection, but it can also introduce critical risks and safety\nconcerns, including potential personal data leakage and automated generation of\nnew malware. Building on recent findings that fine-tuning LLMs with\npseudo-malicious cyber security data significantly compromises their safety,\nthis paper presents a comprehensive validation and extension of these safety\nrisks using a different evaluation framework. We employ the garak red teaming\nframework with the OWASP Top 10 for LLM Applications to assess four open-source\nLLMs: Mistral 7B, Llama 3 8B, Gemma 2 9B, and DeepSeek R1 8B. Our evaluation\nconfirms and extends previous findings, showing that fine-tuning reduces safety\nresilience across all tested LLMs (e.g., the failure rate of Mistral 7B against\nprompt injection increases from 9.1% to 68.7%). We further propose and evaluate\na novel safety alignment approach that carefully rewords instruction-response\npairs to include explicit safety precautions and ethical considerations. This\nwork validates previous safety concerns through independent evaluation and\nintroduces new methods for mitigating these risks, contributing towards the\ndevelopment of secure, trustworthy, and ethically aligned LLMs. This approach\ndemonstrates that it is possible to maintain or even improve model safety while\npreserving technical utility, offering a practical path towards developing\nsafer fine-tuning methodologies."
                },
                "authors": [
                    {
                        "name": "Adel ElZemity"
                    },
                    {
                        "name": "Budi Arief"
                    },
                    {
                        "name": "Shujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Shujun Li"
                },
                "author": "Shujun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09974v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09974v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21670v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21670v3",
                "updated": "2025-09-17T13:25:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    25,
                    13,
                    2,
                    260,
                    0
                ],
                "published": "2025-03-27T16:36:39Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    36,
                    39,
                    3,
                    86,
                    0
                ],
                "title": "COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in\n  Hindi-English Code-Mixing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in\n  Hindi-English Code-Mixing"
                },
                "summary": "We introduce COMI-LINGUA, the largest manually annotated Hindi-English\ncode-mixed dataset, comprising 125K+ high-quality instances across five core\nNLP tasks: Matrix Language Identification, Token-level Language Identification,\nPart-Of-Speech Tagging, Named Entity Recognition, and Machine Translation. Each\ninstance is annotated by three bilingual annotators, yielding over 376K expert\nannotations with strong inter-annotator agreement (Fleiss' Kappa $\\geq$ 0.81).\nThe rigorously preprocessed and filtered dataset covers both Devanagari and\nRoman scripts and spans diverse domains, ensuring real-world linguistic\ncoverage. Evaluation reveals that closed-source LLMs significantly outperform\ntraditional tools and open-source models in zero-shot settings. Notably,\none-shot prompting consistently boosts performance across tasks, especially in\nstructure-sensitive predictions like POS and NER. Fine-tuning state-of-the-art\nLLMs on COMI-LINGUA demonstrates substantial improvements, achieving up to\n95.25 F1 in NER, 98.77 F1 in MLI, and competitive MT performance, setting new\nbenchmarks for Hinglish code-mixed text. COMI-LINGUA is publicly available at\nthis URL: https://huggingface.co/datasets/LingoIITGN/COMI-LINGUA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce COMI-LINGUA, the largest manually annotated Hindi-English\ncode-mixed dataset, comprising 125K+ high-quality instances across five core\nNLP tasks: Matrix Language Identification, Token-level Language Identification,\nPart-Of-Speech Tagging, Named Entity Recognition, and Machine Translation. Each\ninstance is annotated by three bilingual annotators, yielding over 376K expert\nannotations with strong inter-annotator agreement (Fleiss' Kappa $\\geq$ 0.81).\nThe rigorously preprocessed and filtered dataset covers both Devanagari and\nRoman scripts and spans diverse domains, ensuring real-world linguistic\ncoverage. Evaluation reveals that closed-source LLMs significantly outperform\ntraditional tools and open-source models in zero-shot settings. Notably,\none-shot prompting consistently boosts performance across tasks, especially in\nstructure-sensitive predictions like POS and NER. Fine-tuning state-of-the-art\nLLMs on COMI-LINGUA demonstrates substantial improvements, achieving up to\n95.25 F1 in NER, 98.77 F1 in MLI, and competitive MT performance, setting new\nbenchmarks for Hinglish code-mixed text. COMI-LINGUA is publicly available at\nthis URL: https://huggingface.co/datasets/LingoIITGN/COMI-LINGUA."
                },
                "authors": [
                    {
                        "name": "Rajvee Sheth"
                    },
                    {
                        "name": "Himanshu Beniwal"
                    },
                    {
                        "name": "Mayank Singh"
                    }
                ],
                "author_detail": {
                    "name": "Mayank Singh"
                },
                "author": "Mayank Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21670v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21670v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13949v1",
                "updated": "2025-09-17T13:19:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    19,
                    59,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T13:19:59Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    19,
                    59,
                    2,
                    260,
                    0
                ],
                "title": "SHaRe-RL: Structured, Interactive Reinforcement Learning for\n  Contact-Rich Industrial Assembly Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SHaRe-RL: Structured, Interactive Reinforcement Learning for\n  Contact-Rich Industrial Assembly Tasks"
                },
                "summary": "High-mix low-volume (HMLV) industrial assembly, common in small and\nmedium-sized enterprises (SMEs), requires the same precision, safety, and\nreliability as high-volume automation while remaining flexible to product\nvariation and environmental uncertainty. Current robotic systems struggle to\nmeet these demands. Manual programming is brittle and costly to adapt, while\nlearning-based methods suffer from poor sample efficiency and unsafe\nexploration in contact-rich tasks. To address this, we present SHaRe-RL, a\nreinforcement learning framework that leverages multiple sources of prior\nknowledge. By (i) structuring skills into manipulation primitives, (ii)\nincorporating human demonstrations and online corrections, and (iii) bounding\ninteraction forces with per-axis compliance, SHaRe-RL enables efficient and\nsafe online learning for long-horizon, contact-rich industrial assembly tasks.\nExperiments on the insertion of industrial Harting connector modules with\n0.2-0.4 mm clearance demonstrate that SHaRe-RL achieves reliable performance\nwithin practical time budgets. Our results show that process expertise, without\nrequiring robotics or RL knowledge, can meaningfully contribute to learning,\nenabling safer, more robust, and more economically viable deployment of RL for\nindustrial assembly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-mix low-volume (HMLV) industrial assembly, common in small and\nmedium-sized enterprises (SMEs), requires the same precision, safety, and\nreliability as high-volume automation while remaining flexible to product\nvariation and environmental uncertainty. Current robotic systems struggle to\nmeet these demands. Manual programming is brittle and costly to adapt, while\nlearning-based methods suffer from poor sample efficiency and unsafe\nexploration in contact-rich tasks. To address this, we present SHaRe-RL, a\nreinforcement learning framework that leverages multiple sources of prior\nknowledge. By (i) structuring skills into manipulation primitives, (ii)\nincorporating human demonstrations and online corrections, and (iii) bounding\ninteraction forces with per-axis compliance, SHaRe-RL enables efficient and\nsafe online learning for long-horizon, contact-rich industrial assembly tasks.\nExperiments on the insertion of industrial Harting connector modules with\n0.2-0.4 mm clearance demonstrate that SHaRe-RL achieves reliable performance\nwithin practical time budgets. Our results show that process expertise, without\nrequiring robotics or RL knowledge, can meaningfully contribute to learning,\nenabling safer, more robust, and more economically viable deployment of RL for\nindustrial assembly."
                },
                "authors": [
                    {
                        "name": "Jannick Stranghöner"
                    },
                    {
                        "name": "Philipp Hartmann"
                    },
                    {
                        "name": "Marco Braun"
                    },
                    {
                        "name": "Sebastian Wrede"
                    },
                    {
                        "name": "Klaus Neumann"
                    }
                ],
                "author_detail": {
                    "name": "Klaus Neumann"
                },
                "author": "Klaus Neumann",
                "arxiv_comment": "8 pages, 5 figures, submitted to the IEEE International Conference on\n  Robotics and Automation (ICRA) 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09334v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09334v3",
                "updated": "2025-09-17T13:19:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    19,
                    14,
                    2,
                    260,
                    0
                ],
                "published": "2025-03-12T12:29:27Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    29,
                    27,
                    2,
                    71,
                    0
                ],
                "title": "CyberLLMInstruct: A Pseudo-malicious Dataset Revealing\n  Safety-performance Trade-offs in Cyber Security LLM Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CyberLLMInstruct: A Pseudo-malicious Dataset Revealing\n  Safety-performance Trade-offs in Cyber Security LLM Fine-tuning"
                },
                "summary": "The integration of large language models (LLMs) into cyber security\napplications presents both opportunities and critical safety risks. We\nintroduce CyberLLMInstruct, a dataset of 54,928 pseudo-malicious\ninstruction-response pairs spanning cyber security tasks including malware\nanalysis, phishing simulations, and zero-day vulnerabilities. Our comprehensive\nevaluation using seven open-source LLMs reveals a critical trade-off: while\nfine-tuning improves cyber security task performance (achieving up to 92.50%\naccuracy on CyberMetric), it severely compromises safety resilience across all\ntested models and attack vectors (e.g., Llama 3.1 8B's security score against\nprompt injection drops from 0.95 to 0.15). The dataset incorporates diverse\nsources including CTF challenges, academic papers, industry reports, and CVE\ndatabases to ensure comprehensive coverage of cyber security domains. Our\nfindings highlight the unique challenges of securing LLMs in adversarial\ndomains and establish the critical need for developing fine-tuning\nmethodologies that balance performance gains with safety preservation in\nsecurity-sensitive domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language models (LLMs) into cyber security\napplications presents both opportunities and critical safety risks. We\nintroduce CyberLLMInstruct, a dataset of 54,928 pseudo-malicious\ninstruction-response pairs spanning cyber security tasks including malware\nanalysis, phishing simulations, and zero-day vulnerabilities. Our comprehensive\nevaluation using seven open-source LLMs reveals a critical trade-off: while\nfine-tuning improves cyber security task performance (achieving up to 92.50%\naccuracy on CyberMetric), it severely compromises safety resilience across all\ntested models and attack vectors (e.g., Llama 3.1 8B's security score against\nprompt injection drops from 0.95 to 0.15). The dataset incorporates diverse\nsources including CTF challenges, academic papers, industry reports, and CVE\ndatabases to ensure comprehensive coverage of cyber security domains. Our\nfindings highlight the unique challenges of securing LLMs in adversarial\ndomains and establish the critical need for developing fine-tuning\nmethodologies that balance performance gains with safety preservation in\nsecurity-sensitive domains."
                },
                "authors": [
                    {
                        "name": "Adel ElZemity"
                    },
                    {
                        "name": "Budi Arief"
                    },
                    {
                        "name": "Shujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Shujun Li"
                },
                "author": "Shujun Li",
                "arxiv_doi": "10.1145/3733799.3762968",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3733799.3762968",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.09334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09334v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13943v1",
                "updated": "2025-09-17T13:12:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    12,
                    52,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T13:12:52Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    12,
                    52,
                    2,
                    260,
                    0
                ],
                "title": "Reinforcement Learning for Autonomous Point-to-Point UAV Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning for Autonomous Point-to-Point UAV Navigation"
                },
                "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly used in automated\ninspection, delivery, and navigation tasks that require reliable autonomy. This\nproject develops a reinforcement learning (RL) approach to enable a single UAV\nto autonomously navigate between predefined points without manual intervention.\nThe drone learns navigation policies through trial-and-error interaction, using\na custom reward function that encourages goal-reaching efficiency while\npenalizing collisions and unsafe behavior. The control system integrates ROS\nwith a Gym-compatible training environment, enabling flexible deployment and\ntesting. After training, the learned policy is deployed on a real UAV platform\nand evaluated under practical conditions. Results show that the UAV can\nsuccessfully perform autonomous navigation with minimal human oversight,\ndemonstrating the viability of RL-based control for point-to-point drone\noperations in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned Aerial Vehicles (UAVs) are increasingly used in automated\ninspection, delivery, and navigation tasks that require reliable autonomy. This\nproject develops a reinforcement learning (RL) approach to enable a single UAV\nto autonomously navigate between predefined points without manual intervention.\nThe drone learns navigation policies through trial-and-error interaction, using\na custom reward function that encourages goal-reaching efficiency while\npenalizing collisions and unsafe behavior. The control system integrates ROS\nwith a Gym-compatible training environment, enabling flexible deployment and\ntesting. After training, the learned policy is deployed on a real UAV platform\nand evaluated under practical conditions. Results show that the UAV can\nsuccessfully perform autonomous navigation with minimal human oversight,\ndemonstrating the viability of RL-based control for point-to-point drone\noperations in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Salim Oyinlola"
                    },
                    {
                        "name": "Nitesh Subedi"
                    },
                    {
                        "name": "Soumik Sarkar"
                    }
                ],
                "author_detail": {
                    "name": "Soumik Sarkar"
                },
                "author": "Soumik Sarkar",
                "arxiv_comment": "Presented at the Research Experience for Undergraduates (REU)\n  Symposium at the Translational AI Centre in Iowa State University",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13942v1",
                "updated": "2025-09-17T13:11:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    11,
                    49,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T13:11:49Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    11,
                    49,
                    2,
                    260,
                    0
                ],
                "title": "Evaluating Classical Software Process Models as Coordination Mechanisms\n  for LLM-Based Software Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Classical Software Process Models as Coordination Mechanisms\n  for LLM-Based Software Generation"
                },
                "summary": "[Background] Large Language Model (LLM)-based multi-agent systems (MAS) are\ntransforming software development by enabling autonomous collaboration.\nClassical software processes such asWaterfall, V-Model, and Agile offer\nstructured coordination patterns that can be repurposed to guide these agent\ninteractions. [Aims] This study explores how traditional software development\nprocesses can be adapted as coordination scaffolds for LLM based MAS and\nexamines their impact on code quality, cost, and productivity. [Method] We\nexecuted 11 diverse software projects under three process models and four GPT\nvariants, totaling 132 runs. Each output was evaluated using standardized\nmetrics for size (files, LOC), cost (execution time, token usage), and quality\n(code smells, AI- and human detected bugs). [Results] Both process model and\nLLM choice significantly affected system performance. Waterfall was most\nefficient, V-Model produced the most verbose code, and Agile achieved the\nhighest code quality, albeit at higher computational cost. [Conclusions]\nClassical software processes can be effectively instantiated in LLM-based MAS,\nbut each entails trade-offs across quality, cost, and adaptability. Process\nselection should reflect project goals, whether prioritizing efficiency,\nrobustness, or structured validation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "[Background] Large Language Model (LLM)-based multi-agent systems (MAS) are\ntransforming software development by enabling autonomous collaboration.\nClassical software processes such asWaterfall, V-Model, and Agile offer\nstructured coordination patterns that can be repurposed to guide these agent\ninteractions. [Aims] This study explores how traditional software development\nprocesses can be adapted as coordination scaffolds for LLM based MAS and\nexamines their impact on code quality, cost, and productivity. [Method] We\nexecuted 11 diverse software projects under three process models and four GPT\nvariants, totaling 132 runs. Each output was evaluated using standardized\nmetrics for size (files, LOC), cost (execution time, token usage), and quality\n(code smells, AI- and human detected bugs). [Results] Both process model and\nLLM choice significantly affected system performance. Waterfall was most\nefficient, V-Model produced the most verbose code, and Agile achieved the\nhighest code quality, albeit at higher computational cost. [Conclusions]\nClassical software processes can be effectively instantiated in LLM-based MAS,\nbut each entails trade-offs across quality, cost, and adaptability. Process\nselection should reflect project goals, whether prioritizing efficiency,\nrobustness, or structured validation."
                },
                "authors": [
                    {
                        "name": "Duc Minh Ha"
                    },
                    {
                        "name": "Phu Trac Kien"
                    },
                    {
                        "name": "Tho Quan"
                    },
                    {
                        "name": "Anh Nguyen-Duc"
                    }
                ],
                "author_detail": {
                    "name": "Anh Nguyen-Duc"
                },
                "author": "Anh Nguyen-Duc",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13941v1",
                "updated": "2025-09-17T13:07:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    7,
                    52,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T13:07:52Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    7,
                    52,
                    2,
                    260,
                    0
                ],
                "title": "An Empirical Study on Failures in Automated Issue Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on Failures in Automated Issue Solving"
                },
                "summary": "Automated issue solving seeks to autonomously identify and repair defective\ncode snippets across an entire codebase. SWE-Bench has emerged as the most\nwidely adopted benchmark for evaluating progress in this area. While LLM-based\nagentic tools show great promise, they still fail on a substantial portion of\ntasks. Moreover, current evaluations primarily report aggregate issue-solving\nrates, which obscure the underlying causes of success and failure, making it\nchallenging to diagnose model weaknesses or guide targeted improvements. To\nbridge this gap, we first analyze the performance and efficiency of three SOTA\ntools, spanning both pipeline-based and agentic architectures, in automated\nissue solving tasks of SWE-Bench-Verified under varying task characteristics.\nFurthermore, to move from high-level performance metrics to underlying cause\nanalysis, we conducted a systematic manual analysis of 150 failed instances.\nFrom this analysis, we developed a comprehensive taxonomy of failure modes\ncomprising 3 primary phases, 9 main categories, and 25 fine-grained\nsubcategories. Then we systematically analyze the distribution of the\nidentified failure modes, the results reveal distinct failure fingerprints\nbetween the two architectural paradigms, with the majority of agentic failures\nstemming from flawed reasoning and cognitive deadlocks. Motivated by these\ninsights, we propose a collaborative Expert-Executor framework. It introduces a\nsupervisory Expert agent tasked with providing strategic oversight and\ncourse-correction for a primary Executor agent. This architecture is designed\nto correct flawed reasoning and break the cognitive deadlocks that frequently\nlead to failure. Experiments show that our framework solves 22.2% of previously\nintractable issues for a leading single agent. These findings pave the way for\nbuilding more robust agents through diagnostic evaluation and collaborative\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated issue solving seeks to autonomously identify and repair defective\ncode snippets across an entire codebase. SWE-Bench has emerged as the most\nwidely adopted benchmark for evaluating progress in this area. While LLM-based\nagentic tools show great promise, they still fail on a substantial portion of\ntasks. Moreover, current evaluations primarily report aggregate issue-solving\nrates, which obscure the underlying causes of success and failure, making it\nchallenging to diagnose model weaknesses or guide targeted improvements. To\nbridge this gap, we first analyze the performance and efficiency of three SOTA\ntools, spanning both pipeline-based and agentic architectures, in automated\nissue solving tasks of SWE-Bench-Verified under varying task characteristics.\nFurthermore, to move from high-level performance metrics to underlying cause\nanalysis, we conducted a systematic manual analysis of 150 failed instances.\nFrom this analysis, we developed a comprehensive taxonomy of failure modes\ncomprising 3 primary phases, 9 main categories, and 25 fine-grained\nsubcategories. Then we systematically analyze the distribution of the\nidentified failure modes, the results reveal distinct failure fingerprints\nbetween the two architectural paradigms, with the majority of agentic failures\nstemming from flawed reasoning and cognitive deadlocks. Motivated by these\ninsights, we propose a collaborative Expert-Executor framework. It introduces a\nsupervisory Expert agent tasked with providing strategic oversight and\ncourse-correction for a primary Executor agent. This architecture is designed\nto correct flawed reasoning and break the cognitive deadlocks that frequently\nlead to failure. Experiments show that our framework solves 22.2% of previously\nintractable issues for a leading single agent. These findings pave the way for\nbuilding more robust agents through diagnostic evaluation and collaborative\ndesign."
                },
                "authors": [
                    {
                        "name": "Simiao Liu"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Liehao Li"
                    },
                    {
                        "name": "Xin Tan"
                    },
                    {
                        "name": "Yinghao Zhu"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Li Zhang"
                },
                "author": "Li Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13934v1",
                "updated": "2025-09-17T13:05:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    5,
                    8,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T13:05:08Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    5,
                    8,
                    2,
                    260,
                    0
                ],
                "title": "Large Language Model-Empowered Decision Transformer for UAV-Enabled Data\n  Collection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Empowered Decision Transformer for UAV-Enabled Data\n  Collection"
                },
                "summary": "The deployment of unmanned aerial vehicles (UAVs) for reliable and\nenergy-efficient data collection from spatially distributed devices holds great\npromise in supporting diverse Internet of Things (IoT) applications.\nNevertheless, the limited endurance and communication range of UAVs necessitate\nintelligent trajectory planning. While reinforcement learning (RL) has been\nextensively explored for UAV trajectory optimization, its interactive nature\nentails high costs and risks in real-world environments. Offline RL mitigates\nthese issues but remains susceptible to unstable training and heavily rely on\nexpert-quality datasets. To address these challenges, we formulate a joint UAV\ntrajectory planning and resource allocation problem to maximize energy\nefficiency of data collection. The resource allocation subproblem is first\ntransformed into an equivalent linear programming formulation and solved\noptimally with polynomial-time complexity. Then, we propose a large language\nmodel (LLM)-empowered critic-regularized decision transformer (DT) framework,\ntermed LLM-CRDT, to learn effective UAV control policies. In LLM-CRDT, we\nincorporate critic networks to regularize the DT model training, thereby\nintegrating the sequence modeling capabilities of DT with critic-based value\nguidance to enable learning effective policies from suboptimal datasets.\nFurthermore, to mitigate the data-hungry nature of transformer models, we\nemploy a pre-trained LLM as the transformer backbone of the DT model and adopt\na parameter-efficient fine-tuning strategy, i.e., LoRA, enabling rapid\nadaptation to UAV control tasks with small-scale dataset and low computational\noverhead. Extensive simulations demonstrate that LLM-CRDT outperforms benchmark\nonline and offline RL methods, achieving up to 36.7\\% higher energy efficiency\nthan the current state-of-the-art DT approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of unmanned aerial vehicles (UAVs) for reliable and\nenergy-efficient data collection from spatially distributed devices holds great\npromise in supporting diverse Internet of Things (IoT) applications.\nNevertheless, the limited endurance and communication range of UAVs necessitate\nintelligent trajectory planning. While reinforcement learning (RL) has been\nextensively explored for UAV trajectory optimization, its interactive nature\nentails high costs and risks in real-world environments. Offline RL mitigates\nthese issues but remains susceptible to unstable training and heavily rely on\nexpert-quality datasets. To address these challenges, we formulate a joint UAV\ntrajectory planning and resource allocation problem to maximize energy\nefficiency of data collection. The resource allocation subproblem is first\ntransformed into an equivalent linear programming formulation and solved\noptimally with polynomial-time complexity. Then, we propose a large language\nmodel (LLM)-empowered critic-regularized decision transformer (DT) framework,\ntermed LLM-CRDT, to learn effective UAV control policies. In LLM-CRDT, we\nincorporate critic networks to regularize the DT model training, thereby\nintegrating the sequence modeling capabilities of DT with critic-based value\nguidance to enable learning effective policies from suboptimal datasets.\nFurthermore, to mitigate the data-hungry nature of transformer models, we\nemploy a pre-trained LLM as the transformer backbone of the DT model and adopt\na parameter-efficient fine-tuning strategy, i.e., LoRA, enabling rapid\nadaptation to UAV control tasks with small-scale dataset and low computational\noverhead. Extensive simulations demonstrate that LLM-CRDT outperforms benchmark\nonline and offline RL methods, achieving up to 36.7\\% higher energy efficiency\nthan the current state-of-the-art DT approaches."
                },
                "authors": [
                    {
                        "name": "Zhixion Chen"
                    },
                    {
                        "name": "Jiangzhou Wang"
                    },
                    {
                        "name": "and Hyundong Shin"
                    },
                    {
                        "name": "Arumugam Nallanathan"
                    }
                ],
                "author_detail": {
                    "name": "Arumugam Nallanathan"
                },
                "author": "Arumugam Nallanathan",
                "arxiv_comment": "14pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13933v1",
                "updated": "2025-09-17T13:04:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    4,
                    14,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T13:04:14Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    4,
                    14,
                    2,
                    260,
                    0
                ],
                "title": "Adaptive Client Selection via Q-Learning-based Whittle Index in Wireless\n  Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Client Selection via Q-Learning-based Whittle Index in Wireless\n  Federated Learning"
                },
                "summary": "We consider the client selection problem in wireless Federated Learning (FL),\nwith the objective of reducing the total required time to achieve a certain\nlevel of learning accuracy. Since the server cannot observe the clients'\ndynamic states that can change their computation and communication efficiency,\nwe formulate client selection as a restless multi-armed bandit problem. We\npropose a scalable and efficient approach called the Whittle Index Learning in\nFederated Q-learning (WILF-Q), which uses Q-learning to adaptively learn and\nupdate an approximated Whittle index associated with each client, and then\nselects the clients with the highest indices. Compared to existing approaches,\nWILF-Q does not require explicit knowledge of client state transitions or data\ndistributions, making it well-suited for deployment in practical FL settings.\nExperiment results demonstrate that WILF-Q significantly outperforms existing\nbaseline policies in terms of learning efficiency, providing a robust and\nefficient approach to client selection in wireless FL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the client selection problem in wireless Federated Learning (FL),\nwith the objective of reducing the total required time to achieve a certain\nlevel of learning accuracy. Since the server cannot observe the clients'\ndynamic states that can change their computation and communication efficiency,\nwe formulate client selection as a restless multi-armed bandit problem. We\npropose a scalable and efficient approach called the Whittle Index Learning in\nFederated Q-learning (WILF-Q), which uses Q-learning to adaptively learn and\nupdate an approximated Whittle index associated with each client, and then\nselects the clients with the highest indices. Compared to existing approaches,\nWILF-Q does not require explicit knowledge of client state transitions or data\ndistributions, making it well-suited for deployment in practical FL settings.\nExperiment results demonstrate that WILF-Q significantly outperforms existing\nbaseline policies in terms of learning efficiency, providing a robust and\nefficient approach to client selection in wireless FL."
                },
                "authors": [
                    {
                        "name": "Qiyue Li"
                    },
                    {
                        "name": "Yingxin Liu"
                    },
                    {
                        "name": "Hang Qi"
                    },
                    {
                        "name": "Jieping Luo"
                    },
                    {
                        "name": "Zhizhang Liu"
                    },
                    {
                        "name": "Jingjin Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jingjin Wu"
                },
                "author": "Jingjin Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18655v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18655v3",
                "updated": "2025-09-17T13:01:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    1,
                    21,
                    2,
                    260,
                    0
                ],
                "published": "2025-08-26T03:54:39Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    3,
                    54,
                    39,
                    1,
                    238,
                    0
                ],
                "title": "Empathy Omni: Enabling Empathetic Speech Response Generation through\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empathy Omni: Enabling Empathetic Speech Response Generation through\n  Large Language Models"
                },
                "summary": "With the development of speech large language models (speech LLMs), users can\nnow interact directly with assistants via speech. However, most existing models\nonly convert response content into speech without fully capturing the rich\nemotional cues in user queries, where the same sentence may convey different\nmeanings depending on the expression. Emotional understanding is thus essential\nfor improving human-machine interaction. Most empathetic speech LLMs rely on\nmassive datasets, demanding high computational cost. A key challenge is to\nbuild models that generate empathetic responses with limited data and without\nlarge-scale training. To this end, we propose Emotion Omni, a model that\nunderstands emotional content in user speech and generates empathetic\nresponses. We further developed a data pipeline to construct a 200k emotional\ndialogue dataset supporting empathetic speech assistants. Experiments show that\nEmotion Omni achieves comparable instruction-following ability without\nlarge-scale pretraining, while surpassing existing models in speech quality\n(UTMOS:4.41) and empathy (Emotion GPT Score: 3.97). These results confirm its\nimprovements in both speech fidelity and emotional expressiveness. Demos are\navailable at https://w311411.github.io/omni_demo/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of speech large language models (speech LLMs), users can\nnow interact directly with assistants via speech. However, most existing models\nonly convert response content into speech without fully capturing the rich\nemotional cues in user queries, where the same sentence may convey different\nmeanings depending on the expression. Emotional understanding is thus essential\nfor improving human-machine interaction. Most empathetic speech LLMs rely on\nmassive datasets, demanding high computational cost. A key challenge is to\nbuild models that generate empathetic responses with limited data and without\nlarge-scale training. To this end, we propose Emotion Omni, a model that\nunderstands emotional content in user speech and generates empathetic\nresponses. We further developed a data pipeline to construct a 200k emotional\ndialogue dataset supporting empathetic speech assistants. Experiments show that\nEmotion Omni achieves comparable instruction-following ability without\nlarge-scale pretraining, while surpassing existing models in speech quality\n(UTMOS:4.41) and empathy (Emotion GPT Score: 3.97). These results confirm its\nimprovements in both speech fidelity and emotional expressiveness. Demos are\navailable at https://w311411.github.io/omni_demo/."
                },
                "authors": [
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Guangyan Zhang"
                    },
                    {
                        "name": "Jiale Chen"
                    },
                    {
                        "name": "Jingyu Li"
                    },
                    {
                        "name": "Yuehai Wang"
                    },
                    {
                        "name": "Yiwen Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yiwen Guo"
                },
                "author": "Yiwen Guo",
                "arxiv_comment": "5 pages, 1 figure, submitted to ICASSP 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18655v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18655v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12918v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12918v2",
                "updated": "2025-09-17T12:59:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    12,
                    59,
                    25,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-16T10:11:59Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    11,
                    59,
                    1,
                    259,
                    0
                ],
                "title": "A Novel Compression Framework for YOLOv8: Achieving Real-Time Aerial\n  Object Detection on Edge Devices via Structured Pruning and Channel-Wise\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Compression Framework for YOLOv8: Achieving Real-Time Aerial\n  Object Detection on Edge Devices via Structured Pruning and Channel-Wise\n  Distillation"
                },
                "summary": "Efficient deployment of deep learning models for aerial object detection on\nresource-constrained devices requires significant compression without\ncom-promising performance. In this study, we propose a novel three-stage\ncompression pipeline for the YOLOv8 object detection model, integrating\nsparsity-aware training, structured channel pruning, and Channel-Wise Knowledge\nDistillation (CWD). First, sparsity-aware training introduces dynamic sparsity\nduring model optimization, effectively balancing parameter reduction and\ndetection accuracy. Second, we apply structured channel pruning by leveraging\nbatch normalization scaling factors to eliminate redundant channels,\nsignificantly reducing model size and computational complexity. Finally, to\nmitigate the accuracy drop caused by pruning, we employ CWD to transfer\nknowledge from the original model, using an adjustable temperature and loss\nweighting scheme tailored for small and medium object detection. Extensive\nexperiments on the VisDrone dataset demonstrate the effectiveness of our\napproach across multiple YOLOv8 variants. For YOLOv8m, our method reduces model\nparameters from 25.85M to 6.85M (a 73.51% reduction), FLOPs from 49.6G to\n13.3G, and MACs from 101G to 34.5G, while reducing AP50 by only 2.7%. The\nresulting compressed model achieves 47.9 AP50 and boosts inference speed from\n26 FPS (YOLOv8m baseline) to 45 FPS, enabling real-time deployment on edge\ndevices. We further apply TensorRT as a lightweight optimization step. While\nthis introduces a minor drop in AP50 (from 47.9 to 47.6), it significantly\nimproves inference speed from 45 to 68 FPS, demonstrating the practicality of\nour approach for high-throughput, re-source-constrained scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient deployment of deep learning models for aerial object detection on\nresource-constrained devices requires significant compression without\ncom-promising performance. In this study, we propose a novel three-stage\ncompression pipeline for the YOLOv8 object detection model, integrating\nsparsity-aware training, structured channel pruning, and Channel-Wise Knowledge\nDistillation (CWD). First, sparsity-aware training introduces dynamic sparsity\nduring model optimization, effectively balancing parameter reduction and\ndetection accuracy. Second, we apply structured channel pruning by leveraging\nbatch normalization scaling factors to eliminate redundant channels,\nsignificantly reducing model size and computational complexity. Finally, to\nmitigate the accuracy drop caused by pruning, we employ CWD to transfer\nknowledge from the original model, using an adjustable temperature and loss\nweighting scheme tailored for small and medium object detection. Extensive\nexperiments on the VisDrone dataset demonstrate the effectiveness of our\napproach across multiple YOLOv8 variants. For YOLOv8m, our method reduces model\nparameters from 25.85M to 6.85M (a 73.51% reduction), FLOPs from 49.6G to\n13.3G, and MACs from 101G to 34.5G, while reducing AP50 by only 2.7%. The\nresulting compressed model achieves 47.9 AP50 and boosts inference speed from\n26 FPS (YOLOv8m baseline) to 45 FPS, enabling real-time deployment on edge\ndevices. We further apply TensorRT as a lightweight optimization step. While\nthis introduces a minor drop in AP50 (from 47.9 to 47.6), it significantly\nimproves inference speed from 45 to 68 FPS, demonstrating the practicality of\nour approach for high-throughput, re-source-constrained scenarios."
                },
                "authors": [
                    {
                        "name": "Melika Sabaghian"
                    },
                    {
                        "name": "Mohammad Ali Keyvanrad"
                    },
                    {
                        "name": "Seyyedeh Mahila Moghadami"
                    }
                ],
                "author_detail": {
                    "name": "Seyyedeh Mahila Moghadami"
                },
                "author": "Seyyedeh Mahila Moghadami",
                "arxiv_comment": "28 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12918v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12918v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06652v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06652v2",
                "updated": "2025-09-17T12:55:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    12,
                    55,
                    31,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-08T13:07:35Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    7,
                    35,
                    0,
                    251,
                    0
                ],
                "title": "IntrEx: A Dataset for Modeling Engagement in Educational Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IntrEx: A Dataset for Modeling Engagement in Educational Conversations"
                },
                "summary": "Engagement and motivation are crucial for second-language acquisition, yet\nmaintaining learner interest in educational conversations remains a challenge.\nWhile prior research has explored what makes educational texts interesting,\nstill little is known about the linguistic features that drive engagement in\nconversations. To address this gap, we introduce IntrEx, the first large\ndataset annotated for interestingness and expected interestingness in\nteacher-student interactions. Built upon the Teacher-Student Chatroom Corpus\n(TSCC), IntrEx extends prior work by incorporating sequence-level annotations,\nallowing for the study of engagement beyond isolated turns to capture how\ninterest evolves over extended dialogues. We employ a rigorous annotation\nprocess with over 100 second-language learners, using a comparison-based rating\napproach inspired by reinforcement learning from human feedback (RLHF) to\nimprove agreement. We investigate whether large language models (LLMs) can\npredict human interestingness judgments. We find that LLMs (7B/8B parameters)\nfine-tuned on interestingness ratings outperform larger proprietary models like\nGPT-4o, demonstrating the potential for specialised datasets to model\nengagement in educational settings. Finally, we analyze how linguistic and\ncognitive factors, such as concreteness, comprehensibility (readability), and\nuptake, influence engagement in educational dialogues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engagement and motivation are crucial for second-language acquisition, yet\nmaintaining learner interest in educational conversations remains a challenge.\nWhile prior research has explored what makes educational texts interesting,\nstill little is known about the linguistic features that drive engagement in\nconversations. To address this gap, we introduce IntrEx, the first large\ndataset annotated for interestingness and expected interestingness in\nteacher-student interactions. Built upon the Teacher-Student Chatroom Corpus\n(TSCC), IntrEx extends prior work by incorporating sequence-level annotations,\nallowing for the study of engagement beyond isolated turns to capture how\ninterest evolves over extended dialogues. We employ a rigorous annotation\nprocess with over 100 second-language learners, using a comparison-based rating\napproach inspired by reinforcement learning from human feedback (RLHF) to\nimprove agreement. We investigate whether large language models (LLMs) can\npredict human interestingness judgments. We find that LLMs (7B/8B parameters)\nfine-tuned on interestingness ratings outperform larger proprietary models like\nGPT-4o, demonstrating the potential for specialised datasets to model\nengagement in educational settings. Finally, we analyze how linguistic and\ncognitive factors, such as concreteness, comprehensibility (readability), and\nuptake, influence engagement in educational dialogues."
                },
                "authors": [
                    {
                        "name": "Xingwei Tan"
                    },
                    {
                        "name": "Mahathi Parvatham"
                    },
                    {
                        "name": "Chiara Gambi"
                    },
                    {
                        "name": "Gabriele Pergola"
                    }
                ],
                "author_detail": {
                    "name": "Gabriele Pergola"
                },
                "author": "Gabriele Pergola",
                "arxiv_comment": "EMNLP 2025 Findings camera-ready, 9+7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06652v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06652v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10663v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10663v2",
                "updated": "2025-09-17T11:21:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    11,
                    21,
                    33,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-12T19:42:16Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    19,
                    42,
                    16,
                    4,
                    255,
                    0
                ],
                "title": "Context Copying Modulation: The Role of Entropy Neurons in Managing\n  Parametric and Contextual Knowledge Conflicts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Copying Modulation: The Role of Entropy Neurons in Managing\n  Parametric and Contextual Knowledge Conflicts"
                },
                "summary": "The behavior of Large Language Models (LLMs) when facing contextual\ninformation that conflicts with their internal parametric knowledge is\ninconsistent, with no generally accepted explanation for the expected outcome\ndistribution. Recent work has identified in autoregressive transformer models a\nclass of neurons -- called entropy neurons -- that produce a significant effect\non the model output entropy while having an overall moderate impact on the\nranking of the predicted tokens. In this paper, we investigate the preliminary\nclaim that these neurons are involved in inhibiting context copying behavior in\ntransformers by looking at their role in resolving conflicts between contextual\nand parametric information. We show that entropy neurons are responsible for\nsuppressing context copying across a range of LLMs, and that ablating them\nleads to a significant change in the generation process. These results enhance\nour understanding of the internal dynamics of LLMs when handling conflicting\ninformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The behavior of Large Language Models (LLMs) when facing contextual\ninformation that conflicts with their internal parametric knowledge is\ninconsistent, with no generally accepted explanation for the expected outcome\ndistribution. Recent work has identified in autoregressive transformer models a\nclass of neurons -- called entropy neurons -- that produce a significant effect\non the model output entropy while having an overall moderate impact on the\nranking of the predicted tokens. In this paper, we investigate the preliminary\nclaim that these neurons are involved in inhibiting context copying behavior in\ntransformers by looking at their role in resolving conflicts between contextual\nand parametric information. We show that entropy neurons are responsible for\nsuppressing context copying across a range of LLMs, and that ablating them\nleads to a significant change in the generation process. These results enhance\nour understanding of the internal dynamics of LLMs when handling conflicting\ninformation."
                },
                "authors": [
                    {
                        "name": "Zineddine Tighidet"
                    },
                    {
                        "name": "Andrea Mogini"
                    },
                    {
                        "name": "Hedi Ben-younes"
                    },
                    {
                        "name": "Jiali Mei"
                    },
                    {
                        "name": "Patrick Gallinari"
                    },
                    {
                        "name": "Benjamin Piwowarski"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Piwowarski"
                },
                "author": "Benjamin Piwowarski",
                "arxiv_comment": "Accepted at EMNLP 2025",
                "arxiv_journal_ref": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10663v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10663v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17514v2",
                "updated": "2025-09-17T11:19:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    11,
                    19,
                    18,
                    2,
                    260,
                    0
                ],
                "published": "2025-07-23T13:51:23Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    13,
                    51,
                    23,
                    2,
                    204,
                    0
                ],
                "title": "TAI Scan Tool: A RAG-Based Tool With Minimalistic Input for Trustworthy\n  AI Self-Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAI Scan Tool: A RAG-Based Tool With Minimalistic Input for Trustworthy\n  AI Self-Assessment"
                },
                "summary": "This paper introduces the TAI Scan Tool, a RAG-based TAI self-assessment tool\nwith minimalistic input. The current version of the tool supports the legal TAI\nassessment, with a particular emphasis on facilitating compliance with the AI\nAct. It involves a two-step approach with a pre-screening and an assessment\nphase. The assessment output of the system includes insight regarding the\nrisk-level of the AI system according to the AI Act, while at the same time\nretrieving relevant articles to aid with compliance and notify on their\nobligations. Our qualitative evaluation using use-case scenarios yields\npromising results, correctly predicting risk levels while retrieving relevant\narticles across three distinct semantic groups. Furthermore, interpretation of\nresults shows that the tool's reasoning relies on comparison with the setting\nof high-risk systems, a behaviour attributed to their deployment requiring\ncareful consideration, and therefore frequently presented within the AI Act.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the TAI Scan Tool, a RAG-based TAI self-assessment tool\nwith minimalistic input. The current version of the tool supports the legal TAI\nassessment, with a particular emphasis on facilitating compliance with the AI\nAct. It involves a two-step approach with a pre-screening and an assessment\nphase. The assessment output of the system includes insight regarding the\nrisk-level of the AI system according to the AI Act, while at the same time\nretrieving relevant articles to aid with compliance and notify on their\nobligations. Our qualitative evaluation using use-case scenarios yields\npromising results, correctly predicting risk levels while retrieving relevant\narticles across three distinct semantic groups. Furthermore, interpretation of\nresults shows that the tool's reasoning relies on comparison with the setting\nof high-risk systems, a behaviour attributed to their deployment requiring\ncareful consideration, and therefore frequently presented within the AI Act."
                },
                "authors": [
                    {
                        "name": "Athanasios Davvetas"
                    },
                    {
                        "name": "Xenia Ziouvelou"
                    },
                    {
                        "name": "Ypatia Dami"
                    },
                    {
                        "name": "Alexios Kaponis"
                    },
                    {
                        "name": "Konstantina Giouvanopoulou"
                    },
                    {
                        "name": "Michael Papademas"
                    }
                ],
                "author_detail": {
                    "name": "Michael Papademas"
                },
                "author": "Michael Papademas",
                "arxiv_comment": "9 pages, 1 figure, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13905v1",
                "updated": "2025-09-17T11:11:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    11,
                    11,
                    27,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T11:11:27Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    11,
                    11,
                    27,
                    2,
                    260,
                    0
                ],
                "title": "Do Large Language Models Understand Word Senses?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Understand Word Senses?"
                },
                "summary": "Understanding the meaning of words in context is a fundamental capability for\nLarge Language Models (LLMs). Despite extensive evaluation efforts, the extent\nto which LLMs show evidence that they truly grasp word senses remains\nunderexplored. In this paper, we address this gap by evaluating both i) the\nWord Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs,\ncomparing their performance to state-of-the-art systems specifically designed\nfor the task, and ii) the ability of two top-performing open- and closed-source\nLLMs to understand word senses in three generative settings: definition\ngeneration, free-form explanation, and example generation. Notably, we find\nthat, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve\nperformance on par with specialized WSD systems, while also demonstrating\ngreater robustness across domains and levels of difficulty. In the generation\ntasks, results reveal that LLMs can explain the meaning of words in context up\nto 98\\% accuracy, with the highest performance observed in the free-form\nexplanation task, which best aligns with their generative capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the meaning of words in context is a fundamental capability for\nLarge Language Models (LLMs). Despite extensive evaluation efforts, the extent\nto which LLMs show evidence that they truly grasp word senses remains\nunderexplored. In this paper, we address this gap by evaluating both i) the\nWord Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs,\ncomparing their performance to state-of-the-art systems specifically designed\nfor the task, and ii) the ability of two top-performing open- and closed-source\nLLMs to understand word senses in three generative settings: definition\ngeneration, free-form explanation, and example generation. Notably, we find\nthat, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve\nperformance on par with specialized WSD systems, while also demonstrating\ngreater robustness across domains and levels of difficulty. In the generation\ntasks, results reveal that LLMs can explain the meaning of words in context up\nto 98\\% accuracy, with the highest performance observed in the free-form\nexplanation task, which best aligns with their generative capabilities."
                },
                "authors": [
                    {
                        "name": "Domenico Meconi"
                    },
                    {
                        "name": "Simone Stirpe"
                    },
                    {
                        "name": "Federico Martelli"
                    },
                    {
                        "name": "Leonardo Lavalle"
                    },
                    {
                        "name": "Roberto Navigli"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Navigli"
                },
                "author": "Roberto Navigli",
                "arxiv_comment": "20 pages, to be published in EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11062v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11062v2",
                "updated": "2025-09-17T11:06:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    11,
                    6,
                    49,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-14T03:05:54Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    3,
                    5,
                    54,
                    6,
                    257,
                    0
                ],
                "title": "Auto-Slides: An Interactive Multi-Agent System for Creating and\n  Customizing Research Presentations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-Slides: An Interactive Multi-Agent System for Creating and\n  Customizing Research Presentations"
                },
                "summary": "The rapid progress of large language models (LLMs) has opened new\nopportunities for education. While learners can interact with academic papers\nthrough LLM-powered dialogue, limitations still exist: absence of structured\norganization and high text reliance can impede systematic understanding and\nengagement with complex concepts. To address these challenges, we propose\nAuto-Slides, an LLM-driven system that converts research papers into\npedagogically structured, multimodal slides (e.g., diagrams and tables).\nDrawing on cognitive science, it creates a presentation-oriented narrative and\nallows iterative refinement via an interactive editor, in order to match\nlearners' knowledge level and goals. Auto-Slides further incorporates\nverification and knowledge retrieval mechanisms to ensure accuracy and\ncontextual completeness. Through extensive user studies, Auto-Slides enhances\nlearners' comprehension and engagement compared to conventional LLM-based\nreading. Our contributions lie in designing a multi-agent framework for\ntransforming academic papers into pedagogically optimized slides and\nintroducing interactive customization for personalized learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress of large language models (LLMs) has opened new\nopportunities for education. While learners can interact with academic papers\nthrough LLM-powered dialogue, limitations still exist: absence of structured\norganization and high text reliance can impede systematic understanding and\nengagement with complex concepts. To address these challenges, we propose\nAuto-Slides, an LLM-driven system that converts research papers into\npedagogically structured, multimodal slides (e.g., diagrams and tables).\nDrawing on cognitive science, it creates a presentation-oriented narrative and\nallows iterative refinement via an interactive editor, in order to match\nlearners' knowledge level and goals. Auto-Slides further incorporates\nverification and knowledge retrieval mechanisms to ensure accuracy and\ncontextual completeness. Through extensive user studies, Auto-Slides enhances\nlearners' comprehension and engagement compared to conventional LLM-based\nreading. Our contributions lie in designing a multi-agent framework for\ntransforming academic papers into pedagogically optimized slides and\nintroducing interactive customization for personalized learning."
                },
                "authors": [
                    {
                        "name": "Yuheng Yang"
                    },
                    {
                        "name": "Wenjia Jiang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Chi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chi Zhang"
                },
                "author": "Chi Zhang",
                "arxiv_comment": "Project Homepage: https://auto-slides.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11062v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11062v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13901v1",
                "updated": "2025-09-17T11:04:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    11,
                    4,
                    30,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T11:04:30Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    11,
                    4,
                    30,
                    2,
                    260,
                    0
                ],
                "title": "Performance Evaluation of Intent-Based Networking Scenarios: A GitOps\n  and Nephio Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Evaluation of Intent-Based Networking Scenarios: A GitOps\n  and Nephio Approach"
                },
                "summary": "GitOps has emerged as a foundational paradigm for managing cloud-native\ninfrastructures by enabling declarative configuration, version-controlled\nstate, and automated reconciliation between intents and runtime deployments.\nDespite its widespread adoption, the performance and scalability of GitOps\ntools in Intent-Based Networking (IBN) scenarios are insufficiently evaluated.\nThis paper presents a reproducible, metric-driven benchmarking, assessing the\nlatency and resource overheads of three widely used GitOps operators: Argo CD,\nFlux CD, and ConfigSync. We conduct controlled experiments under both single-\nand multi-intent scenarios, capturing key performance indicators such as\nlatency and resource consumption. Our results highlight trade-offs between the\ntools in terms of determinism, resource efficiency, and responsiveness. We\nfurther investigate a realistic orchestration scenario, using Nephio as our\norchestrator, to quantify the processing latency and overhead in declarative\nend-to-end deployment pipelines. Our findings can offer valuable insights for\ntool selection and optimisation in future autonomous network orchestration\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GitOps has emerged as a foundational paradigm for managing cloud-native\ninfrastructures by enabling declarative configuration, version-controlled\nstate, and automated reconciliation between intents and runtime deployments.\nDespite its widespread adoption, the performance and scalability of GitOps\ntools in Intent-Based Networking (IBN) scenarios are insufficiently evaluated.\nThis paper presents a reproducible, metric-driven benchmarking, assessing the\nlatency and resource overheads of three widely used GitOps operators: Argo CD,\nFlux CD, and ConfigSync. We conduct controlled experiments under both single-\nand multi-intent scenarios, capturing key performance indicators such as\nlatency and resource consumption. Our results highlight trade-offs between the\ntools in terms of determinism, resource efficiency, and responsiveness. We\nfurther investigate a realistic orchestration scenario, using Nephio as our\norchestrator, to quantify the processing latency and overhead in declarative\nend-to-end deployment pipelines. Our findings can offer valuable insights for\ntool selection and optimisation in future autonomous network orchestration\nsystems."
                },
                "authors": [
                    {
                        "name": "Saptarshi Ghosh"
                    },
                    {
                        "name": "Ioannis Mavromatis"
                    },
                    {
                        "name": "Konstantinos Antonakoglou"
                    },
                    {
                        "name": "Konstantinos Katsaros"
                    }
                ],
                "author_detail": {
                    "name": "Konstantinos Katsaros"
                },
                "author": "Konstantinos Katsaros",
                "arxiv_comment": "Accepted for publication at IEEE CSCN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05170v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05170v2",
                "updated": "2025-09-17T10:56:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    10,
                    56,
                    50,
                    2,
                    260,
                    0
                ],
                "published": "2025-08-07T09:04:10Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    4,
                    10,
                    3,
                    219,
                    0
                ],
                "title": "Posterior-GRPO: Rewarding Reasoning Processes in Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Posterior-GRPO: Rewarding Reasoning Processes in Code Generation"
                },
                "summary": "Reinforcement learning (RL) has significantly advanced code generation for\nlarge language models (LLMs). However, current paradigms rely on outcome-based\nrewards from test cases, neglecting the quality of the intermediate reasoning\nprocess. While supervising the reasoning process directly is a promising\ndirection, it is highly susceptible to reward hacking, where the policy model\nlearns to exploit the reasoning reward signal without improving final outcomes.\nTo address this, we introduce a unified framework that can effectively\nincorporate the quality of the reasoning process during RL. First, to enable\nreasoning evaluation, we develop LCB-RB, a benchmark comprising preference\npairs of superior and inferior reasoning processes. Second, to accurately score\nreasoning quality, we introduce an Optimized-Degraded based (OD-based) method\nfor reward model training. This method generates high-quality preference pairs\nby systematically optimizing and degrading initial reasoning paths along\ncurated dimensions of reasoning quality, such as factual accuracy, logical\nrigor, and coherence. A 7B parameter reward model with this method achieves\nstate-of-the-art (SOTA) performance on LCB-RB and generalizes well to other\nbenchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method\nthat conditions process-based rewards on task success. By selectively applying\nrewards to the reasoning processes of only successful outcomes, P-GRPO\neffectively mitigates reward hacking and aligns the model's internal reasoning\nwith final code correctness. A 7B parameter model with P-GRPO achieves superior\nperformance across diverse code generation tasks, outperforming outcome-only\nbaselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further\ndemonstrate the generalizability of our approach by extending it to\nmathematical tasks. Our models, dataset, and code are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has significantly advanced code generation for\nlarge language models (LLMs). However, current paradigms rely on outcome-based\nrewards from test cases, neglecting the quality of the intermediate reasoning\nprocess. While supervising the reasoning process directly is a promising\ndirection, it is highly susceptible to reward hacking, where the policy model\nlearns to exploit the reasoning reward signal without improving final outcomes.\nTo address this, we introduce a unified framework that can effectively\nincorporate the quality of the reasoning process during RL. First, to enable\nreasoning evaluation, we develop LCB-RB, a benchmark comprising preference\npairs of superior and inferior reasoning processes. Second, to accurately score\nreasoning quality, we introduce an Optimized-Degraded based (OD-based) method\nfor reward model training. This method generates high-quality preference pairs\nby systematically optimizing and degrading initial reasoning paths along\ncurated dimensions of reasoning quality, such as factual accuracy, logical\nrigor, and coherence. A 7B parameter reward model with this method achieves\nstate-of-the-art (SOTA) performance on LCB-RB and generalizes well to other\nbenchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method\nthat conditions process-based rewards on task success. By selectively applying\nrewards to the reasoning processes of only successful outcomes, P-GRPO\neffectively mitigates reward hacking and aligns the model's internal reasoning\nwith final code correctness. A 7B parameter model with P-GRPO achieves superior\nperformance across diverse code generation tasks, outperforming outcome-only\nbaselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further\ndemonstrate the generalizability of our approach by extending it to\nmathematical tasks. Our models, dataset, and code are publicly available."
                },
                "authors": [
                    {
                        "name": "Lishui Fan"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Mouxiang Chen"
                    },
                    {
                        "name": "Zhongxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhongxin Liu"
                },
                "author": "Zhongxin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05170v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05170v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13899v1",
                "updated": "2025-09-17T10:54:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    10,
                    54,
                    17,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T10:54:17Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    10,
                    54,
                    17,
                    2,
                    260,
                    0
                ],
                "title": "AI as a teaching tool and learning partner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI as a teaching tool and learning partner"
                },
                "summary": "The arrival of AI tools and in particular Large Language Models (LLMs) has\nhad a transformative impact on teaching and learning and institutes are still\ntrying to determine how to integrate LLMs into education in constructive ways.\nHere, we explore the adoption of LLM-based tools into two teaching programmes,\none undergraduate and one postgraduate. We provided to our classes (1) a\nLLM-powered chatbot that had access to course materials by RAG and (2)\nAI-generated audio-only podcasts for each week$\\text{'}$s teaching material. At\nthe end of the semester, we surveyed the classes to gauge attitudes towards\nthese tools. The classes were small and from biological courses. The students\nfelt positive about AI generally and that AI tools made a positive impact on\nteaching. Students found the LLM-powered chatbot easy and enjoyable to use and\nfelt that it enhanced their learning. The podcasts were less popular and only a\nsmall proportion of the class listened weekly. The class as a whole was\nindifferent to whether the podcasts should be used more widely across courses,\nbut those who listened enjoyed them and were in favour.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The arrival of AI tools and in particular Large Language Models (LLMs) has\nhad a transformative impact on teaching and learning and institutes are still\ntrying to determine how to integrate LLMs into education in constructive ways.\nHere, we explore the adoption of LLM-based tools into two teaching programmes,\none undergraduate and one postgraduate. We provided to our classes (1) a\nLLM-powered chatbot that had access to course materials by RAG and (2)\nAI-generated audio-only podcasts for each week$\\text{'}$s teaching material. At\nthe end of the semester, we surveyed the classes to gauge attitudes towards\nthese tools. The classes were small and from biological courses. The students\nfelt positive about AI generally and that AI tools made a positive impact on\nteaching. Students found the LLM-powered chatbot easy and enjoyable to use and\nfelt that it enhanced their learning. The podcasts were less popular and only a\nsmall proportion of the class listened weekly. The class as a whole was\nindifferent to whether the podcasts should be used more widely across courses,\nbut those who listened enjoyed them and were in favour."
                },
                "authors": [
                    {
                        "name": "Steven Watterson"
                    },
                    {
                        "name": "Sarah Atkinson"
                    },
                    {
                        "name": "Elaine Murray"
                    },
                    {
                        "name": "Andrew McDowell"
                    }
                ],
                "author_detail": {
                    "name": "Andrew McDowell"
                },
                "author": "Andrew McDowell",
                "arxiv_comment": "6 Pages, 1 Figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13892v1",
                "updated": "2025-09-17T10:42:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    10,
                    42,
                    6,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T10:42:06Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    10,
                    42,
                    6,
                    2,
                    260,
                    0
                ],
                "title": "Synthetic Data Generation for Screen Time and App Usage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Data Generation for Screen Time and App Usage"
                },
                "summary": "Smartphone usage data can provide valuable insights for understanding\ninteraction with technology and human behavior. However, collecting\nlarge-scale, in-the-wild smartphone usage logs is challenging due to high\ncosts, privacy concerns, under representative user samples and biases like\nnon-response that can skew results. These challenges call for exploring\nalternative approaches to obtain smartphone usage datasets. In this context,\nlarge language models (LLMs) such as Open AI's ChatGPT present a novel approach\nfor synthetic smartphone usage data generation, addressing limitations of\nreal-world data collection. We describe a case study on how four prompt\nstrategies influenced the quality of generated smartphone usage data. We\ncontribute with insights on prompt design and measures of data quality,\nreporting a prompting strategy comparison combining two factors, prompt level\nof detail (describing a user persona, describing the expected results\ncharacteristics) and seed data inclusion (with versus without an initial real\nusage example). Our findings suggest that using LLMs to generate structured and\nbehaviorally plausible smartphone use datasets is feasible for some use cases,\nespecially when using detailed prompts. Challenges remain in capturing diverse\nnuances of human behavioral patterns in a single synthetic dataset, and\nevaluating tradeoffs between data fidelity and diversity, suggesting the need\nfor use-case-specific evaluation metrics and future research with more diverse\nseed data and different LLM models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smartphone usage data can provide valuable insights for understanding\ninteraction with technology and human behavior. However, collecting\nlarge-scale, in-the-wild smartphone usage logs is challenging due to high\ncosts, privacy concerns, under representative user samples and biases like\nnon-response that can skew results. These challenges call for exploring\nalternative approaches to obtain smartphone usage datasets. In this context,\nlarge language models (LLMs) such as Open AI's ChatGPT present a novel approach\nfor synthetic smartphone usage data generation, addressing limitations of\nreal-world data collection. We describe a case study on how four prompt\nstrategies influenced the quality of generated smartphone usage data. We\ncontribute with insights on prompt design and measures of data quality,\nreporting a prompting strategy comparison combining two factors, prompt level\nof detail (describing a user persona, describing the expected results\ncharacteristics) and seed data inclusion (with versus without an initial real\nusage example). Our findings suggest that using LLMs to generate structured and\nbehaviorally plausible smartphone use datasets is feasible for some use cases,\nespecially when using detailed prompts. Challenges remain in capturing diverse\nnuances of human behavioral patterns in a single synthetic dataset, and\nevaluating tradeoffs between data fidelity and diversity, suggesting the need\nfor use-case-specific evaluation metrics and future research with more diverse\nseed data and different LLM models."
                },
                "authors": [
                    {
                        "name": "Gustavo Kruger"
                    },
                    {
                        "name": "Nikhil Sachdeva"
                    },
                    {
                        "name": "Michael Sobolev"
                    }
                ],
                "author_detail": {
                    "name": "Michael Sobolev"
                },
                "author": "Michael Sobolev",
                "arxiv_comment": "14 pages",
                "arxiv_journal_ref": "International Conference on Computer-Human Interaction Research\n  and Applications (CHIRA) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23512v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23512v6",
                "updated": "2025-09-17T10:22:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    10,
                    22,
                    35,
                    2,
                    260,
                    0
                ],
                "published": "2025-03-30T16:48:27Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    16,
                    48,
                    27,
                    6,
                    89,
                    0
                ],
                "title": "SCORE: Story Coherence and Retrieval Enhancement for AI Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCORE: Story Coherence and Retrieval Enhancement for AI Narratives"
                },
                "summary": "Large Language Models (LLMs) can generate creative and engaging narratives\nfrom user-specified input, but maintaining coherence and emotional depth\nthroughout these AI-generated stories remains a challenge. In this work, we\npropose SCORE, a framework for Story Coherence and Retrieval Enhancement,\ndesigned to detect and resolve narrative inconsistencies. By tracking key item\nstatuses and generating episode summaries, SCORE uses a Retrieval-Augmented\nGeneration (RAG) approach to identify related episodes and enhance the overall\nstory structure. Experimental results from testing multiple LLM-generated\nstories demonstrate that SCORE significantly improves the consistency and\nstability of narrative coherence compared to baseline GPT models, providing a\nmore robust method for evaluating and refining AI-generated narratives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can generate creative and engaging narratives\nfrom user-specified input, but maintaining coherence and emotional depth\nthroughout these AI-generated stories remains a challenge. In this work, we\npropose SCORE, a framework for Story Coherence and Retrieval Enhancement,\ndesigned to detect and resolve narrative inconsistencies. By tracking key item\nstatuses and generating episode summaries, SCORE uses a Retrieval-Augmented\nGeneration (RAG) approach to identify related episodes and enhance the overall\nstory structure. Experimental results from testing multiple LLM-generated\nstories demonstrate that SCORE significantly improves the consistency and\nstability of narrative coherence compared to baseline GPT models, providing a\nmore robust method for evaluating and refining AI-generated narratives."
                },
                "authors": [
                    {
                        "name": "Qiang Yi"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Jianhui Wang"
                    },
                    {
                        "name": "Xinyuan Song"
                    },
                    {
                        "name": "ShiYao Qian"
                    },
                    {
                        "name": "Xinhang Yuan"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Yijin Wang"
                    },
                    {
                        "name": "Jingqun Tang"
                    },
                    {
                        "name": "Yuchen Li"
                    },
                    {
                        "name": "Junjiang Lin"
                    },
                    {
                        "name": "Hongyang He"
                    },
                    {
                        "name": "Zhen Tian"
                    },
                    {
                        "name": "Tianxiang Xu"
                    },
                    {
                        "name": "Keqin Li"
                    },
                    {
                        "name": "Kuan Lu"
                    },
                    {
                        "name": "Menghao Huo"
                    },
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Miao Zhang"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Jianyuan Ni"
                    }
                ],
                "author_detail": {
                    "name": "Jianyuan Ni"
                },
                "author": "Jianyuan Ni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23512v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23512v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13869v1",
                "updated": "2025-09-17T09:58:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    58,
                    28,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T09:58:28Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    58,
                    28,
                    2,
                    260,
                    0
                ],
                "title": "Do LLMs Align Human Values Regarding Social Biases? Judging and\n  Explaining Social Biases with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Align Human Values Regarding Social Biases? Judging and\n  Explaining Social Biases with LLMs"
                },
                "summary": "Large language models (LLMs) can lead to undesired consequences when\nmisaligned with human values, especially in scenarios involving complex and\nsensitive social biases. Previous studies have revealed the misalignment of\nLLMs with human values using expert-designed or agent-based emulated bias\nscenarios. However, it remains unclear whether the alignment of LLMs with human\nvalues differs across different types of scenarios (e.g., scenarios containing\nnegative vs. non-negative questions). In this study, we investigate the\nalignment of LLMs with human values regarding social biases (HVSB) in different\ntypes of bias scenarios. Through extensive analysis of 12 LLMs from four model\nfamilies and four datasets, we demonstrate that LLMs with large model parameter\nscales do not necessarily have lower misalignment rate and attack success rate.\nMoreover, LLMs show a certain degree of alignment preference for specific types\nof scenarios and the LLMs from the same model family tend to have higher\njudgment consistency. In addition, we study the understanding capacity of LLMs\nwith their explanations of HVSB. We find no significant differences in the\nunderstanding of HVSB across LLMs. We also find LLMs prefer their own generated\nexplanations. Additionally, we endow smaller language models (LMs) with the\nability to explain HVSB. The generation results show that the explanations\ngenerated by the fine-tuned smaller LMs are more readable, but have a\nrelatively lower model agreeability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can lead to undesired consequences when\nmisaligned with human values, especially in scenarios involving complex and\nsensitive social biases. Previous studies have revealed the misalignment of\nLLMs with human values using expert-designed or agent-based emulated bias\nscenarios. However, it remains unclear whether the alignment of LLMs with human\nvalues differs across different types of scenarios (e.g., scenarios containing\nnegative vs. non-negative questions). In this study, we investigate the\nalignment of LLMs with human values regarding social biases (HVSB) in different\ntypes of bias scenarios. Through extensive analysis of 12 LLMs from four model\nfamilies and four datasets, we demonstrate that LLMs with large model parameter\nscales do not necessarily have lower misalignment rate and attack success rate.\nMoreover, LLMs show a certain degree of alignment preference for specific types\nof scenarios and the LLMs from the same model family tend to have higher\njudgment consistency. In addition, we study the understanding capacity of LLMs\nwith their explanations of HVSB. We find no significant differences in the\nunderstanding of HVSB across LLMs. We also find LLMs prefer their own generated\nexplanations. Additionally, we endow smaller language models (LMs) with the\nability to explain HVSB. The generation results show that the explanations\ngenerated by the fine-tuned smaller LMs are more readable, but have a\nrelatively lower model agreeability."
                },
                "authors": [
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Chenhui Chu"
                    }
                ],
                "author_detail": {
                    "name": "Chenhui Chu"
                },
                "author": "Chenhui Chu",
                "arxiv_comment": "38 pages, 31 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13868v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13868v1",
                "updated": "2025-09-17T09:58:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    58,
                    26,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T09:58:26Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    58,
                    26,
                    2,
                    260,
                    0
                ],
                "title": "Are Prompts All You Need? Evaluating Prompt-Based Large Language Models\n  (LLM)s for Software Requirements Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Prompts All You Need? Evaluating Prompt-Based Large Language Models\n  (LLM)s for Software Requirements Classification"
                },
                "summary": "Requirements classification assigns natural language requirements to\npredefined classes, such as functional and non functional. Accurate\nclassification reduces risk and improves software quality. Most existing models\nrely on supervised learning, which needs large labeled data that are costly,\nslow to create, and domain dependent; they also generalize poorly and often\nrequire retraining for each task. This study tests whether prompt based large\nlanguage models can reduce data needs. We benchmark several models and\nprompting styles (zero shot, few shot, persona, and chain of thought) across\nmultiple tasks on two English datasets, PROMISE and SecReq. For each task we\ncompare model prompt configurations and then compare the best LLM setups with a\nstrong fine tuned transformer baseline. Results show that prompt based LLMs,\nespecially with few shot prompts, can match or exceed the baseline. Adding a\npersona, or persona plus chain of thought, can yield further gains. We conclude\nthat prompt based LLMs are a practical and scalable option that reduces\ndependence on large annotations and can improve generalizability across tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Requirements classification assigns natural language requirements to\npredefined classes, such as functional and non functional. Accurate\nclassification reduces risk and improves software quality. Most existing models\nrely on supervised learning, which needs large labeled data that are costly,\nslow to create, and domain dependent; they also generalize poorly and often\nrequire retraining for each task. This study tests whether prompt based large\nlanguage models can reduce data needs. We benchmark several models and\nprompting styles (zero shot, few shot, persona, and chain of thought) across\nmultiple tasks on two English datasets, PROMISE and SecReq. For each task we\ncompare model prompt configurations and then compare the best LLM setups with a\nstrong fine tuned transformer baseline. Results show that prompt based LLMs,\nespecially with few shot prompts, can match or exceed the baseline. Adding a\npersona, or persona plus chain of thought, can yield further gains. We conclude\nthat prompt based LLMs are a practical and scalable option that reduces\ndependence on large annotations and can improve generalizability across tasks."
                },
                "authors": [
                    {
                        "name": "Manal Binkhonain"
                    },
                    {
                        "name": "Reem Alfayaz"
                    }
                ],
                "author_detail": {
                    "name": "Reem Alfayaz"
                },
                "author": "Reem Alfayaz",
                "arxiv_doi": "10.1007/s00766-025-00451-8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s00766-025-00451-8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.13868v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13868v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "33 pages, 12 figures",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13858v1",
                "updated": "2025-09-17T09:48:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    48,
                    39,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T09:48:39Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    48,
                    39,
                    2,
                    260,
                    0
                ],
                "title": "EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics"
                },
                "summary": "Dataset distillation aims to synthesize a compact dataset from the original\nlarge-scale one, enabling highly efficient learning while preserving\ncompetitive model performance. However, traditional techniques primarily\ncapture low-level visual features, neglecting the high-level semantic and\nstructural information inherent in images. In this paper, we propose EDITS, a\nnovel framework that exploits the implicit textual semantics within the image\ndata to achieve enhanced distillation. First, external texts generated by a\nVision Language Model (VLM) are fused with image features through a Global\nSemantic Query module, forming the prior clustered buffer. Local Semantic\nAwareness then selects representative samples from the buffer to construct\nimage and text prototypes, with the latter produced by guiding a Large Language\nModel (LLM) with meticulously crafted prompt. Ultimately, Dual Prototype\nGuidance strategy generates the final synthetic dataset through a diffusion\nmodel. Extensive experiments confirm the effectiveness of our method.Source\ncode is available in: https://github.com/einsteinxia/EDITS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dataset distillation aims to synthesize a compact dataset from the original\nlarge-scale one, enabling highly efficient learning while preserving\ncompetitive model performance. However, traditional techniques primarily\ncapture low-level visual features, neglecting the high-level semantic and\nstructural information inherent in images. In this paper, we propose EDITS, a\nnovel framework that exploits the implicit textual semantics within the image\ndata to achieve enhanced distillation. First, external texts generated by a\nVision Language Model (VLM) are fused with image features through a Global\nSemantic Query module, forming the prior clustered buffer. Local Semantic\nAwareness then selects representative samples from the buffer to construct\nimage and text prototypes, with the latter produced by guiding a Large Language\nModel (LLM) with meticulously crafted prompt. Ultimately, Dual Prototype\nGuidance strategy generates the final synthetic dataset through a diffusion\nmodel. Extensive experiments confirm the effectiveness of our method.Source\ncode is available in: https://github.com/einsteinxia/EDITS."
                },
                "authors": [
                    {
                        "name": "Qianxin Xia"
                    },
                    {
                        "name": "Jiawei Du"
                    },
                    {
                        "name": "Guoming Lu"
                    },
                    {
                        "name": "Zhiyong Shu"
                    },
                    {
                        "name": "Jielei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jielei Wang"
                },
                "author": "Jielei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21710v2",
                "updated": "2025-09-17T09:36:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    36,
                    5,
                    2,
                    260,
                    0
                ],
                "published": "2025-03-27T17:21:47Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    21,
                    47,
                    3,
                    86,
                    0
                ],
                "title": "KGCompass: Knowledge Graph Enhanced Repository-Level Software Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KGCompass: Knowledge Graph Enhanced Repository-Level Software Repair"
                },
                "summary": "Repository-level software repair faces challenges in bridging semantic gaps\nbetween issue descriptions and code patches. Existing approaches, which\nprimarily rely on large language models (LLMs), are hindered by semantic\nambiguities, limited understanding of structural context, and insufficient\nreasoning capabilities. To address these limitations, we propose KGCompass with\ntwo innovations: (1) a novel repository-aware knowledge graph (KG) that\naccurately links repository artifacts (issues and pull requests) and codebase\nentities (files, classes, and functions), allowing us to effectively narrow\ndown the vast search space to only 20 most relevant functions with accurate\ncandidate fault locations and contextual information, and (2) a path-guided\nrepair mechanism that leverages KG-mined entity paths, tracing through which\nallows us to augment LLMs with relevant contextual information to generate\nprecise patches along with their explanations. Experimental results in the\nSWE-bench Lite demonstrate that KGCompass achieves state-of-the-art single-LLM\nrepair performance (58.3%) and function-level fault location accuracy (56.0%)\nacross open-source approaches with a single repair model, costing only $0.2 per\nrepair. Among the bugs that KGCompass successfully localizes, 89.7% lack\nexplicit location hints in the issue and are found only through multi-hop graph\ntraversal, where pure LLMs struggle to locate bugs accurately. Relative to\npure-LLM baselines, KGCompass lifts the resolved rate by 50.8% on Claude-4\nSonnet, 30.2% on Claude-3.5 Sonnet, 115.7% on DeepSeek-V3, and 156.4% on\nQwen2.5 Max. These consistent improvements demonstrate that this graph-guided\nrepair framework delivers model-agnostic, cost-efficient repair and sets a\nstrong new baseline for repository-level repair.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository-level software repair faces challenges in bridging semantic gaps\nbetween issue descriptions and code patches. Existing approaches, which\nprimarily rely on large language models (LLMs), are hindered by semantic\nambiguities, limited understanding of structural context, and insufficient\nreasoning capabilities. To address these limitations, we propose KGCompass with\ntwo innovations: (1) a novel repository-aware knowledge graph (KG) that\naccurately links repository artifacts (issues and pull requests) and codebase\nentities (files, classes, and functions), allowing us to effectively narrow\ndown the vast search space to only 20 most relevant functions with accurate\ncandidate fault locations and contextual information, and (2) a path-guided\nrepair mechanism that leverages KG-mined entity paths, tracing through which\nallows us to augment LLMs with relevant contextual information to generate\nprecise patches along with their explanations. Experimental results in the\nSWE-bench Lite demonstrate that KGCompass achieves state-of-the-art single-LLM\nrepair performance (58.3%) and function-level fault location accuracy (56.0%)\nacross open-source approaches with a single repair model, costing only $0.2 per\nrepair. Among the bugs that KGCompass successfully localizes, 89.7% lack\nexplicit location hints in the issue and are found only through multi-hop graph\ntraversal, where pure LLMs struggle to locate bugs accurately. Relative to\npure-LLM baselines, KGCompass lifts the resolved rate by 50.8% on Claude-4\nSonnet, 30.2% on Claude-3.5 Sonnet, 115.7% on DeepSeek-V3, and 156.4% on\nQwen2.5 Max. These consistent improvements demonstrate that this graph-guided\nrepair framework delivers model-agnostic, cost-efficient repair and sets a\nstrong new baseline for repository-level repair."
                },
                "authors": [
                    {
                        "name": "Boyang Yang"
                    },
                    {
                        "name": "Jiadong Ren"
                    },
                    {
                        "name": "Shunfu Jin"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Bach Le"
                    },
                    {
                        "name": "Haoye Tian"
                    }
                ],
                "author_detail": {
                    "name": "Haoye Tian"
                },
                "author": "Haoye Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15214v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15214v2",
                "updated": "2025-09-17T09:34:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    34,
                    53,
                    2,
                    260,
                    0
                ],
                "published": "2025-08-21T03:56:38Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    56,
                    38,
                    3,
                    233,
                    0
                ],
                "title": "Self-Guided Function Calling in Large Language Models via Stepwise\n  Experience Recall",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Guided Function Calling in Large Language Models via Stepwise\n  Experience Recall"
                },
                "summary": "Function calling enables large language models (LLMs) to interact with\nexternal systems by leveraging tools and APIs. When faced with multi-step tool\nusage, LLMs still struggle with tool selection, parameter generation, and\ntool-chain planning. Existing methods typically rely on manually designing\ntask-specific demonstrations, or retrieving from a curated library. These\napproaches demand substantial expert effort and prompt engineering becomes\nincreasingly complex and inefficient as tool diversity and task difficulty\nscale. To address these challenges, we propose a self-guided method, Stepwise\nExperience Recall (SEER), which performs fine-grained, stepwise retrieval from\na continually updated experience pool. Instead of relying on static or manually\ncurated library, SEER incrementally augments the experience pool with past\nsuccessful trajectories, enabling continuous expansion of the pool and improved\nmodel performance over time. Evaluated on the ToolQA benchmark, SEER achieves\nan average improvement of 6.1% on easy and 4.7% on hard questions. We further\ntest SEER on $\\tau$-bench, which includes two real-world domains. Powered by\nQwen2.5-7B and Qwen2.5-72B models, SEER demonstrates substantial accuracy gains\nof 7.44% and 23.38%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Function calling enables large language models (LLMs) to interact with\nexternal systems by leveraging tools and APIs. When faced with multi-step tool\nusage, LLMs still struggle with tool selection, parameter generation, and\ntool-chain planning. Existing methods typically rely on manually designing\ntask-specific demonstrations, or retrieving from a curated library. These\napproaches demand substantial expert effort and prompt engineering becomes\nincreasingly complex and inefficient as tool diversity and task difficulty\nscale. To address these challenges, we propose a self-guided method, Stepwise\nExperience Recall (SEER), which performs fine-grained, stepwise retrieval from\na continually updated experience pool. Instead of relying on static or manually\ncurated library, SEER incrementally augments the experience pool with past\nsuccessful trajectories, enabling continuous expansion of the pool and improved\nmodel performance over time. Evaluated on the ToolQA benchmark, SEER achieves\nan average improvement of 6.1% on easy and 4.7% on hard questions. We further\ntest SEER on $\\tau$-bench, which includes two real-world domains. Powered by\nQwen2.5-7B and Qwen2.5-72B models, SEER demonstrates substantial accuracy gains\nof 7.44% and 23.38%, respectively."
                },
                "authors": [
                    {
                        "name": "Sijia Cui"
                    },
                    {
                        "name": "Aiyao He"
                    },
                    {
                        "name": "Shuai Xu"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Yanna Wang"
                    },
                    {
                        "name": "Qingyang Zhang"
                    },
                    {
                        "name": "Yajing Wang"
                    },
                    {
                        "name": "Bo Xu"
                    }
                ],
                "author_detail": {
                    "name": "Bo Xu"
                },
                "author": "Bo Xu",
                "arxiv_comment": "Accepted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15214v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15214v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20923v2",
                "updated": "2025-09-17T09:33:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    33,
                    21,
                    2,
                    260,
                    0
                ],
                "published": "2025-07-28T15:26:43Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    15,
                    26,
                    43,
                    0,
                    209,
                    0
                ],
                "title": "Pareto-Grid-Guided Large Language Models for Fast and High-Quality\n  Heuristics Design in Multi-Objective Combinatorial Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pareto-Grid-Guided Large Language Models for Fast and High-Quality\n  Heuristics Design in Multi-Objective Combinatorial Optimization"
                },
                "summary": "Multi-objective combinatorial optimization problems (MOCOP) frequently arise\nin practical applications that require the simultaneous optimization of\nconflicting objectives. Although traditional evolutionary algorithms can be\neffective, they typically depend on domain knowledge and repeated parameter\ntuning, limiting flexibility when applied to unseen MOCOP instances. Recently,\nintegration of Large Language Models (LLMs) into evolutionary computation has\nopened new avenues for automatic heuristic generation, using their advanced\nlanguage understanding and code synthesis capabilities. Nevertheless, most\nexisting approaches predominantly focus on single-objective tasks, often\nneglecting key considerations such as runtime efficiency and heuristic\ndiversity in multi-objective settings. To bridge this gap, we introduce\nMulti-heuristics for MOCOP via Pareto-Grid-guided Evolution of LLMs (MPaGE), a\nnovel enhancement of the Simple Evolutionary Multiobjective Optimization (SEMO)\nframework that leverages LLMs and Pareto Front Grid (PFG) technique. By\npartitioning the objective space into grids and retaining top-performing\ncandidates to guide heuristic generation, MPaGE utilizes LLMs to prioritize\nheuristics with semantically distinct logical structures during variation, thus\npromoting diversity and mitigating redundancy within the population. Through\nextensive evaluations, MPaGE demonstrates superior performance over existing\nLLM-based frameworks, and achieves competitive results to traditional\nMulti-objective evolutionary algorithms (MOEAs), with significantly faster\nruntime. Our code is available at: https://github.com/langkhachhoha/MPaGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-objective combinatorial optimization problems (MOCOP) frequently arise\nin practical applications that require the simultaneous optimization of\nconflicting objectives. Although traditional evolutionary algorithms can be\neffective, they typically depend on domain knowledge and repeated parameter\ntuning, limiting flexibility when applied to unseen MOCOP instances. Recently,\nintegration of Large Language Models (LLMs) into evolutionary computation has\nopened new avenues for automatic heuristic generation, using their advanced\nlanguage understanding and code synthesis capabilities. Nevertheless, most\nexisting approaches predominantly focus on single-objective tasks, often\nneglecting key considerations such as runtime efficiency and heuristic\ndiversity in multi-objective settings. To bridge this gap, we introduce\nMulti-heuristics for MOCOP via Pareto-Grid-guided Evolution of LLMs (MPaGE), a\nnovel enhancement of the Simple Evolutionary Multiobjective Optimization (SEMO)\nframework that leverages LLMs and Pareto Front Grid (PFG) technique. By\npartitioning the objective space into grids and retaining top-performing\ncandidates to guide heuristic generation, MPaGE utilizes LLMs to prioritize\nheuristics with semantically distinct logical structures during variation, thus\npromoting diversity and mitigating redundancy within the population. Through\nextensive evaluations, MPaGE demonstrates superior performance over existing\nLLM-based frameworks, and achieves competitive results to traditional\nMulti-objective evolutionary algorithms (MOEAs), with significantly faster\nruntime. Our code is available at: https://github.com/langkhachhoha/MPaGE."
                },
                "authors": [
                    {
                        "name": "Minh Hieu Ha"
                    },
                    {
                        "name": "Hung Phan"
                    },
                    {
                        "name": "Tung Duy Doan"
                    },
                    {
                        "name": "Tung Dao"
                    },
                    {
                        "name": "Dao Tran"
                    },
                    {
                        "name": "Huynh Thi Thanh Binh"
                    }
                ],
                "author_detail": {
                    "name": "Huynh Thi Thanh Binh"
                },
                "author": "Huynh Thi Thanh Binh",
                "arxiv_comment": "36 pages, 20 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10408v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10408v3",
                "updated": "2025-09-17T09:28:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    28,
                    46,
                    2,
                    260,
                    0
                ],
                "published": "2025-03-13T14:32:30Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    14,
                    32,
                    30,
                    3,
                    72,
                    0
                ],
                "title": "Out-of-Context Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-Context Reasoning in Large Language Models"
                },
                "summary": "We study how large language models (LLMs) reason about memorized knowledge\nthrough simple binary relations such as equality ($=$), inequality ($<$), and\ninclusion ($\\subset$). Unlike in-context reasoning, the axioms (e.g., $a < b, b\n< c$) are only seen during training and not provided in the task prompt (e.g.,\nevaluating $a < c$). The tasks require one or more reasoning steps, and data\naggregation from one or more sources, showing performance change with task\ncomplexity. We introduce a lightweight technique, out-of-context representation\nlearning, which trains only new token embeddings on axioms and evaluates them\non unseen tasks. Across reflexivity, symmetry, and transitivity tests, LLMs\nmostly perform statistically significant better than chance, making the correct\nanswer extractable when testing multiple phrasing variations, but still fall\nshort of consistent reasoning on every single query. Analysis shows that the\nlearned embeddings are organized in structured ways, suggesting real relational\nunderstanding. Surprisingly, it also indicates that the core reasoning happens\nduring the training, not inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study how large language models (LLMs) reason about memorized knowledge\nthrough simple binary relations such as equality ($=$), inequality ($<$), and\ninclusion ($\\subset$). Unlike in-context reasoning, the axioms (e.g., $a < b, b\n< c$) are only seen during training and not provided in the task prompt (e.g.,\nevaluating $a < c$). The tasks require one or more reasoning steps, and data\naggregation from one or more sources, showing performance change with task\ncomplexity. We introduce a lightweight technique, out-of-context representation\nlearning, which trains only new token embeddings on axioms and evaluates them\non unseen tasks. Across reflexivity, symmetry, and transitivity tests, LLMs\nmostly perform statistically significant better than chance, making the correct\nanswer extractable when testing multiple phrasing variations, but still fall\nshort of consistent reasoning on every single query. Analysis shows that the\nlearned embeddings are organized in structured ways, suggesting real relational\nunderstanding. Surprisingly, it also indicates that the core reasoning happens\nduring the training, not inference."
                },
                "authors": [
                    {
                        "name": "Jonathan Shaki"
                    },
                    {
                        "name": "Emanuele La Malfa"
                    },
                    {
                        "name": "Michael Wooldridge"
                    },
                    {
                        "name": "Sarit Kraus"
                    }
                ],
                "author_detail": {
                    "name": "Sarit Kraus"
                },
                "author": "Sarit Kraus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10408v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10408v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06311v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06311v2",
                "updated": "2025-09-17T09:17:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    17,
                    8,
                    2,
                    260,
                    0
                ],
                "published": "2025-05-08T13:04:45Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    4,
                    45,
                    3,
                    128,
                    0
                ],
                "title": "Defending against Indirect Prompt Injection by Instruction Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defending against Indirect Prompt Injection by Instruction Detection"
                },
                "summary": "The integration of Large Language Models (LLMs) with external sources is\nbecoming increasingly common, with Retrieval-Augmented Generation (RAG) being a\nprominent example. However, this integration introduces vulnerabilities of\nIndirect Prompt Injection (IPI) attacks, where hidden instructions embedded in\nexternal data can manipulate LLMs into executing unintended or harmful actions.\nWe recognize that IPI attacks fundamentally rely on the presence of\ninstructions embedded within external content, which can alter the behavioral\nstates of LLMs. Can the effective detection of such state changes help us\ndefend against IPI attacks? In this paper, we propose InstructDetector, a novel\ndetection-based approach that leverages the behavioral states of LLMs to\nidentify potential IPI attacks. Specifically, we demonstrate the hidden states\nand gradients from intermediate layers provide highly discriminative features\nfor instruction detection. By effectively combining these features,\nInstructDetector achieves a detection accuracy of 99.60% in the in-domain\nsetting and 96.90% in the out-of-domain setting, and reduces the attack success\nrate to just 0.03% on the BIPIA benchmark. The code is publicly available at\nhttps://github.com/MYVAE/Instruction-detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) with external sources is\nbecoming increasingly common, with Retrieval-Augmented Generation (RAG) being a\nprominent example. However, this integration introduces vulnerabilities of\nIndirect Prompt Injection (IPI) attacks, where hidden instructions embedded in\nexternal data can manipulate LLMs into executing unintended or harmful actions.\nWe recognize that IPI attacks fundamentally rely on the presence of\ninstructions embedded within external content, which can alter the behavioral\nstates of LLMs. Can the effective detection of such state changes help us\ndefend against IPI attacks? In this paper, we propose InstructDetector, a novel\ndetection-based approach that leverages the behavioral states of LLMs to\nidentify potential IPI attacks. Specifically, we demonstrate the hidden states\nand gradients from intermediate layers provide highly discriminative features\nfor instruction detection. By effectively combining these features,\nInstructDetector achieves a detection accuracy of 99.60% in the in-domain\nsetting and 96.90% in the out-of-domain setting, and reduces the attack success\nrate to just 0.03% on the BIPIA benchmark. The code is publicly available at\nhttps://github.com/MYVAE/Instruction-detection."
                },
                "authors": [
                    {
                        "name": "Tongyu Wen"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Xiyuan Yang"
                    },
                    {
                        "name": "Haoyu Tang"
                    },
                    {
                        "name": "Yueqi Xie"
                    },
                    {
                        "name": "Lingjuan Lyu"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Fangzhao Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fangzhao Wu"
                },
                "author": "Fangzhao Wu",
                "arxiv_comment": "16 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06311v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06311v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11176v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11176v4",
                "updated": "2025-09-17T09:13:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    13,
                    23,
                    2,
                    260,
                    0
                ],
                "published": "2025-02-16T15:54:53Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    15,
                    54,
                    53,
                    6,
                    47,
                    0
                ],
                "title": "LogiDynamics: Unraveling the Dynamics of Inductive, Abductive and\n  Deductive Logical Inferences in LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogiDynamics: Unraveling the Dynamics of Inductive, Abductive and\n  Deductive Logical Inferences in LLM Reasoning"
                },
                "summary": "Modern large language models (LLMs) employ diverse logical inference\nmechanisms for reasoning, making the strategic optimization of these approaches\ncritical for advancing their capabilities. This paper systematically\ninvestigate the comparative dynamics of inductive (System 1) versus\nabductive/deductive (System 2) inference in LLMs. We utilize a controlled\nanalogical reasoning environment, varying modality (textual, visual, symbolic),\ndifficulty, and task format (MCQ / free-text). Our analysis reveals System 2\npipelines generally excel, particularly in visual/symbolic modalities and\nharder tasks, while System 1 is competitive for textual and easier problems.\nCrucially, task format significantly influences their relative advantage, with\nSystem 1 sometimes outperforming System 2 in free-text rule-execution. These\ncore findings generalize to broader in-context learning. Furthermore, we\ndemonstrate that advanced System 2 strategies like hypothesis selection and\niterative refinement can substantially scale LLM reasoning. This study offers\nfoundational insights and actionable guidelines for strategically deploying\nlogical inference to enhance LLM reasoning. Resources are available at\nhttps://github.com/HKUST-KnowComp/LogiDynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) employ diverse logical inference\nmechanisms for reasoning, making the strategic optimization of these approaches\ncritical for advancing their capabilities. This paper systematically\ninvestigate the comparative dynamics of inductive (System 1) versus\nabductive/deductive (System 2) inference in LLMs. We utilize a controlled\nanalogical reasoning environment, varying modality (textual, visual, symbolic),\ndifficulty, and task format (MCQ / free-text). Our analysis reveals System 2\npipelines generally excel, particularly in visual/symbolic modalities and\nharder tasks, while System 1 is competitive for textual and easier problems.\nCrucially, task format significantly influences their relative advantage, with\nSystem 1 sometimes outperforming System 2 in free-text rule-execution. These\ncore findings generalize to broader in-context learning. Furthermore, we\ndemonstrate that advanced System 2 strategies like hypothesis selection and\niterative refinement can substantially scale LLM reasoning. This study offers\nfoundational insights and actionable guidelines for strategically deploying\nlogical inference to enhance LLM reasoning. Resources are available at\nhttps://github.com/HKUST-KnowComp/LogiDynamics."
                },
                "authors": [
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Jiayang Cheng"
                    },
                    {
                        "name": "Chunyang Li"
                    },
                    {
                        "name": "Haochen Shi"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Jiaxin Bai"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Ginny Y. Wong"
                    },
                    {
                        "name": "Simon See"
                    }
                ],
                "author_detail": {
                    "name": "Simon See"
                },
                "author": "Simon See",
                "arxiv_comment": "EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11176v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11176v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11824v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11824v4",
                "updated": "2025-09-17T09:06:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    6,
                    58,
                    2,
                    260,
                    0
                ],
                "published": "2024-08-05T06:31:39Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    6,
                    31,
                    39,
                    0,
                    218,
                    0
                ],
                "title": "AppAgent v2: Advanced Agent for Flexible Mobile Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AppAgent v2: Advanced Agent for Flexible Mobile Interactions"
                },
                "summary": "With the advancement of Multimodal Large Language Models (MLLM), LLM-driven\nvisual agents are increasingly impacting software interfaces, particularly\nthose with graphical user interfaces. This work introduces a novel LLM-based\nmultimodal agent framework for mobile devices. This framework, capable of\nnavigating mobile devices, emulates human-like interactions. Our agent\nconstructs a flexible action space that enhances adaptability across various\napplications including parser, text and vision descriptions. The agent operates\nthrough two main phases: exploration and deployment. During the exploration\nphase, functionalities of user interface elements are documented either through\nagent-driven or manual explorations into a customized structured knowledge\nbase. In the deployment phase, RAG technology enables efficient retrieval and\nupdate from this knowledge base, thereby empowering the agent to perform tasks\neffectively and accurately. This includes performing complex, multi-step\noperations across various applications, thereby demonstrating the framework's\nadaptability and precision in handling customized task workflows. Our\nexperimental results across various benchmarks demonstrate the framework's\nsuperior performance, confirming its effectiveness in real-world scenarios. Our\ncode will be open source soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of Multimodal Large Language Models (MLLM), LLM-driven\nvisual agents are increasingly impacting software interfaces, particularly\nthose with graphical user interfaces. This work introduces a novel LLM-based\nmultimodal agent framework for mobile devices. This framework, capable of\nnavigating mobile devices, emulates human-like interactions. Our agent\nconstructs a flexible action space that enhances adaptability across various\napplications including parser, text and vision descriptions. The agent operates\nthrough two main phases: exploration and deployment. During the exploration\nphase, functionalities of user interface elements are documented either through\nagent-driven or manual explorations into a customized structured knowledge\nbase. In the deployment phase, RAG technology enables efficient retrieval and\nupdate from this knowledge base, thereby empowering the agent to perform tasks\neffectively and accurately. This includes performing complex, multi-step\noperations across various applications, thereby demonstrating the framework's\nadaptability and precision in handling customized task workflows. Our\nexperimental results across various benchmarks demonstrate the framework's\nsuperior performance, confirming its effectiveness in real-world scenarios. Our\ncode will be open source soon."
                },
                "authors": [
                    {
                        "name": "Yanda Li"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Wenjia Jiang"
                    },
                    {
                        "name": "Wanqi Yang"
                    },
                    {
                        "name": "Bin Fu"
                    },
                    {
                        "name": "Pei Cheng"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Ling Chen"
                    },
                    {
                        "name": "Yunchao Wei"
                    }
                ],
                "author_detail": {
                    "name": "Yunchao Wei"
                },
                "author": "Yunchao Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11824v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11824v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13259v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13259v3",
                "updated": "2025-09-17T09:06:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    6,
                    42,
                    2,
                    260,
                    0
                ],
                "published": "2025-05-19T15:41:32Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    41,
                    32,
                    0,
                    139,
                    0
                ],
                "title": "From Automation to Autonomy: A Survey on Large Language Models in\n  Scientific Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Automation to Autonomy: A Survey on Large Language Models in\n  Scientific Discovery"
                },
                "summary": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific\ndiscovery, evolving from task-specific automation tools into increasingly\nautonomous agents and fundamentally redefining research processes and human-AI\ncollaboration. This survey systematically charts this burgeoning field, placing\na central focus on the changing roles and escalating capabilities of LLMs in\nscience. Through the lens of the scientific method, we introduce a foundational\nthree-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating\nautonomy and evolving responsibilities within the research lifecycle. We\nfurther identify pivotal challenges and future research trajectories such as\nrobotic automation, self-improvement, and ethical governance. Overall, this\nsurvey provides a conceptual architecture and strategic foresight to navigate\nand shape the future of AI-driven scientific discovery, fostering both rapid\ninnovation and responsible advancement. Github Repository:\nhttps://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific\ndiscovery, evolving from task-specific automation tools into increasingly\nautonomous agents and fundamentally redefining research processes and human-AI\ncollaboration. This survey systematically charts this burgeoning field, placing\na central focus on the changing roles and escalating capabilities of LLMs in\nscience. Through the lens of the scientific method, we introduce a foundational\nthree-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating\nautonomy and evolving responsibilities within the research lifecycle. We\nfurther identify pivotal challenges and future research trajectories such as\nrobotic automation, self-improvement, and ethical governance. Overall, this\nsurvey provides a conceptual architecture and strategic foresight to navigate\nand shape the future of AI-driven scientific discovery, fostering both rapid\ninnovation and responsible advancement. Github Repository:\nhttps://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery."
                },
                "authors": [
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Zheye Deng"
                    },
                    {
                        "name": "Hong Ting Tsang"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Jiaxin Bai"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13259v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13259v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13835v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13835v1",
                "updated": "2025-09-17T09:05:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    5,
                    37,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T09:05:37Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    5,
                    37,
                    2,
                    260,
                    0
                ],
                "title": "Large Language Models Discriminate Against Speakers of German Dialects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Discriminate Against Speakers of German Dialects"
                },
                "summary": "Dialects represent a significant component of human culture and are found\nacross all regions of the world. In Germany, more than 40% of the population\nspeaks a regional dialect (Adler and Hansen, 2022). However, despite cultural\nimportance, individuals speaking dialects often face negative societal\nstereotypes. We examine whether such stereotypes are mirrored by large language\nmodels (LLMs). We draw on the sociolinguistic literature on dialect perception\nto analyze traits commonly associated with dialect speakers. Based on these\ntraits, we assess the dialect naming bias and dialect usage bias expressed by\nLLMs in two tasks: an association task and a decision task. To assess a model's\ndialect usage bias, we construct a novel evaluation corpus that pairs sentences\nfrom seven regional German dialects (e.g., Alemannic and Bavarian) with their\nstandard German counterparts. We find that: (1) in the association task, all\nevaluated LLMs exhibit significant dialect naming and dialect usage bias\nagainst German dialect speakers, reflected in negative adjective associations;\n(2) all models reproduce these dialect naming and dialect usage biases in their\ndecision making; and (3) contrary to prior work showing minimal bias with\nexplicit demographic mentions, we find that explicitly labeling linguistic\ndemographics--German dialect speakers--amplifies bias more than implicit cues\nlike dialect usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialects represent a significant component of human culture and are found\nacross all regions of the world. In Germany, more than 40% of the population\nspeaks a regional dialect (Adler and Hansen, 2022). However, despite cultural\nimportance, individuals speaking dialects often face negative societal\nstereotypes. We examine whether such stereotypes are mirrored by large language\nmodels (LLMs). We draw on the sociolinguistic literature on dialect perception\nto analyze traits commonly associated with dialect speakers. Based on these\ntraits, we assess the dialect naming bias and dialect usage bias expressed by\nLLMs in two tasks: an association task and a decision task. To assess a model's\ndialect usage bias, we construct a novel evaluation corpus that pairs sentences\nfrom seven regional German dialects (e.g., Alemannic and Bavarian) with their\nstandard German counterparts. We find that: (1) in the association task, all\nevaluated LLMs exhibit significant dialect naming and dialect usage bias\nagainst German dialect speakers, reflected in negative adjective associations;\n(2) all models reproduce these dialect naming and dialect usage biases in their\ndecision making; and (3) contrary to prior work showing minimal bias with\nexplicit demographic mentions, we find that explicitly labeling linguistic\ndemographics--German dialect speakers--amplifies bias more than implicit cues\nlike dialect usage."
                },
                "authors": [
                    {
                        "name": "Minh Duc Bui"
                    },
                    {
                        "name": "Carolin Holtermann"
                    },
                    {
                        "name": "Valentin Hofmann"
                    },
                    {
                        "name": "Anne Lauscher"
                    },
                    {
                        "name": "Katharina von der Wense"
                    }
                ],
                "author_detail": {
                    "name": "Katharina von der Wense"
                },
                "author": "Katharina von der Wense",
                "arxiv_comment": "Accepted to EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13835v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13835v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00308v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00308v2",
                "updated": "2025-09-17T08:35:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    8,
                    35,
                    13,
                    2,
                    260,
                    0
                ],
                "published": "2025-05-30T23:37:10Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    23,
                    37,
                    10,
                    4,
                    150,
                    0
                ],
                "title": "MythTriage: Scalable Detection of Opioid Use Disorder Myths on a\n  Video-Sharing Platform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MythTriage: Scalable Detection of Opioid Use Disorder Myths on a\n  Video-Sharing Platform"
                },
                "summary": "Understanding the prevalence of misinformation in health topics online can\ninform public health policies and interventions. However, measuring such\nmisinformation at scale remains a challenge, particularly for high-stakes but\nunderstudied topics like opioid-use disorder (OUD)--a leading cause of death in\nthe U.S. We present the first large-scale study of OUD-related myths on\nYouTube, a widely-used platform for health information. With clinical experts,\nwe validate 8 pervasive myths and release an expert-labeled video dataset. To\nscale labeling, we introduce MythTriage, an efficient triage pipeline that uses\na lightweight model for routine cases and defers harder ones to a\nhigh-performing, but costlier, large language model (LLM). MythTriage achieves\nup to 0.86 macro F1-score while estimated to reduce annotation time and\nfinancial cost by over 76% compared to experts and full LLM labeling. We\nanalyze 2.9K search results and 343K recommendations, uncovering how myths\npersist on YouTube and offering actionable insights for public health and\nplatform moderation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the prevalence of misinformation in health topics online can\ninform public health policies and interventions. However, measuring such\nmisinformation at scale remains a challenge, particularly for high-stakes but\nunderstudied topics like opioid-use disorder (OUD)--a leading cause of death in\nthe U.S. We present the first large-scale study of OUD-related myths on\nYouTube, a widely-used platform for health information. With clinical experts,\nwe validate 8 pervasive myths and release an expert-labeled video dataset. To\nscale labeling, we introduce MythTriage, an efficient triage pipeline that uses\na lightweight model for routine cases and defers harder ones to a\nhigh-performing, but costlier, large language model (LLM). MythTriage achieves\nup to 0.86 macro F1-score while estimated to reduce annotation time and\nfinancial cost by over 76% compared to experts and full LLM labeling. We\nanalyze 2.9K search results and 343K recommendations, uncovering how myths\npersist on YouTube and offering actionable insights for public health and\nplatform moderation."
                },
                "authors": [
                    {
                        "name": "Hayoung Jung"
                    },
                    {
                        "name": "Shravika Mittal"
                    },
                    {
                        "name": "Ananya Aatreya"
                    },
                    {
                        "name": "Navreet Kaur"
                    },
                    {
                        "name": "Munmun De Choudhury"
                    },
                    {
                        "name": "Tanushree Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Tanushree Mitra"
                },
                "author": "Tanushree Mitra",
                "arxiv_comment": "To appear at EMNLP 2025. Please cite EMNLP version when proceedings\n  are available",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00308v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00308v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00132v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00132v4",
                "updated": "2025-09-17T08:32:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    8,
                    32,
                    45,
                    2,
                    260,
                    0
                ],
                "published": "2025-03-31T18:33:55Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    18,
                    33,
                    55,
                    0,
                    90,
                    0
                ],
                "title": "Contextualize-then-Aggregate: Circuits for In-Context Learning in\n  Gemma-2 2B",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextualize-then-Aggregate: Circuits for In-Context Learning in\n  Gemma-2 2B"
                },
                "summary": "In-Context Learning (ICL) is an intriguing ability of large language models\n(LLMs). Despite a substantial amount of work on its behavioral aspects and how\nit emerges in miniature setups, it remains unclear which mechanism assembles\ntask information from the individual examples in a fewshot prompt. We use\ncausal interventions to identify information flow in Gemma-2 2B for five\nnaturalistic ICL tasks. We find that the model infers task information using a\ntwo-step strategy we call contextualize-then-aggregate: In the lower layers,\nthe model builds up representations of individual fewshot examples, which are\ncontextualized by preceding examples through connections between fewshot input\nand output tokens across the sequence. In the higher layers, these\nrepresentations are aggregated to identify the task and prepare prediction of\nthe next output. The importance of the contextualization step differs between\ntasks, and it may become more important in the presence of ambiguous examples.\nOverall, by providing rigorous causal analysis, our results shed light on the\nmechanisms through which ICL happens in language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning (ICL) is an intriguing ability of large language models\n(LLMs). Despite a substantial amount of work on its behavioral aspects and how\nit emerges in miniature setups, it remains unclear which mechanism assembles\ntask information from the individual examples in a fewshot prompt. We use\ncausal interventions to identify information flow in Gemma-2 2B for five\nnaturalistic ICL tasks. We find that the model infers task information using a\ntwo-step strategy we call contextualize-then-aggregate: In the lower layers,\nthe model builds up representations of individual fewshot examples, which are\ncontextualized by preceding examples through connections between fewshot input\nand output tokens across the sequence. In the higher layers, these\nrepresentations are aggregated to identify the task and prepare prediction of\nthe next output. The importance of the contextualization step differs between\ntasks, and it may become more important in the presence of ambiguous examples.\nOverall, by providing rigorous causal analysis, our results shed light on the\nmechanisms through which ICL happens in language models."
                },
                "authors": [
                    {
                        "name": "Aleksandra Bakalova"
                    },
                    {
                        "name": "Yana Veitsman"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Michael Hahn"
                    }
                ],
                "author_detail": {
                    "name": "Michael Hahn"
                },
                "author": "Michael Hahn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00132v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00132v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13814v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13814v1",
                "updated": "2025-09-17T08:29:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    8,
                    29,
                    57,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T08:29:57Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    8,
                    29,
                    57,
                    2,
                    260,
                    0
                ],
                "title": "Findings of the Third Automatic Minuting (AutoMin) Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Findings of the Third Automatic Minuting (AutoMin) Challenge"
                },
                "summary": "This paper presents the third edition of AutoMin, a shared task on automatic\nmeeting summarization into minutes. In 2025, AutoMin featured the main task of\nminuting, the creation of structured meeting minutes, as well as a new task:\nquestion answering (QA) based on meeting transcripts.\n  The minuting task covered two languages, English and Czech, and two domains:\nproject meetings and European Parliament sessions. The QA task focused solely\non project meetings and was available in two settings: monolingual QA in\nEnglish, and cross-lingual QA, where questions were asked and answered in Czech\nbased on English meetings.\n  Participation in 2025 was more limited compared to previous years, with only\none team joining the minuting task and two teams participating in QA. However,\nas organizers, we included multiple baseline systems to enable a comprehensive\nevaluation of current (2025) large language models (LLMs) on both tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the third edition of AutoMin, a shared task on automatic\nmeeting summarization into minutes. In 2025, AutoMin featured the main task of\nminuting, the creation of structured meeting minutes, as well as a new task:\nquestion answering (QA) based on meeting transcripts.\n  The minuting task covered two languages, English and Czech, and two domains:\nproject meetings and European Parliament sessions. The QA task focused solely\non project meetings and was available in two settings: monolingual QA in\nEnglish, and cross-lingual QA, where questions were asked and answered in Czech\nbased on English meetings.\n  Participation in 2025 was more limited compared to previous years, with only\none team joining the minuting task and two teams participating in QA. However,\nas organizers, we included multiple baseline systems to enable a comprehensive\nevaluation of current (2025) large language models (LLMs) on both tasks."
                },
                "authors": [
                    {
                        "name": "Kartik Shinde"
                    },
                    {
                        "name": "Laurent Besacier"
                    },
                    {
                        "name": "Ondrej Bojar"
                    },
                    {
                        "name": "Thibaut Thonet"
                    },
                    {
                        "name": "Tirthankar Ghosal"
                    }
                ],
                "author_detail": {
                    "name": "Tirthankar Ghosal"
                },
                "author": "Tirthankar Ghosal",
                "arxiv_comment": "Automin 2025 Website: https://ufal.github.io/automin-2025/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13814v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13813v1",
                "updated": "2025-09-17T08:28:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    8,
                    28,
                    7,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T08:28:07Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    8,
                    28,
                    7,
                    2,
                    260,
                    0
                ],
                "title": "Geometric Uncertainty for Detecting and Correcting Hallucinations in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric Uncertainty for Detecting and Correcting Hallucinations in\n  LLMs"
                },
                "summary": "Large language models demonstrate impressive results across diverse tasks but\nare still known to hallucinate, generating linguistically plausible but\nincorrect answers to questions. Uncertainty quantification has been proposed as\na strategy for hallucination detection, but no existing black-box approach\nprovides estimates for both global and local uncertainty. The former attributes\nuncertainty to a batch of responses, while the latter attributes uncertainty to\nindividual responses. Current local methods typically rely on white-box access\nto internal model states, whilst black-box methods only provide global\nuncertainty estimates. We introduce a geometric framework to address this,\nbased on archetypal analysis of batches of responses sampled with only\nblack-box model access. At the global level, we propose Geometric Volume, which\nmeasures the convex hull volume of archetypes derived from response embeddings.\nAt the local level, we propose Geometric Suspicion, which ranks responses by\nreliability and enables hallucination reduction through preferential response\nselection. Unlike prior dispersion methods which yield only a single global\nscore, our approach provides semantic boundary points which have utility for\nattributing reliability to individual responses. Experiments show that our\nframework performs comparably to or better than prior methods on short form\nquestion-answering datasets, and achieves superior results on medical datasets\nwhere hallucinations carry particularly critical risks. We also provide\ntheoretical justification by proving a link between convex hull volume and\nentropy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models demonstrate impressive results across diverse tasks but\nare still known to hallucinate, generating linguistically plausible but\nincorrect answers to questions. Uncertainty quantification has been proposed as\na strategy for hallucination detection, but no existing black-box approach\nprovides estimates for both global and local uncertainty. The former attributes\nuncertainty to a batch of responses, while the latter attributes uncertainty to\nindividual responses. Current local methods typically rely on white-box access\nto internal model states, whilst black-box methods only provide global\nuncertainty estimates. We introduce a geometric framework to address this,\nbased on archetypal analysis of batches of responses sampled with only\nblack-box model access. At the global level, we propose Geometric Volume, which\nmeasures the convex hull volume of archetypes derived from response embeddings.\nAt the local level, we propose Geometric Suspicion, which ranks responses by\nreliability and enables hallucination reduction through preferential response\nselection. Unlike prior dispersion methods which yield only a single global\nscore, our approach provides semantic boundary points which have utility for\nattributing reliability to individual responses. Experiments show that our\nframework performs comparably to or better than prior methods on short form\nquestion-answering datasets, and achieves superior results on medical datasets\nwhere hallucinations carry particularly critical risks. We also provide\ntheoretical justification by proving a link between convex hull volume and\nentropy."
                },
                "authors": [
                    {
                        "name": "Edward Phillips"
                    },
                    {
                        "name": "Sean Wu"
                    },
                    {
                        "name": "Soheila Molaei"
                    },
                    {
                        "name": "Danielle Belgrave"
                    },
                    {
                        "name": "Anshul Thakur"
                    },
                    {
                        "name": "David Clifton"
                    }
                ],
                "author_detail": {
                    "name": "David Clifton"
                },
                "author": "David Clifton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21233v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21233v2",
                "updated": "2025-09-17T08:06:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    8,
                    6,
                    44,
                    2,
                    260,
                    0
                ],
                "published": "2025-05-27T14:16:52Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    16,
                    52,
                    1,
                    147,
                    0
                ],
                "title": "CROP: Contextual Region-Oriented Visual Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CROP: Contextual Region-Oriented Visual Token Pruning"
                },
                "summary": "Current VLM-based VQA methods often process entire images, leading to\nexcessive visual tokens that include redundant information irrelevant to the\nposed question. This abundance of unnecessary image details creates numerous\nvisual tokens, drastically increasing memory and computational requirements in\nVLMs. To address this, we propose Contextual Region-Oriented Visual Token\nPruning (CROP), a novel framework to compress visual tokens through a two-step\nprocess: Localization and Pruning. Specifically, CROP first employs an\nefficient model to identify the contextual region relevant to the input query.\nSubsequently, two distinct strategies are introduced for pruning: (1) Pre-LLM\nCompression (PLC), which adaptively compresses different image regions with\nvarying ratios, and (2) Inner-LLM Pruning (ILP), a training-free method that\nprunes tokens within early LLM layers guided by the identified contextual\nregion. Extensive experiments on a wide range of VQA tasks demonstrate that\nCROP significantly outperforms existing visual token pruning methods and\nachieves state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current VLM-based VQA methods often process entire images, leading to\nexcessive visual tokens that include redundant information irrelevant to the\nposed question. This abundance of unnecessary image details creates numerous\nvisual tokens, drastically increasing memory and computational requirements in\nVLMs. To address this, we propose Contextual Region-Oriented Visual Token\nPruning (CROP), a novel framework to compress visual tokens through a two-step\nprocess: Localization and Pruning. Specifically, CROP first employs an\nefficient model to identify the contextual region relevant to the input query.\nSubsequently, two distinct strategies are introduced for pruning: (1) Pre-LLM\nCompression (PLC), which adaptively compresses different image regions with\nvarying ratios, and (2) Inner-LLM Pruning (ILP), a training-free method that\nprunes tokens within early LLM layers guided by the identified contextual\nregion. Extensive experiments on a wide range of VQA tasks demonstrate that\nCROP significantly outperforms existing visual token pruning methods and\nachieves state-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Jiawei Guo"
                    },
                    {
                        "name": "Feifei Zhai"
                    },
                    {
                        "name": "Pu Jian"
                    },
                    {
                        "name": "Qianrun Wei"
                    },
                    {
                        "name": "Yu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yu Zhou"
                },
                "author": "Yu Zhou",
                "arxiv_comment": "EMNLP2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21233v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21233v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13792v1",
                "updated": "2025-09-17T08:03:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    8,
                    3,
                    5,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T08:03:05Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    8,
                    3,
                    5,
                    2,
                    260,
                    0
                ],
                "title": "Bridging the Synthetic-Real Gap: Supervised Domain Adaptation for Robust\n  Spacecraft 6-DoF Pose Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Synthetic-Real Gap: Supervised Domain Adaptation for Robust\n  Spacecraft 6-DoF Pose Estimation"
                },
                "summary": "Spacecraft Pose Estimation (SPE) is a fundamental capability for autonomous\nspace operations such as rendezvous, docking, and in-orbit servicing. Hybrid\npipelines that combine object detection, keypoint regression, and\nPerspective-n-Point (PnP) solvers have recently achieved strong results on\nsynthetic datasets, yet their performance deteriorates sharply on real or\nlab-generated imagery due to the persistent synthetic-to-real domain gap.\nExisting unsupervised domain adaptation approaches aim to mitigate this issue\nbut often underperform when a modest number of labeled target samples are\navailable. In this work, we propose the first Supervised Domain Adaptation\n(SDA) framework tailored for SPE keypoint regression. Building on the Learning\nInvariant Representation and Risk (LIRR) paradigm, our method jointly optimizes\ndomain-invariant representations and task-specific risk using both labeled\nsynthetic and limited labeled real data, thereby reducing generalization error\nunder domain shift. Extensive experiments on the SPEED+ benchmark demonstrate\nthat our approach consistently outperforms source-only, fine-tuning, and oracle\nbaselines. Notably, with only 5% labeled target data, our method matches or\nsurpasses oracle performance trained on larger fractions of labeled data. The\nframework is lightweight, backbone-agnostic, and computationally efficient,\noffering a practical pathway toward robust and deployable spacecraft pose\nestimation in real-world space environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spacecraft Pose Estimation (SPE) is a fundamental capability for autonomous\nspace operations such as rendezvous, docking, and in-orbit servicing. Hybrid\npipelines that combine object detection, keypoint regression, and\nPerspective-n-Point (PnP) solvers have recently achieved strong results on\nsynthetic datasets, yet their performance deteriorates sharply on real or\nlab-generated imagery due to the persistent synthetic-to-real domain gap.\nExisting unsupervised domain adaptation approaches aim to mitigate this issue\nbut often underperform when a modest number of labeled target samples are\navailable. In this work, we propose the first Supervised Domain Adaptation\n(SDA) framework tailored for SPE keypoint regression. Building on the Learning\nInvariant Representation and Risk (LIRR) paradigm, our method jointly optimizes\ndomain-invariant representations and task-specific risk using both labeled\nsynthetic and limited labeled real data, thereby reducing generalization error\nunder domain shift. Extensive experiments on the SPEED+ benchmark demonstrate\nthat our approach consistently outperforms source-only, fine-tuning, and oracle\nbaselines. Notably, with only 5% labeled target data, our method matches or\nsurpasses oracle performance trained on larger fractions of labeled data. The\nframework is lightweight, backbone-agnostic, and computationally efficient,\noffering a practical pathway toward robust and deployable spacecraft pose\nestimation in real-world space environments."
                },
                "authors": [
                    {
                        "name": "Inder Pal Singh"
                    },
                    {
                        "name": "Nidhal Eddine Chenni"
                    },
                    {
                        "name": "Abd El Rahman Shabayek"
                    },
                    {
                        "name": "Arunkumar Rathinam"
                    },
                    {
                        "name": "Djamila Aouada"
                    }
                ],
                "author_detail": {
                    "name": "Djamila Aouada"
                },
                "author": "Djamila Aouada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13790v1",
                "updated": "2025-09-17T07:58:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    58,
                    59,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T07:58:59Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    58,
                    59,
                    2,
                    260,
                    0
                ],
                "title": "Teaching According to Talents! Instruction Tuning LLMs with\n  Competence-Aware Curriculum Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching According to Talents! Instruction Tuning LLMs with\n  Competence-Aware Curriculum Learning"
                },
                "summary": "Efficient instruction tuning aims to enhance the ultimate performance of\nlarge language models (LLMs) trained on a given instruction dataset. Curriculum\nlearning as a typical data organization strategy has shown preliminary\neffectiveness in instruction tuning. However, current curriculum tuning methods\nsuffer from the curriculum rigidity, since they rely solely on static heuristic\ndifficulty metrics. These methods fail to adapt to the evolving capabilities of\nmodels during training, resulting in a fixed and potentially sub-optimal\nlearning trajectory. To address the issue, Competence-Aware Multi-Perspective\ncUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS\noffers several advantages: (1) Dynamic selection for sub-curriculum. (2)\nCompetency-aware adjustment to the curriculum schedule. (3) Multiple\ndifficulty-based scheduling. Extensive experiments prove the superior\nperformance of CAMPUS, compared to other state-of-the-art baselines for\nefficient instruction tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient instruction tuning aims to enhance the ultimate performance of\nlarge language models (LLMs) trained on a given instruction dataset. Curriculum\nlearning as a typical data organization strategy has shown preliminary\neffectiveness in instruction tuning. However, current curriculum tuning methods\nsuffer from the curriculum rigidity, since they rely solely on static heuristic\ndifficulty metrics. These methods fail to adapt to the evolving capabilities of\nmodels during training, resulting in a fixed and potentially sub-optimal\nlearning trajectory. To address the issue, Competence-Aware Multi-Perspective\ncUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS\noffers several advantages: (1) Dynamic selection for sub-curriculum. (2)\nCompetency-aware adjustment to the curriculum schedule. (3) Multiple\ndifficulty-based scheduling. Extensive experiments prove the superior\nperformance of CAMPUS, compared to other state-of-the-art baselines for\nefficient instruction tuning."
                },
                "authors": [
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Tingwei Lu"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Yankai Chen"
                    },
                    {
                        "name": "Wei-Chieh Huang"
                    },
                    {
                        "name": "Wenhao Jiang"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Hai-Tao Zheng"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13786v1",
                "updated": "2025-09-17T07:55:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    55,
                    58,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T07:55:58Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    55,
                    58,
                    2,
                    260,
                    0
                ],
                "title": "Efficient Quantization-Aware Neural Receivers: Beyond Post-Training\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Quantization-Aware Neural Receivers: Beyond Post-Training\n  Quantization"
                },
                "summary": "As wireless communication systems advance toward Sixth Generation (6G) Radio\nAccess Networks (RAN), Deep Learning (DL)-based neural receivers are emerging\nas transformative solutions for Physical Layer (PHY) processing, delivering\nsuperior Block Error Rate (BLER) performance compared to traditional\nmodel-based approaches. Practical deployment on resource-constrained hardware,\nhowever, requires efficient quantization to reduce latency, energy, and memory\nwithout sacrificing reliability. We extend Post-Training Quantization (PTQ)\nbaselines with Quantization-Aware Training (QAT), which incorporates\nlow-precision simulation during training for robustness at ultra-low bitwidths.\nOur study applies QAT/PTQ to a neural receiver architecture and evaluates\nacross 3GPP Clustered Delay Line (CDL)-B/D channels in LoS and NLoS\nenvironments at user velocities up to 40 m/s. Results show that 4-bit and 8-bit\nQAT models achieve BLERs similar to that of FP32 models at 10% target BLER. QAT\nmodels are also shown to outperform PTQ models by up to 3 dB, and yield 8x\ncompression. These results demonstrate that QAT is a key enabler of\nlow-complexity and latency-constrained inference at the PHY layer, facilitating\nreal-time processing in 6G edge devices",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As wireless communication systems advance toward Sixth Generation (6G) Radio\nAccess Networks (RAN), Deep Learning (DL)-based neural receivers are emerging\nas transformative solutions for Physical Layer (PHY) processing, delivering\nsuperior Block Error Rate (BLER) performance compared to traditional\nmodel-based approaches. Practical deployment on resource-constrained hardware,\nhowever, requires efficient quantization to reduce latency, energy, and memory\nwithout sacrificing reliability. We extend Post-Training Quantization (PTQ)\nbaselines with Quantization-Aware Training (QAT), which incorporates\nlow-precision simulation during training for robustness at ultra-low bitwidths.\nOur study applies QAT/PTQ to a neural receiver architecture and evaluates\nacross 3GPP Clustered Delay Line (CDL)-B/D channels in LoS and NLoS\nenvironments at user velocities up to 40 m/s. Results show that 4-bit and 8-bit\nQAT models achieve BLERs similar to that of FP32 models at 10% target BLER. QAT\nmodels are also shown to outperform PTQ models by up to 3 dB, and yield 8x\ncompression. These results demonstrate that QAT is a key enabler of\nlow-complexity and latency-constrained inference at the PHY layer, facilitating\nreal-time processing in 6G edge devices"
                },
                "authors": [
                    {
                        "name": "SaiKrishna Saketh Yellapragada"
                    },
                    {
                        "name": "Esa Ollila"
                    },
                    {
                        "name": "Mario Costa"
                    }
                ],
                "author_detail": {
                    "name": "Mario Costa"
                },
                "author": "Mario Costa",
                "arxiv_comment": "Submitted for 51st International Conference on Acoustics, Speech, and\n  Signal Processing, ICASSP 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13785v1",
                "updated": "2025-09-17T07:55:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    55,
                    39,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T07:55:39Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    55,
                    39,
                    2,
                    260,
                    0
                ],
                "title": "Summary on The Multilingual Conversational Speech Language Model\n  Challenge: Datasets, Tasks, Baselines, and Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Summary on The Multilingual Conversational Speech Language Model\n  Challenge: Datasets, Tasks, Baselines, and Methods"
                },
                "summary": "This paper summarizes the Interspeech2025 Multilingual Conversational Speech\nLanguage Model (MLC-SLM) challenge, which aims to advance the exploration of\nbuilding effective multilingual conversational speech LLMs (SLLMs). We provide\na detailed description of the task settings for the MLC-SLM challenge, the\nreleased real-world multilingual conversational speech dataset totaling\napproximately 1,604 hours, and the baseline systems for participants. The\nMLC-SLM challenge attracts 78 teams from 13 countries to participate, with 489\nvalid leaderboard results and 14 technical reports for the two tasks. We\ndistill valuable insights on building multilingual conversational SLLMs based\non submissions from participants, aiming to contribute to the advancement of\nthe community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper summarizes the Interspeech2025 Multilingual Conversational Speech\nLanguage Model (MLC-SLM) challenge, which aims to advance the exploration of\nbuilding effective multilingual conversational speech LLMs (SLLMs). We provide\na detailed description of the task settings for the MLC-SLM challenge, the\nreleased real-world multilingual conversational speech dataset totaling\napproximately 1,604 hours, and the baseline systems for participants. The\nMLC-SLM challenge attracts 78 teams from 13 countries to participate, with 489\nvalid leaderboard results and 14 technical reports for the two tasks. We\ndistill valuable insights on building multilingual conversational SLLMs based\non submissions from participants, aiming to contribute to the advancement of\nthe community."
                },
                "authors": [
                    {
                        "name": "Bingshen Mu"
                    },
                    {
                        "name": "Pengcheng Guo"
                    },
                    {
                        "name": "Zhaokai Sun"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Hexin Liu"
                    },
                    {
                        "name": "Mingchen Shao"
                    },
                    {
                        "name": "Lei Xie"
                    },
                    {
                        "name": "Eng Siong Chng"
                    },
                    {
                        "name": "Longshuai Xiao"
                    },
                    {
                        "name": "Qiangze Feng"
                    },
                    {
                        "name": "Daliang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Daliang Wang"
                },
                "author": "Daliang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15022v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15022v4",
                "updated": "2025-09-17T07:52:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    52,
                    58,
                    2,
                    260,
                    0
                ],
                "published": "2025-02-20T20:16:34Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    20,
                    16,
                    34,
                    3,
                    51,
                    0
                ],
                "title": "Mind the Style Gap: Meta-Evaluation of Style and Attribute Transfer\n  Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Style Gap: Meta-Evaluation of Style and Attribute Transfer\n  Metrics"
                },
                "summary": "Large language models (LLMs) make it easy to rewrite a text in any style --\ne.g. to make it more polite, persuasive, or more positive -- but evaluation\nthereof is not straightforward. A challenge lies in measuring content\npreservation: that content not attributable to style change is retained. This\npaper presents a large meta-evaluation of metrics for evaluating style and\nattribute transfer, focusing on content preservation. We find that\nmeta-evaluation studies on existing datasets lead to misleading conclusions\nabout the suitability of metrics for content preservation. Widely used metrics\nshow a high correlation with human judgments despite being deemed unsuitable\nfor the task -- because they do not abstract from style changes when evaluating\ncontent preservation. We show that the overly high correlations with human\njudgment stem from the nature of the test data. To address this issue, we\nintroduce a new, challenging test set specifically designed for evaluating\ncontent preservation metrics for style transfer. We construct the data by\ncreating high variation in the content preservation. Using this dataset, we\ndemonstrate that suitable metrics for content preservation for style transfer\nindeed are style-aware. To support efficient evaluation, we propose a new\nstyle-aware method that utilises small language models, obtaining a higher\nalignment with human judgements than prompting a model of a similar size as an\nautorater. ater.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) make it easy to rewrite a text in any style --\ne.g. to make it more polite, persuasive, or more positive -- but evaluation\nthereof is not straightforward. A challenge lies in measuring content\npreservation: that content not attributable to style change is retained. This\npaper presents a large meta-evaluation of metrics for evaluating style and\nattribute transfer, focusing on content preservation. We find that\nmeta-evaluation studies on existing datasets lead to misleading conclusions\nabout the suitability of metrics for content preservation. Widely used metrics\nshow a high correlation with human judgments despite being deemed unsuitable\nfor the task -- because they do not abstract from style changes when evaluating\ncontent preservation. We show that the overly high correlations with human\njudgment stem from the nature of the test data. To address this issue, we\nintroduce a new, challenging test set specifically designed for evaluating\ncontent preservation metrics for style transfer. We construct the data by\ncreating high variation in the content preservation. Using this dataset, we\ndemonstrate that suitable metrics for content preservation for style transfer\nindeed are style-aware. To support efficient evaluation, we propose a new\nstyle-aware method that utilises small language models, obtaining a higher\nalignment with human judgements than prompting a model of a similar size as an\nautorater. ater."
                },
                "authors": [
                    {
                        "name": "Amalie Brogaard Pauli"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    },
                    {
                        "name": "Ira Assent"
                    }
                ],
                "author_detail": {
                    "name": "Ira Assent"
                },
                "author": "Ira Assent",
                "arxiv_comment": "Accepted at EMNLP Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15022v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15022v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.07107v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.07107v5",
                "updated": "2025-09-17T07:52:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    52,
                    25,
                    2,
                    260,
                    0
                ],
                "published": "2023-08-14T12:47:22Z",
                "published_parsed": [
                    2023,
                    8,
                    14,
                    12,
                    47,
                    22,
                    0,
                    226,
                    0
                ],
                "title": "Large Language Models for Information Retrieval: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Information Retrieval: A Survey"
                },
                "summary": "As a primary means of information acquisition, information retrieval (IR)\nsystems, such as search engines, have integrated themselves into our daily\nlives. These systems also serve as components of dialogue, question-answering,\nand recommender systems. The trajectory of IR has evolved dynamically from its\norigins in term-based methods to its integration with advanced neural models.\nWhile the neural models excel at capturing complex contextual signals and\nsemantic nuances, thereby reshaping the IR landscape, they still face\nchallenges such as data scarcity, interpretability, and the generation of\ncontextually plausible yet potentially inaccurate responses. This evolution\nrequires a combination of both traditional methods (such as term-based sparse\nretrieval methods with rapid response) and modern neural architectures (such as\nlanguage models with powerful language understanding capacity). Meanwhile, the\nemergence of large language models (LLMs), typified by ChatGPT and GPT-4, has\nrevolutionized natural language processing due to their remarkable language\nunderstanding, generation, generalization, and reasoning abilities.\nConsequently, recent research has sought to leverage LLMs to improve IR\nsystems. Given the rapid evolution of this research trajectory, it is necessary\nto consolidate existing methodologies and provide nuanced insights through a\ncomprehensive overview. In this survey, we delve into the confluence of LLMs\nand IR systems, including crucial aspects such as query rewriters, retrievers,\nrerankers, and readers. Additionally, we explore promising directions, such as\nsearch agents, within this expanding field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a primary means of information acquisition, information retrieval (IR)\nsystems, such as search engines, have integrated themselves into our daily\nlives. These systems also serve as components of dialogue, question-answering,\nand recommender systems. The trajectory of IR has evolved dynamically from its\norigins in term-based methods to its integration with advanced neural models.\nWhile the neural models excel at capturing complex contextual signals and\nsemantic nuances, thereby reshaping the IR landscape, they still face\nchallenges such as data scarcity, interpretability, and the generation of\ncontextually plausible yet potentially inaccurate responses. This evolution\nrequires a combination of both traditional methods (such as term-based sparse\nretrieval methods with rapid response) and modern neural architectures (such as\nlanguage models with powerful language understanding capacity). Meanwhile, the\nemergence of large language models (LLMs), typified by ChatGPT and GPT-4, has\nrevolutionized natural language processing due to their remarkable language\nunderstanding, generation, generalization, and reasoning abilities.\nConsequently, recent research has sought to leverage LLMs to improve IR\nsystems. Given the rapid evolution of this research trajectory, it is necessary\nto consolidate existing methodologies and provide nuanced insights through a\ncomprehensive overview. In this survey, we delve into the confluence of LLMs\nand IR systems, including crucial aspects such as query rewriters, retrievers,\nrerankers, and readers. Additionally, we explore promising directions, such as\nsearch agents, within this expanding field."
                },
                "authors": [
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Huaying Yuan"
                    },
                    {
                        "name": "Shuting Wang"
                    },
                    {
                        "name": "Jiongnan Liu"
                    },
                    {
                        "name": "Wenhan Liu"
                    },
                    {
                        "name": "Chenlong Deng"
                    },
                    {
                        "name": "Haonan Chen"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_doi": "10.1145/3748304",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3748304",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2308.07107v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.07107v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Updated to version 4; Accepted by ACM TOIS",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12824v2",
                "updated": "2025-09-17T07:49:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    49,
                    58,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-16T08:49:53Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    8,
                    49,
                    53,
                    1,
                    259,
                    0
                ],
                "title": "DiffHash: Text-Guided Targeted Attack via Diffusion Models against Deep\n  Hashing Image Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffHash: Text-Guided Targeted Attack via Diffusion Models against Deep\n  Hashing Image Retrieval"
                },
                "summary": "Deep hashing models have been widely adopted to tackle the challenges of\nlarge-scale image retrieval. However, these approaches face serious security\nrisks due to their vulnerability to adversarial examples. Despite the\nincreasing exploration of targeted attacks on deep hashing models, existing\napproaches still suffer from a lack of multimodal guidance, reliance on\nlabeling information and dependence on pixel-level operations for attacks. To\naddress these limitations, we proposed DiffHash, a novel diffusion-based\ntargeted attack for deep hashing. Unlike traditional pixel-based attacks that\ndirectly modify specific pixels and lack multimodal guidance, our approach\nfocuses on optimizing the latent representations of images, guided by text\ninformation generated by a Large Language Model (LLM) for the target image.\nFurthermore, we designed a multi-space hash alignment network to align the\nhigh-dimension image space and text space to the low-dimension binary hash\nspace. During reconstruction, we also incorporated text-guided attention\nmechanisms to refine adversarial examples, ensuring them aligned with the\ntarget semantics while maintaining visual plausibility. Extensive experiments\nhave demonstrated that our method outperforms state-of-the-art (SOTA) targeted\nattack methods, achieving better black-box transferability and offering more\nexcellent stability across datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep hashing models have been widely adopted to tackle the challenges of\nlarge-scale image retrieval. However, these approaches face serious security\nrisks due to their vulnerability to adversarial examples. Despite the\nincreasing exploration of targeted attacks on deep hashing models, existing\napproaches still suffer from a lack of multimodal guidance, reliance on\nlabeling information and dependence on pixel-level operations for attacks. To\naddress these limitations, we proposed DiffHash, a novel diffusion-based\ntargeted attack for deep hashing. Unlike traditional pixel-based attacks that\ndirectly modify specific pixels and lack multimodal guidance, our approach\nfocuses on optimizing the latent representations of images, guided by text\ninformation generated by a Large Language Model (LLM) for the target image.\nFurthermore, we designed a multi-space hash alignment network to align the\nhigh-dimension image space and text space to the low-dimension binary hash\nspace. During reconstruction, we also incorporated text-guided attention\nmechanisms to refine adversarial examples, ensuring them aligned with the\ntarget semantics while maintaining visual plausibility. Extensive experiments\nhave demonstrated that our method outperforms state-of-the-art (SOTA) targeted\nattack methods, achieving better black-box transferability and offering more\nexcellent stability across datasets."
                },
                "authors": [
                    {
                        "name": "Zechao Liu"
                    },
                    {
                        "name": "Zheng Zhou"
                    },
                    {
                        "name": "Xiangkun Chen"
                    },
                    {
                        "name": "Tao Liang"
                    },
                    {
                        "name": "Dapeng Lang"
                    }
                ],
                "author_detail": {
                    "name": "Dapeng Lang"
                },
                "author": "Dapeng Lang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13780v1",
                "updated": "2025-09-17T07:49:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    49,
                    12,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T07:49:12Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    49,
                    12,
                    2,
                    260,
                    0
                ],
                "title": "Behavior Foundation Model for Humanoid Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Behavior Foundation Model for Humanoid Robots"
                },
                "summary": "Whole-body control (WBC) of humanoid robots has witnessed remarkable progress\nin skill versatility, enabling a wide range of applications such as locomotion,\nteleoperation, and motion tracking. Despite these achievements, existing WBC\nframeworks remain largely task-specific, relying heavily on labor-intensive\nreward engineering and demonstrating limited generalization across tasks and\nskills. These limitations hinder their response to arbitrary control modes and\nrestrict their deployment in complex, real-world scenarios. To address these\nchallenges, we revisit existing WBC systems and identify a shared objective\nacross diverse tasks: the generation of appropriate behaviors that guide the\nrobot toward desired goal states. Building on this insight, we propose the\nBehavior Foundation Model (BFM), a generative model pretrained on large-scale\nbehavioral datasets to capture broad, reusable behavioral knowledge for\nhumanoid robots. BFM integrates a masked online distillation framework with a\nConditional Variational Autoencoder (CVAE) to model behavioral distributions,\nthereby enabling flexible operation across diverse control modes and efficient\nacquisition of novel behaviors without retraining from scratch. Extensive\nexperiments in both simulation and on a physical humanoid platform demonstrate\nthat BFM generalizes robustly across diverse WBC tasks while rapidly adapting\nto new behaviors. These results establish BFM as a promising step toward a\nfoundation model for general-purpose humanoid control.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whole-body control (WBC) of humanoid robots has witnessed remarkable progress\nin skill versatility, enabling a wide range of applications such as locomotion,\nteleoperation, and motion tracking. Despite these achievements, existing WBC\nframeworks remain largely task-specific, relying heavily on labor-intensive\nreward engineering and demonstrating limited generalization across tasks and\nskills. These limitations hinder their response to arbitrary control modes and\nrestrict their deployment in complex, real-world scenarios. To address these\nchallenges, we revisit existing WBC systems and identify a shared objective\nacross diverse tasks: the generation of appropriate behaviors that guide the\nrobot toward desired goal states. Building on this insight, we propose the\nBehavior Foundation Model (BFM), a generative model pretrained on large-scale\nbehavioral datasets to capture broad, reusable behavioral knowledge for\nhumanoid robots. BFM integrates a masked online distillation framework with a\nConditional Variational Autoencoder (CVAE) to model behavioral distributions,\nthereby enabling flexible operation across diverse control modes and efficient\nacquisition of novel behaviors without retraining from scratch. Extensive\nexperiments in both simulation and on a physical humanoid platform demonstrate\nthat BFM generalizes robustly across diverse WBC tasks while rapidly adapting\nto new behaviors. These results establish BFM as a promising step toward a\nfoundation model for general-purpose humanoid control."
                },
                "authors": [
                    {
                        "name": "Weishuai Zeng"
                    },
                    {
                        "name": "Shunlin Lu"
                    },
                    {
                        "name": "Kangning Yin"
                    },
                    {
                        "name": "Xiaojie Niu"
                    },
                    {
                        "name": "Minyue Dai"
                    },
                    {
                        "name": "Jingbo Wang"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13775v2",
                "updated": "2025-09-18T08:09:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    8,
                    9,
                    19,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-17T07:45:09Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    45,
                    9,
                    2,
                    260,
                    0
                ],
                "title": "Exploring Data and Parameter Efficient Strategies for Arabic Dialect\n  Identifications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Data and Parameter Efficient Strategies for Arabic Dialect\n  Identifications"
                },
                "summary": "This paper discusses our exploration of different data-efficient and\nparameter-efficient approaches to Arabic Dialect Identification (ADI). In\nparticular, we investigate various soft-prompting strategies, including\nprefix-tuning, prompt-tuning, P-tuning, and P-tuning V2, as well as LoRA\nreparameterizations. For the data-efficient strategy, we analyze hard prompting\nwith zero-shot and few-shot inferences to analyze the dialect identification\ncapabilities of Large Language Models (LLMs). For the parameter-efficient PEFT\napproaches, we conducted our experiments using Arabic-specific encoder models\non several major datasets. We also analyzed the n-shot inferences on\nopen-source decoder-only models, a general multilingual model (Phi-3.5), and an\nArabic-specific one(SILMA). We observed that the LLMs generally struggle to\ndifferentiate the dialectal nuances in the few-shot or zero-shot setups. The\nsoft-prompted encoder variants perform better, while the LoRA-based fine-tuned\nmodels perform best, even surpassing full fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper discusses our exploration of different data-efficient and\nparameter-efficient approaches to Arabic Dialect Identification (ADI). In\nparticular, we investigate various soft-prompting strategies, including\nprefix-tuning, prompt-tuning, P-tuning, and P-tuning V2, as well as LoRA\nreparameterizations. For the data-efficient strategy, we analyze hard prompting\nwith zero-shot and few-shot inferences to analyze the dialect identification\ncapabilities of Large Language Models (LLMs). For the parameter-efficient PEFT\napproaches, we conducted our experiments using Arabic-specific encoder models\non several major datasets. We also analyzed the n-shot inferences on\nopen-source decoder-only models, a general multilingual model (Phi-3.5), and an\nArabic-specific one(SILMA). We observed that the LLMs generally struggle to\ndifferentiate the dialectal nuances in the few-shot or zero-shot setups. The\nsoft-prompted encoder variants perform better, while the LoRA-based fine-tuned\nmodels perform best, even surpassing full fine-tuning."
                },
                "authors": [
                    {
                        "name": "Vani Kanjirangat"
                    },
                    {
                        "name": "Ljiljana Dolamic"
                    },
                    {
                        "name": "Fabio Rinaldi"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Rinaldi"
                },
                "author": "Fabio Rinaldi",
                "arxiv_comment": "4 main pages, 4 additional, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13765v1",
                "updated": "2025-09-17T07:24:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    24,
                    25,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T07:24:25Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    24,
                    25,
                    2,
                    260,
                    0
                ],
                "title": "TENET: An Efficient Sparsity-Aware LUT-Centric Architecture for Ternary\n  LLM Inference On Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TENET: An Efficient Sparsity-Aware LUT-Centric Architecture for Ternary\n  LLM Inference On Edge"
                },
                "summary": "Ternary quantization has emerged as a powerful technique for reducing both\ncomputational and memory footprint of large language models (LLM), enabling\nefficient real-time inference deployment without significantly compromising\nmodel accuracy. Conventional LLM inference platforms (e.g GPUs) cannot\ncapitalize on its benefits, as they (i) lack native support for ternary\narithmetic and memory specialization and (ii) remain severely under-utilized in\nlow-batch, real-time scenarios. In this work, we propose TENET, a sparse-aware\nLUT-centric architecture that co-optimizes algorithm, compute, and memory for\nternary LLM inference. To maximize the efficiency of Ternary Linear layer,\nTENET introduces a Sparse Ternary LUT (STL) core that optimizes ternary\nmixed-precision GEMM using a symmetric precompute lookup table. It also\nfeatures Dynamic Activation N:M Sparsity to exploit the sparsity within the\nactivation of each token. Additionally, we propose a LUT-based 64B:80B ternary\nweight decompression module to fully exploit the memory efficiency of ternary\nvalues. At the system level, we design a heterogeneous TENET accelerator with\nfull programmability that integrates STL cores with high-precision cores. An\nassociated Linear-Projection-aware Sparse Attention dataflow is introduced to\noptimize memory access and hardware utilization. We implement TENET accelerator\nprototype on both FPGA and ASIC platforms. Experiments across various model\nsizes and workloads demonstrate that TENET-FPGA and TENET-ASIC improve energy\nefficiency by 4.3$\\times$ and 21.1$\\times$, respectively, compared to the A100\nGPU. Furthermore, TENET-ASIC achieves a 2.7$\\times$ average speedup compared to\nthe A100 GPU in end-to-end inference latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ternary quantization has emerged as a powerful technique for reducing both\ncomputational and memory footprint of large language models (LLM), enabling\nefficient real-time inference deployment without significantly compromising\nmodel accuracy. Conventional LLM inference platforms (e.g GPUs) cannot\ncapitalize on its benefits, as they (i) lack native support for ternary\narithmetic and memory specialization and (ii) remain severely under-utilized in\nlow-batch, real-time scenarios. In this work, we propose TENET, a sparse-aware\nLUT-centric architecture that co-optimizes algorithm, compute, and memory for\nternary LLM inference. To maximize the efficiency of Ternary Linear layer,\nTENET introduces a Sparse Ternary LUT (STL) core that optimizes ternary\nmixed-precision GEMM using a symmetric precompute lookup table. It also\nfeatures Dynamic Activation N:M Sparsity to exploit the sparsity within the\nactivation of each token. Additionally, we propose a LUT-based 64B:80B ternary\nweight decompression module to fully exploit the memory efficiency of ternary\nvalues. At the system level, we design a heterogeneous TENET accelerator with\nfull programmability that integrates STL cores with high-precision cores. An\nassociated Linear-Projection-aware Sparse Attention dataflow is introduced to\noptimize memory access and hardware utilization. We implement TENET accelerator\nprototype on both FPGA and ASIC platforms. Experiments across various model\nsizes and workloads demonstrate that TENET-FPGA and TENET-ASIC improve energy\nefficiency by 4.3$\\times$ and 21.1$\\times$, respectively, compared to the A100\nGPU. Furthermore, TENET-ASIC achieves a 2.7$\\times$ average speedup compared to\nthe A100 GPU in end-to-end inference latency."
                },
                "authors": [
                    {
                        "name": "Zhirui Huang"
                    },
                    {
                        "name": "Rui Ma"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Ran Shu"
                    },
                    {
                        "name": "Ian Wang"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Chixiao Chen"
                    },
                    {
                        "name": "Yongqiang Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Yongqiang Xiong"
                },
                "author": "Yongqiang Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13764v1",
                "updated": "2025-09-17T07:22:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    22,
                    46,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T07:22:46Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    22,
                    46,
                    2,
                    260,
                    0
                ],
                "title": "A compact Sr magneto-optical trap system for field-deployable optical\n  lattice clocks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A compact Sr magneto-optical trap system for field-deployable optical\n  lattice clocks"
                },
                "summary": "We demonstrate a compact strontium (Sr) magneto-optical trap (MOT) realized\nin a single vacuum chamber without a Zeeman slower or a two-dimensional MOT.\nThe MOT is directly loaded from a thermal atomic beam generated by an atomic\noven. The entire vacuum chamber is maintained by a single ion pump, without\nemploying differential pumping. At an oven temperature of\n$395\\,\\mathrm{{}^\\circ C}$, the number of atoms in the MOT reaches $10^7$ with\na loading rate of $10^7 \\,\\mathrm{atoms\\,s^{-1}}$, while sustaining a\nbackground gas pressure in the $10^{-9} \\,\\mathrm{Torr}$ range. At this oven\ntemperature, the MOT lifetime limited by collisions with background gas is\n$\\sim 5 \\,\\mathrm{s}$, with the atom number primarily constrained by\nlight-assisted two-body collisions. Our MOT system significantly simplifies the\nconstruction of field-deployable optical lattice clocks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate a compact strontium (Sr) magneto-optical trap (MOT) realized\nin a single vacuum chamber without a Zeeman slower or a two-dimensional MOT.\nThe MOT is directly loaded from a thermal atomic beam generated by an atomic\noven. The entire vacuum chamber is maintained by a single ion pump, without\nemploying differential pumping. At an oven temperature of\n$395\\,\\mathrm{{}^\\circ C}$, the number of atoms in the MOT reaches $10^7$ with\na loading rate of $10^7 \\,\\mathrm{atoms\\,s^{-1}}$, while sustaining a\nbackground gas pressure in the $10^{-9} \\,\\mathrm{Torr}$ range. At this oven\ntemperature, the MOT lifetime limited by collisions with background gas is\n$\\sim 5 \\,\\mathrm{s}$, with the atom number primarily constrained by\nlight-assisted two-body collisions. Our MOT system significantly simplifies the\nconstruction of field-deployable optical lattice clocks."
                },
                "authors": [
                    {
                        "name": "Naohiro Okamoto"
                    },
                    {
                        "name": "Takumi Sato"
                    },
                    {
                        "name": "Takatoshi Aoki"
                    },
                    {
                        "name": "Yoshio Torii"
                    }
                ],
                "author_detail": {
                    "name": "Yoshio Torii"
                },
                "author": "Yoshio Torii",
                "arxiv_comment": "15 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13762v1",
                "updated": "2025-09-17T07:16:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    16,
                    51,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T07:16:51Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    16,
                    51,
                    2,
                    260,
                    0
                ],
                "title": "Task-Aware Image Signal Processor for Advanced Visual Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-Aware Image Signal Processor for Advanced Visual Perception"
                },
                "summary": "In recent years, there has been a growing trend in computer vision towards\nexploiting RAW sensor data, which preserves richer information compared to\nconventional low-bit RGB images. Early studies mainly focused on enhancing\nvisual quality, while more recent efforts aim to leverage the abundant\ninformation in RAW data to improve the performance of visual perception tasks\nsuch as object detection and segmentation. However, existing approaches still\nface two key limitations: large-scale ISP networks impose heavy computational\noverhead, while methods based on tuning traditional ISP pipelines are\nrestricted by limited representational capacity.To address these issues, we\npropose Task-Aware Image Signal Processing (TA-ISP), a compact RAW-to-RGB\nframework that produces task-oriented representations for pretrained vision\nmodels. Instead of heavy dense convolutional pipelines, TA-ISP predicts a small\nset of lightweight, multi-scale modulation operators that act at global,\nregional, and pixel scales to reshape image statistics across different spatial\nextents. This factorized control significantly expands the range of spatially\nvarying transforms that can be represented while keeping memory usage,\ncomputation, and latency tightly constrained. Evaluated on several RAW-domain\ndetection and segmentation benchmarks under both daytime and nighttime\nconditions, TA-ISP consistently improves downstream accuracy while markedly\nreducing parameter count and inference time, making it well suited for\ndeployment on resource-constrained devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, there has been a growing trend in computer vision towards\nexploiting RAW sensor data, which preserves richer information compared to\nconventional low-bit RGB images. Early studies mainly focused on enhancing\nvisual quality, while more recent efforts aim to leverage the abundant\ninformation in RAW data to improve the performance of visual perception tasks\nsuch as object detection and segmentation. However, existing approaches still\nface two key limitations: large-scale ISP networks impose heavy computational\noverhead, while methods based on tuning traditional ISP pipelines are\nrestricted by limited representational capacity.To address these issues, we\npropose Task-Aware Image Signal Processing (TA-ISP), a compact RAW-to-RGB\nframework that produces task-oriented representations for pretrained vision\nmodels. Instead of heavy dense convolutional pipelines, TA-ISP predicts a small\nset of lightweight, multi-scale modulation operators that act at global,\nregional, and pixel scales to reshape image statistics across different spatial\nextents. This factorized control significantly expands the range of spatially\nvarying transforms that can be represented while keeping memory usage,\ncomputation, and latency tightly constrained. Evaluated on several RAW-domain\ndetection and segmentation benchmarks under both daytime and nighttime\nconditions, TA-ISP consistently improves downstream accuracy while markedly\nreducing parameter count and inference time, making it well suited for\ndeployment on resource-constrained devices."
                },
                "authors": [
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Jin Xiao"
                    },
                    {
                        "name": "Leheng Zhang"
                    },
                    {
                        "name": "Kexuan Shi"
                    },
                    {
                        "name": "Shuhang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Shuhang Gu"
                },
                "author": "Shuhang Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13761v1",
                "updated": "2025-09-17T07:16:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    16,
                    12,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T07:16:12Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    16,
                    12,
                    2,
                    260,
                    0
                ],
                "title": "THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical\n  Reasoning"
                },
                "summary": "Large Language Models (LLMs) have made remarkable progress in mathematical\nreasoning, but still continue to struggle with high-precision tasks like\nnumerical computation and formal symbolic manipulation. Integrating external\ntools has emerged as a promising approach to bridge this gap. Despite recent\nadvances, existing methods struggle with three key challenges: constructing\ntool-integrated reasoning data, performing fine-grained optimization, and\nenhancing inference. To overcome these limitations, we propose THOR\n(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,\na multi-agent actor-critic-based pipeline for constructing high-quality\ndatasets of tool-integrated reasoning paths, aligning with the policy and\ngeneralizing well across diverse models. Second, to perform fine-grained\nhierarchical optimization, we introduce an RL strategy that jointly optimizes\nfor both trajectory-level problem solving and step-level code generation. This\nis motivated by our key insight that the success of an intermediate tool call\nis a strong predictor of the final answer's correctness. Finally, THOR\nincorporates a self-correction mechanism that leverages immediate tool feedback\nto dynamically revise erroneous reasoning paths during inference. Our approach\ndemonstrates strong generalization across diverse models, performing\neffectively in both reasoning and non-reasoning models. It further achieves\nstate-of-the-art performance for models of a similar scale on multiple\nmathematical benchmarks, while also delivering consistent improvements on code\nbenchmarks. Our code will be publicly available at\nhttps://github.com/JingMog/THOR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made remarkable progress in mathematical\nreasoning, but still continue to struggle with high-precision tasks like\nnumerical computation and formal symbolic manipulation. Integrating external\ntools has emerged as a promising approach to bridge this gap. Despite recent\nadvances, existing methods struggle with three key challenges: constructing\ntool-integrated reasoning data, performing fine-grained optimization, and\nenhancing inference. To overcome these limitations, we propose THOR\n(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,\na multi-agent actor-critic-based pipeline for constructing high-quality\ndatasets of tool-integrated reasoning paths, aligning with the policy and\ngeneralizing well across diverse models. Second, to perform fine-grained\nhierarchical optimization, we introduce an RL strategy that jointly optimizes\nfor both trajectory-level problem solving and step-level code generation. This\nis motivated by our key insight that the success of an intermediate tool call\nis a strong predictor of the final answer's correctness. Finally, THOR\nincorporates a self-correction mechanism that leverages immediate tool feedback\nto dynamically revise erroneous reasoning paths during inference. Our approach\ndemonstrates strong generalization across diverse models, performing\neffectively in both reasoning and non-reasoning models. It further achieves\nstate-of-the-art performance for models of a similar scale on multiple\nmathematical benchmarks, while also delivering consistent improvements on code\nbenchmarks. Our code will be publicly available at\nhttps://github.com/JingMog/THOR."
                },
                "authors": [
                    {
                        "name": "Qikai Chang"
                    },
                    {
                        "name": "Zhenrong Zhang"
                    },
                    {
                        "name": "Pengfei Hu"
                    },
                    {
                        "name": "Jiefeng Ma"
                    },
                    {
                        "name": "Yicheng Pan"
                    },
                    {
                        "name": "Jianshu Zhang"
                    },
                    {
                        "name": "Jun Du"
                    },
                    {
                        "name": "Quan Liu"
                    },
                    {
                        "name": "Jianqing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianqing Gao"
                },
                "author": "Jianqing Gao",
                "arxiv_comment": "22 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13760v1",
                "updated": "2025-09-17T07:16:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    16,
                    6,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T07:16:06Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    16,
                    6,
                    2,
                    260,
                    0
                ],
                "title": "Iterative Prompt Refinement for Safer Text-to-Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative Prompt Refinement for Safer Text-to-Image Generation"
                },
                "summary": "Text-to-Image (T2I) models have made remarkable progress in generating images\nfrom text prompts, but their output quality and safety still depend heavily on\nhow prompts are phrased. Existing safety methods typically refine prompts using\nlarge language models (LLMs), but they overlook the images produced, which can\nresult in unsafe outputs or unnecessary changes to already safe prompts. To\naddress this, we propose an iterative prompt refinement algorithm that uses\nVision Language Models (VLMs) to analyze both the input prompts and the\ngenerated images. By leveraging visual feedback, our method refines prompts\nmore effectively, improving safety while maintaining user intent and\nreliability comparable to existing LLM-based approaches. Additionally, we\nintroduce a new dataset labeled with both textual and visual safety signals\nusing off-the-shelf multi-modal LLM, enabling supervised fine-tuning.\nExperimental results demonstrate that our approach produces safer outputs\nwithout compromising alignment with user intent, offering a practical solution\nfor generating safer T2I content. Our code is available at\nhttps://github.com/ku-dmlab/IPR. \\textbf{\\textcolor{red}WARNING: This paper\ncontains examples of harmful or inappropriate images generated by models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-Image (T2I) models have made remarkable progress in generating images\nfrom text prompts, but their output quality and safety still depend heavily on\nhow prompts are phrased. Existing safety methods typically refine prompts using\nlarge language models (LLMs), but they overlook the images produced, which can\nresult in unsafe outputs or unnecessary changes to already safe prompts. To\naddress this, we propose an iterative prompt refinement algorithm that uses\nVision Language Models (VLMs) to analyze both the input prompts and the\ngenerated images. By leveraging visual feedback, our method refines prompts\nmore effectively, improving safety while maintaining user intent and\nreliability comparable to existing LLM-based approaches. Additionally, we\nintroduce a new dataset labeled with both textual and visual safety signals\nusing off-the-shelf multi-modal LLM, enabling supervised fine-tuning.\nExperimental results demonstrate that our approach produces safer outputs\nwithout compromising alignment with user intent, offering a practical solution\nfor generating safer T2I content. Our code is available at\nhttps://github.com/ku-dmlab/IPR. \\textbf{\\textcolor{red}WARNING: This paper\ncontains examples of harmful or inappropriate images generated by models."
                },
                "authors": [
                    {
                        "name": "Jinwoo Jeon"
                    },
                    {
                        "name": "JunHyeok Oh"
                    },
                    {
                        "name": "Hayeong Lee"
                    },
                    {
                        "name": "Byung-Jun Lee"
                    }
                ],
                "author_detail": {
                    "name": "Byung-Jun Lee"
                },
                "author": "Byung-Jun Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05262v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05262v3",
                "updated": "2025-09-17T07:14:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    14,
                    56,
                    2,
                    260,
                    0
                ],
                "published": "2025-04-07T16:57:10Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    57,
                    10,
                    0,
                    97,
                    0
                ],
                "title": "Do Large Language Models Truly Grasp Addition? A Rule-Focused Diagnostic\n  Using Two-Integer Arithmetic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Truly Grasp Addition? A Rule-Focused Diagnostic\n  Using Two-Integer Arithmetic"
                },
                "summary": "Large language models (LLMs) achieve impressive results on advanced\nmathematics benchmarks but sometimes fail on basic arithmetic tasks, raising\nthe question of whether they have truly grasped fundamental arithmetic rules or\nare merely relying on pattern matching. To unravel this issue, we\nsystematically probe LLMs' understanding of two-integer addition ($0$ to\n$2^{64}$) by testing three crucial properties: commutativity ($A+B=B+A$),\nrepresentation invariance via symbolic remapping (e.g., $7 \\mapsto Y$), and\nconsistent accuracy scaling with operand length. Our evaluation of 12 leading\nLLMs reveals a stark disconnect: while models achieve high numeric accuracy\n(73.8-99.8%), they systematically fail these diagnostics. Specifically,\naccuracy plummets to $\\le 7.5$% with symbolic inputs, commutativity is violated\nin up to 20% of cases, and accuracy scaling is non-monotonic. Interventions\nfurther expose this pattern-matching reliance: explicitly providing rules\ndegrades performance by 29.49%, while prompting for explanations before\nanswering merely maintains baseline accuracy. These findings demonstrate that\ncurrent LLMs address elementary addition via pattern matching, not robust rule\ninduction, motivating new diagnostic benchmarks and innovations in model\narchitecture and training to cultivate genuine mathematical reasoning. Our\ndataset and generating code are available at\nhttps://github.com/kuri-leo/llm-arithmetic-diagnostic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve impressive results on advanced\nmathematics benchmarks but sometimes fail on basic arithmetic tasks, raising\nthe question of whether they have truly grasped fundamental arithmetic rules or\nare merely relying on pattern matching. To unravel this issue, we\nsystematically probe LLMs' understanding of two-integer addition ($0$ to\n$2^{64}$) by testing three crucial properties: commutativity ($A+B=B+A$),\nrepresentation invariance via symbolic remapping (e.g., $7 \\mapsto Y$), and\nconsistent accuracy scaling with operand length. Our evaluation of 12 leading\nLLMs reveals a stark disconnect: while models achieve high numeric accuracy\n(73.8-99.8%), they systematically fail these diagnostics. Specifically,\naccuracy plummets to $\\le 7.5$% with symbolic inputs, commutativity is violated\nin up to 20% of cases, and accuracy scaling is non-monotonic. Interventions\nfurther expose this pattern-matching reliance: explicitly providing rules\ndegrades performance by 29.49%, while prompting for explanations before\nanswering merely maintains baseline accuracy. These findings demonstrate that\ncurrent LLMs address elementary addition via pattern matching, not robust rule\ninduction, motivating new diagnostic benchmarks and innovations in model\narchitecture and training to cultivate genuine mathematical reasoning. Our\ndataset and generating code are available at\nhttps://github.com/kuri-leo/llm-arithmetic-diagnostic."
                },
                "authors": [
                    {
                        "name": "Yang Yan"
                    },
                    {
                        "name": "Yu Lu"
                    },
                    {
                        "name": "Renjun Xu"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "arxiv_comment": "Accepted by EMNLP'25 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05262v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05262v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13758v1",
                "updated": "2025-09-17T07:13:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    13,
                    12,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T07:13:12Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    13,
                    12,
                    2,
                    260,
                    0
                ],
                "title": "A Study on Thinking Patterns of Large Reasoning Models in Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Study on Thinking Patterns of Large Reasoning Models in Code\n  Generation"
                },
                "summary": "Currently, many large language models (LLMs) are utilized for software\nengineering tasks such as code generation. The emergence of more advanced\nmodels known as large reasoning models (LRMs), such as OpenAI's o3, DeepSeek\nR1, and Qwen3. They have demonstrated the capability of performing multi-step\nreasoning. Despite the advancement in LRMs, little attention has been paid to\nsystematically analyzing the reasoning patterns these models exhibit and how\nsuch patterns influence the generated code. This paper presents a comprehensive\nstudy aimed at investigating and uncovering the reasoning behavior of LRMs\nduring code generation. We prompted several state-of-the-art LRMs of varying\nsizes with code generation tasks and applied open coding to manually annotate\nthe reasoning traces. From this analysis, we derive a taxonomy of LRM reasoning\nbehaviors, encompassing 15 reasoning actions across four phases.\n  Our empirical study based on the taxonomy reveals a series of findings.\nFirst, we identify common reasoning patterns, showing that LRMs generally\nfollow a human-like coding workflow, with more complex tasks eliciting\nadditional actions such as scaffolding, flaw detection, and style checks.\nSecond, we compare reasoning across models, finding that Qwen3 exhibits\niterative reasoning while DeepSeek-R1-7B follows a more linear, waterfall-like\napproach. Third, we analyze the relationship between reasoning and code\ncorrectness, showing that actions such as unit test creation and scaffold\ngeneration strongly support functional outcomes, with LRMs adapting strategies\nbased on task context. Finally, we evaluate lightweight prompting strategies\ninformed by these findings, demonstrating the potential of context- and\nreasoning-oriented prompts to improve LRM-generated code. Our results offer\ninsights and practical implications for advancing automatic code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Currently, many large language models (LLMs) are utilized for software\nengineering tasks such as code generation. The emergence of more advanced\nmodels known as large reasoning models (LRMs), such as OpenAI's o3, DeepSeek\nR1, and Qwen3. They have demonstrated the capability of performing multi-step\nreasoning. Despite the advancement in LRMs, little attention has been paid to\nsystematically analyzing the reasoning patterns these models exhibit and how\nsuch patterns influence the generated code. This paper presents a comprehensive\nstudy aimed at investigating and uncovering the reasoning behavior of LRMs\nduring code generation. We prompted several state-of-the-art LRMs of varying\nsizes with code generation tasks and applied open coding to manually annotate\nthe reasoning traces. From this analysis, we derive a taxonomy of LRM reasoning\nbehaviors, encompassing 15 reasoning actions across four phases.\n  Our empirical study based on the taxonomy reveals a series of findings.\nFirst, we identify common reasoning patterns, showing that LRMs generally\nfollow a human-like coding workflow, with more complex tasks eliciting\nadditional actions such as scaffolding, flaw detection, and style checks.\nSecond, we compare reasoning across models, finding that Qwen3 exhibits\niterative reasoning while DeepSeek-R1-7B follows a more linear, waterfall-like\napproach. Third, we analyze the relationship between reasoning and code\ncorrectness, showing that actions such as unit test creation and scaffold\ngeneration strongly support functional outcomes, with LRMs adapting strategies\nbased on task context. Finally, we evaluate lightweight prompting strategies\ninformed by these findings, demonstrating the potential of context- and\nreasoning-oriented prompts to improve LRM-generated code. Our results offer\ninsights and practical implications for advancing automatic code generation."
                },
                "authors": [
                    {
                        "name": "Kevin Halim"
                    },
                    {
                        "name": "Sin G. Teo"
                    },
                    {
                        "name": "Ruitao Feng"
                    },
                    {
                        "name": "Zhenpeng Chen"
                    },
                    {
                        "name": "Yang Gu"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13753v1",
                "updated": "2025-09-17T07:11:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    11,
                    45,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T07:11:45Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    11,
                    45,
                    2,
                    260,
                    0
                ],
                "title": "ST-LINK: Spatially-Aware Large Language Models for Spatio-Temporal\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ST-LINK: Spatially-Aware Large Language Models for Spatio-Temporal\n  Forecasting"
                },
                "summary": "Traffic forecasting represents a crucial problem within intelligent\ntransportation systems. In recent research, Large Language Models (LLMs) have\nemerged as a promising method, but their intrinsic design, tailored primarily\nfor sequential token processing, introduces notable challenges in effectively\ncapturing spatial dependencies. Specifically, the inherent limitations of LLMs\nin modeling spatial relationships and their architectural incompatibility with\ngraph-structured spatial data remain largely unaddressed. To overcome these\nlimitations, we introduce ST-LINK, a novel framework that enhances the\ncapability of Large Language Models to capture spatio-temporal dependencies.\nIts key components are Spatially-Enhanced Attention (SE-Attention) and the\nMemory Retrieval Feed-Forward Network (MRFFN). SE-Attention extends rotary\nposition embeddings to integrate spatial correlations as direct rotational\ntransformations within the attention mechanism. This approach maximizes spatial\nlearning while preserving the LLM's inherent sequential processing structure.\nMeanwhile, MRFFN dynamically retrieves and utilizes key historical patterns to\ncapture complex temporal dependencies and improve the stability of long-term\nforecasting. Comprehensive experiments on benchmark datasets demonstrate that\nST-LINK surpasses conventional deep learning and LLM approaches, and\neffectively captures both regular traffic patterns and abrupt changes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic forecasting represents a crucial problem within intelligent\ntransportation systems. In recent research, Large Language Models (LLMs) have\nemerged as a promising method, but their intrinsic design, tailored primarily\nfor sequential token processing, introduces notable challenges in effectively\ncapturing spatial dependencies. Specifically, the inherent limitations of LLMs\nin modeling spatial relationships and their architectural incompatibility with\ngraph-structured spatial data remain largely unaddressed. To overcome these\nlimitations, we introduce ST-LINK, a novel framework that enhances the\ncapability of Large Language Models to capture spatio-temporal dependencies.\nIts key components are Spatially-Enhanced Attention (SE-Attention) and the\nMemory Retrieval Feed-Forward Network (MRFFN). SE-Attention extends rotary\nposition embeddings to integrate spatial correlations as direct rotational\ntransformations within the attention mechanism. This approach maximizes spatial\nlearning while preserving the LLM's inherent sequential processing structure.\nMeanwhile, MRFFN dynamically retrieves and utilizes key historical patterns to\ncapture complex temporal dependencies and improve the stability of long-term\nforecasting. Comprehensive experiments on benchmark datasets demonstrate that\nST-LINK surpasses conventional deep learning and LLM approaches, and\neffectively captures both regular traffic patterns and abrupt changes."
                },
                "authors": [
                    {
                        "name": "Hyotaek Jeon"
                    },
                    {
                        "name": "Hyunwook Lee"
                    },
                    {
                        "name": "Juwon Kim"
                    },
                    {
                        "name": "Sungahn Ko"
                    }
                ],
                "author_detail": {
                    "name": "Sungahn Ko"
                },
                "author": "Sungahn Ko",
                "arxiv_comment": "11 pages, 4 figures, Accepted to CIKM 2025. Code:\n  https://github.com/HyoTaek98/ST_LINK",
                "arxiv_journal_ref": "The 34th ACM International Conference on Information and Knowledge\n  Management (CIKM 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]