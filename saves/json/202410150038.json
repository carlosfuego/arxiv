[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.08895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08895v1",
                "updated": "2024-10-11T15:12:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    12,
                    30,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T15:12:30Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    12,
                    30,
                    4,
                    285,
                    0
                ],
                "title": "Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation"
                },
                "summary": "Cache-based approaches stand out as both effective and efficient for adapting\nvision-language models (VLMs). Nonetheless, the existing cache model overlooks\nthree crucial aspects. 1) Pre-trained VLMs are mainly optimized for image-text\nsimilarity, neglecting the importance of image-image similarity, leading to a\ngap between pre-training and adaptation. 2) The current cache model is based on\nthe Nadaraya-Watson (N-W) estimator, which disregards the intricate\nrelationships among training samples while constructing weight function. 3)\nUnder the condition of limited samples, the logits generated by cache model are\nof high uncertainty, directly using these logits without accounting for the\nconfidence could be problematic. This work presents three calibration modules\naimed at addressing the above challenges. Similarity Calibration refines the\nimage-image similarity by using unlabeled images. We add a learnable projection\nlayer with residual connection on top of the pre-trained image encoder of CLIP\nand optimize the parameters by minimizing self-supervised contrastive loss.\nWeight Calibration introduces a precision matrix into the weight function to\nadequately model the relation between training samples, transforming the\nexisting cache model to a Gaussian Process (GP) regressor, which could be more\naccurate than N-W estimator. Confidence Calibration leverages the predictive\nvariances computed by GP Regression to dynamically re-scale the logits of cache\nmodel, ensuring that the cache model's outputs are appropriately adjusted based\non their confidence levels. Besides, to reduce the high complexity of GPs, we\nfurther propose a group-based learning strategy. Integrating the above designs,\nwe propose both training-free and training-required variants. Extensive\nexperiments on 11 few-shot classification datasets validate that the proposed\nmethods can achieve state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-based approaches stand out as both effective and efficient for adapting\nvision-language models (VLMs). Nonetheless, the existing cache model overlooks\nthree crucial aspects. 1) Pre-trained VLMs are mainly optimized for image-text\nsimilarity, neglecting the importance of image-image similarity, leading to a\ngap between pre-training and adaptation. 2) The current cache model is based on\nthe Nadaraya-Watson (N-W) estimator, which disregards the intricate\nrelationships among training samples while constructing weight function. 3)\nUnder the condition of limited samples, the logits generated by cache model are\nof high uncertainty, directly using these logits without accounting for the\nconfidence could be problematic. This work presents three calibration modules\naimed at addressing the above challenges. Similarity Calibration refines the\nimage-image similarity by using unlabeled images. We add a learnable projection\nlayer with residual connection on top of the pre-trained image encoder of CLIP\nand optimize the parameters by minimizing self-supervised contrastive loss.\nWeight Calibration introduces a precision matrix into the weight function to\nadequately model the relation between training samples, transforming the\nexisting cache model to a Gaussian Process (GP) regressor, which could be more\naccurate than N-W estimator. Confidence Calibration leverages the predictive\nvariances computed by GP Regression to dynamically re-scale the logits of cache\nmodel, ensuring that the cache model's outputs are appropriately adjusted based\non their confidence levels. Besides, to reduce the high complexity of GPs, we\nfurther propose a group-based learning strategy. Integrating the above designs,\nwe propose both training-free and training-required variants. Extensive\nexperiments on 11 few-shot classification datasets validate that the proposed\nmethods can achieve state-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Kun Ding"
                    },
                    {
                        "name": "Qiang Yu"
                    },
                    {
                        "name": "Haojian Zhang"
                    },
                    {
                        "name": "Gaofeng Meng"
                    },
                    {
                        "name": "Shiming Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Shiming Xiang"
                },
                "author": "Shiming Xiang",
                "arxiv_comment": "submitted to IJCV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08760v1",
                "updated": "2024-10-11T12:19:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T12:19:18Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "title": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation"
                },
                "summary": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL."
                },
                "authors": [
                    {
                        "name": "Konstantin Burlachenko"
                    },
                    {
                        "name": "Peter Richtárik"
                    }
                ],
                "author_detail": {
                    "name": "Peter Richtárik"
                },
                "author": "Peter Richtárik",
                "arxiv_comment": "55 pages, 12 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4; C.3; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08618v1",
                "updated": "2024-10-11T08:33:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    33,
                    58,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T08:33:58Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    33,
                    58,
                    4,
                    285,
                    0
                ],
                "title": "AsyncFS: Metadata Updates Made Asynchronous for Distributed Filesystems\n  with In-Network Coordination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsyncFS: Metadata Updates Made Asynchronous for Distributed Filesystems\n  with In-Network Coordination"
                },
                "summary": "Distributed filesystems typically employ synchronous metadata updates, facing\ninherent challenges for access efficiency, load balancing, and directory\ncontention, especially under dynamic and skewed workloads. This paper argues\nthat synchronous updates are overly conservative for distributed filesystems.\nWe propose AsyncFS with asynchronous metadata updates, allowing operations to\nreturn early and defer directory updates until respective read to enable\nlatency hiding and conflict resolution. The key challenge is efficiently\nmaintaining the synchronous semantics of metadata updates. To address this,\nAsyncFS is co-designed with a programmable switch, leveraging the constrained\non-switch resources to holistically track directory states in the network with\nnegligible cost. This allows AsyncFS to timely aggregate and efficiently apply\ndelayed updates using batching and consolidation before directory reads.\nEvaluation shows that AsyncFS achieves up to 13.34$\\times$ and 3.85$\\times$\nhigher throughput, and 61.6% and 57.3% lower latency than two state-of-the-art\ndistributed filesystems, InfiniFS and CFS-KV, respectively, on skewed\nworkloads. For real-world workloads, AsyncFS improves end-to-end throughput by\n21.1$\\times$, 1.1$\\times$ and 30.1% over Ceph, IndexFS and CFS-KV,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed filesystems typically employ synchronous metadata updates, facing\ninherent challenges for access efficiency, load balancing, and directory\ncontention, especially under dynamic and skewed workloads. This paper argues\nthat synchronous updates are overly conservative for distributed filesystems.\nWe propose AsyncFS with asynchronous metadata updates, allowing operations to\nreturn early and defer directory updates until respective read to enable\nlatency hiding and conflict resolution. The key challenge is efficiently\nmaintaining the synchronous semantics of metadata updates. To address this,\nAsyncFS is co-designed with a programmable switch, leveraging the constrained\non-switch resources to holistically track directory states in the network with\nnegligible cost. This allows AsyncFS to timely aggregate and efficiently apply\ndelayed updates using batching and consolidation before directory reads.\nEvaluation shows that AsyncFS achieves up to 13.34$\\times$ and 3.85$\\times$\nhigher throughput, and 61.6% and 57.3% lower latency than two state-of-the-art\ndistributed filesystems, InfiniFS and CFS-KV, respectively, on skewed\nworkloads. For real-world workloads, AsyncFS improves end-to-end throughput by\n21.1$\\times$, 1.1$\\times$ and 30.1% over Ceph, IndexFS and CFS-KV,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Qiulin Tian"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Tong Xin"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08584v1",
                "updated": "2024-10-11T07:24:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T07:24:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification and KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification and KV Cache Compression"
                },
                "summary": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs that resolves both computation\nand memory bottlenecks through a dynamic ratio allocation strategy of important\ntokens. This ratio is adaptively determined based on the layer-specific\ndistribution of attention scores, rather than fixed hyper-parameters, thereby\nimproving efficiency for less complex tasks while maintaining high performance\nfor more challenging ones. Then we select important tokens based on their\nnormalized attention scores and perform attention mechanism solely on those\nimportant tokens to accelerate the prefill phase. To mitigate the memory\nbottleneck in the decoding phase, we employ mixed-precision quantization to the\nKV cache, where high-bit quantization is used for caches of important tokens,\nwhile low-bit quantization is applied to those of less importance. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.6$\\times$ and reduce GPU memory usage by 50.0%, with a minimal accuracy\nreduction of only 0.2% on Video-MME benchmark over LongVA-7B model, effectively\nenhancing the generation efficiency of LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs that resolves both computation\nand memory bottlenecks through a dynamic ratio allocation strategy of important\ntokens. This ratio is adaptively determined based on the layer-specific\ndistribution of attention scores, rather than fixed hyper-parameters, thereby\nimproving efficiency for less complex tasks while maintaining high performance\nfor more challenging ones. Then we select important tokens based on their\nnormalized attention scores and perform attention mechanism solely on those\nimportant tokens to accelerate the prefill phase. To mitigate the memory\nbottleneck in the decoding phase, we employ mixed-precision quantization to the\nKV cache, where high-bit quantization is used for caches of important tokens,\nwhile low-bit quantization is applied to those of less importance. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.6$\\times$ and reduce GPU memory usage by 50.0%, with a minimal accuracy\nreduction of only 0.2% on Video-MME benchmark over LongVA-7B model, effectively\nenhancing the generation efficiency of LVLMs."
                },
                "authors": [
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.03462v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.03462v3",
                "updated": "2024-10-11T02:18:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    2,
                    18,
                    24,
                    4,
                    285,
                    0
                ],
                "published": "2024-01-07T11:57:40Z",
                "published_parsed": [
                    2024,
                    1,
                    7,
                    11,
                    57,
                    40,
                    6,
                    7,
                    0
                ],
                "title": "Long Context Compression with Activation Beacon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Context Compression with Activation Beacon"
                },
                "summary": "Long context compression is a critical research problem due to its\nsignificance in reducing the high computational and memory costs associated\nwith LLMs. In this paper, we propose Activation Beacon, a plug-in module for\ntransformer-based LLMs that targets effective, efficient, and flexible\ncompression of long contexts. To achieve this, our method introduces the\nfollowing technical designs. 1) We directly compress the activations (i.e. keys\nand values at every layer), rather than leveraging soft prompts to relay\ninformation (which constitute a major bottleneck to encapsulate the complex\ninformation within long contexts). 2) We tailor the compression workflow, where\neach fine-grained input unit is progressively compressed, enabling high-quality\ncompression and efficient computation during both training and inference. 3) We\ntrain the model through compression-based auto-regression, making full use of\nplain texts and instructional data to optimize the model's compression\nperformance. 4) During training, we randomly sample a compression ratio at each\nstep, teaching the model to support a wide range of compression configurations.\nExtensive evaluations are conducted on various long-context tasks whose lengths\n(e.g., 128K) may far exceed the maximum training length (20K), such as document\nunderstanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing\nmethods struggle to handle these challenging tasks, Activation Beacon maintains\na comparable performance to the uncompressed baseline across various scenarios,\nachieving a 2x acceleration in inference time and an 8x reduction of memory\ncosts for KV cache. Our data, model, and code have been released at\n\\url{https://github.com/FlagOpen/FlagEmbedding/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context compression is a critical research problem due to its\nsignificance in reducing the high computational and memory costs associated\nwith LLMs. In this paper, we propose Activation Beacon, a plug-in module for\ntransformer-based LLMs that targets effective, efficient, and flexible\ncompression of long contexts. To achieve this, our method introduces the\nfollowing technical designs. 1) We directly compress the activations (i.e. keys\nand values at every layer), rather than leveraging soft prompts to relay\ninformation (which constitute a major bottleneck to encapsulate the complex\ninformation within long contexts). 2) We tailor the compression workflow, where\neach fine-grained input unit is progressively compressed, enabling high-quality\ncompression and efficient computation during both training and inference. 3) We\ntrain the model through compression-based auto-regression, making full use of\nplain texts and instructional data to optimize the model's compression\nperformance. 4) During training, we randomly sample a compression ratio at each\nstep, teaching the model to support a wide range of compression configurations.\nExtensive evaluations are conducted on various long-context tasks whose lengths\n(e.g., 128K) may far exceed the maximum training length (20K), such as document\nunderstanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing\nmethods struggle to handle these challenging tasks, Activation Beacon maintains\na comparable performance to the uncompressed baseline across various scenarios,\nachieving a 2x acceleration in inference time and an 8x reduction of memory\ncosts for KV cache. Our data, model, and code have been released at\n\\url{https://github.com/FlagOpen/FlagEmbedding/}."
                },
                "authors": [
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Ninglu Shao"
                    },
                    {
                        "name": "Qiwei Ye"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "arxiv_comment": "Newer version of Activation Beacon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.03462v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.03462v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08391v1",
                "updated": "2024-10-10T21:55:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    21,
                    55,
                    11,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T21:55:11Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    21,
                    55,
                    11,
                    3,
                    284,
                    0
                ],
                "title": "KV Prediction for Improved Time to First Token",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Prediction for Improved Time to First Token"
                },
                "summary": "Inference with transformer-based language models begins with a prompt\nprocessing step. In this step, the model generates the first output token and\nstores the KV cache needed for future generation steps. This prompt processing\nstep can be computationally expensive, taking 10s of seconds or more for\nbillion-parameter models on edge devices when prompt lengths or batch sizes\nrise. This degrades user experience by introducing significant latency into the\nmodel's outputs. To reduce the time spent producing the first output (known as\nthe ``time to first token'', or TTFT) of a pretrained model, we introduce a\nnovel method called KV Prediction. In our method, a small auxiliary model is\nused to process the prompt and produce an approximation of the KV cache used by\na base model. This approximated KV cache is then used with the base model for\nautoregressive generation without the need to query the auxiliary model again.\nWe demonstrate that our method produces a pareto-optimal efficiency-accuracy\ntrade-off when compared to baselines. On TriviaQA, we demonstrate relative\naccuracy improvements in the range of $15\\%-50\\%$ across a range of TTFT FLOPs\nbudgets. We also demonstrate accuracy improvements of up to $30\\%$ on HumanEval\npython code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark\nmodels on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs\ntranslates to a TTFT speedup on hardware. We release our code at\nhttps://github.com/apple/corenet/tree/main/projects/kv-prediction .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with transformer-based language models begins with a prompt\nprocessing step. In this step, the model generates the first output token and\nstores the KV cache needed for future generation steps. This prompt processing\nstep can be computationally expensive, taking 10s of seconds or more for\nbillion-parameter models on edge devices when prompt lengths or batch sizes\nrise. This degrades user experience by introducing significant latency into the\nmodel's outputs. To reduce the time spent producing the first output (known as\nthe ``time to first token'', or TTFT) of a pretrained model, we introduce a\nnovel method called KV Prediction. In our method, a small auxiliary model is\nused to process the prompt and produce an approximation of the KV cache used by\na base model. This approximated KV cache is then used with the base model for\nautoregressive generation without the need to query the auxiliary model again.\nWe demonstrate that our method produces a pareto-optimal efficiency-accuracy\ntrade-off when compared to baselines. On TriviaQA, we demonstrate relative\naccuracy improvements in the range of $15\\%-50\\%$ across a range of TTFT FLOPs\nbudgets. We also demonstrate accuracy improvements of up to $30\\%$ on HumanEval\npython code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark\nmodels on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs\ntranslates to a TTFT speedup on hardware. We release our code at\nhttps://github.com/apple/corenet/tree/main/projects/kv-prediction ."
                },
                "authors": [
                    {
                        "name": "Maxwell Horton"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Chenfan Sun"
                    },
                    {
                        "name": "Yanzi Jin"
                    },
                    {
                        "name": "Sachin Mehta"
                    },
                    {
                        "name": "Mohammad Rastegari"
                    },
                    {
                        "name": "Moin Nabi"
                    }
                ],
                "author_detail": {
                    "name": "Moin Nabi"
                },
                "author": "Moin Nabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11284v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11284v3",
                "updated": "2024-10-10T16:57:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    57,
                    34,
                    3,
                    284,
                    0
                ],
                "published": "2024-04-17T11:48:14Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    11,
                    48,
                    14,
                    2,
                    108,
                    0
                ],
                "title": "Amplifying Main Memory-Based Timing Covert and Side Channels using\n  Processing-in-Memory Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Main Memory-Based Timing Covert and Side Channels using\n  Processing-in-Memory Operations"
                },
                "summary": "The adoption of processing-in-memory (PiM) architectures has been gaining\nmomentum because they provide high performance and low energy consumption by\nalleviating the data movement bottleneck. Yet, the security of such\narchitectures has not been thoroughly explored. The adoption of PiM solutions\nprovides a new way to directly access main memory, which malicious user\napplications can exploit. We show that this new way to access main memory opens\nopportunities for high-throughput timing attacks that are hard-to-mitigate\nwithout significant performance overhead.\n  We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of PiM architectures to establish covert\nand side channels. IMPACT enables high-throughput communication and private\ninformation leakage by exploiting the shared DRAM row buffer. To achieve high\nthroughput, IMPACT (i) eliminates cache bypassing steps required by\nprocessor-centric main memory and cache-based timing attacks and (ii) leverages\nthe intrinsic parallelism of PiM operations. We showcase two applications of\nIMPACT. First, we build two covert-channel attacks that run on the host CPU and\nleverage different PiM approaches to gain direct and fast access to main memory\nand establish high-throughput communication covert channels. Second, we\nshowcase a side-channel attack that leaks private information of concurrently\nrunning victim applications that are accelerated with PiM. Our results\ndemonstrate that (i) our covert channels achieve 12.87 Mb/s and 14.16 Mb/s\ncommunication throughput, respectively, which is up to 4.91x and 5.41x faster\nthan the state-of-the-art main memory-based covert channels, and (ii) our\nside-channel attack allows the attacker to leak secrets with a low error rate.\nTo avoid such covert and side channels in emerging PiM systems, we propose and\nevaluate three defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of processing-in-memory (PiM) architectures has been gaining\nmomentum because they provide high performance and low energy consumption by\nalleviating the data movement bottleneck. Yet, the security of such\narchitectures has not been thoroughly explored. The adoption of PiM solutions\nprovides a new way to directly access main memory, which malicious user\napplications can exploit. We show that this new way to access main memory opens\nopportunities for high-throughput timing attacks that are hard-to-mitigate\nwithout significant performance overhead.\n  We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of PiM architectures to establish covert\nand side channels. IMPACT enables high-throughput communication and private\ninformation leakage by exploiting the shared DRAM row buffer. To achieve high\nthroughput, IMPACT (i) eliminates cache bypassing steps required by\nprocessor-centric main memory and cache-based timing attacks and (ii) leverages\nthe intrinsic parallelism of PiM operations. We showcase two applications of\nIMPACT. First, we build two covert-channel attacks that run on the host CPU and\nleverage different PiM approaches to gain direct and fast access to main memory\nand establish high-throughput communication covert channels. Second, we\nshowcase a side-channel attack that leaks private information of concurrently\nrunning victim applications that are accelerated with PiM. Our results\ndemonstrate that (i) our covert channels achieve 12.87 Mb/s and 14.16 Mb/s\ncommunication throughput, respectively, which is up to 4.91x and 5.41x faster\nthan the state-of-the-art main memory-based covert channels, and (ii) our\nside-channel attack allows the attacker to leak secrets with a low error rate.\nTo avoid such covert and side channels in emerging PiM systems, we propose and\nevaluate three defenses."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellopoulos"
                    },
                    {
                        "name": "F. Nisa Bostanci"
                    },
                    {
                        "name": "Ataberk Olgun"
                    },
                    {
                        "name": "A. Giray Yaglikci"
                    },
                    {
                        "name": "Ismail Emir Yuksel"
                    },
                    {
                        "name": "Nika Mansouri Ghiasi"
                    },
                    {
                        "name": "Zulal Bingol"
                    },
                    {
                        "name": "Mohammad Sadrosadati"
                    },
                    {
                        "name": "Onur Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Onur Mutlu"
                },
                "author": "Onur Mutlu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11284v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11284v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01195v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01195v2",
                "updated": "2024-10-10T11:01:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    1,
                    44,
                    3,
                    284,
                    0
                ],
                "published": "2024-06-03T10:58:32Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    10,
                    58,
                    32,
                    0,
                    155,
                    0
                ],
                "title": "C$^3$P-VoxelMap: Compact, Cumulative and Coalescible Probabilistic Voxel\n  Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C$^3$P-VoxelMap: Compact, Cumulative and Coalescible Probabilistic Voxel\n  Mapping"
                },
                "summary": "This work presents a compact, cumulative and coalescible probabilistic voxel\nmapping method to enhance performance, accuracy and memory efficiency in LiDAR\nodometry. Probabilistic voxel mapping requires storing past point clouds and\nre-iterating on them to update the uncertainty every iteration, which consumes\nlarge memory space and CPU cycles. To solve this problem, we propose a\ntwo-folded strategy. First, we introduce a compact point-free representation\nfor probabilistic voxels and derive a cumulative update of the planar\nuncertainty without caching original point clouds. Our voxel structure only\nkeeps track of a predetermined set of statistics for points that lie inside it.\nThis method reduces the runtime complexity from $O(MN)$ to $O(N)$ and the space\ncomplexity from $O(N)$ to $O(1)$ where $M$ is the number of iterations and $N$\nis the number of points. Second, to further minimize memory usage and enhance\nmapping accuracy, we provide a strategy to dynamically merge voxels associated\nwith the same physical planes by taking advantage of the geometric features in\nthe real world. Rather than scanning for these coalescible voxels constantly at\nevery iteration, our merging strategy accumulates voxels in a\nlocality-sensitive hash and triggers merging lazily. On-demand merging not only\nreduces memory footprint with minimal computational overhead but also improves\nlocalization accuracy thanks to cross-voxel denoising. Experiments exhibit 20%\nhigher accuracy, 20% faster performance and 70% lower memory consumption than\nthe state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a compact, cumulative and coalescible probabilistic voxel\nmapping method to enhance performance, accuracy and memory efficiency in LiDAR\nodometry. Probabilistic voxel mapping requires storing past point clouds and\nre-iterating on them to update the uncertainty every iteration, which consumes\nlarge memory space and CPU cycles. To solve this problem, we propose a\ntwo-folded strategy. First, we introduce a compact point-free representation\nfor probabilistic voxels and derive a cumulative update of the planar\nuncertainty without caching original point clouds. Our voxel structure only\nkeeps track of a predetermined set of statistics for points that lie inside it.\nThis method reduces the runtime complexity from $O(MN)$ to $O(N)$ and the space\ncomplexity from $O(N)$ to $O(1)$ where $M$ is the number of iterations and $N$\nis the number of points. Second, to further minimize memory usage and enhance\nmapping accuracy, we provide a strategy to dynamically merge voxels associated\nwith the same physical planes by taking advantage of the geometric features in\nthe real world. Rather than scanning for these coalescible voxels constantly at\nevery iteration, our merging strategy accumulates voxels in a\nlocality-sensitive hash and triggers merging lazily. On-demand merging not only\nreduces memory footprint with minimal computational overhead but also improves\nlocalization accuracy thanks to cross-voxel denoising. Experiments exhibit 20%\nhigher accuracy, 20% faster performance and 70% lower memory consumption than\nthe state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Xu Yang"
                    },
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Qijie Ge"
                    },
                    {
                        "name": "Lulu Suo"
                    },
                    {
                        "name": "Weijie Tang"
                    },
                    {
                        "name": "Zhengyu Wei"
                    },
                    {
                        "name": "Longxiang Huang"
                    },
                    {
                        "name": "Bo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Wang"
                },
                "author": "Bo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01195v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04793v2",
                "updated": "2024-10-10T05:11:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    5,
                    11,
                    52,
                    3,
                    284,
                    0
                ],
                "published": "2024-04-07T03:08:14Z",
                "published_parsed": [
                    2024,
                    4,
                    7,
                    3,
                    8,
                    14,
                    6,
                    98,
                    0
                ],
                "title": "SqueezeAttention: 2D Management of KV-Cache in LLM Inference via\n  Layer-wise Optimal Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SqueezeAttention: 2D Management of KV-Cache in LLM Inference via\n  Layer-wise Optimal Budget"
                },
                "summary": "Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has\nbeen considered critical to saving the cost of inference. Most of the existing\nKV-cache compression algorithms attempted to sparsify the sequence of tokens by\ntaking advantage of the different importance of tokens. However, most of these\nmethods treat all layers equally, allocating the same KV budget to each layer.\nThis approach is suboptimal, as some layers may be less sensitive to input\ntokens yet still receive the same budget as others. In this work, we found that\nby identifying the importance of attention layers, we could optimize the\nKV-cache jointly from two dimensions, i.e., sequence-wise and layer-wise. Based\non our observations regarding layer-wise importance in inference, we propose\nSqueezeAttention to precisely optimize the allocation of KV-cache budget among\nlayers on-the-fly and then incorporate three representative sequence-wise\nalgorithms to compress the KV-cache for each layer with its very own budget.\nSpecifically, we first measure each layer's importance by calculating the\ncosine similarity of the input prompt differences before and after the\nself-attention layers. Based on this similarity, we then categorize the layers\ninto two groups and adjust their KV budgets accordingly. By optimizing the\nKV-cache from both sequence's and layer's dimensions, SqueezeAttention achieves\naround 30% to 70% of the memory reductions and up to 2.2 times of throughput\nimprovements in a wide range of LLMs and benchmarks. The code is available at\nhttps://github.com/hetailang/SqueezeAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has\nbeen considered critical to saving the cost of inference. Most of the existing\nKV-cache compression algorithms attempted to sparsify the sequence of tokens by\ntaking advantage of the different importance of tokens. However, most of these\nmethods treat all layers equally, allocating the same KV budget to each layer.\nThis approach is suboptimal, as some layers may be less sensitive to input\ntokens yet still receive the same budget as others. In this work, we found that\nby identifying the importance of attention layers, we could optimize the\nKV-cache jointly from two dimensions, i.e., sequence-wise and layer-wise. Based\non our observations regarding layer-wise importance in inference, we propose\nSqueezeAttention to precisely optimize the allocation of KV-cache budget among\nlayers on-the-fly and then incorporate three representative sequence-wise\nalgorithms to compress the KV-cache for each layer with its very own budget.\nSpecifically, we first measure each layer's importance by calculating the\ncosine similarity of the input prompt differences before and after the\nself-attention layers. Based on this similarity, we then categorize the layers\ninto two groups and adjust their KV budgets accordingly. By optimizing the\nKV-cache from both sequence's and layer's dimensions, SqueezeAttention achieves\naround 30% to 70% of the memory reductions and up to 2.2 times of throughput\nimprovements in a wide range of LLMs and benchmarks. The code is available at\nhttps://github.com/hetailang/SqueezeAttention."
                },
                "authors": [
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Shaoduo Gan"
                    }
                ],
                "author_detail": {
                    "name": "Shaoduo Gan"
                },
                "author": "Shaoduo Gan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07590v1",
                "updated": "2024-10-10T03:52:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    52,
                    54,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T03:52:54Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    52,
                    54,
                    3,
                    284,
                    0
                ],
                "title": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed\n  KV Caches for Chunked Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed\n  KV Caches for Chunked Text"
                },
                "summary": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process\nnumerous retrieved document chunks for prefill which requires a large volume of\ncomputation, therefore leading to significant latency in time-to-first-token\n(TTFT). To reduce the computation overhead as well as TTFT, we introduce\nTurboRAG, a novel RAG system that redesigns the inference paradigm of the\ncurrent RAG system by first pre-computing and storing the key-value (KV) caches\nof documents offline, and then directly retrieving the saved KV cache for\nprefill. Hence, online computation of KV caches is eliminated during inference.\nIn addition, we provide a number of insights into the mask matrix and\npositional embedding mechanisms, plus fine-tune a pretrained language model to\nmaintain model accuracy of TurboRAG. Our approach is applicable to most\nexisting large language models and their applications without any requirement\nin modification of models and inference systems. Experimental results across a\nsuite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x\ncompared to the conventional RAG systems (on an average of 8.6x), but reserving\ncomparable performance to the standard RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process\nnumerous retrieved document chunks for prefill which requires a large volume of\ncomputation, therefore leading to significant latency in time-to-first-token\n(TTFT). To reduce the computation overhead as well as TTFT, we introduce\nTurboRAG, a novel RAG system that redesigns the inference paradigm of the\ncurrent RAG system by first pre-computing and storing the key-value (KV) caches\nof documents offline, and then directly retrieving the saved KV cache for\nprefill. Hence, online computation of KV caches is eliminated during inference.\nIn addition, we provide a number of insights into the mask matrix and\npositional embedding mechanisms, plus fine-tune a pretrained language model to\nmaintain model accuracy of TurboRAG. Our approach is applicable to most\nexisting large language models and their applications without any requirement\nin modification of models and inference systems. Experimental results across a\nsuite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x\ncompared to the conventional RAG systems (on an average of 8.6x), but reserving\ncomparable performance to the standard RAG systems."
                },
                "authors": [
                    {
                        "name": "Songshuo Lu"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Yutian Rong"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Yaohua Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yaohua Tang"
                },
                "author": "Yaohua Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07579v1",
                "updated": "2024-10-10T03:28:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    28,
                    46,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T03:28:46Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    28,
                    46,
                    3,
                    284,
                    0
                ],
                "title": "Teddy: Efficient Large-Scale Dataset Distillation via\n  Taylor-Approximated Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teddy: Efficient Large-Scale Dataset Distillation via\n  Taylor-Approximated Matching"
                },
                "summary": "Dataset distillation or condensation refers to compressing a large-scale\ndataset into a much smaller one, enabling models trained on this synthetic\ndataset to generalize effectively on real data. Tackling this challenge, as\ndefined, relies on a bi-level optimization algorithm: a novel model is trained\nin each iteration within a nested loop, with gradients propagated through an\nunrolled computation graph. However, this approach incurs high memory and time\ncomplexity, posing difficulties in scaling up to large datasets such as\nImageNet. Addressing these concerns, this paper introduces Teddy, a\nTaylor-approximated dataset distillation framework designed to handle\nlarge-scale dataset and enhance efficiency. On the one hand, backed up by\ntheoretical analysis, we propose a memory-efficient approximation derived from\nTaylor expansion, which transforms the original form dependent on multi-step\ngradients to a first-order one. On the other hand, rather than repeatedly\ntraining a novel model in each iteration, we unveil that employing a pre-cached\npool of weak models, which can be generated from a single base model, enhances\nboth time efficiency and performance concurrently, particularly when dealing\nwith large-scale datasets. Extensive experiments demonstrate that the proposed\nTeddy attains state-of-the-art efficiency and performance on the Tiny-ImageNet\nand original-sized ImageNet-1K dataset, notably surpassing prior methods by up\nto 12.8%, while reducing 46.6% runtime. Our code will be available at\nhttps://github.com/Lexie-YU/Teddy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dataset distillation or condensation refers to compressing a large-scale\ndataset into a much smaller one, enabling models trained on this synthetic\ndataset to generalize effectively on real data. Tackling this challenge, as\ndefined, relies on a bi-level optimization algorithm: a novel model is trained\nin each iteration within a nested loop, with gradients propagated through an\nunrolled computation graph. However, this approach incurs high memory and time\ncomplexity, posing difficulties in scaling up to large datasets such as\nImageNet. Addressing these concerns, this paper introduces Teddy, a\nTaylor-approximated dataset distillation framework designed to handle\nlarge-scale dataset and enhance efficiency. On the one hand, backed up by\ntheoretical analysis, we propose a memory-efficient approximation derived from\nTaylor expansion, which transforms the original form dependent on multi-step\ngradients to a first-order one. On the other hand, rather than repeatedly\ntraining a novel model in each iteration, we unveil that employing a pre-cached\npool of weak models, which can be generated from a single base model, enhances\nboth time efficiency and performance concurrently, particularly when dealing\nwith large-scale datasets. Extensive experiments demonstrate that the proposed\nTeddy attains state-of-the-art efficiency and performance on the Tiny-ImageNet\nand original-sized ImageNet-1K dataset, notably surpassing prior methods by up\nto 12.8%, while reducing 46.6% runtime. Our code will be available at\nhttps://github.com/Lexie-YU/Teddy."
                },
                "authors": [
                    {
                        "name": "Ruonan Yu"
                    },
                    {
                        "name": "Songhua Liu"
                    },
                    {
                        "name": "Jingwen Ye"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "Accepted by ECCV2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19519v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19519v3",
                "updated": "2024-10-09T15:57:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    57,
                    3,
                    2,
                    283,
                    0
                ],
                "published": "2024-03-28T15:52:15Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    15,
                    52,
                    15,
                    3,
                    88,
                    0
                ],
                "title": "Laser Interactions with Gas Jets: EMP Emission and Nozzle Damage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laser Interactions with Gas Jets: EMP Emission and Nozzle Damage"
                },
                "summary": "Understanding the physics of electromagnetic pulse emission and nozzle damage\nis critical for the long-term operation of laser experiments with gas targets,\nparticularly at facilities looking to produce stable sources of radiation at\nhigh repetition rate. We present a theoretical model of plasma formation and\nelectrostatic charging when high-power lasers are focused inside gases. The\nmodel can be used to estimate the amplitude of gigahertz electromagnetic pulses\n(EMPs) produced by the laser and the extent of damage to the gas jet nozzle.\nLooking at a range of laser and target properties relevant to existing\nhigh-power laser systems, we find that EMP fields of tens to hundreds of kV/m\ncan be generated several metres from the gas jet. Model predictions are\ncompared with measurements of EMP, plasma formation and nozzle damage from two\nexperiments on the VEGA-3 laser and one experiment on the Vulcan Petawatt\nlaser.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the physics of electromagnetic pulse emission and nozzle damage\nis critical for the long-term operation of laser experiments with gas targets,\nparticularly at facilities looking to produce stable sources of radiation at\nhigh repetition rate. We present a theoretical model of plasma formation and\nelectrostatic charging when high-power lasers are focused inside gases. The\nmodel can be used to estimate the amplitude of gigahertz electromagnetic pulses\n(EMPs) produced by the laser and the extent of damage to the gas jet nozzle.\nLooking at a range of laser and target properties relevant to existing\nhigh-power laser systems, we find that EMP fields of tens to hundreds of kV/m\ncan be generated several metres from the gas jet. Model predictions are\ncompared with measurements of EMP, plasma formation and nozzle damage from two\nexperiments on the VEGA-3 laser and one experiment on the Vulcan Petawatt\nlaser."
                },
                "authors": [
                    {
                        "name": "Philip Wykeham Bradford"
                    },
                    {
                        "name": "Valeria Ospina-Bohorquez"
                    },
                    {
                        "name": "Michael Ehret"
                    },
                    {
                        "name": "Jose-Luis Henares"
                    },
                    {
                        "name": "Pilar Puyuelo-Valdes"
                    },
                    {
                        "name": "Tomasz Chodukowski"
                    },
                    {
                        "name": "Tadeusz Pisarczyk"
                    },
                    {
                        "name": "Zofia Rusiniak"
                    },
                    {
                        "name": "Carlos Salgado-Lopez"
                    },
                    {
                        "name": "Christos Vlachos"
                    },
                    {
                        "name": "Massimiliano Sciscio"
                    },
                    {
                        "name": "Martina Salvadori"
                    },
                    {
                        "name": "Claudio Verona"
                    },
                    {
                        "name": "George Hicks"
                    },
                    {
                        "name": "Oliver Ettlinger"
                    },
                    {
                        "name": "Zulfikar Najmudin"
                    },
                    {
                        "name": "Jean-Raphael Marques"
                    },
                    {
                        "name": "Laurent Gremillet"
                    },
                    {
                        "name": "Joao Jorge Santos"
                    },
                    {
                        "name": "Fabrizio Consoli"
                    },
                    {
                        "name": "Vladimir Tikhonchuk"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir Tikhonchuk"
                },
                "author": "Vladimir Tikhonchuk",
                "arxiv_comment": "18 pages (total), 12 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19519v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19519v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06934v1",
                "updated": "2024-10-09T14:28:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    28,
                    59,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T14:28:59Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    28,
                    59,
                    2,
                    283,
                    0
                ],
                "title": "VEC-Sim: A Simulation Platform for Evaluating Service Caching and\n  Computation Offloading Policies in Vehicular Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VEC-Sim: A Simulation Platform for Evaluating Service Caching and\n  Computation Offloading Policies in Vehicular Edge Networks"
                },
                "summary": "Computer simulation platforms offer an alternative solution by emulating\ncomplex systems in a controlled manner. However, existing Edge Computing (EC)\nsimulators, as well as general-purpose vehicular network simulators, are not\ntailored for VEC and lack dedicated support for modeling the distinct access\npattern, entity mobility trajectory and other unique characteristics of VEC\nnetworks. To fill this gap, this paper proposes VEC-Sim, a versatile simulation\nplatform for in-depth evaluation and analysis of various service caching and\ncomputation offloading policies in VEC networks. VEC-Sim incorporates realistic\nmechanisms to replicate real-world access patterns, including service feature\nvector, vehicle mobility modeling, evolving service popularity, new service\nupload and user preference shifts, etc. Moreover, its modular architecture and\nextensive Application Programming Interfaces (APIs) allow seamless integration\nof customized scheduling policies and user-defined metrics. A comprehensive\nevaluation of VEC-Sim's capabilities is undertaken in comparison to real-world\nground truths. Results prove it to be accurate in reproducing classical\nscheduling algorithms and extremely effective in conducting case studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer simulation platforms offer an alternative solution by emulating\ncomplex systems in a controlled manner. However, existing Edge Computing (EC)\nsimulators, as well as general-purpose vehicular network simulators, are not\ntailored for VEC and lack dedicated support for modeling the distinct access\npattern, entity mobility trajectory and other unique characteristics of VEC\nnetworks. To fill this gap, this paper proposes VEC-Sim, a versatile simulation\nplatform for in-depth evaluation and analysis of various service caching and\ncomputation offloading policies in VEC networks. VEC-Sim incorporates realistic\nmechanisms to replicate real-world access patterns, including service feature\nvector, vehicle mobility modeling, evolving service popularity, new service\nupload and user preference shifts, etc. Moreover, its modular architecture and\nextensive Application Programming Interfaces (APIs) allow seamless integration\nof customized scheduling policies and user-defined metrics. A comprehensive\nevaluation of VEC-Sim's capabilities is undertaken in comparison to real-world\nground truths. Results prove it to be accurate in reproducing classical\nscheduling algorithms and extremely effective in conducting case studies."
                },
                "authors": [
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Xiaolong Xu"
                    },
                    {
                        "name": "Muhammad Bilal"
                    },
                    {
                        "name": "Xiangwei Wang"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Siyu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Siyu Wu"
                },
                "author": "Siyu Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00428v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00428v3",
                "updated": "2024-10-09T11:40:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    11,
                    40,
                    31,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-01T06:23:17Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    6,
                    23,
                    17,
                    1,
                    275,
                    0
                ],
                "title": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management"
                },
                "summary": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience."
                },
                "authors": [
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Ziqing Wang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Zhenxuan Pan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenxuan Pan"
                },
                "author": "Zhenxuan Pan",
                "arxiv_comment": "11 pages, 7 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00428v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00428v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06627v1",
                "updated": "2024-10-09T07:22:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    7,
                    22,
                    40,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T07:22:40Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    7,
                    22,
                    40,
                    2,
                    283,
                    0
                ],
                "title": "Variations in Multi-Agent Actor-Critic Frameworks for Joint\n  Optimizations in UAV Swarm Networks: Recent Evolution, Challenges, and\n  Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variations in Multi-Agent Actor-Critic Frameworks for Joint\n  Optimizations in UAV Swarm Networks: Recent Evolution, Challenges, and\n  Directions"
                },
                "summary": "Autonomous unmanned aerial vehicle (UAV) swarm networks (UAVSNs) can\neffectively execute surveillance, connectivity, and computing services to\nground users (GUs). These missions require trajectory planning, UAV-GUs\nassociation, task offloading, next-hop selection, and resources such as\ntransmit power, bandwidth, caching, and computing allocation to improve network\nperformances. Owing to the highly dynamic topology, limited resources, and\nnon-availability of global knowledge, optimizing network performance in UAVSNs\nis very intricate. Hence, it requires an adaptive joint optimization framework\nthat can tackle both discrete and continuous decision variables to ensure\noptimal network performance under dynamic constraints. Multi-agent deep\nreinforcement learning-based adaptive actor-critic framework can efficiently\naddress these problems. This paper investigates the recent evolutions of\nactor-critic frameworks to deal with joint optimization problems in UAVSNs. In\naddition, challenges and potential solutions are addressed as research\ndirections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous unmanned aerial vehicle (UAV) swarm networks (UAVSNs) can\neffectively execute surveillance, connectivity, and computing services to\nground users (GUs). These missions require trajectory planning, UAV-GUs\nassociation, task offloading, next-hop selection, and resources such as\ntransmit power, bandwidth, caching, and computing allocation to improve network\nperformances. Owing to the highly dynamic topology, limited resources, and\nnon-availability of global knowledge, optimizing network performance in UAVSNs\nis very intricate. Hence, it requires an adaptive joint optimization framework\nthat can tackle both discrete and continuous decision variables to ensure\noptimal network performance under dynamic constraints. Multi-agent deep\nreinforcement learning-based adaptive actor-critic framework can efficiently\naddress these problems. This paper investigates the recent evolutions of\nactor-critic frameworks to deal with joint optimization problems in UAVSNs. In\naddition, challenges and potential solutions are addressed as research\ndirections."
                },
                "authors": [
                    {
                        "name": "Muhammad Morshed Alam"
                    },
                    {
                        "name": "Muhammad Yeasir Aarafat"
                    },
                    {
                        "name": "Tamim Hossain"
                    }
                ],
                "author_detail": {
                    "name": "Tamim Hossain"
                },
                "author": "Tamim Hossain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13941v2",
                "updated": "2024-10-09T04:11:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    4,
                    11,
                    28,
                    2,
                    283,
                    0
                ],
                "published": "2024-06-20T02:20:21Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    2,
                    20,
                    21,
                    3,
                    172,
                    0
                ],
                "title": "UpDLRM: Accelerating Personalized Recommendation using Real-World PIM\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDLRM: Accelerating Personalized Recommendation using Real-World PIM\n  Architecture"
                },
                "summary": "Deep Learning Recommendation Models (DLRMs) have gained popularity in\nrecommendation systems due to their effectiveness in handling large-scale\nrecommendation tasks. The embedding layers of DLRMs have become the performance\nbottleneck due to their intensive needs on memory capacity and memory\nbandwidth. In this paper, we propose UpDLRM, which utilizes real-world\nprocessingin-memory (PIM) hardware, UPMEM DPU, to boost the memory bandwidth\nand reduce recommendation latency. The parallel nature of the DPU memory can\nprovide high aggregated bandwidth for the large number of irregular memory\naccesses in embedding lookups, thus offering great potential to reduce the\ninference latency. To fully utilize the DPU memory bandwidth, we further\nstudied the embedding table partitioning problem to achieve good\nworkload-balance and efficient data caching. Evaluations using real-world\ndatasets show that, UpDLRM achieves much lower inference time for DLRM compared\nto both CPU-only and CPU-GPU hybrid counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Recommendation Models (DLRMs) have gained popularity in\nrecommendation systems due to their effectiveness in handling large-scale\nrecommendation tasks. The embedding layers of DLRMs have become the performance\nbottleneck due to their intensive needs on memory capacity and memory\nbandwidth. In this paper, we propose UpDLRM, which utilizes real-world\nprocessingin-memory (PIM) hardware, UPMEM DPU, to boost the memory bandwidth\nand reduce recommendation latency. The parallel nature of the DPU memory can\nprovide high aggregated bandwidth for the large number of irregular memory\naccesses in embedding lookups, thus offering great potential to reduce the\ninference latency. To fully utilize the DPU memory bandwidth, we further\nstudied the embedding table partitioning problem to achieve good\nworkload-balance and efficient data caching. Evaluations using real-world\ndatasets show that, UpDLRM achieves much lower inference time for DLRM compared\nto both CPU-only and CPU-GPU hybrid counterparts."
                },
                "authors": [
                    {
                        "name": "Sitian Chen"
                    },
                    {
                        "name": "Haobin Tan"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Yusen Li"
                    },
                    {
                        "name": "Pavan Balaji"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Balaji"
                },
                "author": "Pavan Balaji",
                "arxiv_doi": "10.1145/3649329.3658266",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3649329.3658266",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.13941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by DAC 2024",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06497v1",
                "updated": "2024-10-09T02:51:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    2,
                    51,
                    27,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T02:51:27Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    2,
                    51,
                    27,
                    2,
                    283,
                    0
                ],
                "title": "ERCache: An Efficient and Reliable Caching Framework for Large-Scale\n  User Representations in Meta's Ads System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERCache: An Efficient and Reliable Caching Framework for Large-Scale\n  User Representations in Meta's Ads System"
                },
                "summary": "The increasing complexity of deep learning models used for calculating user\nrepresentations presents significant challenges, particularly with limited\ncomputational resources and strict service-level agreements (SLAs). Previous\nresearch efforts have focused on optimizing model inference but have overlooked\na critical question: is it necessary to perform user model inference for every\nad request in large-scale social networks? To address this question and these\nchallenges, we first analyze user access patterns at Meta and find that most\nuser model inferences occur within a short timeframe. T his observation reveals\na triangular relationship among model complexity, embedding freshness, and\nservice SLAs. Building on this insight, we designed, implemented, and evaluated\nERCache, an efficient and robust caching framework for large-scale user\nrepresentations in ads recommendation systems on social networks. ERCache\ncategorizes cache into direct and failover types and applies customized\nsettings and eviction policies for each model, effectively balancing model\ncomplexity, embedding freshness, and service SLAs, even considering the\nstaleness introduced by caching. ERCache has been deployed at Meta for over six\nmonths, supporting more than 30 ranking models while efficiently conserving\ncomputational resources and complying with service SLA requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of deep learning models used for calculating user\nrepresentations presents significant challenges, particularly with limited\ncomputational resources and strict service-level agreements (SLAs). Previous\nresearch efforts have focused on optimizing model inference but have overlooked\na critical question: is it necessary to perform user model inference for every\nad request in large-scale social networks? To address this question and these\nchallenges, we first analyze user access patterns at Meta and find that most\nuser model inferences occur within a short timeframe. T his observation reveals\na triangular relationship among model complexity, embedding freshness, and\nservice SLAs. Building on this insight, we designed, implemented, and evaluated\nERCache, an efficient and robust caching framework for large-scale user\nrepresentations in ads recommendation systems on social networks. ERCache\ncategorizes cache into direct and failover types and applies customized\nsettings and eviction policies for each model, effectively balancing model\ncomplexity, embedding freshness, and service SLAs, even considering the\nstaleness introduced by caching. ERCache has been deployed at Meta for over six\nmonths, supporting more than 30 ranking models while efficiently conserving\ncomputational resources and complying with service SLA requirements."
                },
                "authors": [
                    {
                        "name": "Fang Zhou"
                    },
                    {
                        "name": "Yaning Huang"
                    },
                    {
                        "name": "Dong Liang"
                    },
                    {
                        "name": "Dai Li"
                    },
                    {
                        "name": "Zhongke Zhang"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Xiao Xin"
                    },
                    {
                        "name": "Abdallah Aboelela"
                    },
                    {
                        "name": "Zheliang Jiang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Jeff Song"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Chen Liang"
                    },
                    {
                        "name": "Huayu Li"
                    },
                    {
                        "name": "ChongLin Sun"
                    },
                    {
                        "name": "Hang Yang"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Zhan Shu"
                    },
                    {
                        "name": "Mindi Yuan"
                    },
                    {
                        "name": "Emanuele Maccherani"
                    },
                    {
                        "name": "Taha Hayat"
                    },
                    {
                        "name": "John Guo"
                    },
                    {
                        "name": "Varna Puvvada"
                    },
                    {
                        "name": "Uladzimir Pashkevich"
                    }
                ],
                "author_detail": {
                    "name": "Uladzimir Pashkevich"
                },
                "author": "Uladzimir Pashkevich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10443v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10443v4",
                "updated": "2024-10-09T01:12:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    1,
                    12,
                    19,
                    2,
                    283,
                    0
                ],
                "published": "2024-05-16T21:07:42Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    21,
                    7,
                    42,
                    3,
                    137,
                    0
                ],
                "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation"
                },
                "summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost."
                },
                "authors": [
                    {
                        "name": "Matthew Raffel"
                    },
                    {
                        "name": "Victor Agostinelli"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "arxiv_comment": "Accepted at EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10443v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10443v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01527v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01527v2",
                "updated": "2024-10-08T19:34:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    19,
                    34,
                    3,
                    1,
                    282,
                    0
                ],
                "published": "2024-07-01T17:59:47Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    17,
                    59,
                    47,
                    0,
                    183,
                    0
                ],
                "title": "KV Cache Compression, But What Must We Give in Return? A Comprehensive\n  Benchmark of Long Context Capable Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Compression, But What Must We Give in Return? A Comprehensive\n  Benchmark of Long Context Capable Approaches"
                },
                "summary": "Long context capability is a crucial competency for large language models\n(LLMs) as it mitigates the human struggle to digest long-form texts. This\ncapability enables complex task-solving scenarios such as book summarization,\ncode assistance, and many more tasks that are traditionally manpower-intensive.\nHowever, transformer-based LLMs face significant challenges with long context\ninput due to the growing size of the KV cache and the intrinsic complexity of\nattending to extended inputs; where multiple schools of efficiency-driven\napproaches - such as KV cache quantization, token dropping, prompt compression,\nlinear-time sequence models, and hybrid architectures - have been proposed to\nproduce efficient yet long context-capable models. Despite these advancements,\nno existing work has comprehensively benchmarked these methods in a reasonably\naligned environment. In this work, we fill this gap by providing a taxonomy of\ncurrent methods and evaluating 10+ state-of-the-art approaches across seven\ncategories of long context tasks. Our work reveals numerous previously unknown\nphenomena and offers insights - as well as a friendly workbench - for the\nfuture development of long context-capable LLMs. The source code is available\nat https://github.com/henryzhongsc/longctx_bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context capability is a crucial competency for large language models\n(LLMs) as it mitigates the human struggle to digest long-form texts. This\ncapability enables complex task-solving scenarios such as book summarization,\ncode assistance, and many more tasks that are traditionally manpower-intensive.\nHowever, transformer-based LLMs face significant challenges with long context\ninput due to the growing size of the KV cache and the intrinsic complexity of\nattending to extended inputs; where multiple schools of efficiency-driven\napproaches - such as KV cache quantization, token dropping, prompt compression,\nlinear-time sequence models, and hybrid architectures - have been proposed to\nproduce efficient yet long context-capable models. Despite these advancements,\nno existing work has comprehensively benchmarked these methods in a reasonably\naligned environment. In this work, we fill this gap by providing a taxonomy of\ncurrent methods and evaluating 10+ state-of-the-art approaches across seven\ncategories of long context tasks. Our work reveals numerous previously unknown\nphenomena and offers insights - as well as a friendly workbench - for the\nfuture development of long context-capable LLMs. The source code is available\nat https://github.com/henryzhongsc/longctx_bench."
                },
                "authors": [
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongyi Liu"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Yu-Neng Chuang"
                    },
                    {
                        "name": "Songchen Li"
                    },
                    {
                        "name": "Guanchu Wang"
                    },
                    {
                        "name": "Duy Le"
                    },
                    {
                        "name": "Hongye Jin"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01527v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01527v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05927v1",
                "updated": "2024-10-08T11:28:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    11,
                    28,
                    30,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T11:28:30Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    11,
                    28,
                    30,
                    1,
                    282,
                    0
                ],
                "title": "Numerical analysis of partial discharge ignition in H2 bubbles floating\n  in dielectric oils, for High-Voltage Solid State Transformer applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical analysis of partial discharge ignition in H2 bubbles floating\n  in dielectric oils, for High-Voltage Solid State Transformer applications"
                },
                "summary": "We report on a self-consistent numerical analysis campaign of partial\ndischarge (PD) ignition in H2 bubbles floating in biobased dielectric oils. We\ninvestigate various configurations (bubble sizes, bubble position, existence of\nprotrusion) on a cylinder-to-cylinder setup that emulates a specific SST module\n(from SSTAR Horizon Europe project) under transient overvoltage as well as in\nits design operational conditions (VRMS = 66 kV, AC excitation of 50 Hz). Our\nresults on electrical characteristics and plasma dynamics leading to the PD\nignition, indicate that under transient overvoltage and for mm size bubbles\n(diameter 1 -4.5 mm), the smaller the bubble the less the inception voltage,\nwhile the peak inception voltage is higher than 70 kV. The existence of\nmetallic protrusion can affect the inception voltage of a remote floating\nbubble only slightly and when this is close to the sharp tip. The extreme\nscenario of a protrusion in contact (inside) a gas bubble severely affects the\ninsulation properties and drops the PD inception voltage remarkably. The larger\nthe bubble and the sharper the tip of the protrusion the lower the inception\npeak voltage, that can reach values well below 40 kV. On the contrary and under\ndesign operation, larger bubbles increase the severity and probability of PD\nevents, leading to lower instantaneous inception voltages. Current pulses\nproduced in bubbles can quickly transit to intense streamer discharges (which\ncan also transit to catastrophic arcing) if the operational frequency is\nreduced and/or under transient, HF overvoltage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on a self-consistent numerical analysis campaign of partial\ndischarge (PD) ignition in H2 bubbles floating in biobased dielectric oils. We\ninvestigate various configurations (bubble sizes, bubble position, existence of\nprotrusion) on a cylinder-to-cylinder setup that emulates a specific SST module\n(from SSTAR Horizon Europe project) under transient overvoltage as well as in\nits design operational conditions (VRMS = 66 kV, AC excitation of 50 Hz). Our\nresults on electrical characteristics and plasma dynamics leading to the PD\nignition, indicate that under transient overvoltage and for mm size bubbles\n(diameter 1 -4.5 mm), the smaller the bubble the less the inception voltage,\nwhile the peak inception voltage is higher than 70 kV. The existence of\nmetallic protrusion can affect the inception voltage of a remote floating\nbubble only slightly and when this is close to the sharp tip. The extreme\nscenario of a protrusion in contact (inside) a gas bubble severely affects the\ninsulation properties and drops the PD inception voltage remarkably. The larger\nthe bubble and the sharper the tip of the protrusion the lower the inception\npeak voltage, that can reach values well below 40 kV. On the contrary and under\ndesign operation, larger bubbles increase the severity and probability of PD\nevents, leading to lower instantaneous inception voltages. Current pulses\nproduced in bubbles can quickly transit to intense streamer discharges (which\ncan also transit to catastrophic arcing) if the operational frequency is\nreduced and/or under transient, HF overvoltage."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kourtzanidis"
                    },
                    {
                        "name": "Panagiotis Dimitrakellis"
                    },
                    {
                        "name": "Dimitrios Rakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Rakopoulos"
                },
                "author": "Dimitrios Rakopoulos",
                "arxiv_comment": "Submitted to IEEE Transactions on Dielectrics and Electrical\n  Insulation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05863v1",
                "updated": "2024-10-08T09:53:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    53,
                    10,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T09:53:10Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    53,
                    10,
                    1,
                    282,
                    0
                ],
                "title": "Enhancing Playback Performance in Video Recommender Systems with an\n  On-Device Gating and Ranking Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Playback Performance in Video Recommender Systems with an\n  On-Device Gating and Ranking Framework"
                },
                "summary": "Video recommender systems (RSs) have gained increasing attention in recent\nyears. Existing mainstream RSs focus on optimizing the matching function\nbetween users and items. However, we noticed that users frequently encounter\nplayback issues such as slow loading or stuttering while browsing the videos,\nespecially in weak network conditions, which will lead to a subpar browsing\nexperience, and may cause users to leave, even when the video content and\nrecommendations are superior. It is quite a serious issue, yet easily\noverlooked. To tackle this issue, we propose an on-device Gating and Ranking\nFramework (GRF) that cooperates with server-side RS. Specifically, we utilize a\ngate model to identify videos that may have playback issues in real-time, and\nthen we employ a ranking model to select the optimal result from a\nlocally-cached pool to replace the stuttering videos. Our solution has been\nfully deployed on Kwai, a large-scale short video platform with hundreds of\nmillions of users globally. Moreover, it significantly enhances video playback\nperformance and improves overall user experience and retention rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video recommender systems (RSs) have gained increasing attention in recent\nyears. Existing mainstream RSs focus on optimizing the matching function\nbetween users and items. However, we noticed that users frequently encounter\nplayback issues such as slow loading or stuttering while browsing the videos,\nespecially in weak network conditions, which will lead to a subpar browsing\nexperience, and may cause users to leave, even when the video content and\nrecommendations are superior. It is quite a serious issue, yet easily\noverlooked. To tackle this issue, we propose an on-device Gating and Ranking\nFramework (GRF) that cooperates with server-side RS. Specifically, we utilize a\ngate model to identify videos that may have playback issues in real-time, and\nthen we employ a ranking model to select the optimal result from a\nlocally-cached pool to replace the stuttering videos. Our solution has been\nfully deployed on Kwai, a large-scale short video platform with hundreds of\nmillions of users globally. Moreover, it significantly enhances video playback\nperformance and improves overall user experience and retention rates."
                },
                "authors": [
                    {
                        "name": "Yunfei Yang"
                    },
                    {
                        "name": "Zhenghao Qi"
                    },
                    {
                        "name": "Honghuan Wu"
                    },
                    {
                        "name": "Qi Song"
                    },
                    {
                        "name": "Tieyao Zhang"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Yimin Tu"
                    },
                    {
                        "name": "Kaiqiao Zhan"
                    },
                    {
                        "name": "Ben Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ben Wang"
                },
                "author": "Ben Wang",
                "arxiv_comment": "CIKM 2024 applied research track, 7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05854v1",
                "updated": "2024-10-08T09:46:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    46,
                    38,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T09:46:38Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    46,
                    38,
                    1,
                    282,
                    0
                ],
                "title": "A Scalable State Sharing Protocol for Low-Resource Validator Nodes in\n  Blockchain Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable State Sharing Protocol for Low-Resource Validator Nodes in\n  Blockchain Networks"
                },
                "summary": "The perpetual growth of data stored on popular blockchains such as Ethereum\nleads to significant scalability challenges and substantial storage costs for\noperators of full nodes. Increasing costs may lead to fewer independently\noperated nodes in the network, which poses risks to decentralization (and hence\nnetwork security), but also pushes decentralized app developers towards\ncentrally hosted API services.\n  This paper introduces a new protocol that allows validator nodes to\nparticipate in a blockchain network without the need to store the full state of\nthe network on each node. The key idea is to use the blockchain network as both\na replicated state machine and as a distributed storage system. By distributing\nstates across nodes and enabling efficient data retrieval through a\nKademlia-inspired routing protocol, we reduce storage costs for validators.\nCryptographic proofs (such as Merkle proofs) are used to allow nodes to verify\ndata stored by other nodes without having to trust those nodes directly. While\nthe protocol trades off data storage for increased network bandwidth, we show\nhow gossiping and caching can minimize the increased bandwidth needs.\n  To validate our state sharing protocol, we conduct an extensive quantitative\nanalysis of Ethereum's data storage and data access patterns. Our findings\nindicate that while our protocol significantly lowers storage needs, it comes\nwith an increased bandwidth usage ranging from 1.5 MB to 5 MB per block,\ntranslating to an additional monthly bandwidth of 319 GB to 1,065 GB. Despite\nthis, the size remains small enough such that it can be passed to all nodes and\nvalidated within Ethereum's 12-second block validation window. Further analysis\nshows that Merkle proofs are the most significant contributor to the additional\nbandwidth. To address this concern, we also analyze the impact of switching to\nthe more space-efficient Verkle Proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The perpetual growth of data stored on popular blockchains such as Ethereum\nleads to significant scalability challenges and substantial storage costs for\noperators of full nodes. Increasing costs may lead to fewer independently\noperated nodes in the network, which poses risks to decentralization (and hence\nnetwork security), but also pushes decentralized app developers towards\ncentrally hosted API services.\n  This paper introduces a new protocol that allows validator nodes to\nparticipate in a blockchain network without the need to store the full state of\nthe network on each node. The key idea is to use the blockchain network as both\na replicated state machine and as a distributed storage system. By distributing\nstates across nodes and enabling efficient data retrieval through a\nKademlia-inspired routing protocol, we reduce storage costs for validators.\nCryptographic proofs (such as Merkle proofs) are used to allow nodes to verify\ndata stored by other nodes without having to trust those nodes directly. While\nthe protocol trades off data storage for increased network bandwidth, we show\nhow gossiping and caching can minimize the increased bandwidth needs.\n  To validate our state sharing protocol, we conduct an extensive quantitative\nanalysis of Ethereum's data storage and data access patterns. Our findings\nindicate that while our protocol significantly lowers storage needs, it comes\nwith an increased bandwidth usage ranging from 1.5 MB to 5 MB per block,\ntranslating to an additional monthly bandwidth of 319 GB to 1,065 GB. Despite\nthis, the size remains small enough such that it can be passed to all nodes and\nvalidated within Ethereum's 12-second block validation window. Further analysis\nshows that Merkle proofs are the most significant contributor to the additional\nbandwidth. To address this concern, we also analyze the impact of switching to\nthe more space-efficient Verkle Proofs."
                },
                "authors": [
                    {
                        "name": "Ruben Hias"
                    },
                    {
                        "name": "Weihong Wang"
                    },
                    {
                        "name": "Jan Vanhoof"
                    },
                    {
                        "name": "Tom Van Cutsem"
                    }
                ],
                "author_detail": {
                    "name": "Tom Van Cutsem"
                },
                "author": "Tom Van Cutsem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12018v2",
                "updated": "2024-10-08T04:25:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    4,
                    25,
                    41,
                    1,
                    282,
                    0
                ],
                "published": "2024-06-17T18:34:58Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    18,
                    34,
                    58,
                    0,
                    169,
                    0
                ],
                "title": "CItruS: Chunked Instruction-aware State Eviction for Long Sequence\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CItruS: Chunked Instruction-aware State Eviction for Long Sequence\n  Modeling"
                },
                "summary": "Long sequence modeling has gained broad interest as large language models\n(LLMs) continue to advance. Recent research has identified that a large portion\nof hidden states within the key-value caches of Transformer models can be\ndiscarded (also termed evicted) without affecting the perplexity performance in\ngenerating long sequences. However, we show that these methods, despite\npreserving perplexity performance, often drop information that is important for\nsolving downstream tasks, a problem which we call information neglect. To\naddress this issue, we introduce Chunked Instruction-aware State Eviction\n(CItruS), a novel modeling technique that integrates the attention preferences\nuseful for a downstream task into the eviction process of hidden states. In\naddition, we design a method for chunked sequence processing to further improve\nefficiency. Our training-free method exhibits superior performance on long\nsequence comprehension and retrieval tasks over several strong baselines under\nthe same memory budget, while preserving language modeling perplexity. The code\nand data have been released at https://github.com/ybai-nlp/CItruS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long sequence modeling has gained broad interest as large language models\n(LLMs) continue to advance. Recent research has identified that a large portion\nof hidden states within the key-value caches of Transformer models can be\ndiscarded (also termed evicted) without affecting the perplexity performance in\ngenerating long sequences. However, we show that these methods, despite\npreserving perplexity performance, often drop information that is important for\nsolving downstream tasks, a problem which we call information neglect. To\naddress this issue, we introduce Chunked Instruction-aware State Eviction\n(CItruS), a novel modeling technique that integrates the attention preferences\nuseful for a downstream task into the eviction process of hidden states. In\naddition, we design a method for chunked sequence processing to further improve\nefficiency. Our training-free method exhibits superior performance on long\nsequence comprehension and retrieval tasks over several strong baselines under\nthe same memory budget, while preserving language modeling perplexity. The code\nand data have been released at https://github.com/ybai-nlp/CItruS."
                },
                "authors": [
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Xiyuan Zou"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Sanxing Chen"
                    },
                    {
                        "name": "Marc-Antoine Rondeau"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Jackie Chi Kit Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Jackie Chi Kit Cheung"
                },
                "author": "Jackie Chi Kit Cheung",
                "arxiv_comment": "EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05265v1",
                "updated": "2024-10-07T17:59:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:59:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs"
                },
                "summary": "Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}."
                },
                "authors": [
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "A PTQ method to significantly boost the performance of static\n  activation quantization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05516v3",
                "updated": "2024-10-07T17:21:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    21,
                    57,
                    0,
                    281,
                    0
                ],
                "published": "2023-12-09T09:55:07Z",
                "published_parsed": [
                    2023,
                    12,
                    9,
                    9,
                    55,
                    7,
                    5,
                    343,
                    0
                ],
                "title": "Stateful Large Language Model Serving with Pensieve",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stateful Large Language Model Serving with Pensieve"
                },
                "summary": "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency."
                },
                "authors": [
                    {
                        "name": "Lingfan Yu"
                    },
                    {
                        "name": "Jinkun Lin"
                    },
                    {
                        "name": "Jinyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Li"
                },
                "author": "Jinyang Li",
                "arxiv_doi": "10.1145/3689031.3696086",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689031.3696086",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.05516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00161v2",
                "updated": "2024-10-07T15:07:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    7,
                    9,
                    0,
                    281,
                    0
                ],
                "published": "2024-09-30T19:09:13Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    19,
                    9,
                    13,
                    0,
                    274,
                    0
                ],
                "title": "KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head"
                },
                "summary": "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches."
                },
                "authors": [
                    {
                        "name": "Isaac Rehg"
                    }
                ],
                "author_detail": {
                    "name": "Isaac Rehg"
                },
                "author": "Isaac Rehg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05076v1",
                "updated": "2024-10-07T14:30:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    30,
                    27,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:30:27Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    30,
                    27,
                    0,
                    281,
                    0
                ],
                "title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention"
                },
                "summary": "Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x."
                },
                "authors": [
                    {
                        "name": "Lijie Yang"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Zhuofu Chen"
                    },
                    {
                        "name": "Zikun Li"
                    },
                    {
                        "name": "Zhihao Jia"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Jia"
                },
                "author": "Zhihao Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05033v1",
                "updated": "2024-10-07T13:33:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    33,
                    23,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T13:33:23Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    33,
                    23,
                    0,
                    281,
                    0
                ],
                "title": "Extended Functional Representation Lemma: A Tool For Privacy, Semantic\n  Representation, Caching, and Compression Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extended Functional Representation Lemma: A Tool For Privacy, Semantic\n  Representation, Caching, and Compression Design"
                },
                "summary": "This paper provides an overview of a problem in information-theoretic privacy\nmechanism design, addressing two scenarios in which private data is either\nobservable or hidden. In each scenario, different privacy measures are used,\nincluding bounded mutual information and two types of per-letter privacy\nconstraints. Considering the first scenario, an agent observes useful data that\nis correlated with private data, and wants to disclose the useful information\nto a user. Due to the privacy concerns, direct disclosure is prohibited. Hence,\na privacy mechanism is designed to generate disclosed data which maximizes the\nrevealed information about the useful data while satisfying a privacy\nconstraint. In the second scenario, the agent has additionally access to the\nprivate data. We discuss how the Functional Representation Lemma, the Strong\nFunctional Representation Lemma, and their extended versions are useful for\ndesigning low-complexity privacy mechanisms that achieve optimal\nprivacy-utility trade-offs under certain constraints. Furthermore, another\nprivacy design problem is presented where part of the private attribute is more\nprivate than the remaining part. Finally, we provide applications including\nsemantic communications, caching and delivery, and compression designs, where\nthe approach can be applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides an overview of a problem in information-theoretic privacy\nmechanism design, addressing two scenarios in which private data is either\nobservable or hidden. In each scenario, different privacy measures are used,\nincluding bounded mutual information and two types of per-letter privacy\nconstraints. Considering the first scenario, an agent observes useful data that\nis correlated with private data, and wants to disclose the useful information\nto a user. Due to the privacy concerns, direct disclosure is prohibited. Hence,\na privacy mechanism is designed to generate disclosed data which maximizes the\nrevealed information about the useful data while satisfying a privacy\nconstraint. In the second scenario, the agent has additionally access to the\nprivate data. We discuss how the Functional Representation Lemma, the Strong\nFunctional Representation Lemma, and their extended versions are useful for\ndesigning low-complexity privacy mechanisms that achieve optimal\nprivacy-utility trade-offs under certain constraints. Furthermore, another\nprivacy design problem is presented where part of the private attribute is more\nprivate than the remaining part. Finally, we provide applications including\nsemantic communications, caching and delivery, and compression designs, where\nthe approach can be applied."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2212.12475",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05004v1",
                "updated": "2024-10-07T13:03:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    3,
                    45,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T13:03:45Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    3,
                    45,
                    0,
                    281,
                    0
                ],
                "title": "Fast State Restoration in LLM Serving with HCache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast State Restoration in LLM Serving with HCache"
                },
                "summary": "The growing complexity of LLM usage today, e.g., multi-round conversation and\nretrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)\nreusable across user requests. Given the capacity constraints of GPU memory,\nonly a limited number of contexts can be cached on GPU for reusing. Existing\ninference systems typically evict part of the KV cache and restore it by\nrecomputing it from the original tokens or offloading it to host storage for\nlater retrieval, both of which introduce substantial computational or I/O\noverheads. We propose HCache, a novel LLM state restoration method. Its key\nidea is to restore LLM states from intermediate activations and thus utilize\ncomputational and I/O resources with low overhead. We enhance HCache with two\ntechniques, including i) a bubble-free restoration scheduler that integrates\nresource-complementary methods to optimize the balance between computation and\nIO tasks; and ii) a chunk-based storage manager to address the layout mismatch\nissue (i.e., layer-before-token saving versus token-before-layer restoration).\nOur evaluations, conducted using real-world tasks, show that HCache reduces the\nTTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less\nstorage space; compared to token recomputation, HCache achieves up to 5.73X\nreduction in TTFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing complexity of LLM usage today, e.g., multi-round conversation and\nretrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)\nreusable across user requests. Given the capacity constraints of GPU memory,\nonly a limited number of contexts can be cached on GPU for reusing. Existing\ninference systems typically evict part of the KV cache and restore it by\nrecomputing it from the original tokens or offloading it to host storage for\nlater retrieval, both of which introduce substantial computational or I/O\noverheads. We propose HCache, a novel LLM state restoration method. Its key\nidea is to restore LLM states from intermediate activations and thus utilize\ncomputational and I/O resources with low overhead. We enhance HCache with two\ntechniques, including i) a bubble-free restoration scheduler that integrates\nresource-complementary methods to optimize the balance between computation and\nIO tasks; and ii) a chunk-based storage manager to address the layout mismatch\nissue (i.e., layer-before-token saving versus token-before-layer restoration).\nOur evaluations, conducted using real-world tasks, show that HCache reduces the\nTTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less\nstorage space; compared to token recomputation, HCache achieves up to 5.73X\nreduction in TTFT."
                },
                "authors": [
                    {
                        "name": "Shiwei Gao"
                    },
                    {
                        "name": "Youmin Chen"
                    },
                    {
                        "name": "Jiwu Shu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwu Shu"
                },
                "author": "Jiwu Shu",
                "arxiv_comment": "EuroSys 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16406v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16406v3",
                "updated": "2024-10-07T01:27:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    1,
                    27,
                    59,
                    0,
                    281,
                    0
                ],
                "published": "2024-05-26T02:15:49Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    2,
                    15,
                    49,
                    6,
                    147,
                    0
                ],
                "title": "SpinQuant: LLM quantization with learned rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpinQuant: LLM quantization with learned rotations"
                },
                "summary": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot."
                },
                "authors": [
                    {
                        "name": "Zechun Liu"
                    },
                    {
                        "name": "Changsheng Zhao"
                    },
                    {
                        "name": "Igor Fedorov"
                    },
                    {
                        "name": "Bilge Soran"
                    },
                    {
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "name": "Raghuraman Krishnamoorthi"
                    },
                    {
                        "name": "Vikas Chandra"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Tijmen Blankevoort"
                    }
                ],
                "author_detail": {
                    "name": "Tijmen Blankevoort"
                },
                "author": "Tijmen Blankevoort",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16406v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16406v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v4",
                "updated": "2024-10-06T22:13:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    6,
                    22,
                    13,
                    16,
                    6,
                    280,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04603v1",
                "updated": "2024-10-06T19:36:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "published": "2024-10-06T19:36:34Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "title": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics"
                },
                "summary": "Liquid Argon Time Projection Chamber (LArTPC) is an exceptional dual\ncalorimeter capable of estimating the energy of incident particles through both\nthe ionization charge and the scintillation light. Our studies show that due to\nthe mechanisms of charge recombination and light generation involved in the\nenergy dissipation in liquid argon, light calorimetry in LArTPCs is inherently\nself-compensating: the missing energy in the hadronic component is compensated\nfor by the extra recombination luminescence compared to the electromagnetic\ncomponent. Good compensation of the electron-to-hadron response ratio (e/h)\naround unity can be achieved across a broad range of drift electric fields from\n0.2 to 1.8 kV/cm.This inherent self-compensation enhances the appeal of light\ncalorimetry in LArTPCs, complementing the well-established charge calorimetry.\nUsing GeV neutrinos as a case study, we show that light calorimetry can achieve\nan energy resolution comparable to the more sophisticated charge imaging\ncalorimetry. The synergy between light and charge calorimetry offers a novel\napproach to evaluating and mitigating systematic uncertainties in energy\nmeasurements with LArTPCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Liquid Argon Time Projection Chamber (LArTPC) is an exceptional dual\ncalorimeter capable of estimating the energy of incident particles through both\nthe ionization charge and the scintillation light. Our studies show that due to\nthe mechanisms of charge recombination and light generation involved in the\nenergy dissipation in liquid argon, light calorimetry in LArTPCs is inherently\nself-compensating: the missing energy in the hadronic component is compensated\nfor by the extra recombination luminescence compared to the electromagnetic\ncomponent. Good compensation of the electron-to-hadron response ratio (e/h)\naround unity can be achieved across a broad range of drift electric fields from\n0.2 to 1.8 kV/cm.This inherent self-compensation enhances the appeal of light\ncalorimetry in LArTPCs, complementing the well-established charge calorimetry.\nUsing GeV neutrinos as a case study, we show that light calorimetry can achieve\nan energy resolution comparable to the more sophisticated charge imaging\ncalorimetry. The synergy between light and charge calorimetry offers a novel\napproach to evaluating and mitigating systematic uncertainties in energy\nmeasurements with LArTPCs."
                },
                "authors": [
                    {
                        "name": "Xuyang Ning"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Ciro Riccio"
                    },
                    {
                        "name": "Jay Hyun Jo"
                    }
                ],
                "author_detail": {
                    "name": "Jay Hyun Jo"
                },
                "author": "Jay Hyun Jo",
                "arxiv_comment": "15 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04252v1",
                "updated": "2024-10-05T18:20:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    5,
                    18,
                    20,
                    37,
                    5,
                    279,
                    0
                ],
                "published": "2024-10-05T18:20:37Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    18,
                    20,
                    37,
                    5,
                    279,
                    0
                ],
                "title": "Lazy Qubit Reordering for Accelerating Parallel State-Vector-based\n  Quantum Circuit Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lazy Qubit Reordering for Accelerating Parallel State-Vector-based\n  Quantum Circuit Simulation"
                },
                "summary": "This paper proposes two quantum operation scheduling methods for accelerating\nparallel state-vector-based quantum circuit simulation using multiple graphics\nprocessing units (GPUs). The proposed methods reduce all-to-all communication\ncaused by qubit reordering (QR), which can dominate the overhead of parallel\nsimulation. Our approach eliminates redundant QRs by introducing intentional\ndelays in QR communications such that multiple QRs can be aggregated into a\nsingle QR. The delays are carefully introduced based on the principles of\ntime-space tiling, or a cache optimization technique for classical computers,\nwhich we use to arrange the execution order of quantum operations. Moreover, we\npresent an extended scheduling method for the hierarchical interconnection of\nGPU cluster systems to avoid slow inter-node communication. We develop these\nmethods tailored for two primary procedures in variational quantum eigensolver\n(VQE) simulation: quantum state update (QSU) and expectation value computation\n(EVC). Experimental validation on 32-GPU executions demonstrates acceleration\nin QSU and EVC -- up to 54$\\times$ and 606$\\times$, respectively -- compared to\nexisting methods. Moreover, our extended scheduling method further reduced\ncommunication time by up to 15\\% in a two-layered interconnected cluster\nsystem. Our approach is useful for any quantum circuit simulations, including\nQSU and/or EVC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes two quantum operation scheduling methods for accelerating\nparallel state-vector-based quantum circuit simulation using multiple graphics\nprocessing units (GPUs). The proposed methods reduce all-to-all communication\ncaused by qubit reordering (QR), which can dominate the overhead of parallel\nsimulation. Our approach eliminates redundant QRs by introducing intentional\ndelays in QR communications such that multiple QRs can be aggregated into a\nsingle QR. The delays are carefully introduced based on the principles of\ntime-space tiling, or a cache optimization technique for classical computers,\nwhich we use to arrange the execution order of quantum operations. Moreover, we\npresent an extended scheduling method for the hierarchical interconnection of\nGPU cluster systems to avoid slow inter-node communication. We develop these\nmethods tailored for two primary procedures in variational quantum eigensolver\n(VQE) simulation: quantum state update (QSU) and expectation value computation\n(EVC). Experimental validation on 32-GPU executions demonstrates acceleration\nin QSU and EVC -- up to 54$\\times$ and 606$\\times$, respectively -- compared to\nexisting methods. Moreover, our extended scheduling method further reduced\ncommunication time by up to 15\\% in a two-layered interconnected cluster\nsystem. Our approach is useful for any quantum circuit simulations, including\nQSU and/or EVC."
                },
                "authors": [
                    {
                        "name": "Yusuke Teranishi"
                    },
                    {
                        "name": "Shoma Hiraoka"
                    },
                    {
                        "name": "Wataru Mizukami"
                    },
                    {
                        "name": "Masao Okita"
                    },
                    {
                        "name": "Fumihiko Ino"
                    }
                ],
                "author_detail": {
                    "name": "Fumihiko Ino"
                },
                "author": "Fumihiko Ino",
                "arxiv_comment": "24 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v1",
                "updated": "2024-10-05T03:47:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v1",
                "updated": "2024-10-04T22:45:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v2",
                "updated": "2024-10-04T10:14:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    10,
                    14,
                    17,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Cache\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Cache\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) have gained prominence for outstanding\nscalability and extraordinary performance in generative tasks. However, their\nconsiderable inference costs impede practical deployment. The feature cache\nmechanism, which involves storing and retrieving redundant computations across\ntimesteps, holds promise for reducing per-step inference time in diffusion\nmodels. Most existing caching methods for DiT are manually designed. Although\nthe learning-based approach attempts to optimize strategies adaptively, it\nsuffers from discrepancies between training and inference, which hampers both\nthe performance and acceleration ratio. Upon detailed analysis, we pinpoint\nthat these discrepancies primarily stem from two aspects: (1) Prior Timestep\nDisregard, where training ignores the effect of cache usage at earlier\ntimesteps, and (2) Objective Mismatch, where the training target (align\npredicted noise in each timestep) deviates from the goal of inference (generate\nthe high-quality image). To alleviate these discrepancies, we propose\nHarmoniCa, a novel method that Harmonizes training and inference with a novel\nlearning-based Caching framework built upon Step-Wise Denoising Training (SDT)\nand Image Error Proxy-Guided Objective (IEPO). Compared to the traditional\ntraining paradigm, the newly proposed SDT maintains the continuity of the\ndenoising process, enabling the model to leverage information from prior\ntimesteps during training, similar to the way it operates during inference.\nFurthermore, we design IEPO, which integrates an efficient proxy mechanism to\napproximate the final image error caused by reusing the cached feature.\nTherefore, IEPO helps balance final image quality and cache utilization,\nresolving the issue of training that only considers the impact of cache usage\non the predicted output at each timestep.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have gained prominence for outstanding\nscalability and extraordinary performance in generative tasks. However, their\nconsiderable inference costs impede practical deployment. The feature cache\nmechanism, which involves storing and retrieving redundant computations across\ntimesteps, holds promise for reducing per-step inference time in diffusion\nmodels. Most existing caching methods for DiT are manually designed. Although\nthe learning-based approach attempts to optimize strategies adaptively, it\nsuffers from discrepancies between training and inference, which hampers both\nthe performance and acceleration ratio. Upon detailed analysis, we pinpoint\nthat these discrepancies primarily stem from two aspects: (1) Prior Timestep\nDisregard, where training ignores the effect of cache usage at earlier\ntimesteps, and (2) Objective Mismatch, where the training target (align\npredicted noise in each timestep) deviates from the goal of inference (generate\nthe high-quality image). To alleviate these discrepancies, we propose\nHarmoniCa, a novel method that Harmonizes training and inference with a novel\nlearning-based Caching framework built upon Step-Wise Denoising Training (SDT)\nand Image Error Proxy-Guided Objective (IEPO). Compared to the traditional\ntraining paradigm, the newly proposed SDT maintains the continuity of the\ndenoising process, enabling the model to leverage information from prior\ntimesteps during training, similar to the way it operates during inference.\nFurthermore, we design IEPO, which integrates an efficient proxy mechanism to\napproximate the final image error caused by reusing the cached feature.\nTherefore, IEPO helps balance final image quality and cache utilization,\nresolving the issue of training that only considers the impact of cache usage\non the predicted output at each timestep."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Code will be released soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02369v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02369v2",
                "updated": "2024-10-04T07:54:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    7,
                    54,
                    58,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-03T10:33:49Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    33,
                    49,
                    3,
                    277,
                    0
                ],
                "title": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation"
                },
                "summary": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zekai Luo"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Guangkai Xu"
                    },
                    {
                        "name": "Xinlong Wang"
                    },
                    {
                        "name": "Chunhua Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chunhua Shen"
                },
                "author": "Chunhua Shen",
                "arxiv_comment": "Accepted to Proc. Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02369v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12016v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12016v2",
                "updated": "2024-10-04T06:26:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    6,
                    26,
                    20,
                    4,
                    278,
                    0
                ],
                "published": "2024-06-17T18:33:44Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    18,
                    33,
                    44,
                    0,
                    169,
                    0
                ],
                "title": "Prefixing Attention Sinks can Mitigate Activation Outliers for Large\n  Language Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefixing Attention Sinks can Mitigate Activation Outliers for Large\n  Language Model Quantization"
                },
                "summary": "Despite recent advances in LLM quantization, activation quantization remains\nto be challenging due to the activation outliers. Conventional remedies, e.g.,\nmixing precisions for different channels, introduce extra overhead and reduce\nthe speedup. In this work, we develop a simple yet effective strategy to\nfacilitate per-tensor activation quantization by preventing the generation of\nproblematic tokens. Precisely, we propose a method to find a set of key-value\ncache, coined CushionCache, which mitigates outliers in subsequent tokens when\ninserted as a prefix. CushionCache works in two steps: First, we greedily\nsearch for a prompt token sequence that minimizes the maximum activation values\nin subsequent tokens. Then, we further tune the token cache to regularize the\nactivations of subsequent tokens to be more quantization-friendly. The proposed\nmethod successfully addresses activation outliers of LLMs, providing a\nsubstantial performance boost for per-tensor activation quantization methods.\nWe thoroughly evaluate our method over a wide range of models and benchmarks\nand find that it significantly surpasses the established baseline of per-tensor\nW8A8 quantization and can be seamlessly integrated with the recent activation\nquantization method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advances in LLM quantization, activation quantization remains\nto be challenging due to the activation outliers. Conventional remedies, e.g.,\nmixing precisions for different channels, introduce extra overhead and reduce\nthe speedup. In this work, we develop a simple yet effective strategy to\nfacilitate per-tensor activation quantization by preventing the generation of\nproblematic tokens. Precisely, we propose a method to find a set of key-value\ncache, coined CushionCache, which mitigates outliers in subsequent tokens when\ninserted as a prefix. CushionCache works in two steps: First, we greedily\nsearch for a prompt token sequence that minimizes the maximum activation values\nin subsequent tokens. Then, we further tune the token cache to regularize the\nactivations of subsequent tokens to be more quantization-friendly. The proposed\nmethod successfully addresses activation outliers of LLMs, providing a\nsubstantial performance boost for per-tensor activation quantization methods.\nWe thoroughly evaluate our method over a wide range of models and benchmarks\nand find that it significantly surpasses the established baseline of per-tensor\nW8A8 quantization and can be seamlessly integrated with the recent activation\nquantization method."
                },
                "authors": [
                    {
                        "name": "Seungwoo Son"
                    },
                    {
                        "name": "Wonpyo Park"
                    },
                    {
                        "name": "Woohyun Han"
                    },
                    {
                        "name": "Kyuyeun Kim"
                    },
                    {
                        "name": "Jaeho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jaeho Lee"
                },
                "author": "Jaeho Lee",
                "arxiv_comment": "EMNLP 2024 Main (Long)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12016v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12016v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03111v1",
                "updated": "2024-10-04T03:10:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    3,
                    10,
                    53,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T03:10:53Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    3,
                    10,
                    53,
                    4,
                    278,
                    0
                ],
                "title": "LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive\n  Compression Strategy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive\n  Compression Strategy"
                },
                "summary": "The Key-Value (KV) cache is a crucial component in serving transformer-based\nautoregressive large language models (LLMs), enabling faster inference by\nstoring previously computed KV vectors. However, its memory consumption scales\nlinearly with sequence length and batch size, posing a significant bottleneck\nin LLM deployment. Existing approaches to mitigate this issue include: (1)\nefficient attention variants integrated in upcycling stages, which requires\nextensive parameter tuning thus unsuitable for pre-trained LLMs; (2) KV cache\ncompression at test time, primarily through token eviction policies, which\noften overlook inter-layer dependencies and can be task-specific.\n  This paper introduces an orthogonal approach to KV cache compression. We\npropose a low-rank approximation of KV weight matrices, allowing for plug-in\nintegration with existing transformer-based LLMs without model retraining. To\neffectively compress KV cache at the weight level, we adjust for layerwise\nsensitivity and introduce a progressive compression strategy, which is\nsupported by our theoretical analysis on how compression errors accumulate in\ndeep networks. Our method is designed to function without model tuning in\nupcycling stages or task-specific profiling in test stages. Extensive\nexperiments with LLaMA models ranging from 8B to 70B parameters across various\ntasks show that our approach significantly reduces the GPU memory footprint\nwhile maintaining performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache is a crucial component in serving transformer-based\nautoregressive large language models (LLMs), enabling faster inference by\nstoring previously computed KV vectors. However, its memory consumption scales\nlinearly with sequence length and batch size, posing a significant bottleneck\nin LLM deployment. Existing approaches to mitigate this issue include: (1)\nefficient attention variants integrated in upcycling stages, which requires\nextensive parameter tuning thus unsuitable for pre-trained LLMs; (2) KV cache\ncompression at test time, primarily through token eviction policies, which\noften overlook inter-layer dependencies and can be task-specific.\n  This paper introduces an orthogonal approach to KV cache compression. We\npropose a low-rank approximation of KV weight matrices, allowing for plug-in\nintegration with existing transformer-based LLMs without model retraining. To\neffectively compress KV cache at the weight level, we adjust for layerwise\nsensitivity and introduce a progressive compression strategy, which is\nsupported by our theoretical analysis on how compression errors accumulate in\ndeep networks. Our method is designed to function without model tuning in\nupcycling stages or task-specific profiling in test stages. Extensive\nexperiments with LLaMA models ranging from 8B to 70B parameters across various\ntasks show that our approach significantly reduces the GPU memory footprint\nwhile maintaining performance."
                },
                "authors": [
                    {
                        "name": "Rongzhi Zhang"
                    },
                    {
                        "name": "Kuang Wang"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Shuohang Wang"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Yelong Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yelong Shen"
                },
                "author": "Yelong Shen",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03090v1",
                "updated": "2024-10-04T02:32:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    2,
                    32,
                    36,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T02:32:36Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    2,
                    32,
                    36,
                    4,
                    278,
                    0
                ],
                "title": "UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large\n  Language Model Inference"
                },
                "summary": "Deploying large language models (LLMs) is challenging due to their high\nmemory and computational demands, especially during long-context inference.\nWhile key-value (KV) caching accelerates inference by reusing previously\ncomputed keys and values, it also introduces significant memory overhead.\nExisting KV cache compression methods such as eviction and merging typically\ncompress the KV cache after it is generated and overlook the eviction of hidden\nstates, failing to improve the speed of the prefilling stage. Additionally,\napplying a uniform compression rate across different attention heads can harm\ncrucial retrieval heads in needle-in-a-haystack tasks due to excessive\ncompression. In this paper, we propose UNComp, an uncertainty-aware compression\nscheme that leverages matrix entropy to estimate model uncertainty across\nlayers and heads at the token sequence level. By grouping layers and heads\nbased on their uncertainty, UNComp adaptively compresses both the hidden states\nand the KV cache. Our method achieves a 1.6x speedup in the prefilling stage\nand reduces the KV cache to 4.74% of its original size, resulting in a 6.4x\nincrease in throughput and a 1.4x speedup in inference with only a 1.41%\nperformance loss. Remarkably, in needle-in-a-haystack tasks, UNComp outperforms\nthe full-size KV cache even when compressed to 9.38% of its original size. Our\napproach offers an efficient, training-free Grouped-Query Attention paradigm\nthat can be seamlessly integrated into existing KV cache schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) is challenging due to their high\nmemory and computational demands, especially during long-context inference.\nWhile key-value (KV) caching accelerates inference by reusing previously\ncomputed keys and values, it also introduces significant memory overhead.\nExisting KV cache compression methods such as eviction and merging typically\ncompress the KV cache after it is generated and overlook the eviction of hidden\nstates, failing to improve the speed of the prefilling stage. Additionally,\napplying a uniform compression rate across different attention heads can harm\ncrucial retrieval heads in needle-in-a-haystack tasks due to excessive\ncompression. In this paper, we propose UNComp, an uncertainty-aware compression\nscheme that leverages matrix entropy to estimate model uncertainty across\nlayers and heads at the token sequence level. By grouping layers and heads\nbased on their uncertainty, UNComp adaptively compresses both the hidden states\nand the KV cache. Our method achieves a 1.6x speedup in the prefilling stage\nand reduces the KV cache to 4.74% of its original size, resulting in a 6.4x\nincrease in throughput and a 1.4x speedup in inference with only a 1.41%\nperformance loss. Remarkably, in needle-in-a-haystack tasks, UNComp outperforms\nthe full-size KV cache even when compressed to 9.38% of its original size. Our\napproach offers an efficient, training-free Grouped-Query Attention paradigm\nthat can be seamlessly integrated into existing KV cache schemes."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03065v1",
                "updated": "2024-10-04T01:11:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    1,
                    11,
                    9,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T01:11:09Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    1,
                    11,
                    9,
                    4,
                    278,
                    0
                ],
                "title": "Compute Or Load KV Cache? Why Not Both?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Or Load KV Cache? Why Not Both?"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nincreased context window sizes, enabling sophisticated applications but also\nintroducing substantial computational overheads, particularly computing\nkey-value (KV) cache in the prefill stage. Prefix caching has emerged to save\nGPU power in this scenario, which saves KV cache at disks and reuse them across\nmultiple queries. However, traditional prefix caching mechanisms often suffer\nfrom substantial latency because the speed of loading KV cache from disks to\nGPU memory is bottlenecked by the throughput of I/O devices. To optimize the\nlatency of long-context prefill, we propose Cake, a novel KV cache loader,\nwhich employs a bidirectional parallelized KV cache generation strategy. Upon\nreceiving a prefill task, Cake simultaneously and dynamically loads saved KV\ncache from prefix cache locations and computes KV cache on local GPUs,\nmaximizing the utilization of available computation and I/O bandwidth\nresources. Additionally, Cake automatically adapts to diverse system statuses\nwithout manual parameter. tuning. In experiments on various prompt datasets,\nGPUs, and I/O devices, Cake offers up to 68.1% Time To First Token (TTFT)\nreduction compare with compute-only method and 94.6% TTFT reduction compare\nwith I/O-only method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nincreased context window sizes, enabling sophisticated applications but also\nintroducing substantial computational overheads, particularly computing\nkey-value (KV) cache in the prefill stage. Prefix caching has emerged to save\nGPU power in this scenario, which saves KV cache at disks and reuse them across\nmultiple queries. However, traditional prefix caching mechanisms often suffer\nfrom substantial latency because the speed of loading KV cache from disks to\nGPU memory is bottlenecked by the throughput of I/O devices. To optimize the\nlatency of long-context prefill, we propose Cake, a novel KV cache loader,\nwhich employs a bidirectional parallelized KV cache generation strategy. Upon\nreceiving a prefill task, Cake simultaneously and dynamically loads saved KV\ncache from prefix cache locations and computes KV cache on local GPUs,\nmaximizing the utilization of available computation and I/O bandwidth\nresources. Additionally, Cake automatically adapts to diverse system statuses\nwithout manual parameter. tuning. In experiments on various prompt datasets,\nGPUs, and I/O devices, Cake offers up to 68.1% Time To First Token (TTFT)\nreduction compare with compute-only method and 94.6% TTFT reduction compare\nwith I/O-only method."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00242v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00242v3",
                "updated": "2024-10-03T22:17:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    22,
                    17,
                    1,
                    3,
                    277,
                    0
                ],
                "published": "2024-03-30T04:34:54Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    4,
                    34,
                    54,
                    5,
                    90,
                    0
                ],
                "title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference"
                },
                "summary": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99 KV cache IO and\nnearly 100 IO for partial results during attention calculation, DeFT achieves\nup to 2.52/3.82x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99 KV cache IO and\nnearly 100 IO for partial results during attention calculation, DeFT achieves\nup to 2.52/3.82x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms."
                },
                "authors": [
                    {
                        "name": "Jinwei Yao"
                    },
                    {
                        "name": "Kaiqi Chen"
                    },
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "Jiaxuan You"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Zeke Wang"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "arxiv_comment": "Update DeFT-v3 with more ablation studies. DeFT-v1 was accepted by\n  ICLR'24 AGI Workshop ( https://openreview.net/forum?id=HqfLHoX8bR ). Code\n  will be released soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00242v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00242v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15651v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15651v2",
                "updated": "2024-10-03T22:11:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    22,
                    11,
                    19,
                    3,
                    277,
                    0
                ],
                "published": "2024-03-22T23:47:19Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    23,
                    47,
                    19,
                    4,
                    82,
                    0
                ],
                "title": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering"
                },
                "summary": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room."
                },
                "authors": [
                    {
                        "name": "Jiaye Wu"
                    },
                    {
                        "name": "Saeed Hadadan"
                    },
                    {
                        "name": "Geng Lin"
                    },
                    {
                        "name": "Matthias Zwicker"
                    },
                    {
                        "name": "David Jacobs"
                    },
                    {
                        "name": "Roni Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Roni Sengupta"
                },
                "author": "Roni Sengupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15651v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15651v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02751v1",
                "updated": "2024-10-03T17:58:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    58,
                    11,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:58:11Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    58,
                    11,
                    3,
                    277,
                    0
                ],
                "title": "ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for\n  Embodied AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for\n  Embodied AI"
                },
                "summary": "Intelligent embodied agents need to quickly adapt to new scenarios by\nintegrating long histories of experience into decision-making. For instance, a\nrobot in an unfamiliar house initially wouldn't know the locations of objects\nneeded for tasks and might perform inefficiently. However, as it gathers more\nexperience, it should learn the layout of its environment and remember where\nobjects are, allowing it to complete new tasks more efficiently. To enable such\nrapid adaptation to new tasks, we present ReLIC, a new approach for in-context\nreinforcement learning (RL) for embodied agents. With ReLIC, agents are capable\nof adapting to new environments using 64,000 steps of in-context experience\nwith full attention while being trained through self-generated experience via\nRL. We achieve this by proposing a novel policy update scheme for on-policy RL\ncalled \"partial updates'' as well as a Sink-KV mechanism that enables effective\nutilization of a long observation history for embodied agents. Our method\noutperforms a variety of meta-RL baselines in adapting to unseen houses in an\nembodied multi-object navigation task. In addition, we find that ReLIC is\ncapable of few-shot imitation learning despite never being trained with expert\ndemonstrations. We also provide a comprehensive analysis of ReLIC, highlighting\nthat the combination of large-scale RL training, the proposed partial updates\nscheme, and the Sink-KV are essential for effective in-context learning. The\ncode for ReLIC and all our experiments is at https://github.com/aielawady/relic",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent embodied agents need to quickly adapt to new scenarios by\nintegrating long histories of experience into decision-making. For instance, a\nrobot in an unfamiliar house initially wouldn't know the locations of objects\nneeded for tasks and might perform inefficiently. However, as it gathers more\nexperience, it should learn the layout of its environment and remember where\nobjects are, allowing it to complete new tasks more efficiently. To enable such\nrapid adaptation to new tasks, we present ReLIC, a new approach for in-context\nreinforcement learning (RL) for embodied agents. With ReLIC, agents are capable\nof adapting to new environments using 64,000 steps of in-context experience\nwith full attention while being trained through self-generated experience via\nRL. We achieve this by proposing a novel policy update scheme for on-policy RL\ncalled \"partial updates'' as well as a Sink-KV mechanism that enables effective\nutilization of a long observation history for embodied agents. Our method\noutperforms a variety of meta-RL baselines in adapting to unseen houses in an\nembodied multi-object navigation task. In addition, we find that ReLIC is\ncapable of few-shot imitation learning despite never being trained with expert\ndemonstrations. We also provide a comprehensive analysis of ReLIC, highlighting\nthat the combination of large-scale RL training, the proposed partial updates\nscheme, and the Sink-KV are essential for effective in-context learning. The\ncode for ReLIC and all our experiments is at https://github.com/aielawady/relic"
                },
                "authors": [
                    {
                        "name": "Ahmad Elawady"
                    },
                    {
                        "name": "Gunjan Chhablani"
                    },
                    {
                        "name": "Ram Ramrakhya"
                    },
                    {
                        "name": "Karmesh Yadav"
                    },
                    {
                        "name": "Dhruv Batra"
                    },
                    {
                        "name": "Zsolt Kira"
                    },
                    {
                        "name": "Andrew Szot"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Szot"
                },
                "author": "Andrew Szot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00023v2",
                "updated": "2024-10-03T17:50:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    50,
                    33,
                    3,
                    277,
                    0
                ],
                "published": "2024-05-08T06:30:58Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    6,
                    30,
                    58,
                    2,
                    129,
                    0
                ],
                "title": "Preble: Efficient Distributed Prompt Scheduling for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preble: Efficient Distributed Prompt Scheduling for LLM Serving"
                },
                "summary": "Prompts to large language models (LLMs) have evolved beyond simple user\nquestions. For LLMs to solve complex problems, today's practices are to include\ndomain-specific instructions, illustration of tool usages, and/or long context\nsuch as textbook chapters in prompts. As such, many parts of prompts are\nrepetitive across requests. Recent works propose to cache and reuse KV state of\nprompts. However, they are all confined to a single-GPU optimization, while\nproduction LLM serving systems are distributed by nature.\n  This paper proposes Preble, the first distributed LLM serving platform that\ntargets and optimizes for prompt sharing. We designed a distributed scheduling\nsystem that co-optimizes KV state reuse and computation load-balancing with a\nnew scheduling algorithm and a hierarchical scheduling mechanism. Our\nevaluation of Preble with real workloads and request arrival patterns on two\nopen-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X\nto 14.5X on average latency and 2X to 10X on p99 latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompts to large language models (LLMs) have evolved beyond simple user\nquestions. For LLMs to solve complex problems, today's practices are to include\ndomain-specific instructions, illustration of tool usages, and/or long context\nsuch as textbook chapters in prompts. As such, many parts of prompts are\nrepetitive across requests. Recent works propose to cache and reuse KV state of\nprompts. However, they are all confined to a single-GPU optimization, while\nproduction LLM serving systems are distributed by nature.\n  This paper proposes Preble, the first distributed LLM serving platform that\ntargets and optimizes for prompt sharing. We designed a distributed scheduling\nsystem that co-optimizes KV state reuse and computation load-balancing with a\nnew scheduling algorithm and a hierarchical scheduling mechanism. Our\nevaluation of Preble with real workloads and request arrival patterns on two\nopen-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X\nto 14.5X on average latency and 2X to 10X on p99 latency."
                },
                "authors": [
                    {
                        "name": "Vikranth Srivatsa"
                    },
                    {
                        "name": "Zijian He"
                    },
                    {
                        "name": "Reyna Abhyankar"
                    },
                    {
                        "name": "Dongming Li"
                    },
                    {
                        "name": "Yiying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiying Zhang"
                },
                "author": "Yiying Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02599v1",
                "updated": "2024-10-03T15:41:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    41,
                    31,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T15:41:31Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    41,
                    31,
                    3,
                    277,
                    0
                ],
                "title": "Disaggregated Memory with SmartNIC Offloading: a Case Study on Graph\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Memory with SmartNIC Offloading: a Case Study on Graph\n  Processing"
                },
                "summary": "Disaggregated memory breaks the boundary of monolithic servers to enable\nmemory provisioning on demand. Using network-attached memory to provide memory\nexpansion for memory-intensive applications on compute nodes can improve the\noverall memory utilization on a cluster and reduce the total cost of ownership.\nHowever, current software solutions for leveraging network-attached memory must\nconsume resources on the compute node for memory management tasks. Emerging\noff-path smartNICs provide general-purpose programmability at low-cost\nlow-power cores. This work provides a general architecture design that enables\nnetwork-attached memory and offloading tasks onto off-path programmable\nSmartNIC. We provide a prototype implementation called SODA on Nvidia BlueField\nDPU. SODA adapts communication paths and data transfer alternatives, pipelines\ndata movement stages, and enables customizable data caching and prefetching\noptimizations. We evaluate SODA in five representative graph applications on\nreal-world graphs. Our results show that SODA can achieve up to 7.9x speedup\ncompared to node-local SSD and reduce network traffic by 42% compared to\ndisaggregated memory without SmartNIC offloading at similar or better\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory breaks the boundary of monolithic servers to enable\nmemory provisioning on demand. Using network-attached memory to provide memory\nexpansion for memory-intensive applications on compute nodes can improve the\noverall memory utilization on a cluster and reduce the total cost of ownership.\nHowever, current software solutions for leveraging network-attached memory must\nconsume resources on the compute node for memory management tasks. Emerging\noff-path smartNICs provide general-purpose programmability at low-cost\nlow-power cores. This work provides a general architecture design that enables\nnetwork-attached memory and offloading tasks onto off-path programmable\nSmartNIC. We provide a prototype implementation called SODA on Nvidia BlueField\nDPU. SODA adapts communication paths and data transfer alternatives, pipelines\ndata movement stages, and enables customizable data caching and prefetching\noptimizations. We evaluate SODA in five representative graph applications on\nreal-world graphs. Our results show that SODA can achieve up to 7.9x speedup\ncompared to node-local SSD and reduce network traffic by 42% compared to\ndisaggregated memory without SmartNIC offloading at similar or better\nperformance."
                },
                "authors": [
                    {
                        "name": "Jacob Wahlgren"
                    },
                    {
                        "name": "Gabin Schieffer"
                    },
                    {
                        "name": "Maya Gokhale"
                    },
                    {
                        "name": "Roger Pearce"
                    },
                    {
                        "name": "Ivy Peng"
                    }
                ],
                "author_detail": {
                    "name": "Ivy Peng"
                },
                "author": "Ivy Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02527v1",
                "updated": "2024-10-03T14:35:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    35,
                    35,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:35:35Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    35,
                    35,
                    3,
                    277,
                    0
                ],
                "title": "Learning from Offline Foundation Features with Tensor Augmentations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Offline Foundation Features with Tensor Augmentations"
                },
                "summary": "We introduce Learning from Offline Foundation Features with Tensor\nAugmentations (LOFF-TA), an efficient training scheme designed to harness the\ncapabilities of foundation models in limited resource settings where their\ndirect development is not feasible. LOFF-TA involves training a compact\nclassifier on cached feature embeddings from a frozen foundation model,\nresulting in up to $37\\times$ faster training and up to $26\\times$ reduced GPU\nmemory usage. Because the embeddings of augmented images would be too numerous\nto store, yet the augmentation process is essential for training, we propose to\napply tensor augmentations to the cached embeddings of the original\nnon-augmented images. LOFF-TA makes it possible to leverage the power of\nfoundation models, regardless of their size, in settings with limited\ncomputational capacity. Moreover, LOFF-TA can be used to apply foundation\nmodels to high-resolution images without increasing compute. In certain\nscenarios, we find that training with LOFF-TA yields better results than\ndirectly fine-tuning the foundation model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Learning from Offline Foundation Features with Tensor\nAugmentations (LOFF-TA), an efficient training scheme designed to harness the\ncapabilities of foundation models in limited resource settings where their\ndirect development is not feasible. LOFF-TA involves training a compact\nclassifier on cached feature embeddings from a frozen foundation model,\nresulting in up to $37\\times$ faster training and up to $26\\times$ reduced GPU\nmemory usage. Because the embeddings of augmented images would be too numerous\nto store, yet the augmentation process is essential for training, we propose to\napply tensor augmentations to the cached embeddings of the original\nnon-augmented images. LOFF-TA makes it possible to leverage the power of\nfoundation models, regardless of their size, in settings with limited\ncomputational capacity. Moreover, LOFF-TA can be used to apply foundation\nmodels to high-resolution images without increasing compute. In certain\nscenarios, we find that training with LOFF-TA yields better results than\ndirectly fine-tuning the foundation model."
                },
                "authors": [
                    {
                        "name": "Emir Konuk"
                    },
                    {
                        "name": "Christos Matsoukas"
                    },
                    {
                        "name": "Moein Sorkhei"
                    },
                    {
                        "name": "Phitchapha Lertsiravaramet"
                    },
                    {
                        "name": "Kevin Smith"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Smith"
                },
                "author": "Kevin Smith",
                "arxiv_comment": "Accepted to the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v2",
                "updated": "2024-10-03T11:47:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    11,
                    47,
                    21,
                    3,
                    277,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "Gaspard Beaufort"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02069v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02069v3",
                "updated": "2024-10-03T08:46:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    8,
                    46,
                    42,
                    3,
                    277,
                    0
                ],
                "published": "2024-06-04T07:51:30Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    7,
                    51,
                    30,
                    1,
                    156,
                    0
                ],
                "title": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling"
                },
                "summary": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100% Acc. performance, matching that of a full KV cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100% Acc. performance, matching that of a full KV cache."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Baobao Chang"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02069v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02069v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v2",
                "updated": "2024-10-03T03:03:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    3,
                    3,
                    29,
                    3,
                    277,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01805v1",
                "updated": "2024-10-02T17:59:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T17:59:52Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads"
                },
                "summary": "Large language models (LLMs) have shown remarkable advances in supporting\nlong-context comprehension and processing tasks. However, scaling the\ngeneration inference of LLMs to such long contexts incurs significant\nadditional computation load, and demands a substantial GPU memory footprint to\nmaintain the key-value (KV) cache of transformer-based LLMs. Existing KV cache\ncompression methods, such as quantization, face memory bottlenecks as context\nlength increases, while static-sized caches, such as eviction, suffer from\ninefficient policies. These limitations restrict deployment on consumer-grade\ndevices like a single Nvidia 4090 GPU. To overcome this, we propose Locret, a\nframework for long-context LLM inference that introduces retaining heads to\nevaluate the causal importance of KV cache units, allowing for more accurate\neviction within a fixed cache size. Locret is fine-tuned on top of the frozen\nbackbone LLM using a minimal amount of data from standard long-context SFT\ndatasets. During inference, we evict low-importance cache units along with a\nchunked prefill pattern, significantly reducing peak GPU memory usage. We\nconduct an extensive empirical study to evaluate Locret, where the experimental\nresults show that Locret outperforms the recent competitive approaches,\nincluding InfLLM, Quantization, SirLLM, and MInference, in terms of memory\nefficiency and the quality of generated contents -- Locret achieves over a 20x\nand 8x KV cache compression ratio compared to the full KV cache for\nPhi-3-mini-128K and Llama-3.1-8B-instruct. Additionally, Locret can be combined\nwith other methods, such as quantization and token merging. To our knowledge,\nLocret is the first framework capable of deploying Llama-3.1-8B or similar\nmodels on a single Nvidia 4090 GPU, enabling 128K long-context inference\nwithout compromising generation quality, and requiring little additional system\noptimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable advances in supporting\nlong-context comprehension and processing tasks. However, scaling the\ngeneration inference of LLMs to such long contexts incurs significant\nadditional computation load, and demands a substantial GPU memory footprint to\nmaintain the key-value (KV) cache of transformer-based LLMs. Existing KV cache\ncompression methods, such as quantization, face memory bottlenecks as context\nlength increases, while static-sized caches, such as eviction, suffer from\ninefficient policies. These limitations restrict deployment on consumer-grade\ndevices like a single Nvidia 4090 GPU. To overcome this, we propose Locret, a\nframework for long-context LLM inference that introduces retaining heads to\nevaluate the causal importance of KV cache units, allowing for more accurate\neviction within a fixed cache size. Locret is fine-tuned on top of the frozen\nbackbone LLM using a minimal amount of data from standard long-context SFT\ndatasets. During inference, we evict low-importance cache units along with a\nchunked prefill pattern, significantly reducing peak GPU memory usage. We\nconduct an extensive empirical study to evaluate Locret, where the experimental\nresults show that Locret outperforms the recent competitive approaches,\nincluding InfLLM, Quantization, SirLLM, and MInference, in terms of memory\nefficiency and the quality of generated contents -- Locret achieves over a 20x\nand 8x KV cache compression ratio compared to the full KV cache for\nPhi-3-mini-128K and Llama-3.1-8B-instruct. Additionally, Locret can be combined\nwith other methods, such as quantization and token merging. To our knowledge,\nLocret is the first framework capable of deploying Llama-3.1-8B or similar\nmodels on a single Nvidia 4090 GPU, enabling 128K long-context inference\nwithout compromising generation quality, and requiring little additional system\noptimizations."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprints",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01760v1",
                "updated": "2024-10-02T17:14:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    14,
                    47,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T17:14:47Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    14,
                    47,
                    2,
                    276,
                    0
                ],
                "title": "Competitive Ratio of Online Caching with Predictions: Lower and Upper\n  Bounds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Competitive Ratio of Online Caching with Predictions: Lower and Upper\n  Bounds"
                },
                "summary": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant."
                },
                "authors": [
                    {
                        "name": "Daniel Skachkov"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    },
                    {
                        "name": "Yuri Dorn"
                    },
                    {
                        "name": "Alexander Demin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Demin"
                },
                "author": "Alexander Demin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v1",
                "updated": "2024-10-02T15:22:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill: a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from linear to square root\nrelative to the context length. Additionally, FutureFill requires a prefill\ncache sized only by the number of tokens generated, which is smaller than the\ncache requirements for standard convolutional and attention-based models. We\nvalidate our theoretical findings with experimental evidence demonstrating\ncorrectness and efficiency gains in a synthetic generation task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill: a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from linear to square root\nrelative to the context length. Additionally, FutureFill requires a prefill\ncache sized only by the number of tokens generated, which is smaller than the\ncache requirements for standard convolutional and attention-based models. We\nvalidate our theoretical findings with experimental evidence demonstrating\ncorrectness and efficiency gains in a synthetic generation task."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01518v1",
                "updated": "2024-10-02T13:09:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    13,
                    9,
                    41,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T13:09:41Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    13,
                    9,
                    41,
                    2,
                    276,
                    0
                ],
                "title": "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs"
                },
                "summary": "Handling long input contexts remains a significant challenge for Large\nLanguage Models (LLMs), particularly in resource-constrained environments such\nas mobile devices. Our work aims to address this limitation by introducing\nInfiniPot, a novel KV cache control framework designed to enable pre-trained\nLLMs to manage extensive sequences within fixed memory constraints efficiently,\nwithout requiring additional training. InfiniPot leverages Continual Context\nDistillation (CCD), an iterative process that compresses and retains essential\ninformation through novel importance metrics, effectively maintaining critical\ndata even without access to future context. Our comprehensive evaluations\nindicate that InfiniPot significantly outperforms models trained for long\ncontexts in various NLP tasks, establishing its efficacy and versatility. This\nwork represents a substantial advancement toward making LLMs applicable to a\nbroader range of real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handling long input contexts remains a significant challenge for Large\nLanguage Models (LLMs), particularly in resource-constrained environments such\nas mobile devices. Our work aims to address this limitation by introducing\nInfiniPot, a novel KV cache control framework designed to enable pre-trained\nLLMs to manage extensive sequences within fixed memory constraints efficiently,\nwithout requiring additional training. InfiniPot leverages Continual Context\nDistillation (CCD), an iterative process that compresses and retains essential\ninformation through novel importance metrics, effectively maintaining critical\ndata even without access to future context. Our comprehensive evaluations\nindicate that InfiniPot significantly outperforms models trained for long\ncontexts in various NLP tasks, establishing its efficacy and versatility. This\nwork represents a substantial advancement toward making LLMs applicable to a\nbroader range of real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Jungwook Choi"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "arxiv_comment": "EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01485v1",
                "updated": "2024-10-02T12:35:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T12:35:53Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts"
                },
                "summary": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12335v2",
                "updated": "2024-10-02T00:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    0,
                    19,
                    13,
                    2,
                    276,
                    0
                ],
                "published": "2024-06-18T07:01:11Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    7,
                    1,
                    11,
                    1,
                    170,
                    0
                ],
                "title": "Attention Score is not All You Need for Token Importance Indicator in KV\n  Cache Reduction: Value Also Matters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Score is not All You Need for Token Importance Indicator in KV\n  Cache Reduction: Value Also Matters"
                },
                "summary": "Scaling the context size of large language models (LLMs) enables them to\nperform various new tasks, e.g., book summarization. However, the memory cost\nof the Key and Value (KV) cache in attention significantly limits the practical\napplications of LLMs. Recent works have explored token pruning for KV cache\nreduction in LLMs, relying solely on attention scores as a token importance\nindicator. However, our investigation into value vector norms revealed a\nnotably non-uniform pattern questioning their reliance only on attention\nscores. Inspired by this, we propose a new method: Value-Aware Token Pruning\n(VATP) which uses both attention scores and the $ \\ell_{1} $ norm of value\nvectors to evaluate token importance. Extensive experiments on LLaMA2-7B-chat\nand Vicuna-v1.5-7B across 16 LongBench tasks demonstrate that VATP outperforms\nattention-score-only baselines in over 12 tasks, confirming the effectiveness\nof incorporating value vector norms into token importance evaluation of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling the context size of large language models (LLMs) enables them to\nperform various new tasks, e.g., book summarization. However, the memory cost\nof the Key and Value (KV) cache in attention significantly limits the practical\napplications of LLMs. Recent works have explored token pruning for KV cache\nreduction in LLMs, relying solely on attention scores as a token importance\nindicator. However, our investigation into value vector norms revealed a\nnotably non-uniform pattern questioning their reliance only on attention\nscores. Inspired by this, we propose a new method: Value-Aware Token Pruning\n(VATP) which uses both attention scores and the $ \\ell_{1} $ norm of value\nvectors to evaluate token importance. Extensive experiments on LLaMA2-7B-chat\nand Vicuna-v1.5-7B across 16 LongBench tasks demonstrate that VATP outperforms\nattention-score-only baselines in over 12 tasks, confirming the effectiveness\nof incorporating value vector norms into token importance evaluation of LLMs."
                },
                "authors": [
                    {
                        "name": "Zhiyu Guo"
                    },
                    {
                        "name": "Hidetaka Kamigaito"
                    },
                    {
                        "name": "Taro Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Taro Watanabe"
                },
                "author": "Taro Watanabe",
                "arxiv_comment": "Accepted at EMNLP 2024 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00644v1",
                "updated": "2024-10-01T12:55:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    12,
                    55,
                    47,
                    1,
                    275,
                    0
                ],
                "published": "2024-10-01T12:55:47Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    12,
                    55,
                    47,
                    1,
                    275,
                    0
                ],
                "title": "PARSIR: a Package for Effective Parallel Discrete Event Simulation on\n  Multi-processor Machines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PARSIR: a Package for Effective Parallel Discrete Event Simulation on\n  Multi-processor Machines"
                },
                "summary": "In this article we present PARSIR (PARallel SImulation Runner), a package\nthat enables the effective exploitation of shared-memory multi-processor\nmachines for running discrete event simulation models. PARSIR is a\ncompile/run-time environment for discrete event simulation models developed\nwith the {\\tt C} programming language. The architecture of PARSIR has been\ndesigned in order to keep low the amount of CPU-cycles required for running\nmodels. This is achieved via the combination of a set of techniques like: 1)\ncausally consistent batch-processing of simulation events at an individual\nsimulation object for caching effectiveness; 2) high likelihood of disjoint\naccess parallelism; 3) the favoring of memory accesses on local NUMA\n(Non-Uniform-Memory-Access) nodes in the architecture, while still enabling\nwell balanced workload distribution via work-stealing from remote nodes; 4) the\nuse of RMW (Read-Modify-Write) machine instructions for fast access to\nsimulation engine data required by the worker threads for managing the\nconcurrent simulation objects and distributing the workload. Furthermore, any\narchitectural solution embedded in the PARSIR engine is fully transparent to\nthe application level code implementing the simulation model. We also provide\nexperimental results showing the effectiveness of PARSIR when running the\nreference PHOLD benchmark on a NUMA shared-memory multi-processor machine\nequipped with 40 CPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article we present PARSIR (PARallel SImulation Runner), a package\nthat enables the effective exploitation of shared-memory multi-processor\nmachines for running discrete event simulation models. PARSIR is a\ncompile/run-time environment for discrete event simulation models developed\nwith the {\\tt C} programming language. The architecture of PARSIR has been\ndesigned in order to keep low the amount of CPU-cycles required for running\nmodels. This is achieved via the combination of a set of techniques like: 1)\ncausally consistent batch-processing of simulation events at an individual\nsimulation object for caching effectiveness; 2) high likelihood of disjoint\naccess parallelism; 3) the favoring of memory accesses on local NUMA\n(Non-Uniform-Memory-Access) nodes in the architecture, while still enabling\nwell balanced workload distribution via work-stealing from remote nodes; 4) the\nuse of RMW (Read-Modify-Write) machine instructions for fast access to\nsimulation engine data required by the worker threads for managing the\nconcurrent simulation objects and distributing the workload. Furthermore, any\narchitectural solution embedded in the PARSIR engine is fully transparent to\nthe application level code implementing the simulation model. We also provide\nexperimental results showing the effectiveness of PARSIR when running the\nreference PHOLD benchmark on a NUMA shared-memory multi-processor machine\nequipped with 40 CPUs."
                },
                "authors": [
                    {
                        "name": "Francesco Quaglia"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Quaglia"
                },
                "author": "Francesco Quaglia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00455v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00455v1",
                "updated": "2024-10-01T07:19:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    7,
                    19,
                    21,
                    1,
                    275,
                    0
                ],
                "published": "2024-10-01T07:19:21Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    7,
                    19,
                    21,
                    1,
                    275,
                    0
                ],
                "title": "Fine-Grained Vectorized Merge Sorting on RISC-V: From Register to Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Vectorized Merge Sorting on RISC-V: From Register to Cache"
                },
                "summary": "Merge sort as a divide-sort-merge paradigm has been widely applied in\ncomputer science fields. As modern reduced instruction set computing\narchitectures like the fifth generation (RISC-V) regard multiple registers as a\nvector register group for wide instruction parallelism, optimizing merge sort\nwith this vectorized property is becoming increasingly common. In this paper,\nwe overhaul the divide-sort-merge paradigm, from its register-level sort to the\ncache-aware merge, to develop a fine-grained RISC-V vectorized merge sort\n(RVMS). From the register-level view, the inline vectorized transpose\ninstruction is missed in RISC-V, so implementing it efficiently is non-trivial.\nBesides, the vectorized comparisons do not always work well in the merging\nnetworks. Both issues primarily stem from the expensive data shuffle\ninstruction. To bypass it, RVMS strides to take register data as the proxy of\ndata shuffle to accelerate the transpose operation, and meanwhile replaces\nvectorized comparisons with scalar cousin for more light real value swap. On\nthe other hand, as cache-aware merge makes larger data merge in the cache, most\nmerge schemes have two drawbacks: the in-cache merge usually has low cache\nutilization, while the out-of-cache merging network remains an ineffectively\nsymmetric structure. To this end, we propose the half-merge scheme to employ\nthe auxiliary space of in-place merge to halve the footprint of naive merge\nsort, and meanwhile copy one sequence to this space to avoid the former data\nexchange. Furthermore, an asymmetric merging network is developed to adapt to\ntwo different input sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Merge sort as a divide-sort-merge paradigm has been widely applied in\ncomputer science fields. As modern reduced instruction set computing\narchitectures like the fifth generation (RISC-V) regard multiple registers as a\nvector register group for wide instruction parallelism, optimizing merge sort\nwith this vectorized property is becoming increasingly common. In this paper,\nwe overhaul the divide-sort-merge paradigm, from its register-level sort to the\ncache-aware merge, to develop a fine-grained RISC-V vectorized merge sort\n(RVMS). From the register-level view, the inline vectorized transpose\ninstruction is missed in RISC-V, so implementing it efficiently is non-trivial.\nBesides, the vectorized comparisons do not always work well in the merging\nnetworks. Both issues primarily stem from the expensive data shuffle\ninstruction. To bypass it, RVMS strides to take register data as the proxy of\ndata shuffle to accelerate the transpose operation, and meanwhile replaces\nvectorized comparisons with scalar cousin for more light real value swap. On\nthe other hand, as cache-aware merge makes larger data merge in the cache, most\nmerge schemes have two drawbacks: the in-cache merge usually has low cache\nutilization, while the out-of-cache merging network remains an ineffectively\nsymmetric structure. To this end, we propose the half-merge scheme to employ\nthe auxiliary space of in-place merge to halve the footprint of naive merge\nsort, and meanwhile copy one sequence to this space to avoid the former data\nexchange. Furthermore, an asymmetric merging network is developed to adapt to\ntwo different input sizes."
                },
                "authors": [
                    {
                        "name": "Jin Zhang"
                    },
                    {
                        "name": "Jincheng Zhou"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Di Ma"
                    },
                    {
                        "name": "Chunye Gong"
                    }
                ],
                "author_detail": {
                    "name": "Chunye Gong"
                },
                "author": "Chunye Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00455v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v3",
                "updated": "2024-10-01T03:40:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    40,
                    8,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00359v1",
                "updated": "2024-10-01T03:14:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    14,
                    12,
                    1,
                    275,
                    0
                ],
                "published": "2024-10-01T03:14:12Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    14,
                    12,
                    1,
                    275,
                    0
                ],
                "title": "Self-controller: Controlling LLMs with Multi-round Step-by-step\n  Self-awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-controller: Controlling LLMs with Multi-round Step-by-step\n  Self-awareness"
                },
                "summary": "The applications of large language models (LLMs) have been widely spread\nacross all domains. However, the basic abilities such as the controllability of\nLLMs are still limited. To address this, we propose \"Self-controller\", a novel\nagentic framework bringing self-awareness into LLMs' reasoning logic. The core\nidea of this work is to maintain states based on the LLM's response, letting\nthe LLM become self-aware of current status and think step by step in a\nmulti-round chain-of-thought paradigm. Our experiment on the state of textual\nlength has shown the controllability and effectiveness of the Self-controller.\nWe further implement a binary search algorithm to accelerate the generation\nprocess based on the linearity and monotonicity of the textual length state.\nAnother advantage of the Self-controller comes with DeepSeek's Context Caching\ntechnology, which significantly saves computational token consumption when a\ncluster of conversations shares the same prefix of context. Theoretically, we\nprove that in this scenario the extra time complexity is $O(c \\log n)$. Results\nof the back-of-the-envelope estimation suggest that the token consumption of\nour method is no more than twice as much as that of the trivial single-round\ngeneration. Furthermore, our ablation study on word constraints demonstrates\nthe Self-controller's consistent controllability across all foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The applications of large language models (LLMs) have been widely spread\nacross all domains. However, the basic abilities such as the controllability of\nLLMs are still limited. To address this, we propose \"Self-controller\", a novel\nagentic framework bringing self-awareness into LLMs' reasoning logic. The core\nidea of this work is to maintain states based on the LLM's response, letting\nthe LLM become self-aware of current status and think step by step in a\nmulti-round chain-of-thought paradigm. Our experiment on the state of textual\nlength has shown the controllability and effectiveness of the Self-controller.\nWe further implement a binary search algorithm to accelerate the generation\nprocess based on the linearity and monotonicity of the textual length state.\nAnother advantage of the Self-controller comes with DeepSeek's Context Caching\ntechnology, which significantly saves computational token consumption when a\ncluster of conversations shares the same prefix of context. Theoretically, we\nprove that in this scenario the extra time complexity is $O(c \\log n)$. Results\nof the back-of-the-envelope estimation suggest that the token consumption of\nour method is no more than twice as much as that of the trivial single-round\ngeneration. Furthermore, our ablation study on word constraints demonstrates\nthe Self-controller's consistent controllability across all foundation models."
                },
                "authors": [
                    {
                        "name": "Xiao Peng"
                    },
                    {
                        "name": "Xufan Geng"
                    }
                ],
                "author_detail": {
                    "name": "Xufan Geng"
                },
                "author": "Xufan Geng",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v4",
                "updated": "2024-09-30T22:44:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    22,
                    44,
                    58,
                    0,
                    274,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2209.09166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.09166v2",
                "updated": "2024-09-30T18:23:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    18,
                    23,
                    7,
                    0,
                    274,
                    0
                ],
                "published": "2022-09-19T16:35:28Z",
                "published_parsed": [
                    2022,
                    9,
                    19,
                    16,
                    35,
                    28,
                    0,
                    262,
                    0
                ],
                "title": "Cache-Oblivious Representation of B-Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Oblivious Representation of B-Tree Structures"
                },
                "summary": "We propose a general data structure CORoBTS for storing B-tree-like search\ntrees dynamically in a cache-oblivious way combining the van Emde Boas memory\nlayout with packed memory array.\n  In the use of the vEB layout mostly search complexity was considered, so far.\nWe show the complexity of depth-first search of a subtree and contiguous memory\narea and provide better insight into the relationship between positions of\nvertices in tree and in memory. We describe how to build an arbitrary tree in\nvEB layout if we can simulate its depth-first search. Similarly, we examine\nbatch updates of packed memory array.\n  In CORoBTS, the stored search tree has to satisfy that all leaves are at the\nsame depth and vertices have arity between the chosen constants $a$ and $b$.\nThe data structure allows searching with an optimal I/O complexity\n$\\mathcal{O}(\\log_B{N})$ and is stored in linear space. It provides operations\nfor inserting and removing a subtree; both have an amortized I/O complexity\n$\\mathcal{O}(S\\cdot(\\log^2 N)/B + \\log_B N\\cdot\\log\\log S + 1)$ and amortized\ntime complexity $\\mathcal{O}(S\\cdot\\log^2 N)$, where $S$ is the size of the\nsubtree and $N$ the size of the whole stored tree. Rebuilding an existing\nsubtree saves the multiplicative $\\mathcal{O}(\\log^2 N)$ in both complexities\nif the number of vertices on individual tree levels is not changed; it is paid\nonly for the inserted/removed vertices otherwise.\n  Modifying cache-oblivious partially persistent array proposed by Davoodi et\nal. [ESA, pages 296-308. Springer, 2014] to use CORoBTS improves its space\ncomplexity from $\\mathcal{O}(U^{\\log_2 3} + V \\log U)$ to $\\mathcal{O}(U + V\n\\log U)$, where $U$ is the maximal size of the array and $V$ is the number of\nversions; the data locality and I/O complexity of both present and persistent\nreads are kept unchanged; I/O complexity of writes is worsened by a\npolylogarithmic factor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a general data structure CORoBTS for storing B-tree-like search\ntrees dynamically in a cache-oblivious way combining the van Emde Boas memory\nlayout with packed memory array.\n  In the use of the vEB layout mostly search complexity was considered, so far.\nWe show the complexity of depth-first search of a subtree and contiguous memory\narea and provide better insight into the relationship between positions of\nvertices in tree and in memory. We describe how to build an arbitrary tree in\nvEB layout if we can simulate its depth-first search. Similarly, we examine\nbatch updates of packed memory array.\n  In CORoBTS, the stored search tree has to satisfy that all leaves are at the\nsame depth and vertices have arity between the chosen constants $a$ and $b$.\nThe data structure allows searching with an optimal I/O complexity\n$\\mathcal{O}(\\log_B{N})$ and is stored in linear space. It provides operations\nfor inserting and removing a subtree; both have an amortized I/O complexity\n$\\mathcal{O}(S\\cdot(\\log^2 N)/B + \\log_B N\\cdot\\log\\log S + 1)$ and amortized\ntime complexity $\\mathcal{O}(S\\cdot\\log^2 N)$, where $S$ is the size of the\nsubtree and $N$ the size of the whole stored tree. Rebuilding an existing\nsubtree saves the multiplicative $\\mathcal{O}(\\log^2 N)$ in both complexities\nif the number of vertices on individual tree levels is not changed; it is paid\nonly for the inserted/removed vertices otherwise.\n  Modifying cache-oblivious partially persistent array proposed by Davoodi et\nal. [ESA, pages 296-308. Springer, 2014] to use CORoBTS improves its space\ncomplexity from $\\mathcal{O}(U^{\\log_2 3} + V \\log U)$ to $\\mathcal{O}(U + V\n\\log U)$, where $U$ is the maximal size of the array and $V$ is the number of\nversions; the data locality and I/O complexity of both present and persistent\nreads are kept unchanged; I/O complexity of writes is worsened by a\npolylogarithmic factor."
                },
                "authors": [
                    {
                        "name": "Lukáš Ondráček"
                    },
                    {
                        "name": "Ondřej Mička"
                    }
                ],
                "author_detail": {
                    "name": "Ondřej Mička"
                },
                "author": "Ondřej Mička",
                "arxiv_comment": "30 pages + 7 pages of algorithms, 9 figures; changes: paper structure\n  improved, general (sub)tree (re)build added, DFS alg. simplified, build\n  complexity lowered,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2209.09166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.09166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "E.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20433v1",
                "updated": "2024-09-30T15:53:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T15:53:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Impact of Device Caching and Handovers on the Performance of 3D UAV\n  Networks with Blockages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of Device Caching and Handovers on the Performance of 3D UAV\n  Networks with Blockages"
                },
                "summary": "We investigate an urban network characterized by blockages, where unmanned\naerial vehicles (UAVs) offer ad-hoc coverage to mobile users with distinct\nservice rate requirements. The UAV-BSs are modeled using a two-dimensional\n(2-D) marked-poisson point process (MPPP), where the marks represent the\naltitude of each UAV-base station (UAV-BS). Initially, we model the network\nblockages and analyze the association probabilities of line-of-sight (LoS) and\nnon-line-of-sight (NLoS) UAV-BSs using stochastic geometry. Subsequently, we\nderive the bth moment of the conditional success probability (CSP) and employ a\nmeta distribution (MD)-based analytical framework of signal-to-interference\nnoise ratio (SINR) taking into account the blockage distribution in the\nnetwork. Furthermore, we proposea cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE). We evaluate the HO rate and average\nthroughput experienced by users ensuring their service rate requirements are\nmet. We demonstrate that LoS associations decrease as the network density\nincreases due to the substantial increase of NLoS UAV-BSs in the network.\nAdditionally, we show that the presence of blockages does not necessarily have\na negative impact on network reliability",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate an urban network characterized by blockages, where unmanned\naerial vehicles (UAVs) offer ad-hoc coverage to mobile users with distinct\nservice rate requirements. The UAV-BSs are modeled using a two-dimensional\n(2-D) marked-poisson point process (MPPP), where the marks represent the\naltitude of each UAV-base station (UAV-BS). Initially, we model the network\nblockages and analyze the association probabilities of line-of-sight (LoS) and\nnon-line-of-sight (NLoS) UAV-BSs using stochastic geometry. Subsequently, we\nderive the bth moment of the conditional success probability (CSP) and employ a\nmeta distribution (MD)-based analytical framework of signal-to-interference\nnoise ratio (SINR) taking into account the blockage distribution in the\nnetwork. Furthermore, we proposea cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE). We evaluate the HO rate and average\nthroughput experienced by users ensuring their service rate requirements are\nmet. We demonstrate that LoS associations decrease as the network density\nincreases due to the substantial increase of NLoS UAV-BSs in the network.\nAdditionally, we show that the presence of blockages does not necessarily have\na negative impact on network reliability"
                },
                "authors": [
                    {
                        "name": "Neetu R R"
                    },
                    {
                        "name": "Gourab Ghatak"
                    },
                    {
                        "name": "Vivek Ashok Bohara"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Ashok Bohara"
                },
                "author": "Vivek Ashok Bohara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08894v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08894v2",
                "updated": "2024-09-30T14:38:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    14,
                    38,
                    41,
                    0,
                    274,
                    0
                ],
                "published": "2023-10-13T06:58:07Z",
                "published_parsed": [
                    2023,
                    10,
                    13,
                    6,
                    58,
                    7,
                    4,
                    286,
                    0
                ],
                "title": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around"
                },
                "summary": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting."
                },
                "authors": [
                    {
                        "name": "Elizabath Peter"
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "A new construction of caching and delivery arrays is added which is\n  optimal (in Section IV.D). A new section (Section V) is also added which\n  contains performance comparison with existing schemes. 16 pages (double\n  column), 6 Figures and one table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08894v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08894v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20133v1",
                "updated": "2024-09-30T09:33:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    33,
                    37,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T09:33:37Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    33,
                    37,
                    0,
                    274,
                    0
                ],
                "title": "Improving Achievability of Cache-Aided Private Variable-Length Coding\n  with Zero Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Achievability of Cache-Aided Private Variable-Length Coding\n  with Zero Leakage"
                },
                "summary": "A statistical cache-aided compression problem with a privacy constraint is\nstudied, where a server has access to a database of $N$ files, $(Y_1,...,Y_N)$,\neach of size $F$ bits and is linked through a shared channel to $K$ users,\nwhere each has access to a local cache memory of size $MF$ bits. During the\nplacement phase, the server fills the users' caches without prior knowledge of\ntheir demands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file in database $Y_i$ is\narbitrarily correlated with a private attribute $X$, and an adversary is\nassumed to have access to the shared channel. The users and the server have\naccess to a shared key $W$. The goal is to design the cache contents and the\ndelivered message $\\cal C$ such that the average length of $\\mathcal{C}$ is\nminimized, while satisfying: i. The response $\\cal C$ does not reveal any\ninformation about $X$, i.e., $I(X;\\mathcal{C})=0$; ii. User $i$ can decode its\ndemand, $Y_{d_i}$, by using the shared key $W$, $\\cal C$, and its local cache\n$Z_i$. In a previous work, we have proposed a variable-length coding scheme\nthat combines privacy-aware compression with coded caching techniques. In this\npaper, we propose a new achievability scheme using minimum entropy coupling\nconcept and a greedy entropy-based algorithm. We show that the proposed scheme\nimproves the previous results. Moreover, considering two special cases we\nimprove the obtained bounds using the common information concept.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A statistical cache-aided compression problem with a privacy constraint is\nstudied, where a server has access to a database of $N$ files, $(Y_1,...,Y_N)$,\neach of size $F$ bits and is linked through a shared channel to $K$ users,\nwhere each has access to a local cache memory of size $MF$ bits. During the\nplacement phase, the server fills the users' caches without prior knowledge of\ntheir demands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file in database $Y_i$ is\narbitrarily correlated with a private attribute $X$, and an adversary is\nassumed to have access to the shared channel. The users and the server have\naccess to a shared key $W$. The goal is to design the cache contents and the\ndelivered message $\\cal C$ such that the average length of $\\mathcal{C}$ is\nminimized, while satisfying: i. The response $\\cal C$ does not reveal any\ninformation about $X$, i.e., $I(X;\\mathcal{C})=0$; ii. User $i$ can decode its\ndemand, $Y_{d_i}$, by using the shared key $W$, $\\cal C$, and its local cache\n$Z_i$. In a previous work, we have proposed a variable-length coding scheme\nthat combines privacy-aware compression with coded caching techniques. In this\npaper, we propose a new achievability scheme using minimum entropy coupling\nconcept and a greedy entropy-based algorithm. We show that the proposed scheme\nimproves the previous results. Moreover, considering two special cases we\nimprove the obtained bounds using the common information concept."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v1",
                "updated": "2024-09-30T06:55:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19720v1",
                "updated": "2024-09-29T14:31:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    29,
                    14,
                    31,
                    52,
                    6,
                    273,
                    0
                ],
                "published": "2024-09-29T14:31:52Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    14,
                    31,
                    52,
                    6,
                    273,
                    0
                ],
                "title": "FAST: A Dual-tier Few-Shot Learning Paradigm for Whole Slide Image\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAST: A Dual-tier Few-Shot Learning Paradigm for Whole Slide Image\n  Classification"
                },
                "summary": "The expensive fine-grained annotation and data scarcity have become the\nprimary obstacles for the widespread adoption of deep learning-based Whole\nSlide Images (WSI) classification algorithms in clinical practice. Unlike\nfew-shot learning methods in natural images that can leverage the labels of\neach image, existing few-shot WSI classification methods only utilize a small\nnumber of fine-grained labels or weakly supervised slide labels for training in\norder to avoid expensive fine-grained annotation. They lack sufficient mining\nof available WSIs, severely limiting WSI classification performance. To address\nthe above issues, we propose a novel and efficient dual-tier few-shot learning\nparadigm for WSI classification, named FAST. FAST consists of a dual-level\nannotation strategy and a dual-branch classification framework. Firstly, to\navoid expensive fine-grained annotation, we collect a very small number of WSIs\nat the slide level, and annotate an extremely small number of patches. Then, to\nfully mining the available WSIs, we use all the patches and available patch\nlabels to build a cache branch, which utilizes the labeled patches to learn the\nlabels of unlabeled patches and through knowledge retrieval for patch\nclassification. In addition to the cache branch, we also construct a prior\nbranch that includes learnable prompt vectors, using the text encoder of\nvisual-language models for patch classification. Finally, we integrate the\nresults from both branches to achieve WSI classification. Extensive experiments\non binary and multi-class datasets demonstrate that our proposed method\nsignificantly surpasses existing few-shot classification methods and approaches\nthe accuracy of fully supervised methods with only 0.22$\\%$ annotation costs.\nAll codes and models will be publicly available on\nhttps://github.com/fukexue/FAST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expensive fine-grained annotation and data scarcity have become the\nprimary obstacles for the widespread adoption of deep learning-based Whole\nSlide Images (WSI) classification algorithms in clinical practice. Unlike\nfew-shot learning methods in natural images that can leverage the labels of\neach image, existing few-shot WSI classification methods only utilize a small\nnumber of fine-grained labels or weakly supervised slide labels for training in\norder to avoid expensive fine-grained annotation. They lack sufficient mining\nof available WSIs, severely limiting WSI classification performance. To address\nthe above issues, we propose a novel and efficient dual-tier few-shot learning\nparadigm for WSI classification, named FAST. FAST consists of a dual-level\nannotation strategy and a dual-branch classification framework. Firstly, to\navoid expensive fine-grained annotation, we collect a very small number of WSIs\nat the slide level, and annotate an extremely small number of patches. Then, to\nfully mining the available WSIs, we use all the patches and available patch\nlabels to build a cache branch, which utilizes the labeled patches to learn the\nlabels of unlabeled patches and through knowledge retrieval for patch\nclassification. In addition to the cache branch, we also construct a prior\nbranch that includes learnable prompt vectors, using the text encoder of\nvisual-language models for patch classification. Finally, we integrate the\nresults from both branches to achieve WSI classification. Extensive experiments\non binary and multi-class datasets demonstrate that our proposed method\nsignificantly surpasses existing few-shot classification methods and approaches\nthe accuracy of fully supervised methods with only 0.22$\\%$ annotation costs.\nAll codes and models will be publicly available on\nhttps://github.com/fukexue/FAST."
                },
                "authors": [
                    {
                        "name": "Kexue Fu"
                    },
                    {
                        "name": "Xiaoyuan Luo"
                    },
                    {
                        "name": "Linhao Qu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Ilias Maglogiannis"
                    },
                    {
                        "name": "Longxiang Gao"
                    },
                    {
                        "name": "Manning Wang"
                    }
                ],
                "author_detail": {
                    "name": "Manning Wang"
                },
                "author": "Manning Wang",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19694v1",
                "updated": "2024-09-29T12:53:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    29,
                    12,
                    53,
                    29,
                    6,
                    273,
                    0
                ],
                "published": "2024-09-29T12:53:29Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    12,
                    53,
                    29,
                    6,
                    273,
                    0
                ],
                "title": "Development of a 3D-printed canine head phantom for veterinary\n  radiotherapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of a 3D-printed canine head phantom for veterinary\n  radiotherapy"
                },
                "summary": "Purpose: To develop the Ultimate Phantom Dog for Orthovoltage Glioma\nTreatment (UPDOG), an anatomically-correct phantom which mimics a dog's head,\nfor quality assurance (QA) of kilovoltage (kV) radiotherapy treatments.\n  Methods: A computed tomography (CT) scan of a canine glioma patient was\nsegmented into bone and soft tissue using 3DSlicer. The segments were converted\nto stereolithographic (STL) files and smoothed in Fusion360. A slit to\naccommodate a radiochromic film (RCF) was added at the location of the glioma\ntumor. UPDOG was 3D printed on a polyjet printer using VeroUltraWhite ($\\rho$ =\n1.19-1.20 g/cm\\textsuperscript{3}) for the bone and Agilus30 ($\\rho$ =\n1.14-1.15 g/cm\\textsuperscript{3}) for the soft tissue. CT scans of UPDOG were\nacquired on a clinical CT scanner. An LD-V1 RCF was inserted into UPDOG and\nirradiated with a kV x-ray source from two angles. The delivered dose to the\nRCF was compared to Monte Carlo (MC) simulations performed in TOPAS.\n  Results: The bone and soft tissue segments in UPDOG were mimicked the patient\nanatomy well with tube voltage-dependent CT numbers. The contrast in HU was of\n49, 47 and 50 HU for the 80, 100, and 120 kVp scans, respectively, sufficient\nfor anatomy visualization. The irradiations delivered a maximum dose to RCF of\n284 mGy which was compared to the results of MC simulations using a depth dose\ncurve and central-axis (CAX) beam profiles. The mean difference in CAX profiles\nand PDD between RCF and MC results was 15.9\\% and 2.3\\%, respectively.\n  Conclusions: We have demonstrated that UPDOG is a useful QA tool for kV\ncanine radiotherapy. UPDOG successfully anatomically mimicked the dog anatomy,\nwith a reduced but sufficient bone contrast. We showed that dose delivered to a\ncanine glioma with kV x-rays can be successfully measured with an RCF\npositioned at the tumor location.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: To develop the Ultimate Phantom Dog for Orthovoltage Glioma\nTreatment (UPDOG), an anatomically-correct phantom which mimics a dog's head,\nfor quality assurance (QA) of kilovoltage (kV) radiotherapy treatments.\n  Methods: A computed tomography (CT) scan of a canine glioma patient was\nsegmented into bone and soft tissue using 3DSlicer. The segments were converted\nto stereolithographic (STL) files and smoothed in Fusion360. A slit to\naccommodate a radiochromic film (RCF) was added at the location of the glioma\ntumor. UPDOG was 3D printed on a polyjet printer using VeroUltraWhite ($\\rho$ =\n1.19-1.20 g/cm\\textsuperscript{3}) for the bone and Agilus30 ($\\rho$ =\n1.14-1.15 g/cm\\textsuperscript{3}) for the soft tissue. CT scans of UPDOG were\nacquired on a clinical CT scanner. An LD-V1 RCF was inserted into UPDOG and\nirradiated with a kV x-ray source from two angles. The delivered dose to the\nRCF was compared to Monte Carlo (MC) simulations performed in TOPAS.\n  Results: The bone and soft tissue segments in UPDOG were mimicked the patient\nanatomy well with tube voltage-dependent CT numbers. The contrast in HU was of\n49, 47 and 50 HU for the 80, 100, and 120 kVp scans, respectively, sufficient\nfor anatomy visualization. The irradiations delivered a maximum dose to RCF of\n284 mGy which was compared to the results of MC simulations using a depth dose\ncurve and central-axis (CAX) beam profiles. The mean difference in CAX profiles\nand PDD between RCF and MC results was 15.9\\% and 2.3\\%, respectively.\n  Conclusions: We have demonstrated that UPDOG is a useful QA tool for kV\ncanine radiotherapy. UPDOG successfully anatomically mimicked the dog anatomy,\nwith a reduced but sufficient bone contrast. We showed that dose delivered to a\ncanine glioma with kV x-rays can be successfully measured with an RCF\npositioned at the tumor location."
                },
                "authors": [
                    {
                        "name": "Sandhya Rottoo"
                    },
                    {
                        "name": "Luke Frangella"
                    },
                    {
                        "name": "Magdalena Bazalova-Carter"
                    },
                    {
                        "name": "Olivia Masella"
                    }
                ],
                "author_detail": {
                    "name": "Olivia Masella"
                },
                "author": "Olivia Masella",
                "arxiv_comment": "9 pages, 6 figures. Submitted to Biomedical Physics & Engineering\n  Express",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19478v1",
                "updated": "2024-09-28T23:01:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    23,
                    1,
                    48,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T23:01:48Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    23,
                    1,
                    48,
                    5,
                    272,
                    0
                ],
                "title": "RTL2M$μ$PATH: Multi-$μ$PATH Synthesis with Applications to Hardware\n  Security Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTL2M$μ$PATH: Multi-$μ$PATH Synthesis with Applications to Hardware\n  Security Verification"
                },
                "summary": "The Check tools automate formal memory consistency model and security\nverification of processors by analyzing abstract models of microarchitectures,\ncalled $\\mu$SPEC models. Despite the efficacy of this approach, a verification\ngap between $\\mu$SPEC models, which must be manually written, and RTL limits\nthe Check tools' broad adoption. Our prior work, called RTL2$\\mu$SPEC, narrows\nthis gap by automatically synthesizing formally verified $\\mu$SPEC models from\nSystemVerilog implementations of simple processors. But, RTL2$\\mu$SPEC assumes\ninput designs where an instruction (e.g., a load) cannot exhibit more than one\nmicroarchitectural execution path ($\\mu$PATH, e.g., a cache hit or miss path)\n-- its single-execution-path assumption.\n  In this paper, we first propose an automated approach and tool, called\nRTL2M$\\mu$PATH, that resolves RTL2$\\mu$SPEC's single-execution-path assumption.\nGiven a SystemVerilog processor design, instruction encodings, and modest\ndesign metadata, RTL2M$\\mu$PATH finds a complete set of formally verified\n$\\mu$PATHs for each instruction. Next, we make an important observation: an\ninstruction that can exhibit more than one $\\mu$PATH strongly indicates the\npresence of a microarchitectural side channel in the input design. Based on\nthis observation, we then propose an automated approach and tool, called\nSynthLC, that extends RTL2M$\\mu$PATH with a symbolic information flow analysis\nto support synthesizing a variety of formally verified leakage contracts from\nSystemVerilog processor designs. Leakage contracts are foundational to\nstate-of-the-art defenses against hardware side-channel attacks. SynthLC is the\nfirst automated methodology for formally verifying hardware adherence to them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Check tools automate formal memory consistency model and security\nverification of processors by analyzing abstract models of microarchitectures,\ncalled $\\mu$SPEC models. Despite the efficacy of this approach, a verification\ngap between $\\mu$SPEC models, which must be manually written, and RTL limits\nthe Check tools' broad adoption. Our prior work, called RTL2$\\mu$SPEC, narrows\nthis gap by automatically synthesizing formally verified $\\mu$SPEC models from\nSystemVerilog implementations of simple processors. But, RTL2$\\mu$SPEC assumes\ninput designs where an instruction (e.g., a load) cannot exhibit more than one\nmicroarchitectural execution path ($\\mu$PATH, e.g., a cache hit or miss path)\n-- its single-execution-path assumption.\n  In this paper, we first propose an automated approach and tool, called\nRTL2M$\\mu$PATH, that resolves RTL2$\\mu$SPEC's single-execution-path assumption.\nGiven a SystemVerilog processor design, instruction encodings, and modest\ndesign metadata, RTL2M$\\mu$PATH finds a complete set of formally verified\n$\\mu$PATHs for each instruction. Next, we make an important observation: an\ninstruction that can exhibit more than one $\\mu$PATH strongly indicates the\npresence of a microarchitectural side channel in the input design. Based on\nthis observation, we then propose an automated approach and tool, called\nSynthLC, that extends RTL2M$\\mu$PATH with a symbolic information flow analysis\nto support synthesizing a variety of formally verified leakage contracts from\nSystemVerilog processor designs. Leakage contracts are foundational to\nstate-of-the-art defenses against hardware side-channel attacks. SynthLC is the\nfirst automated methodology for formally verifying hardware adherence to them."
                },
                "authors": [
                    {
                        "name": "Yao Hsiao"
                    },
                    {
                        "name": "Nikos Nikoleris"
                    },
                    {
                        "name": "Artem Khyzha"
                    },
                    {
                        "name": "Dominic P. Mulligan"
                    },
                    {
                        "name": "Gustavo Petri"
                    },
                    {
                        "name": "Christopher W. Fletcher"
                    },
                    {
                        "name": "Caroline Trippel"
                    }
                ],
                "author_detail": {
                    "name": "Caroline Trippel"
                },
                "author": "Caroline Trippel",
                "arxiv_comment": "Authors' version; to appear in the Proceedings of the 57th Annual\n  IEEE/ACM International Symposium on Microarchitecture 57th (MICRO 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19375v1",
                "updated": "2024-09-28T15:03:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    28,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T15:03:28Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    28,
                    5,
                    272,
                    0
                ],
                "title": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models"
                },
                "summary": "Vision-language foundation models (e.g., CLIP) have shown remarkable\nperformance across a wide range of tasks. However, deploying these models may\nbe unreliable when significant distribution gaps exist between the training and\ntest data. The training-free test-time dynamic adapter (TDA) is a promising\napproach to address this issue by storing representative test samples to guide\nthe classification of subsequent ones. However, TDA only naively maintains a\nlimited number of reference samples in the cache, leading to severe test-time\ncatastrophic forgetting when the cache is updated by dropping samples. In this\npaper, we propose a simple yet effective method for DistributiOnal Test-time\nAdaptation (Dota). Instead of naively memorizing representative test samples,\nDota continually estimates the distributions of test samples, allowing the\nmodel to continually adapt to the deployment environment. The test-time\nposterior probabilities are then computed using the estimated distributions\nbased on Bayes' theorem for adaptation purposes. To further enhance the\nadaptability on the uncertain samples, we introduce a new human-in-the-loop\nparadigm which identifies uncertain samples, collects human-feedback, and\nincorporates it into the Dota framework. Extensive experiments validate that\nDota enables CLIP to continually learn, resulting in a significant improvement\ncompared to current state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language foundation models (e.g., CLIP) have shown remarkable\nperformance across a wide range of tasks. However, deploying these models may\nbe unreliable when significant distribution gaps exist between the training and\ntest data. The training-free test-time dynamic adapter (TDA) is a promising\napproach to address this issue by storing representative test samples to guide\nthe classification of subsequent ones. However, TDA only naively maintains a\nlimited number of reference samples in the cache, leading to severe test-time\ncatastrophic forgetting when the cache is updated by dropping samples. In this\npaper, we propose a simple yet effective method for DistributiOnal Test-time\nAdaptation (Dota). Instead of naively memorizing representative test samples,\nDota continually estimates the distributions of test samples, allowing the\nmodel to continually adapt to the deployment environment. The test-time\nposterior probabilities are then computed using the estimated distributions\nbased on Bayes' theorem for adaptation purposes. To further enhance the\nadaptability on the uncertain samples, we introduce a new human-in-the-loop\nparadigm which identifies uncertain samples, collects human-feedback, and\nincorporates it into the Dota framework. Extensive experiments validate that\nDota enables CLIP to continually learn, resulting in a significant improvement\ncompared to current state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Zongbo Han"
                    },
                    {
                        "name": "Jialong Yang"
                    },
                    {
                        "name": "Junfan Li"
                    },
                    {
                        "name": "Qinghua Hu"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Changqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zhang"
                },
                "author": "Changqing Zhang",
                "arxiv_comment": "In submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19315v1",
                "updated": "2024-09-28T11:00:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T11:00:11Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "title": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models"
                },
                "summary": "Transformer neural networks, driven by self-attention mechanisms, are core\ncomponents of foundational and Large Language Models. In generative\ntransformers, self-attention uses cache memory to store token projections,\navoiding recomputation at each time step. However, GPU-stored projections must\nbe loaded into SRAM for each new generation step, causing latency and energy\nbottlenecks for long sequences. In this work, we propose a fast and\nenergy-efficient hardware implementation of self-attention using analog\nin-memory computing based on gain cell memories. Volatile gain cell memories\ncan be efficiently written to store new tokens during sequence generation,\nwhile performing analog signed weight multiplications to compute the\ndot-products required for self-attention. We implement Sliding Window\nAttention, which keeps memory of a finite set of past steps. A charge-to-pulse\nconverter for array readout eliminates the need for analog-to-digital\nconversion between self-attention stages. Using a co-designed initialization\nalgorithm to adapt pre-trained weights to gain cell non-idealities, we achieve\nNLP performance comparable to ChatGPT-2 with minimal training iterations,\ndespite hardware constraints. Our end-to-end hardware design includes digital\ncontrols, estimating area, latency, and energy. The system reduces attention\nlatency by up to two orders of magnitude and energy consumption by up to five\norders compared to GPUs, marking a significant step toward ultra-fast,\nlow-power sequence generation in Large Language Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer neural networks, driven by self-attention mechanisms, are core\ncomponents of foundational and Large Language Models. In generative\ntransformers, self-attention uses cache memory to store token projections,\navoiding recomputation at each time step. However, GPU-stored projections must\nbe loaded into SRAM for each new generation step, causing latency and energy\nbottlenecks for long sequences. In this work, we propose a fast and\nenergy-efficient hardware implementation of self-attention using analog\nin-memory computing based on gain cell memories. Volatile gain cell memories\ncan be efficiently written to store new tokens during sequence generation,\nwhile performing analog signed weight multiplications to compute the\ndot-products required for self-attention. We implement Sliding Window\nAttention, which keeps memory of a finite set of past steps. A charge-to-pulse\nconverter for array readout eliminates the need for analog-to-digital\nconversion between self-attention stages. Using a co-designed initialization\nalgorithm to adapt pre-trained weights to gain cell non-idealities, we achieve\nNLP performance comparable to ChatGPT-2 with minimal training iterations,\ndespite hardware constraints. Our end-to-end hardware design includes digital\ncontrols, estimating area, latency, and energy. The system reduces attention\nlatency by up to two orders of magnitude and energy consumption by up to five\norders compared to GPUs, marking a significant step toward ultra-fast,\nlow-power sequence generation in Large Language Models."
                },
                "authors": [
                    {
                        "name": "Nathan Leroux"
                    },
                    {
                        "name": "Paul-Philipp Manea"
                    },
                    {
                        "name": "Chirag Sudarshan"
                    },
                    {
                        "name": "Jan Finkbeiner"
                    },
                    {
                        "name": "Sebastian Siegel"
                    },
                    {
                        "name": "John Paul Strachan"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "arxiv_comment": "25 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18523v1",
                "updated": "2024-09-27T08:05:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    5,
                    34,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T08:05:34Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    5,
                    34,
                    4,
                    271,
                    0
                ],
                "title": "Token Caching for Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Caching for Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion transformers have gained substantial interest in diffusion\ngenerative modeling due to their outstanding performance. However, their high\ncomputational cost, arising from the quadratic computational complexity of\nattention mechanisms and multi-step inference, presents a significant\nbottleneck. To address this challenge, we propose TokenCache, a novel\npost-training acceleration method that leverages the token-based multi-block\narchitecture of transformers to reduce redundant computations among tokens\nacross inference steps. TokenCache specifically addresses three critical\nquestions in the context of diffusion transformers: (1) which tokens should be\npruned to eliminate redundancy, (2) which blocks should be targeted for\nefficient pruning, and (3) at which time steps caching should be applied to\nbalance speed and quality. In response to these challenges, TokenCache\nintroduces a Cache Predictor that assigns importance scores to tokens, enabling\nselective pruning without compromising model performance. Furthermore, we\npropose an adaptive block selection strategy to focus on blocks with minimal\nimpact on the network's output, along with a Two-Phase Round-Robin (TPRR)\nscheduling policy to optimize caching intervals throughout the denoising\nprocess. Experimental results across various models demonstrate that TokenCache\nachieves an effective trade-off between generation quality and inference speed\nfor diffusion transformers. Our code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have gained substantial interest in diffusion\ngenerative modeling due to their outstanding performance. However, their high\ncomputational cost, arising from the quadratic computational complexity of\nattention mechanisms and multi-step inference, presents a significant\nbottleneck. To address this challenge, we propose TokenCache, a novel\npost-training acceleration method that leverages the token-based multi-block\narchitecture of transformers to reduce redundant computations among tokens\nacross inference steps. TokenCache specifically addresses three critical\nquestions in the context of diffusion transformers: (1) which tokens should be\npruned to eliminate redundancy, (2) which blocks should be targeted for\nefficient pruning, and (3) at which time steps caching should be applied to\nbalance speed and quality. In response to these challenges, TokenCache\nintroduces a Cache Predictor that assigns importance scores to tokens, enabling\nselective pruning without compromising model performance. Furthermore, we\npropose an adaptive block selection strategy to focus on blocks with minimal\nimpact on the network's output, along with a Two-Phase Round-Robin (TPRR)\nscheduling policy to optimize caching intervals throughout the denoising\nprocess. Experimental results across various models demonstrate that TokenCache\nachieves an effective trade-off between generation quality and inference speed\nfor diffusion transformers. Our code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Jinming Lou"
                    },
                    {
                        "name": "Wenyang Luo"
                    },
                    {
                        "name": "Yufan Liu"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Xinmiao Ding"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Jiajiong Cao"
                    },
                    {
                        "name": "Yuming Li"
                    },
                    {
                        "name": "Chenguang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chenguang Ma"
                },
                "author": "Chenguang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14360v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14360v3",
                "updated": "2024-10-12T02:11:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    2,
                    11,
                    14,
                    5,
                    286,
                    0
                ],
                "published": "2024-09-22T08:30:43Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    8,
                    30,
                    43,
                    6,
                    266,
                    0
                ],
                "title": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs"
                },
                "summary": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies."
                },
                "authors": [
                    {
                        "name": "Xufeng Yang"
                    },
                    {
                        "name": "Zhengjian Cong"
                    },
                    {
                        "name": "Congming Gao"
                    }
                ],
                "author_detail": {
                    "name": "Congming Gao"
                },
                "author": "Congming Gao",
                "arxiv_comment": "This paper has been submitted to NAS'24 (The 17th International\n  Conference on Networking, Architecture and Storage)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14360v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14360v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17606v1",
                "updated": "2024-09-26T07:44:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T07:44:47Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "title": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support"
                },
                "summary": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan."
                },
                "authors": [
                    {
                        "name": "Tim Fischer"
                    },
                    {
                        "name": "Michael Rogenmoser"
                    },
                    {
                        "name": "Thomas Benz"
                    },
                    {
                        "name": "Frank K. Gürkaynak"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17374v1",
                "updated": "2024-09-25T21:37:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    21,
                    37,
                    1,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T21:37:01Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    21,
                    37,
                    1,
                    2,
                    269,
                    0
                ],
                "title": "NiOx/\\b{eta}-Ga2O3 Heterojunction Diode Achieving Breakdown Voltage >3\n  kV with Plasma Etch Field-Termination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NiOx/\\b{eta}-Ga2O3 Heterojunction Diode Achieving Breakdown Voltage >3\n  kV with Plasma Etch Field-Termination"
                },
                "summary": "This work reports the fabrication and characterization of a\nNiOx/\\b{eta}-Ga2O3 heterojunction diode (HJD) that uses a metallic nickel (Ni)\ntarget to deposit NiOx layers via reactive RF magnetron sputtering and lift-off\nprocessing with >3 kV breakdown voltage, record-low reverse current leakage\nunder high reverse bias, and high junction electric fields (>3.34 MV/cm). The\nheterojunction diodes are fabricated via bilayer NiOx sputtering followed by\nself-aligned mesa-etching for field-termination on both large (1-mm2) and small\narea (100-{\\mu}m diameter) devices. The HJD exhibits a ~135 A/cm2 forward\ncurrent density at 5 V with a rectifying ratio of ~1010. The minimum\ndifferential specific on-resistance is measured to be 17.26 m{\\Omega} cm2. The\nbreakdown voltage on 100-{\\mu}m diameter pads was measured to be greater than 3\nkV with a noise floor-level reverse leakage current density (10-8~10-6 A/cm2)\nuntil 3 kV, accomplishing a parallel-plane junction electric field to be at\nleast 3.34 MV/cm at 3 kV with a power figure of merit (PFOM) >0.52 GW/cm2.\nTemperature-dependent forward current density-voltage (J-V) measurements are\nperformed from room temperature (25 C) to 200 C which showed a temperature\ncoefficient of resistance ({\\alpha}) equaling 1.56, higher than that of\n\\b{eta}-Ga2O3 Schottky barrier diodes (SBDs), indicating potential conductivity\ndegradation within NiOx at elevated temperatures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work reports the fabrication and characterization of a\nNiOx/\\b{eta}-Ga2O3 heterojunction diode (HJD) that uses a metallic nickel (Ni)\ntarget to deposit NiOx layers via reactive RF magnetron sputtering and lift-off\nprocessing with >3 kV breakdown voltage, record-low reverse current leakage\nunder high reverse bias, and high junction electric fields (>3.34 MV/cm). The\nheterojunction diodes are fabricated via bilayer NiOx sputtering followed by\nself-aligned mesa-etching for field-termination on both large (1-mm2) and small\narea (100-{\\mu}m diameter) devices. The HJD exhibits a ~135 A/cm2 forward\ncurrent density at 5 V with a rectifying ratio of ~1010. The minimum\ndifferential specific on-resistance is measured to be 17.26 m{\\Omega} cm2. The\nbreakdown voltage on 100-{\\mu}m diameter pads was measured to be greater than 3\nkV with a noise floor-level reverse leakage current density (10-8~10-6 A/cm2)\nuntil 3 kV, accomplishing a parallel-plane junction electric field to be at\nleast 3.34 MV/cm at 3 kV with a power figure of merit (PFOM) >0.52 GW/cm2.\nTemperature-dependent forward current density-voltage (J-V) measurements are\nperformed from room temperature (25 C) to 200 C which showed a temperature\ncoefficient of resistance ({\\alpha}) equaling 1.56, higher than that of\n\\b{eta}-Ga2O3 Schottky barrier diodes (SBDs), indicating potential conductivity\ndegradation within NiOx at elevated temperatures."
                },
                "authors": [
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Saurav Roy"
                    },
                    {
                        "name": "Carl Peterson"
                    },
                    {
                        "name": "Arkka Bhattacharyya"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "arxiv_comment": "6 pages, 5 figures, APL Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v1",
                "updated": "2024-09-25T18:21:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Mnemosyne: Parallelization Strategies for Efficiently Serving\n  Multi-Million Context Length LLM Inference Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mnemosyne: Parallelization Strategies for Efficiently Serving\n  Multi-Million Context Length LLM Inference Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) evolve to handle increasingly longer\ncontexts, serving inference requests for context lengths in the range of\nmillions of tokens presents unique challenges. While existing techniques are\neffective for training, they fail to address the unique challenges of\ninference, such as varying prefill and decode phases and their associated\nlatency constraints - like Time to First Token (TTFT) and Time Between Tokens\n(TBT). Furthermore, there are no long context inference solutions that allow\nbatching requests to increase the hardware utilization today.\n  In this paper, we propose three key innovations for efficient interactive\nlong context LLM inference, without resorting to any approximation: adaptive\nchunking to reduce prefill overheads in mixed batching, Sequence Pipeline\nParallelism (SPP) to lower TTFT, and KV Cache Parallelism (KVP) to minimize\nTBT. These contributions are combined into a 3D parallelism strategy, enabling\nMnemosyne to scale interactive inference to context lengths at least up to 10\nmillion tokens with high throughput enabled with batching. To our knowledge,\nMnemosyne is the first to be able to achieve support for 10 million long\ncontext inference efficiently, while satisfying production-grade SLOs on TBT\n(30ms) on contexts up to and including 10 million.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve to handle increasingly longer\ncontexts, serving inference requests for context lengths in the range of\nmillions of tokens presents unique challenges. While existing techniques are\neffective for training, they fail to address the unique challenges of\ninference, such as varying prefill and decode phases and their associated\nlatency constraints - like Time to First Token (TTFT) and Time Between Tokens\n(TBT). Furthermore, there are no long context inference solutions that allow\nbatching requests to increase the hardware utilization today.\n  In this paper, we propose three key innovations for efficient interactive\nlong context LLM inference, without resorting to any approximation: adaptive\nchunking to reduce prefill overheads in mixed batching, Sequence Pipeline\nParallelism (SPP) to lower TTFT, and KV Cache Parallelism (KVP) to minimize\nTBT. These contributions are combined into a 3D parallelism strategy, enabling\nMnemosyne to scale interactive inference to context lengths at least up to 10\nmillion tokens with high throughput enabled with batching. To our knowledge,\nMnemosyne is the first to be able to achieve support for 10 million long\ncontext inference efficiently, while satisfying production-grade SLOs on TBT\n(30ms) on contexts up to and including 10 million."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17136v1",
                "updated": "2024-09-25T17:55:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    55,
                    7,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T17:55:07Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    55,
                    7,
                    2,
                    269,
                    0
                ],
                "title": "Adaptive Cost Model for Query Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cost Model for Query Optimization"
                },
                "summary": "The principal component of conventional database query optimizers is a cost\nmodel that is used to estimate expected performance of query plans. The\naccuracy of the cost model has direct impact on the optimality of execution\nplans selected by the optimizer and thus, on the resulting query latency.\nSeveral common parameters of cost models in modern DBMS are related to the\nperformance of CPU and I/O and are typically set by a database administrator\nupon system tuning. However these performance characteristics are not stable\nand therefore, a single point estimation may not suffice for all DB load\nregimes. In this paper, we propose an Adaptive Cost Model (ACM) which\ndynamically optimizes CPU- and I/O-related plan cost parameters at DB runtime.\nBy continuously monitoring query execution statistics and the state of DB\nbuffer cache ACM adjusts cost parameters without the need for manual\nintervention from a database administrator. This allows for responding to\nchanges in the workload and system performance ensuring more optimal query\nexecution plans. We describe the main ideas in the implementation of ACM and\nreport on a preliminary experimental evaluation showing 20\\% end-to-end latency\nimprovement on TPC-H benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The principal component of conventional database query optimizers is a cost\nmodel that is used to estimate expected performance of query plans. The\naccuracy of the cost model has direct impact on the optimality of execution\nplans selected by the optimizer and thus, on the resulting query latency.\nSeveral common parameters of cost models in modern DBMS are related to the\nperformance of CPU and I/O and are typically set by a database administrator\nupon system tuning. However these performance characteristics are not stable\nand therefore, a single point estimation may not suffice for all DB load\nregimes. In this paper, we propose an Adaptive Cost Model (ACM) which\ndynamically optimizes CPU- and I/O-related plan cost parameters at DB runtime.\nBy continuously monitoring query execution statistics and the state of DB\nbuffer cache ACM adjusts cost parameters without the need for manual\nintervention from a database administrator. This allows for responding to\nchanges in the workload and system performance ensuring more optimal query\nexecution plans. We describe the main ideas in the implementation of ACM and\nreport on a preliminary experimental evaluation showing 20\\% end-to-end latency\nimprovement on TPC-H benchmark."
                },
                "authors": [
                    {
                        "name": "Nikita Vasilenko"
                    },
                    {
                        "name": "Alexander Demin"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05, 68P15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16743v1",
                "updated": "2024-09-25T08:52:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    52,
                    7,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T08:52:07Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    52,
                    7,
                    2,
                    269,
                    0
                ],
                "title": "Event-Triggered Non-Linear Control of Offshore MMC Grids for\n  Asymmetrical AC Faults",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event-Triggered Non-Linear Control of Offshore MMC Grids for\n  Asymmetrical AC Faults"
                },
                "summary": "Fault ride-through capability studies of MMC-HVDC connected wind power plants\nhave focused primarily on the DC link and onshore AC grid faults. Offshore AC\nfaults, mainly asymmetrical faults have not gained much attention in the\nliterature despite being included in the future development at national levels\nin the ENTSO-E HVDC code. The proposed work gives an event-triggered control to\nstabilize the system once the offshore AC fault has occurred, identified, and\nisolated. Different types of control actions such as proportional-integral (PI)\ncontroller and super-twisted sliding mode control (STSMC) are used to smoothly\ntransition the post-fault system to a new steady state operating point by\nsuppressing the negative sequence control. Initially, the effect of a negative\nsequence current control scheme on the transient behavior of the power system\nwith a PI controller is discussed in this paper. Further, a non-linear control\nstrategy (STSMC) is proposed which gives quicker convergence of the system\npost-fault in comparison to PI control action. These post-fault control\noperations are only triggered in the presence of a fault in the system, i.e.,\nthey are event-triggered. The validity of the proposed strategy is demonstrated\nby simulation on a $\\pm$525 kV, three-terminal meshed MMC-HVDC system model in\nReal Time Digital Simulator (RTDS).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fault ride-through capability studies of MMC-HVDC connected wind power plants\nhave focused primarily on the DC link and onshore AC grid faults. Offshore AC\nfaults, mainly asymmetrical faults have not gained much attention in the\nliterature despite being included in the future development at national levels\nin the ENTSO-E HVDC code. The proposed work gives an event-triggered control to\nstabilize the system once the offshore AC fault has occurred, identified, and\nisolated. Different types of control actions such as proportional-integral (PI)\ncontroller and super-twisted sliding mode control (STSMC) are used to smoothly\ntransition the post-fault system to a new steady state operating point by\nsuppressing the negative sequence control. Initially, the effect of a negative\nsequence current control scheme on the transient behavior of the power system\nwith a PI controller is discussed in this paper. Further, a non-linear control\nstrategy (STSMC) is proposed which gives quicker convergence of the system\npost-fault in comparison to PI control action. These post-fault control\noperations are only triggered in the presence of a fault in the system, i.e.,\nthey are event-triggered. The validity of the proposed strategy is demonstrated\nby simulation on a $\\pm$525 kV, three-terminal meshed MMC-HVDC system model in\nReal Time Digital Simulator (RTDS)."
                },
                "authors": [
                    {
                        "name": "Naajein Cherat"
                    },
                    {
                        "name": "Vaibhav Nougain"
                    },
                    {
                        "name": "Milovan Majstorović"
                    },
                    {
                        "name": "Peter Palensky"
                    },
                    {
                        "name": "Aleksandra Lekić"
                    }
                ],
                "author_detail": {
                    "name": "Aleksandra Lekić"
                },
                "author": "Aleksandra Lekić",
                "arxiv_journal_ref": "ISGT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16546v1",
                "updated": "2024-09-25T01:39:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T01:39:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization"
                },
                "summary": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision."
                },
                "authors": [
                    {
                        "name": "Yifan Tan"
                    },
                    {
                        "name": "Haoze Wang"
                    },
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16258v1",
                "updated": "2024-09-24T17:28:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    28,
                    47,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T17:28:47Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    28,
                    47,
                    1,
                    268,
                    0
                ],
                "title": "SWARM: Replicating Shared Disaggregated-Memory Data in No Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWARM: Replicating Shared Disaggregated-Memory Data in No Time"
                },
                "summary": "Memory disaggregation is an emerging data center architecture that improves\nresource utilization and scalability. Replication is key to ensure the fault\ntolerance of applications, but replicating shared data in disaggregated memory\nis hard. We propose SWARM (Swift WAit-free Replication in disaggregated\nMemory), the first replication scheme for in-disaggregated-memory shared\nobjects to provide (1) single-roundtrip reads and writes in the common case,\n(2) strong consistency (linearizability), and (3) strong liveness\n(wait-freedom). SWARM makes two independent contributions. The first is\nSafe-Guess, a novel wait-free replication protocol with single-roundtrip\noperations. The second is In-n-Out, a novel technique to provide conditional\natomic update and atomic retrieval of large buffers in disaggregated memory in\none roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly\nconsistent and highly available disaggregated key-value store. We evaluate\nSWARM-KV and find that it has marginal latency overhead compared to an\nunreplicated key-value store, and that it offers much lower latency and better\navailability than FUSEE, a state-of-the-art replicated disaggregated key-value\nstore.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory disaggregation is an emerging data center architecture that improves\nresource utilization and scalability. Replication is key to ensure the fault\ntolerance of applications, but replicating shared data in disaggregated memory\nis hard. We propose SWARM (Swift WAit-free Replication in disaggregated\nMemory), the first replication scheme for in-disaggregated-memory shared\nobjects to provide (1) single-roundtrip reads and writes in the common case,\n(2) strong consistency (linearizability), and (3) strong liveness\n(wait-freedom). SWARM makes two independent contributions. The first is\nSafe-Guess, a novel wait-free replication protocol with single-roundtrip\noperations. The second is In-n-Out, a novel technique to provide conditional\natomic update and atomic retrieval of large buffers in disaggregated memory in\none roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly\nconsistent and highly available disaggregated key-value store. We evaluate\nSWARM-KV and find that it has marginal latency overhead compared to an\nunreplicated key-value store, and that it offers much lower latency and better\navailability than FUSEE, a state-of-the-art replicated disaggregated key-value\nstore."
                },
                "authors": [
                    {
                        "name": "Antoine Murat"
                    },
                    {
                        "name": "Clément Burgelin"
                    },
                    {
                        "name": "Athanasios Xygkis"
                    },
                    {
                        "name": "Igor Zablotchi"
                    },
                    {
                        "name": "Marcos K. Aguilera"
                    },
                    {
                        "name": "Rachid Guerraoui"
                    }
                ],
                "author_detail": {
                    "name": "Rachid Guerraoui"
                },
                "author": "Rachid Guerraoui",
                "arxiv_doi": "10.1145/3694715.3695945",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3694715.3695945",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.16258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To appear in the proceedings of SOSP '24",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16110v1",
                "updated": "2024-09-24T14:16:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    16,
                    26,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T14:16:26Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    16,
                    26,
                    1,
                    268,
                    0
                ],
                "title": "Wind lulls and slews; consequences for the stability of future UK\n  electricity systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wind lulls and slews; consequences for the stability of future UK\n  electricity systems"
                },
                "summary": "As the United Kingdom wind fleet increases in size, wind lulls and slews will\nincreasingly challenge the stability of its electricity system. The paper\ndescribes the use of models based on real time records and including solar\nslews, to investigate the most extreme wind variations likely to be encountered\nin future, enabling strategies to be devised to mitigate them. Wind lulls are\nsurprisingly frequent, occasionally lasting a week or more, and are always\nlikely to be beyond the capabilities of stored or imported electrical energy to\nmitigate them. The models indicate that there will be a continuing need for gas\npowered generation to mitigate wind lulls. Currently, Combined Cycle Gas\nTurbines (CCGTs) provide most of the dispatchable generation. However, CCGTs\nare not sufficiently fast acting to cope with the wind and solar slews\nanticipated in future. The paper suggests that a range of already proven\nfast-acting sources of dispatchable generation, including Open Cycle Gas\nTurbines (OCGTs), Internal Combustion Gas-Fired Reciprocating engines (ICGRs)\nand stored electrical energy systems, should be capable of coping with the\nlargest wind and solar slews likely to be encountered up to the year 2035.\nExamples are given of the recent introduction of these fast-acting sources of\ngeneration which, it is suggested, will progressively replace CCGTs as the wind\nand solar fleets increase in size. Moreover, we see the pattern of recent\ninvestments, summarised in the paper, as a good indication of likely future\ninvestments, with OCGT investments mainly serving the 440 kV grid, and ICGRs\nand stored electrical energy more local networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the United Kingdom wind fleet increases in size, wind lulls and slews will\nincreasingly challenge the stability of its electricity system. The paper\ndescribes the use of models based on real time records and including solar\nslews, to investigate the most extreme wind variations likely to be encountered\nin future, enabling strategies to be devised to mitigate them. Wind lulls are\nsurprisingly frequent, occasionally lasting a week or more, and are always\nlikely to be beyond the capabilities of stored or imported electrical energy to\nmitigate them. The models indicate that there will be a continuing need for gas\npowered generation to mitigate wind lulls. Currently, Combined Cycle Gas\nTurbines (CCGTs) provide most of the dispatchable generation. However, CCGTs\nare not sufficiently fast acting to cope with the wind and solar slews\nanticipated in future. The paper suggests that a range of already proven\nfast-acting sources of dispatchable generation, including Open Cycle Gas\nTurbines (OCGTs), Internal Combustion Gas-Fired Reciprocating engines (ICGRs)\nand stored electrical energy systems, should be capable of coping with the\nlargest wind and solar slews likely to be encountered up to the year 2035.\nExamples are given of the recent introduction of these fast-acting sources of\ngeneration which, it is suggested, will progressively replace CCGTs as the wind\nand solar fleets increase in size. Moreover, we see the pattern of recent\ninvestments, summarised in the paper, as a good indication of likely future\ninvestments, with OCGT investments mainly serving the 440 kV grid, and ICGRs\nand stored electrical energy more local networks."
                },
                "authors": [
                    {
                        "name": "Anthony D Stephens"
                    },
                    {
                        "name": "David R Walwyn"
                    }
                ],
                "author_detail": {
                    "name": "David R Walwyn"
                },
                "author": "David R Walwyn",
                "arxiv_comment": "13 pages, 8 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v3",
                "updated": "2024-09-24T11:37:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    37,
                    43,
                    1,
                    268,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15523v1",
                "updated": "2024-09-23T20:16:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    16,
                    49,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T20:16:49Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    16,
                    49,
                    0,
                    267,
                    0
                ],
                "title": "SEAL: Suite for Evaluating API-use of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEAL: Suite for Evaluating API-use of LLMs"
                },
                "summary": "Large language models (LLMs) have limitations in handling tasks that require\nreal-time access to external APIs. While several benchmarks like ToolBench and\nAPIGen have been developed to assess LLMs' API-use capabilities, they often\nsuffer from issues such as lack of generalizability, limited multi-step\nreasoning coverage, and instability due to real-time API fluctuations. In this\npaper, we introduce SEAL, an end-to-end testbed designed to evaluate LLMs in\nreal-world API usage. SEAL standardizes existing benchmarks, integrates an\nagent system for testing API retrieval and planning, and addresses the\ninstability of real-time APIs by introducing a GPT-4-powered API simulator with\ncaching for deterministic evaluations. Our testbed provides a comprehensive\nevaluation pipeline that covers API retrieval, API calls, and final responses,\noffering a reliable framework for structured performance comparison in diverse\nreal-world scenarios. SEAL is publicly available, with ongoing updates for new\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have limitations in handling tasks that require\nreal-time access to external APIs. While several benchmarks like ToolBench and\nAPIGen have been developed to assess LLMs' API-use capabilities, they often\nsuffer from issues such as lack of generalizability, limited multi-step\nreasoning coverage, and instability due to real-time API fluctuations. In this\npaper, we introduce SEAL, an end-to-end testbed designed to evaluate LLMs in\nreal-world API usage. SEAL standardizes existing benchmarks, integrates an\nagent system for testing API retrieval and planning, and addresses the\ninstability of real-time APIs by introducing a GPT-4-powered API simulator with\ncaching for deterministic evaluations. Our testbed provides a comprehensive\nevaluation pipeline that covers API retrieval, API calls, and final responses,\noffering a reliable framework for structured performance comparison in diverse\nreal-world scenarios. SEAL is publicly available, with ongoing updates for new\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Woojeong Kim"
                    },
                    {
                        "name": "Ashish Jagmohan"
                    },
                    {
                        "name": "Aditya Vempaty"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Vempaty"
                },
                "author": "Aditya Vempaty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18322v2",
                "updated": "2024-09-23T20:09:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    9,
                    28,
                    0,
                    267,
                    0
                ],
                "published": "2024-04-28T21:23:40Z",
                "published_parsed": [
                    2024,
                    4,
                    28,
                    21,
                    23,
                    40,
                    6,
                    119,
                    0
                ],
                "title": "BlockLLM: Multi-tenant Finer-grained Serving for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockLLM: Multi-tenant Finer-grained Serving for Large Language Models"
                },
                "summary": "The increasing demand for Large Language Models (LLMs) across various\napplications has led to a significant shift in the design of deep learning\nserving systems. Deploying LLMs, particularly in multi-tenant environments,\nposes substantial challenges due to their high computational and memory\ndemands. We introduce BlockLLM, a serving system that leverages component\nsharing among fine-tuned LLM models to provide an efficient and flexible\nsolution for LLM workloads. BlockLLM partitions models into finer-grained\nblocks, enabling the reuse of model components and independent provisioning to\nimprove computation efficiency. BlockLLM comprises an offline block zoo for\nstoring blocks and an online system to serve requests through chains of blocks.\nIt offers multi-fold flexibilities: (1) Adaptive assembly of blocks on-the-fly\nthrough equivalence evaluation among blocks in the zoo; (2) Per-block batch\nsize configuration and best-effort KV cache coordination at the individual\nblock level; (3) Speculative execution and locality-aware block placement to\nreduce communication costs from dynamic block resource allocation. Our\nevaluation shows that BlockLLM reduces memory and storage footprints and\nimproves computational efficiency, outperforming existing serving approach in\n95%ile latency and GPU utilization by 33.5% and 20.1%, respectively, with\nminimal impact on accuracy",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for Large Language Models (LLMs) across various\napplications has led to a significant shift in the design of deep learning\nserving systems. Deploying LLMs, particularly in multi-tenant environments,\nposes substantial challenges due to their high computational and memory\ndemands. We introduce BlockLLM, a serving system that leverages component\nsharing among fine-tuned LLM models to provide an efficient and flexible\nsolution for LLM workloads. BlockLLM partitions models into finer-grained\nblocks, enabling the reuse of model components and independent provisioning to\nimprove computation efficiency. BlockLLM comprises an offline block zoo for\nstoring blocks and an online system to serve requests through chains of blocks.\nIt offers multi-fold flexibilities: (1) Adaptive assembly of blocks on-the-fly\nthrough equivalence evaluation among blocks in the zoo; (2) Per-block batch\nsize configuration and best-effort KV cache coordination at the individual\nblock level; (3) Speculative execution and locality-aware block placement to\nreduce communication costs from dynamic block resource allocation. Our\nevaluation shows that BlockLLM reduces memory and storage footprints and\nimproves computational efficiency, outperforming existing serving approach in\n95%ile latency and GPU utilization by 33.5% and 20.1%, respectively, with\nminimal impact on accuracy"
                },
                "authors": [
                    {
                        "name": "Bodun Hu"
                    },
                    {
                        "name": "Jiamin Li"
                    },
                    {
                        "name": "Le Xu"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Akshay Jajoo"
                    },
                    {
                        "name": "Geon-Woo Kim"
                    },
                    {
                        "name": "Hong Xu"
                    },
                    {
                        "name": "Aditya Akella"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Akella"
                },
                "author": "Aditya Akella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13122v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13122v2",
                "updated": "2024-09-23T19:53:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    19,
                    53,
                    37,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-19T23:38:59Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    23,
                    38,
                    59,
                    3,
                    263,
                    0
                ],
                "title": "RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal\n  Reinforcement and Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal\n  Reinforcement and Retrieval-Augmented Generation"
                },
                "summary": "In real-world software engineering tasks, solving a problem often requires\nunderstanding and modifying multiple functions, classes, and files across a\nlarge codebase. Therefore, on the repository level, it is crucial to extract\nthe relevant information to achieve accurate code completion effectively.\nExisting code completion tools have achieved some success, but they struggle to\noptimize the retrieval and generation process dynamically. In this paper, we\npropose RepoGenReflex, a generic, dynamic, effective framework to address this\nchallenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with\nVerbal Reinforcement Learning (VRL), it can dynamically choose the optimal\nresults for repository-level code completion. RepoGenReflex uses Reflector to\ngive directional feedback to the next loop. RepoGenReflex chooses the optimal\nresults stored in the Experience cache based on the RAG-VRL loop. To validate\nthe framework's generalization ability, we propose a new benchmark RepoGenEval,\nwhich consists of the latest, high-quality real-world repositories in line\ncompletion scenarios. Our experiments demonstrate that RepoGenReflex achieves\nsignificant improvements after optimizing the Reflector component, resulting in\nenhanced accuracy and relevance of code completions. Additionally,\nRepoGenReflex consistently demonstrates superior performance and effectiveness\nacross standard code completion tasks, highlighting the robustness and\nadaptability of our framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world software engineering tasks, solving a problem often requires\nunderstanding and modifying multiple functions, classes, and files across a\nlarge codebase. Therefore, on the repository level, it is crucial to extract\nthe relevant information to achieve accurate code completion effectively.\nExisting code completion tools have achieved some success, but they struggle to\noptimize the retrieval and generation process dynamically. In this paper, we\npropose RepoGenReflex, a generic, dynamic, effective framework to address this\nchallenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with\nVerbal Reinforcement Learning (VRL), it can dynamically choose the optimal\nresults for repository-level code completion. RepoGenReflex uses Reflector to\ngive directional feedback to the next loop. RepoGenReflex chooses the optimal\nresults stored in the Experience cache based on the RAG-VRL loop. To validate\nthe framework's generalization ability, we propose a new benchmark RepoGenEval,\nwhich consists of the latest, high-quality real-world repositories in line\ncompletion scenarios. Our experiments demonstrate that RepoGenReflex achieves\nsignificant improvements after optimizing the Reflector component, resulting in\nenhanced accuracy and relevance of code completions. Additionally,\nRepoGenReflex consistently demonstrates superior performance and effectiveness\nacross standard code completion tasks, highlighting the robustness and\nadaptability of our framework."
                },
                "authors": [
                    {
                        "name": "Jicheng Wang"
                    },
                    {
                        "name": "Yifeng He"
                    },
                    {
                        "name": "Hao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Chen"
                },
                "author": "Hao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13122v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13122v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15441v1",
                "updated": "2024-09-23T18:06:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    18,
                    6,
                    32,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T18:06:32Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    18,
                    6,
                    32,
                    0,
                    267,
                    0
                ],
                "title": "Steward: Natural Language Web Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steward: Natural Language Web Automation"
                },
                "summary": "Recently, large language models (LLMs) have demonstrated exceptional\ncapabilities in serving as the foundation for AI assistants. One emerging\napplication of LLMs, navigating through websites and interacting with UI\nelements across various web pages, remains somewhat underexplored. We introduce\nSteward, a novel LLM-powered web automation tool designed to serve as a\ncost-effective, scalable, end-to-end solution for automating web interactions.\nTraditional browser automation frameworks like Selenium, Puppeteer, and\nPlaywright are not scalable for extensive web interaction tasks, such as\nstudying recommendation algorithms on platforms like YouTube and Twitter. These\nframeworks require manual coding of interactions, limiting their utility in\nlarge-scale or dynamic contexts. Steward addresses these limitations by\nintegrating LLM capabilities with browser automation, allowing for natural\nlanguage-driven interaction with websites. Steward operates by receiving\nnatural language instructions and reactively planning and executing a sequence\nof actions on websites, looping until completion, making it a practical tool\nfor developers and researchers to use. It achieves high efficiency, completing\nactions in 8.52 to 10.14 seconds at a cost of $0.028 per action or an average\nof $0.18 per task, which is further reduced to 4.8 seconds and $0.022 through a\ncaching mechanism. It runs tasks on real websites with a 40% completion success\nrate. We discuss various design and implementation challenges, including state\nrepresentation, action sequence selection, system responsiveness, detecting\ntask completion, and caching implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have demonstrated exceptional\ncapabilities in serving as the foundation for AI assistants. One emerging\napplication of LLMs, navigating through websites and interacting with UI\nelements across various web pages, remains somewhat underexplored. We introduce\nSteward, a novel LLM-powered web automation tool designed to serve as a\ncost-effective, scalable, end-to-end solution for automating web interactions.\nTraditional browser automation frameworks like Selenium, Puppeteer, and\nPlaywright are not scalable for extensive web interaction tasks, such as\nstudying recommendation algorithms on platforms like YouTube and Twitter. These\nframeworks require manual coding of interactions, limiting their utility in\nlarge-scale or dynamic contexts. Steward addresses these limitations by\nintegrating LLM capabilities with browser automation, allowing for natural\nlanguage-driven interaction with websites. Steward operates by receiving\nnatural language instructions and reactively planning and executing a sequence\nof actions on websites, looping until completion, making it a practical tool\nfor developers and researchers to use. It achieves high efficiency, completing\nactions in 8.52 to 10.14 seconds at a cost of $0.028 per action or an average\nof $0.18 per task, which is further reduced to 4.8 seconds and $0.022 through a\ncaching mechanism. It runs tasks on real websites with a 40% completion success\nrate. We discuss various design and implementation challenges, including state\nrepresentation, action sequence selection, system responsiveness, detecting\ntask completion, and caching implementation."
                },
                "authors": [
                    {
                        "name": "Brian Tang"
                    },
                    {
                        "name": "Kang G. Shin"
                    }
                ],
                "author_detail": {
                    "name": "Kang G. Shin"
                },
                "author": "Kang G. Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15104v1",
                "updated": "2024-09-23T15:16:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    16,
                    29,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T15:16:29Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    16,
                    29,
                    0,
                    267,
                    0
                ],
                "title": "CSPS: A Communication-Efficient Sequence-Parallelism based Serving\n  System for Transformer based Models with Long Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSPS: A Communication-Efficient Sequence-Parallelism based Serving\n  System for Transformer based Models with Long Prompts"
                },
                "summary": "Long-sequence generative large-language model (LLM) applications have become\nincreasingly popular. In this paper, through trace-based experiments, we found\nthat the existing method for long sequences results in a high\nTime-To-First-Token (TTFT) due to sequential chunk processing, long\nTime-Between-Tokens (TBT) from batching long-sequence prefills and decodes, and\nlow throughput due to constrained key-value cache (KVC) for long sequences. To\naddress these issues, we propose two Sequence-Parallelism (SP) architectures\nfor both tensor parallelism (TP) and non-TP. However, SP introduces two\nchallenges: 1) network communication and computation become performance\nbottlenecks; 2) the latter two issues above are mitigated but not resolved, and\nSP's resultant KV value distribution across GPUs still requires communication\nfor decode, increasing TBT. Hence, we propose a Communication-efficient Sparse\nAttention (CSA) and communication-computation-communication three-phase\npipelining. We also propose SP-based decode that processes decode separately\nfrom prefill, distributes KV values of a request across different GPUs, and\nnovelly moves Query (Q) values instead of KV values to reduce communication\noverhead. These methods constitute a communication-efficient\nSequence-Parallelism based LLM Serving System (SPS2). Our trace-driven\nevaluation demonstrates that SPS2 improves the average TTFT, TBT, and response\ntime by up to 7.5x, 1.92x, and 9.8x and improves the prefill and decode\nthroughput by 8.2x and 5.2x while maintaining the accuracy compared to\nSarathi-Serve. We distributed our source code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-sequence generative large-language model (LLM) applications have become\nincreasingly popular. In this paper, through trace-based experiments, we found\nthat the existing method for long sequences results in a high\nTime-To-First-Token (TTFT) due to sequential chunk processing, long\nTime-Between-Tokens (TBT) from batching long-sequence prefills and decodes, and\nlow throughput due to constrained key-value cache (KVC) for long sequences. To\naddress these issues, we propose two Sequence-Parallelism (SP) architectures\nfor both tensor parallelism (TP) and non-TP. However, SP introduces two\nchallenges: 1) network communication and computation become performance\nbottlenecks; 2) the latter two issues above are mitigated but not resolved, and\nSP's resultant KV value distribution across GPUs still requires communication\nfor decode, increasing TBT. Hence, we propose a Communication-efficient Sparse\nAttention (CSA) and communication-computation-communication three-phase\npipelining. We also propose SP-based decode that processes decode separately\nfrom prefill, distributes KV values of a request across different GPUs, and\nnovelly moves Query (Q) values instead of KV values to reduce communication\noverhead. These methods constitute a communication-efficient\nSequence-Parallelism based LLM Serving System (SPS2). Our trace-driven\nevaluation demonstrates that SPS2 improves the average TTFT, TBT, and response\ntime by up to 7.5x, 1.92x, and 9.8x and improves the prefill and decode\nthroughput by 8.2x and 5.2x while maintaining the accuracy compared to\nSarathi-Serve. We distributed our source code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15012v1",
                "updated": "2024-09-23T13:37:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    13,
                    37,
                    25,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T13:37:25Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    13,
                    37,
                    25,
                    0,
                    267,
                    0
                ],
                "title": "Inference-Friendly Models With MixAttention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Friendly Models With MixAttention"
                },
                "summary": "The size of the key-value (KV) cache plays a critical role in determining\nboth the maximum context length and the number of concurrent requests supported\nduring inference in modern language models. The KV cache size grows\nproportionally with the number of attention heads and the tokens processed,\nleading to increased memory consumption and slower inference for long inputs.\nIn this work, we explore the use of MixAttention, a model architecture\nmodification closely related to a blog published by Character.AI. MixAttention\ncombines sliding window attention, where only a small subset of recent tokens\nis stored in the KV cache, with KV cache sharing across layers. Our experiments\ndemonstrate that MixAttention significantly reduces memory usage and improves\ninference speed without sacrificing model performance in both short and\nlong-context tasks. We also explore various configurations of this\narchitecture, identifying those that maintain quality across evaluation metrics\nwhile optimizing resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The size of the key-value (KV) cache plays a critical role in determining\nboth the maximum context length and the number of concurrent requests supported\nduring inference in modern language models. The KV cache size grows\nproportionally with the number of attention heads and the tokens processed,\nleading to increased memory consumption and slower inference for long inputs.\nIn this work, we explore the use of MixAttention, a model architecture\nmodification closely related to a blog published by Character.AI. MixAttention\ncombines sliding window attention, where only a small subset of recent tokens\nis stored in the KV cache, with KV cache sharing across layers. Our experiments\ndemonstrate that MixAttention significantly reduces memory usage and improves\ninference speed without sacrificing model performance in both short and\nlong-context tasks. We also explore various configurations of this\narchitecture, identifying those that maintain quality across evaluation metrics\nwhile optimizing resource efficiency."
                },
                "authors": [
                    {
                        "name": "Shashank Rajput"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Sean Owen"
                    },
                    {
                        "name": "Vitaliy Chiley"
                    }
                ],
                "author_detail": {
                    "name": "Vitaliy Chiley"
                },
                "author": "Vitaliy Chiley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14968v1",
                "updated": "2024-09-23T12:37:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    12,
                    37,
                    56,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T12:37:56Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    12,
                    37,
                    56,
                    0,
                    267,
                    0
                ],
                "title": "Mutation-Based Deep Learning Framework Testing Method in JavaScript\n  Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutation-Based Deep Learning Framework Testing Method in JavaScript\n  Environment"
                },
                "summary": "In recent years, Deep Learning (DL) applications in JavaScript environment\nhave become increasingly popular. As the infrastructure for DL applications,\nJavaScript DL frameworks play a crucial role in the development and deployment.\nIt is essential to ensure the quality of JavaScript DL frameworks. However, the\nbottleneck of limited computational resources in the JavaScript environment\nbrings new challenges to framework testing. Specifically, JavaScript DL\nframeworks are equipped with various optimization mechanisms (e.g., cache\nreuse, inference acceleration) to overcome the bottleneck of limited\ncomputational resources. These optimization mechanisms are overlooked by\nexisting methods, resulting in many bugs in JavaScript DL frameworks being\nmissed. To address the above challenges, we propose a mutation-based JavaScript\nDL framework testing method named DLJSFuzzer. DLJSFuzzer designs 13 tensor\nmutation rules targeting the cache reuse mechanism to generate test input\ntensors. Besides, DLJSFuzzer designs eight model mutation rules targeting the\ninference acceleration mechanism to generate test input models. To evaluate the\neffectiveness of DLJSFuzzer, we conduct experiments on the most widely-used\nJavaScript DL framework, TensorFlow.js. The experimental results show that\nDLJSFuzzer outperforms state-of-the-art methods in both effectiveness and\nefficiency. DLJSFuzzer successfully detects 21 unique crashes and 126 unique\nNaN & Inconsistency bugs. All detected crashes have been reported to the\nopen-source community, with 12 of them already confirmed by developers.\nAdditionally, DLJSFuzzer has improved by over 47% in model generation\nefficiency and over 91% in bug detection efficiency compared to all baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Deep Learning (DL) applications in JavaScript environment\nhave become increasingly popular. As the infrastructure for DL applications,\nJavaScript DL frameworks play a crucial role in the development and deployment.\nIt is essential to ensure the quality of JavaScript DL frameworks. However, the\nbottleneck of limited computational resources in the JavaScript environment\nbrings new challenges to framework testing. Specifically, JavaScript DL\nframeworks are equipped with various optimization mechanisms (e.g., cache\nreuse, inference acceleration) to overcome the bottleneck of limited\ncomputational resources. These optimization mechanisms are overlooked by\nexisting methods, resulting in many bugs in JavaScript DL frameworks being\nmissed. To address the above challenges, we propose a mutation-based JavaScript\nDL framework testing method named DLJSFuzzer. DLJSFuzzer designs 13 tensor\nmutation rules targeting the cache reuse mechanism to generate test input\ntensors. Besides, DLJSFuzzer designs eight model mutation rules targeting the\ninference acceleration mechanism to generate test input models. To evaluate the\neffectiveness of DLJSFuzzer, we conduct experiments on the most widely-used\nJavaScript DL framework, TensorFlow.js. The experimental results show that\nDLJSFuzzer outperforms state-of-the-art methods in both effectiveness and\nefficiency. DLJSFuzzer successfully detects 21 unique crashes and 126 unique\nNaN & Inconsistency bugs. All detected crashes have been reported to the\nopen-source community, with 12 of them already confirmed by developers.\nAdditionally, DLJSFuzzer has improved by over 47% in model generation\nefficiency and over 91% in bug detection efficiency compared to all baselines."
                },
                "authors": [
                    {
                        "name": "Yinglong Zou"
                    },
                    {
                        "name": "Juan Zhai"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Tao Zheng"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14846v1",
                "updated": "2024-09-23T09:22:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T09:22:59Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "title": "A-VL: Adaptive Attention for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A-VL: Adaptive Attention for Large Vision-Language Models"
                },
                "summary": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Mu Yuan"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Puhan Luo"
                    },
                    {
                        "name": "Huiyou Zhan"
                    },
                    {
                        "name": "Ningkang Zhang"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Xiangyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Li"
                },
                "author": "Xiangyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12490v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12490v2",
                "updated": "2024-09-23T02:24:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    2,
                    24,
                    33,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-19T06:09:56Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    6,
                    9,
                    56,
                    3,
                    263,
                    0
                ],
                "title": "CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling\n  Acceleration in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling\n  Acceleration in LLMs"
                },
                "summary": "Large language models have achieved notable success across various domains,\nyet efficient inference is still limited by the quadratic computation\ncomplexity of the attention mechanism. The inference consists of prefilling and\ndecoding phases. Although several attempts have been made to accelerate\ndecoding, the inefficiency of the prefilling phase, especially for long-context\ntasks, remains a challenge. In this paper, we observe a locality in query\ncriticality during the prefilling phase of long-context processing: adjacent\nquery tokens tend to focus on similar subsets of the past Key-Value (KV) cache.\nBased on this observation, we propose CritiPrefill, a criticality-based\nsegment-wise prefilling method. This method partitions the input sequence's\nqueries and KV cache into segments and blocks, utilizing a segment-wise\nalgorithm to estimate the query criticality. By pruning non-critical\ncomputations between query segments and cache blocks in the self-attention\nmechanism, the prefilling process can be significantly accelerated. Extensive\nevaluations on multiple long-context datasets show up to 2.7x speedup on\nLlama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100\nGPU, with minimal quality degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have achieved notable success across various domains,\nyet efficient inference is still limited by the quadratic computation\ncomplexity of the attention mechanism. The inference consists of prefilling and\ndecoding phases. Although several attempts have been made to accelerate\ndecoding, the inefficiency of the prefilling phase, especially for long-context\ntasks, remains a challenge. In this paper, we observe a locality in query\ncriticality during the prefilling phase of long-context processing: adjacent\nquery tokens tend to focus on similar subsets of the past Key-Value (KV) cache.\nBased on this observation, we propose CritiPrefill, a criticality-based\nsegment-wise prefilling method. This method partitions the input sequence's\nqueries and KV cache into segments and blocks, utilizing a segment-wise\nalgorithm to estimate the query criticality. By pruning non-critical\ncomputations between query segments and cache blocks in the self-attention\nmechanism, the prefilling process can be significantly accelerated. Extensive\nevaluations on multiple long-context datasets show up to 2.7x speedup on\nLlama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100\nGPU, with minimal quality degradation."
                },
                "authors": [
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "Xin Jia"
                    },
                    {
                        "name": "Qirong Peng"
                    },
                    {
                        "name": "Guiming Xie"
                    }
                ],
                "author_detail": {
                    "name": "Guiming Xie"
                },
                "author": "Guiming Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12490v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12490v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14350v1",
                "updated": "2024-09-22T07:24:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    22,
                    7,
                    24,
                    2,
                    6,
                    266,
                    0
                ],
                "published": "2024-09-22T07:24:02Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    7,
                    24,
                    2,
                    6,
                    266,
                    0
                ],
                "title": "D2D Coded Caching from Two Classes of Optimal DPDAs using Cross\n  Resolvable Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2D Coded Caching from Two Classes of Optimal DPDAs using Cross\n  Resolvable Designs"
                },
                "summary": "Coded caching in a wireless device-to-device (D2D) network was first studied\nby Ji \\textit{et al.} in [4] (referred to as the JCM scheme). In a D2D network,\na central server first places the data in the user cache memories and all the\nuser's demands are served by inter-user coded multicast transmissions. Low\nsubpacketization level D2D coded caching schemes are desirable for practical\nimplementations. Wang \\textit{et al.} in [7] proposed an array called D2D\nplacement delivery array (DPDA) which characterizes the placement phase and the\ndelivery phase in a D2D network. A lower bound on the transmission load of a\nDPDA is derived and only the JCM scheme achieves this lower bound, but requires\na subpacketization level that grows exponentially with the number of users. Low\nsubpacketization level D2D schemes can be obtained by constructing appropriate\nDPDAs. In this paper, we propose two new classes of DPDA constructions that\ngive low subpacketization level D2D schemes using cross resolvable designs. The\nfirst class of constructed DPDA achieves the known lower bound on the\ntransmission load of DPDA while requiring a subpacketization level lesser than\nthat of the JCM scheme. We propose another lower bound on the transmission load\nof a DPDA and show that the second class of constructed DPDA achieves this\nlower bound.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching in a wireless device-to-device (D2D) network was first studied\nby Ji \\textit{et al.} in [4] (referred to as the JCM scheme). In a D2D network,\na central server first places the data in the user cache memories and all the\nuser's demands are served by inter-user coded multicast transmissions. Low\nsubpacketization level D2D coded caching schemes are desirable for practical\nimplementations. Wang \\textit{et al.} in [7] proposed an array called D2D\nplacement delivery array (DPDA) which characterizes the placement phase and the\ndelivery phase in a D2D network. A lower bound on the transmission load of a\nDPDA is derived and only the JCM scheme achieves this lower bound, but requires\na subpacketization level that grows exponentially with the number of users. Low\nsubpacketization level D2D schemes can be obtained by constructing appropriate\nDPDAs. In this paper, we propose two new classes of DPDA constructions that\ngive low subpacketization level D2D schemes using cross resolvable designs. The\nfirst class of constructed DPDA achieves the known lower bound on the\ntransmission load of DPDA while requiring a subpacketization level lesser than\nthat of the JCM scheme. We propose another lower bound on the transmission load\nof a DPDA and show that the second class of constructed DPDA achieves this\nlower bound."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "9 pages, 3 tables and 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02000v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02000v2",
                "updated": "2024-09-21T20:45:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    20,
                    45,
                    41,
                    5,
                    265,
                    0
                ],
                "published": "2024-07-02T07:15:40Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    7,
                    15,
                    40,
                    1,
                    184,
                    0
                ],
                "title": "Sub-millisecond electric field sensing with an individual rare-earth\n  doped ferroelectric nanocrystal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-millisecond electric field sensing with an individual rare-earth\n  doped ferroelectric nanocrystal"
                },
                "summary": "Understanding the dynamics of electrical signals within neuronal assemblies\nis crucial to unraveling complex brain function. Despite recent advances in\nemploying optically active nanostructures in transmembrane potential sensing,\nthere remains room for improvement in terms of response time and sensitivity.\nHere, we report the development of such a nanosensor capable of detecting\nelectric fields with a submillisecond response time at the single particle\nlevel. We achieve this by using ferroelectric nanocrystals doped with rare\nearth ions producing upconversion (UC). When such a nanocrystal experiences a\nvariation of surrounding electric potential, its surface charge density\nchanges, inducing electric polarization modifications that vary, via converse\npiezoelectric effect, the crystal field around the ions. The latter variation\nis finally converted into UC spectral changes, enabling optical detection of\nelectric potential. To develop such a sensor, we synthesized erbium and\nytterbium-doped barium titanate crystals of size $\\approx160$~nm. We observed\ndistinct changes in the UC spectrum when individual nanocrystals were subjected\nto an external field via a conductive AFM tip, with a response time of\n100~$\\mu$s. Furthermore, our sensor exhibits a remarkable sensitivity of\n4.8~kV/cm/$\\sqrt{\\rm Hz}$, enabling time-resolved detection of fast changing\nelectric field of amplitude comparable to that generated during a neuron action\npotential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the dynamics of electrical signals within neuronal assemblies\nis crucial to unraveling complex brain function. Despite recent advances in\nemploying optically active nanostructures in transmembrane potential sensing,\nthere remains room for improvement in terms of response time and sensitivity.\nHere, we report the development of such a nanosensor capable of detecting\nelectric fields with a submillisecond response time at the single particle\nlevel. We achieve this by using ferroelectric nanocrystals doped with rare\nearth ions producing upconversion (UC). When such a nanocrystal experiences a\nvariation of surrounding electric potential, its surface charge density\nchanges, inducing electric polarization modifications that vary, via converse\npiezoelectric effect, the crystal field around the ions. The latter variation\nis finally converted into UC spectral changes, enabling optical detection of\nelectric potential. To develop such a sensor, we synthesized erbium and\nytterbium-doped barium titanate crystals of size $\\approx160$~nm. We observed\ndistinct changes in the UC spectrum when individual nanocrystals were subjected\nto an external field via a conductive AFM tip, with a response time of\n100~$\\mu$s. Furthermore, our sensor exhibits a remarkable sensitivity of\n4.8~kV/cm/$\\sqrt{\\rm Hz}$, enabling time-resolved detection of fast changing\nelectric field of amplitude comparable to that generated during a neuron action\npotential."
                },
                "authors": [
                    {
                        "name": "Athulya Muraleedharan"
                    },
                    {
                        "name": "Jingye Zou"
                    },
                    {
                        "name": "Maxime Vallet"
                    },
                    {
                        "name": "Abdelali Zaki"
                    },
                    {
                        "name": "Christine Bogicevic"
                    },
                    {
                        "name": "Charles Paillard"
                    },
                    {
                        "name": "Karen Perronet"
                    },
                    {
                        "name": "François Treussart"
                    }
                ],
                "author_detail": {
                    "name": "François Treussart"
                },
                "author": "François Treussart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02000v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02000v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.other",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10593v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10593v2",
                "updated": "2024-09-21T13:01:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    13,
                    1,
                    43,
                    5,
                    265,
                    0
                ],
                "published": "2024-09-16T17:36:50Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    36,
                    50,
                    0,
                    260,
                    0
                ],
                "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios"
                },
                "summary": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%."
                },
                "authors": [
                    {
                        "name": "Luning Wang"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10593v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10593v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v2",
                "updated": "2024-09-21T12:33:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    12,
                    33,
                    0,
                    5,
                    265,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06799v2",
                "updated": "2024-09-21T09:10:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    9,
                    10,
                    2,
                    5,
                    265,
                    0
                ],
                "published": "2024-06-10T21:08:39Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    21,
                    8,
                    39,
                    0,
                    162,
                    0
                ],
                "title": "LLM-dCache: Improving Tool-Augmented LLMs with GPT-Driven Localized Data\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-dCache: Improving Tool-Augmented LLMs with GPT-Driven Localized Data\n  Caching"
                },
                "summary": "As Large Language Models (LLMs) broaden their capabilities to manage\nthousands of API calls, they are confronted with complex data operations across\nvast datasets with significant overhead to the underlying system. In this work,\nwe introduce LLM-dCache to optimize data accesses by treating cache operations\nas callable API functions exposed to the tool-augmented agent. We grant LLMs\nthe autonomy to manage cache decisions via prompting, seamlessly integrating\nwith existing function-calling mechanisms. Tested on an industry-scale\nmassively parallel platform that spans hundreds of GPT endpoints and terabytes\nof imagery, our method improves Copilot times by an average of 1.24x across\nvarious LLMs and prompting techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) broaden their capabilities to manage\nthousands of API calls, they are confronted with complex data operations across\nvast datasets with significant overhead to the underlying system. In this work,\nwe introduce LLM-dCache to optimize data accesses by treating cache operations\nas callable API functions exposed to the tool-augmented agent. We grant LLMs\nthe autonomy to manage cache decisions via prompting, seamlessly integrating\nwith existing function-calling mechanisms. Tested on an industry-scale\nmassively parallel platform that spans hundreds of GPT endpoints and terabytes\nof imagery, our method improves Copilot times by an average of 1.24x across\nvarious LLMs and prompting techniques."
                },
                "authors": [
                    {
                        "name": "Simranjit Singh"
                    },
                    {
                        "name": "Michael Fore"
                    },
                    {
                        "name": "Andreas Karatzas"
                    },
                    {
                        "name": "Chaehong Lee"
                    },
                    {
                        "name": "Yanan Jian"
                    },
                    {
                        "name": "Longfei Shangguan"
                    },
                    {
                        "name": "Fuxun Yu"
                    },
                    {
                        "name": "Iraklis Anagnostopoulos"
                    },
                    {
                        "name": "Dimitrios Stamoulis"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Stamoulis"
                },
                "author": "Dimitrios Stamoulis",
                "arxiv_comment": "ICECS 2024 Camera-Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v2",
                "updated": "2024-09-20T16:59:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    16,
                    59,
                    29,
                    4,
                    264,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "ARCANE: Adaptive Routing with Caching and Network Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive Routing with Caching and Network Exploration"
                },
                "summary": "Most datacenter transport protocols traditionally depend on in-order packet\ndelivery, a legacy design choice that prioritizes simplicity. However,\ntechnological advancements, such as RDMA, now enable the relaxation of this\nrequirement, allowing for more efficient utilization of modern datacenter\ntopologies like FatTree and Dragonfly. With the growing prevalence of AI/ML\nworkloads, the demand for improved link utilization has intensified, creating\nchallenges for single-path load balancers due to problems like ECMP collisions.\nIn this paper, we present ARCANE, a novel, adaptive per-packet traffic\nload-balancing algorithm designed to work seamlessly with existing congestion\ncontrol mechanisms. ARCANE dynamically routes packets to bypass congested areas\nand network failures, all while maintaining a lightweight footprint with\nminimal state requirements. Our evaluation shows that ARCANE delivers\nsignificant performance gains over traditional load-balancing methods,\nincluding packet spraying and other advanced solutions, substantially enhancing\nboth performance and link utilization in modern datacenter networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most datacenter transport protocols traditionally depend on in-order packet\ndelivery, a legacy design choice that prioritizes simplicity. However,\ntechnological advancements, such as RDMA, now enable the relaxation of this\nrequirement, allowing for more efficient utilization of modern datacenter\ntopologies like FatTree and Dragonfly. With the growing prevalence of AI/ML\nworkloads, the demand for improved link utilization has intensified, creating\nchallenges for single-path load balancers due to problems like ECMP collisions.\nIn this paper, we present ARCANE, a novel, adaptive per-packet traffic\nload-balancing algorithm designed to work seamlessly with existing congestion\ncontrol mechanisms. ARCANE dynamically routes packets to bypass congested areas\nand network failures, all while maintaining a lightweight footprint with\nminimal state requirements. Our evaluation shows that ARCANE delivers\nsignificant performance gains over traditional load-balancing methods,\nincluding packet spraying and other advanced solutions, substantially enhancing\nboth performance and link utilization in modern datacenter networks."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v4",
                "updated": "2024-09-20T15:51:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    15,
                    51,
                    17,
                    4,
                    264,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13175v1",
                "updated": "2024-09-20T03:02:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    3,
                    2,
                    42,
                    4,
                    264,
                    0
                ],
                "published": "2024-09-20T03:02:42Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    3,
                    2,
                    42,
                    4,
                    264,
                    0
                ],
                "title": "RPAF: A Reinforcement Prediction-Allocation Framework for Cache\n  Allocation in Large-Scale Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RPAF: A Reinforcement Prediction-Allocation Framework for Cache\n  Allocation in Large-Scale Recommender Systems"
                },
                "summary": "Modern recommender systems are built upon computation-intensive\ninfrastructure, and it is challenging to perform real-time computation for each\nrequest, especially in peak periods, due to the limited computational\nresources. Recommending by user-wise result caches is widely used when the\nsystem cannot afford a real-time recommendation. However, it is challenging to\nallocate real-time and cached recommendations to maximize the users' overall\nengagement. This paper shows two key challenges to cache allocation, i.e., the\nvalue-strategy dependency and the streaming allocation. Then, we propose a\nreinforcement prediction-allocation framework (RPAF) to address these issues.\nRPAF is a reinforcement-learning-based two-stage framework containing\nprediction and allocation stages. The prediction stage estimates the values of\nthe cache choices considering the value-strategy dependency, and the allocation\nstage determines the cache choices for each individual request while satisfying\nthe global budget constraint. We show that the challenge of training RPAF\nincludes globality and the strictness of budget constraints, and a relaxed\nlocal allocator (RLA) is proposed to address this issue. Moreover, a PoolRank\nalgorithm is used in the allocation stage to deal with the streaming allocation\nproblem. Experiments show that RPAF significantly improves users' engagement\nunder computational budget constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern recommender systems are built upon computation-intensive\ninfrastructure, and it is challenging to perform real-time computation for each\nrequest, especially in peak periods, due to the limited computational\nresources. Recommending by user-wise result caches is widely used when the\nsystem cannot afford a real-time recommendation. However, it is challenging to\nallocate real-time and cached recommendations to maximize the users' overall\nengagement. This paper shows two key challenges to cache allocation, i.e., the\nvalue-strategy dependency and the streaming allocation. Then, we propose a\nreinforcement prediction-allocation framework (RPAF) to address these issues.\nRPAF is a reinforcement-learning-based two-stage framework containing\nprediction and allocation stages. The prediction stage estimates the values of\nthe cache choices considering the value-strategy dependency, and the allocation\nstage determines the cache choices for each individual request while satisfying\nthe global budget constraint. We show that the challenge of training RPAF\nincludes globality and the strictness of budget constraints, and a relaxed\nlocal allocator (RLA) is proposed to address this issue. Moreover, a PoolRank\nalgorithm is used in the allocation stage to deal with the streaming allocation\nproblem. Experiments show that RPAF significantly improves users' engagement\nunder computational budget constraints."
                },
                "authors": [
                    {
                        "name": "Shuo Su"
                    },
                    {
                        "name": "Xiaoshuang Chen"
                    },
                    {
                        "name": "Yao Wang"
                    },
                    {
                        "name": "Yulin Wu"
                    },
                    {
                        "name": "Ziqiang Zhang"
                    },
                    {
                        "name": "Kaiqiao Zhan"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12892v1",
                "updated": "2024-09-19T16:31:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    16,
                    31,
                    44,
                    3,
                    263,
                    0
                ],
                "published": "2024-09-19T16:31:44Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    16,
                    31,
                    44,
                    3,
                    263,
                    0
                ],
                "title": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt"
                },
                "summary": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 30% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 30% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS."
                },
                "authors": [
                    {
                        "name": "Lukas Höllein"
                    },
                    {
                        "name": "Aljaž Božič"
                    },
                    {
                        "name": "Michael Zollhöfer"
                    },
                    {
                        "name": "Matthias Nießner"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Nießner"
                },
                "author": "Matthias Nießner",
                "arxiv_comment": "project page: https://lukashoel.github.io/3DGS-LM, video:\n  https://www.youtube.com/watch?v=tDiGuGMssg8, code:\n  https://github.com/lukasHoel/3DGS-LM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.09048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09048v1",
                "updated": "2024-10-11T17:59:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    59,
                    58,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:59:58Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    59,
                    58,
                    4,
                    285,
                    0
                ],
                "title": "Towards Trustworthy LLMs for Code: A Data-Centric Synergistic Auditing\n  Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Trustworthy LLMs for Code: A Data-Centric Synergistic Auditing\n  Framework"
                },
                "summary": "LLM-powered coding and development assistants have become prevalent to\nprogrammers' workflows. However, concerns about the trustworthiness of LLMs for\ncode persist despite their widespread use. Much of the existing research\nfocused on either training or evaluation, raising questions about whether\nstakeholders in training and evaluation align in their understanding of model\ntrustworthiness and whether they can move toward a unified direction. In this\npaper, we propose a vision for a unified trustworthiness auditing framework,\nDataTrust, which adopts a data-centric approach that synergistically emphasizes\nboth training and evaluation data and their correlations. DataTrust aims to\nconnect model trustworthiness indicators in evaluation with data quality\nindicators in training. It autonomously inspects training data and evaluates\nmodel trustworthiness using synthesized data, attributing potential causes from\nspecific evaluation data to corresponding training data and refining indicator\nconnections. Additionally, a trustworthiness arena powered by DataTrust will\nengage crowdsourced input and deliver quantitative outcomes. We outline the\nbenefits that various stakeholders can gain from DataTrust and discuss the\nchallenges and opportunities it presents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-powered coding and development assistants have become prevalent to\nprogrammers' workflows. However, concerns about the trustworthiness of LLMs for\ncode persist despite their widespread use. Much of the existing research\nfocused on either training or evaluation, raising questions about whether\nstakeholders in training and evaluation align in their understanding of model\ntrustworthiness and whether they can move toward a unified direction. In this\npaper, we propose a vision for a unified trustworthiness auditing framework,\nDataTrust, which adopts a data-centric approach that synergistically emphasizes\nboth training and evaluation data and their correlations. DataTrust aims to\nconnect model trustworthiness indicators in evaluation with data quality\nindicators in training. It autonomously inspects training data and evaluates\nmodel trustworthiness using synthesized data, attributing potential causes from\nspecific evaluation data to corresponding training data and refining indicator\nconnections. Additionally, a trustworthiness arena powered by DataTrust will\nengage crowdsourced input and deliver quantitative outcomes. We outline the\nbenefits that various stakeholders can gain from DataTrust and discuss the\nchallenges and opportunities it presents."
                },
                "authors": [
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Zhenpeng Chen"
                    },
                    {
                        "name": "Tianlin Li"
                    },
                    {
                        "name": "Yilun Zhao"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "Short Vision Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15240v2",
                "updated": "2024-10-11T17:59:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    59,
                    32,
                    4,
                    285,
                    0
                ],
                "published": "2024-08-27T17:57:45Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    57,
                    45,
                    1,
                    240,
                    0
                ],
                "title": "Generative Verifiers: Reward Modeling as Next-Token Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Verifiers: Reward Modeling as Next-Token Prediction"
                },
                "summary": "Verifiers or reward models are often used to enhance the reasoning\nperformance of large language models (LLMs). A common approach is the Best-of-N\nmethod, where N candidate solutions generated by the LLM are ranked by a\nverifier, and the best one is selected. While LLM-based verifiers are typically\ntrained as discriminative classifiers to score solutions, they do not utilize\nthe text generation capabilities of pretrained LLMs. To overcome this\nlimitation, we instead propose training verifiers using the ubiquitous\nnext-token prediction objective, jointly on verification and solution\ngeneration. Compared to standard verifiers, such generative verifiers (GenRM)\ncan benefit from several advantages of LLMs: they integrate seamlessly with\ninstruction tuning, enable chain-of-thought reasoning, and can utilize\nadditional test-time compute via majority voting for better verification. We\ndemonstrate that GenRM outperforms discriminative, DPO verifiers, and\nLLM-as-a-Judge, resulting in a 16-40% improvement in the number of problems\nsolved with Best-of-N on algorithmic and math reasoning tasks. Furthermore, we\nfind that training GenRM with synthetic verification rationales is sufficient\nto pick out subtle errors on math problems. Finally, we demonstrate that\ngenerative verifiers scale favorably with model size and inference-time\ncompute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verifiers or reward models are often used to enhance the reasoning\nperformance of large language models (LLMs). A common approach is the Best-of-N\nmethod, where N candidate solutions generated by the LLM are ranked by a\nverifier, and the best one is selected. While LLM-based verifiers are typically\ntrained as discriminative classifiers to score solutions, they do not utilize\nthe text generation capabilities of pretrained LLMs. To overcome this\nlimitation, we instead propose training verifiers using the ubiquitous\nnext-token prediction objective, jointly on verification and solution\ngeneration. Compared to standard verifiers, such generative verifiers (GenRM)\ncan benefit from several advantages of LLMs: they integrate seamlessly with\ninstruction tuning, enable chain-of-thought reasoning, and can utilize\nadditional test-time compute via majority voting for better verification. We\ndemonstrate that GenRM outperforms discriminative, DPO verifiers, and\nLLM-as-a-Judge, resulting in a 16-40% improvement in the number of problems\nsolved with Best-of-N on algorithmic and math reasoning tasks. Furthermore, we\nfind that training GenRM with synthetic verification rationales is sufficient\nto pick out subtle errors on math problems. Finally, we demonstrate that\ngenerative verifiers scale favorably with model size and inference-time\ncompute."
                },
                "authors": [
                    {
                        "name": "Lunjun Zhang"
                    },
                    {
                        "name": "Arian Hosseini"
                    },
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "Mehran Kazemi"
                    },
                    {
                        "name": "Aviral Kumar"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    }
                ],
                "author_detail": {
                    "name": "Rishabh Agarwal"
                },
                "author": "Rishabh Agarwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09047v1",
                "updated": "2024-10-11T17:59:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    59,
                    31,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:59:31Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    59,
                    31,
                    4,
                    285,
                    0
                ],
                "title": "Unraveling and Mitigating Safety Alignment Degradation of\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unraveling and Mitigating Safety Alignment Degradation of\n  Vision-Language Models"
                },
                "summary": "The safety alignment ability of Vision-Language Models (VLMs) is prone to be\ndegraded by the integration of the vision module compared to its LLM backbone.\nWe investigate this phenomenon, dubbed as ''safety alignment degradation'' in\nthis paper, and show that the challenge arises from the representation gap that\nemerges when introducing vision modality to VLMs. In particular, we show that\nthe representations of multi-modal inputs shift away from that of text-only\ninputs which represent the distribution that the LLM backbone is optimized for.\nAt the same time, the safety alignment capabilities, initially developed within\nthe textual embedding space, do not successfully transfer to this new\nmulti-modal representation space. To reduce safety alignment degradation, we\nintroduce Cross-Modality Representation Manipulation (CMRM), an inference time\nrepresentation intervention method for recovering the safety alignment ability\nthat is inherent in the LLM backbone of VLMs, while simultaneously preserving\nthe functional capabilities of VLMs. The empirical results show that our\nframework significantly recovers the alignment ability that is inherited from\nthe LLM backbone with minimal impact on the fluency and linguistic capabilities\nof pre-trained VLMs even without additional training. Specifically, the unsafe\nrate of LLaVA-7B on multi-modal input can be reduced from 61.53% to as low as\n3.15% with only inference-time intervention.\n  WARNING: This paper contains examples of toxic or harmful language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The safety alignment ability of Vision-Language Models (VLMs) is prone to be\ndegraded by the integration of the vision module compared to its LLM backbone.\nWe investigate this phenomenon, dubbed as ''safety alignment degradation'' in\nthis paper, and show that the challenge arises from the representation gap that\nemerges when introducing vision modality to VLMs. In particular, we show that\nthe representations of multi-modal inputs shift away from that of text-only\ninputs which represent the distribution that the LLM backbone is optimized for.\nAt the same time, the safety alignment capabilities, initially developed within\nthe textual embedding space, do not successfully transfer to this new\nmulti-modal representation space. To reduce safety alignment degradation, we\nintroduce Cross-Modality Representation Manipulation (CMRM), an inference time\nrepresentation intervention method for recovering the safety alignment ability\nthat is inherent in the LLM backbone of VLMs, while simultaneously preserving\nthe functional capabilities of VLMs. The empirical results show that our\nframework significantly recovers the alignment ability that is inherited from\nthe LLM backbone with minimal impact on the fluency and linguistic capabilities\nof pre-trained VLMs even without additional training. Specifically, the unsafe\nrate of LLaVA-7B on multi-modal input can be reduced from 61.53% to as low as\n3.15% with only inference-time intervention.\n  WARNING: This paper contains examples of toxic or harmful language."
                },
                "authors": [
                    {
                        "name": "Qin Liu"
                    },
                    {
                        "name": "Chao Shang"
                    },
                    {
                        "name": "Ling Liu"
                    },
                    {
                        "name": "Nikolaos Pappas"
                    },
                    {
                        "name": "Jie Ma"
                    },
                    {
                        "name": "Neha Anna John"
                    },
                    {
                        "name": "Srikanth Doss"
                    },
                    {
                        "name": "Lluis Marquez"
                    },
                    {
                        "name": "Miguel Ballesteros"
                    },
                    {
                        "name": "Yassine Benajiba"
                    }
                ],
                "author_detail": {
                    "name": "Yassine Benajiba"
                },
                "author": "Yassine Benajiba",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09045v1",
                "updated": "2024-10-11T17:58:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    58,
                    2,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:58:02Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    58,
                    2,
                    4,
                    285,
                    0
                ],
                "title": "MiRAGeNews: Multimodal Realistic AI-Generated News Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiRAGeNews: Multimodal Realistic AI-Generated News Detection"
                },
                "summary": "The proliferation of inflammatory or misleading \"fake\" news content has\nbecome increasingly common in recent years. Simultaneously, it has become\neasier than ever to use AI tools to generate photorealistic images depicting\nany scene imaginable. Combining these two -- AI-generated fake news content --\nis particularly potent and dangerous. To combat the spread of AI-generated fake\nnews, we propose the MiRAGeNews Dataset, a dataset of 12,500 high-quality real\nand AI-generated image-caption pairs from state-of-the-art generators. We find\nthat our dataset poses a significant challenge to humans (60% F-1) and\nstate-of-the-art multi-modal LLMs (< 24% F-1). Using our dataset we train a\nmulti-modal detector (MiRAGe) that improves by +5.1% F-1 over state-of-the-art\nbaselines on image-caption pairs from out-of-domain image generators and news\npublishers. We release our code and data to aid future work on detecting\nAI-generated content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of inflammatory or misleading \"fake\" news content has\nbecome increasingly common in recent years. Simultaneously, it has become\neasier than ever to use AI tools to generate photorealistic images depicting\nany scene imaginable. Combining these two -- AI-generated fake news content --\nis particularly potent and dangerous. To combat the spread of AI-generated fake\nnews, we propose the MiRAGeNews Dataset, a dataset of 12,500 high-quality real\nand AI-generated image-caption pairs from state-of-the-art generators. We find\nthat our dataset poses a significant challenge to humans (60% F-1) and\nstate-of-the-art multi-modal LLMs (< 24% F-1). Using our dataset we train a\nmulti-modal detector (MiRAGe) that improves by +5.1% F-1 over state-of-the-art\nbaselines on image-caption pairs from out-of-domain image generators and news\npublishers. We release our code and data to aid future work on detecting\nAI-generated content."
                },
                "authors": [
                    {
                        "name": "Runsheng Huang"
                    },
                    {
                        "name": "Liam Dugan"
                    },
                    {
                        "name": "Yue Yang"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    }
                ],
                "author_detail": {
                    "name": "Chris Callison-Burch"
                },
                "author": "Chris Callison-Burch",
                "arxiv_comment": "EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09043v1",
                "updated": "2024-10-11T17:57:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    57,
                    16,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:57:16Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    57,
                    16,
                    4,
                    285,
                    0
                ],
                "title": "Transforming In-Vehicle Network Intrusion Detection: VAE-based Knowledge\n  Distillation Meets Explainable AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transforming In-Vehicle Network Intrusion Detection: VAE-based Knowledge\n  Distillation Meets Explainable AI"
                },
                "summary": "In the evolving landscape of autonomous vehicles, ensuring robust in-vehicle\nnetwork (IVN) security is paramount. This paper introduces an advanced\nintrusion detection system (IDS) called KD-XVAE that uses a Variational\nAutoencoder (VAE)-based knowledge distillation approach to enhance both\nperformance and efficiency. Our model significantly reduces complexity,\noperating with just 1669 parameters and achieving an inference time of 0.3 ms\nper batch, making it highly suitable for resource-constrained automotive\nenvironments. Evaluations in the HCRL Car-Hacking dataset demonstrate\nexceptional capabilities, attaining perfect scores (Recall, Precision, F1 Score\nof 100%, and FNR of 0%) under multiple attack types, including DoS, Fuzzing,\nGear Spoofing, and RPM Spoofing. Comparative analysis on the CICIoV2024 dataset\nfurther underscores its superiority over traditional machine learning models,\nachieving perfect detection metrics. We furthermore integrate Explainable AI\n(XAI) techniques to ensure transparency in the model's decisions. The VAE\ncompresses the original feature space into a latent space, on which the\ndistilled model is trained. SHAP(SHapley Additive exPlanations) values provide\ninsights into the importance of each latent dimension, mapped back to original\nfeatures for intuitive understanding. Our paper advances the field by\nintegrating state-of-the-art techniques, addressing critical challenges in the\ndeployment of efficient, trustworthy, and reliable IDSes for autonomous\nvehicles, ensuring enhanced protection against emerging cyber threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the evolving landscape of autonomous vehicles, ensuring robust in-vehicle\nnetwork (IVN) security is paramount. This paper introduces an advanced\nintrusion detection system (IDS) called KD-XVAE that uses a Variational\nAutoencoder (VAE)-based knowledge distillation approach to enhance both\nperformance and efficiency. Our model significantly reduces complexity,\noperating with just 1669 parameters and achieving an inference time of 0.3 ms\nper batch, making it highly suitable for resource-constrained automotive\nenvironments. Evaluations in the HCRL Car-Hacking dataset demonstrate\nexceptional capabilities, attaining perfect scores (Recall, Precision, F1 Score\nof 100%, and FNR of 0%) under multiple attack types, including DoS, Fuzzing,\nGear Spoofing, and RPM Spoofing. Comparative analysis on the CICIoV2024 dataset\nfurther underscores its superiority over traditional machine learning models,\nachieving perfect detection metrics. We furthermore integrate Explainable AI\n(XAI) techniques to ensure transparency in the model's decisions. The VAE\ncompresses the original feature space into a latent space, on which the\ndistilled model is trained. SHAP(SHapley Additive exPlanations) values provide\ninsights into the importance of each latent dimension, mapped back to original\nfeatures for intuitive understanding. Our paper advances the field by\nintegrating state-of-the-art techniques, addressing critical challenges in the\ndeployment of efficient, trustworthy, and reliable IDSes for autonomous\nvehicles, ensuring enhanced protection against emerging cyber threats."
                },
                "authors": [
                    {
                        "name": "Muhammet Anil Yagiz"
                    },
                    {
                        "name": "Pedram MohajerAnsari"
                    },
                    {
                        "name": "Mert D. Pese"
                    },
                    {
                        "name": "Polat Goktas"
                    }
                ],
                "author_detail": {
                    "name": "Polat Goktas"
                },
                "author": "Polat Goktas",
                "arxiv_comment": "In Proceedings of the Sixth Workshop on CPSIoT Security and Privacy\n  (CPSIoTSec 24), October 14-18, 2024, Salt Lake City, UT, USA. ACM, New York,\n  NY, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09041v1",
                "updated": "2024-10-11T17:55:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    55,
                    48,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:55:48Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    55,
                    48,
                    4,
                    285,
                    0
                ],
                "title": "Inferring birth versus death dynamics for ecological interactions in\n  stochastic heterogeneous populations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring birth versus death dynamics for ecological interactions in\n  stochastic heterogeneous populations"
                },
                "summary": "In this paper, we study the significance of ecological interactions and\nseparation of birth and death dynamics in heterogeneous stochastic populations.\nInteractions can manifest through the birth dynamics, the death dynamics, or\nsome combination of the two. The underlying mechanisms are important but often\nimplicit in data. We propose an inference method for disambiguating the types\nof interaction and the birth and death processes from time series data of a\nstochastic n-type heterogeneous population. The interspecies interactions\nconsidered can be competitive, antagonistic, or mutualistic. The inference\nmethod is then validated in the example of two-type Lotka-Volterra dynamics.\nUtilizing stochastic fluctuations enables us to estimate additional parameters\nin a stochastic Lotka-Volterra model, which are not identifiable in a\ndeterministic model. We also show that different pairs of birth and death rates\nwith the same net growth rate manifest different time series statistics and\nsurvival probabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study the significance of ecological interactions and\nseparation of birth and death dynamics in heterogeneous stochastic populations.\nInteractions can manifest through the birth dynamics, the death dynamics, or\nsome combination of the two. The underlying mechanisms are important but often\nimplicit in data. We propose an inference method for disambiguating the types\nof interaction and the birth and death processes from time series data of a\nstochastic n-type heterogeneous population. The interspecies interactions\nconsidered can be competitive, antagonistic, or mutualistic. The inference\nmethod is then validated in the example of two-type Lotka-Volterra dynamics.\nUtilizing stochastic fluctuations enables us to estimate additional parameters\nin a stochastic Lotka-Volterra model, which are not identifiable in a\ndeterministic model. We also show that different pairs of birth and death rates\nwith the same net growth rate manifest different time series statistics and\nsurvival probabilities."
                },
                "authors": [
                    {
                        "name": "Erin Beckman"
                    },
                    {
                        "name": "Heyrim Cho"
                    },
                    {
                        "name": "Linh Huynh"
                    }
                ],
                "author_detail": {
                    "name": "Linh Huynh"
                },
                "author": "Linh Huynh",
                "arxiv_comment": "31 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92-08, 92D25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09040v1",
                "updated": "2024-10-11T17:55:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    55,
                    9,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:55:09Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    55,
                    9,
                    4,
                    285,
                    0
                ],
                "title": "AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention\n  Manipulation"
                },
                "summary": "This paper studies the vulnerabilities of transformer-based Large Language\nModels (LLMs) to jailbreaking attacks, focusing specifically on the\noptimization-based Greedy Coordinate Gradient (GCG) strategy. We first observe\na positive correlation between the effectiveness of attacks and the internal\nbehaviors of the models. For instance, attacks tend to be less effective when\nmodels pay more attention to system prompts designed to ensure LLM safety\nalignment. Building on this discovery, we introduce an enhanced method that\nmanipulates models' attention scores to facilitate LLM jailbreaking, which we\nterm AttnGCG. Empirically, AttnGCG shows consistent improvements in attack\nefficacy across diverse LLMs, achieving an average increase of ~7% in the\nLlama-2 series and ~10% in the Gemma series. Our strategy also demonstrates\nrobust attack transferability against both unseen harmful goals and black-box\nLLMs like GPT-3.5 and GPT-4. Moreover, we note our attention-score\nvisualization is more interpretable, allowing us to gain better insights into\nhow our targeted attention manipulation facilitates more effective\njailbreaking. We release the code at\nhttps://github.com/UCSC-VLAA/AttnGCG-attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies the vulnerabilities of transformer-based Large Language\nModels (LLMs) to jailbreaking attacks, focusing specifically on the\noptimization-based Greedy Coordinate Gradient (GCG) strategy. We first observe\na positive correlation between the effectiveness of attacks and the internal\nbehaviors of the models. For instance, attacks tend to be less effective when\nmodels pay more attention to system prompts designed to ensure LLM safety\nalignment. Building on this discovery, we introduce an enhanced method that\nmanipulates models' attention scores to facilitate LLM jailbreaking, which we\nterm AttnGCG. Empirically, AttnGCG shows consistent improvements in attack\nefficacy across diverse LLMs, achieving an average increase of ~7% in the\nLlama-2 series and ~10% in the Gemma series. Our strategy also demonstrates\nrobust attack transferability against both unseen harmful goals and black-box\nLLMs like GPT-3.5 and GPT-4. Moreover, we note our attention-score\nvisualization is more interpretable, allowing us to gain better insights into\nhow our targeted attention manipulation facilitates more effective\njailbreaking. We release the code at\nhttps://github.com/UCSC-VLAA/AttnGCG-attack."
                },
                "authors": [
                    {
                        "name": "Zijun Wang"
                    },
                    {
                        "name": "Haoqin Tu"
                    },
                    {
                        "name": "Jieru Mei"
                    },
                    {
                        "name": "Bingchen Zhao"
                    },
                    {
                        "name": "Yisen Wang"
                    },
                    {
                        "name": "Cihang Xie"
                    }
                ],
                "author_detail": {
                    "name": "Cihang Xie"
                },
                "author": "Cihang Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09038v1",
                "updated": "2024-10-11T17:54:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    54,
                    14,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:54:14Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    54,
                    14,
                    4,
                    285,
                    0
                ],
                "title": "SimpleStrat: Diversifying Language Model Generation with Stratification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimpleStrat: Diversifying Language Model Generation with Stratification"
                },
                "summary": "Generating diverse responses from large language models (LLMs) is crucial for\napplications such as planning/search and synthetic data generation, where\ndiversity provides distinct answers across generations. Prior approaches rely\non increasing temperature to increase diversity. However, contrary to popular\nbelief, we show not only does this approach produce lower quality individual\ngenerations as temperature increases, but it depends on model's next-token\nprobabilities being similar to the true distribution of answers. We propose\n\\method{}, an alternative approach that uses the language model itself to\npartition the space into strata. At inference, a random stratum is selected and\na sample drawn from within the strata. To measure diversity, we introduce\nCoverageQA, a dataset of underspecified questions with multiple equally\nplausible answers, and assess diversity by measuring KL Divergence between the\noutput distribution and uniform distribution over valid ground truth answers.\nAs computing probability per response/solution for proprietary models is\ninfeasible, we measure recall on ground truth solutions. Our evaluation show\nusing SimpleStrat achieves higher recall by 0.05 compared to GPT-4o and 0.36\naverage reduction in KL Divergence compared to Llama 3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating diverse responses from large language models (LLMs) is crucial for\napplications such as planning/search and synthetic data generation, where\ndiversity provides distinct answers across generations. Prior approaches rely\non increasing temperature to increase diversity. However, contrary to popular\nbelief, we show not only does this approach produce lower quality individual\ngenerations as temperature increases, but it depends on model's next-token\nprobabilities being similar to the true distribution of answers. We propose\n\\method{}, an alternative approach that uses the language model itself to\npartition the space into strata. At inference, a random stratum is selected and\na sample drawn from within the strata. To measure diversity, we introduce\nCoverageQA, a dataset of underspecified questions with multiple equally\nplausible answers, and assess diversity by measuring KL Divergence between the\noutput distribution and uniform distribution over valid ground truth answers.\nAs computing probability per response/solution for proprietary models is\ninfeasible, we measure recall on ground truth solutions. Our evaluation show\nusing SimpleStrat achieves higher recall by 0.05 compared to GPT-4o and 0.36\naverage reduction in KL Divergence compared to Llama 3."
                },
                "authors": [
                    {
                        "name": "Justin Wong"
                    },
                    {
                        "name": "Yury Orlovskiy"
                    },
                    {
                        "name": "Michael Luo"
                    },
                    {
                        "name": "Sanjit A. Seshia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09037v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09037v1",
                "updated": "2024-10-11T17:53:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    53,
                    27,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:53:27Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    53,
                    27,
                    4,
                    285,
                    0
                ],
                "title": "Mentor-KD: Making Small Language Models Better Multi-step Reasoners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mentor-KD: Making Small Language Models Better Multi-step Reasoners"
                },
                "summary": "Large Language Models (LLMs) have displayed remarkable performances across\nvarious complex tasks by leveraging Chain-of-Thought (CoT) prompting. Recently,\nstudies have proposed a Knowledge Distillation (KD) approach, reasoning\ndistillation, which transfers such reasoning ability of LLMs through\nfine-tuning language models of multi-step rationales generated by LLM teachers.\nHowever, they have inadequately considered two challenges regarding\ninsufficient distillation sets from the LLM teacher model, in terms of 1) data\nquality and 2) soft label provision. In this paper, we propose Mentor-KD, which\neffectively distills the multi-step reasoning capability of LLMs to smaller LMs\nwhile addressing the aforementioned challenges. Specifically, we exploit a\nmentor, intermediate-sized task-specific fine-tuned model, to augment\nadditional CoT annotations and provide soft labels for the student model during\nreasoning distillation. We conduct extensive experiments and confirm\nMentor-KD's effectiveness across various models and complex reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have displayed remarkable performances across\nvarious complex tasks by leveraging Chain-of-Thought (CoT) prompting. Recently,\nstudies have proposed a Knowledge Distillation (KD) approach, reasoning\ndistillation, which transfers such reasoning ability of LLMs through\nfine-tuning language models of multi-step rationales generated by LLM teachers.\nHowever, they have inadequately considered two challenges regarding\ninsufficient distillation sets from the LLM teacher model, in terms of 1) data\nquality and 2) soft label provision. In this paper, we propose Mentor-KD, which\neffectively distills the multi-step reasoning capability of LLMs to smaller LMs\nwhile addressing the aforementioned challenges. Specifically, we exploit a\nmentor, intermediate-sized task-specific fine-tuned model, to augment\nadditional CoT annotations and provide soft labels for the student model during\nreasoning distillation. We conduct extensive experiments and confirm\nMentor-KD's effectiveness across various models and complex reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Hojae Lee"
                    },
                    {
                        "name": "Junho Kim"
                    },
                    {
                        "name": "SangKeun Lee"
                    }
                ],
                "author_detail": {
                    "name": "SangKeun Lee"
                },
                "author": "SangKeun Lee",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09037v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09037v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09034v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09034v1",
                "updated": "2024-10-11T17:50:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    50,
                    59,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:50:59Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    50,
                    59,
                    4,
                    285,
                    0
                ],
                "title": "PEAR: A Robust and Flexible Automation Framework for Ptychography\n  Enabled by Multiple Large Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PEAR: A Robust and Flexible Automation Framework for Ptychography\n  Enabled by Multiple Large Language Model Agents"
                },
                "summary": "Ptychography is an advanced computational imaging technique in X-ray and\nelectron microscopy. It has been widely adopted across scientific research\nfields, including physics, chemistry, biology, and materials science, as well\nas in industrial applications such as semiconductor characterization. In\npractice, obtaining high-quality ptychographic images requires simultaneous\noptimization of numerous experimental and algorithmic parameters.\nTraditionally, parameter selection often relies on trial and error, leading to\nlow-throughput workflows and potential human bias. In this work, we develop the\n\"Ptychographic Experiment and Analysis Robot\" (PEAR), a framework that\nleverages large language models (LLMs) to automate data analysis in\nptychography. To ensure high robustness and accuracy, PEAR employs multiple LLM\nagents for tasks including knowledge retrieval, code generation, parameter\nrecommendation, and image reasoning. Our study demonstrates that PEAR's\nmulti-agent design significantly improves the workflow success rate, even with\nsmaller open-weight models such as LLaMA 3.1 8B. PEAR also supports various\nautomation levels and is designed to work with customized local knowledge\nbases, ensuring flexibility and adaptability across different research\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ptychography is an advanced computational imaging technique in X-ray and\nelectron microscopy. It has been widely adopted across scientific research\nfields, including physics, chemistry, biology, and materials science, as well\nas in industrial applications such as semiconductor characterization. In\npractice, obtaining high-quality ptychographic images requires simultaneous\noptimization of numerous experimental and algorithmic parameters.\nTraditionally, parameter selection often relies on trial and error, leading to\nlow-throughput workflows and potential human bias. In this work, we develop the\n\"Ptychographic Experiment and Analysis Robot\" (PEAR), a framework that\nleverages large language models (LLMs) to automate data analysis in\nptychography. To ensure high robustness and accuracy, PEAR employs multiple LLM\nagents for tasks including knowledge retrieval, code generation, parameter\nrecommendation, and image reasoning. Our study demonstrates that PEAR's\nmulti-agent design significantly improves the workflow success rate, even with\nsmaller open-weight models such as LLaMA 3.1 8B. PEAR also supports various\nautomation levels and is designed to work with customized local knowledge\nbases, ensuring flexibility and adaptability across different research\nenvironments."
                },
                "authors": [
                    {
                        "name": "Xiangyu Yin"
                    },
                    {
                        "name": "Chuqiao Shi"
                    },
                    {
                        "name": "Yimo Han"
                    },
                    {
                        "name": "Yi Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Jiang"
                },
                "author": "Yi Jiang",
                "arxiv_comment": "18 pages, 5 figures, technical preview report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09034v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09034v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02392v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02392v3",
                "updated": "2024-10-11T17:43:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    43,
                    48,
                    4,
                    285,
                    0
                ],
                "published": "2024-02-04T08:11:45Z",
                "published_parsed": [
                    2024,
                    2,
                    4,
                    8,
                    11,
                    45,
                    6,
                    35,
                    0
                ],
                "title": "DeLLMa: Decision Making Under Uncertainty with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeLLMa: Decision Making Under Uncertainty with Large Language Models"
                },
                "summary": "The potential of large language models (LLMs) as decision support tools is\nincreasingly being explored in fields such as business, engineering, and\nmedicine, which often face challenging tasks of decision-making under\nuncertainty. In this paper, we show that directly prompting LLMs on these types\nof decision-making problems can yield poor results, especially as the problem\ncomplexity increases. To aid in these tasks, we propose DeLLMa (Decision-making\nLarge Language Model assistant), a framework designed to enhance\ndecision-making accuracy in uncertain environments. DeLLMa involves a\nmulti-step reasoning procedure that integrates recent best practices in scaling\ninference-time reasoning, drawing upon principles from decision theory and\nutility theory, to provide an accurate and human-auditable decision-making\nprocess. We validate our procedure on multiple realistic decision-making\nenvironments, demonstrating that DeLLMa can consistently enhance the\ndecision-making performance of leading language models, and achieve up to a 40%\nincrease in accuracy over competing methods. Additionally, we show how\nperformance improves when scaling compute at test time, and carry out human\nevaluations to benchmark components of DeLLMa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The potential of large language models (LLMs) as decision support tools is\nincreasingly being explored in fields such as business, engineering, and\nmedicine, which often face challenging tasks of decision-making under\nuncertainty. In this paper, we show that directly prompting LLMs on these types\nof decision-making problems can yield poor results, especially as the problem\ncomplexity increases. To aid in these tasks, we propose DeLLMa (Decision-making\nLarge Language Model assistant), a framework designed to enhance\ndecision-making accuracy in uncertain environments. DeLLMa involves a\nmulti-step reasoning procedure that integrates recent best practices in scaling\ninference-time reasoning, drawing upon principles from decision theory and\nutility theory, to provide an accurate and human-auditable decision-making\nprocess. We validate our procedure on multiple realistic decision-making\nenvironments, demonstrating that DeLLMa can consistently enhance the\ndecision-making performance of leading language models, and achieve up to a 40%\nincrease in accuracy over competing methods. Additionally, we show how\nperformance improves when scaling compute at test time, and carry out human\nevaluations to benchmark components of DeLLMa."
                },
                "authors": [
                    {
                        "name": "Ollie Liu"
                    },
                    {
                        "name": "Deqing Fu"
                    },
                    {
                        "name": "Dani Yogatama"
                    },
                    {
                        "name": "Willie Neiswanger"
                    }
                ],
                "author_detail": {
                    "name": "Willie Neiswanger"
                },
                "author": "Willie Neiswanger",
                "arxiv_comment": "37 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.02392v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02392v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09024v1",
                "updated": "2024-10-11T17:39:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    39,
                    22,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:39:22Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    39,
                    22,
                    4,
                    285,
                    0
                ],
                "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents"
                },
                "summary": "The robustness of LLMs to jailbreak attacks, where users design prompts to\ncircumvent safety measures and misuse model capabilities, has been studied\nprimarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- which\nuse external tools and can execute multi-stage tasks -- may pose a greater risk\nif misused, but their robustness remains underexplored. To facilitate research\non LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark\nincludes a diverse set of 110 explicitly malicious agent tasks (440 with\naugmentations), covering 11 harm categories including fraud, cybercrime, and\nharassment. In addition to measuring whether models refuse harmful agentic\nrequests, scoring well on AgentHarm requires jailbroken agents to maintain\ntheir capabilities following an attack to complete a multi-step task. We\nevaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly\ncompliant with malicious agent requests without jailbreaking, (2) simple\nuniversal jailbreak templates can be adapted to effectively jailbreak agents,\nand (3) these jailbreaks enable coherent and malicious multi-step agent\nbehavior and retain model capabilities. We publicly release AgentHarm to enable\nsimple and reliable evaluation of attacks and defenses for LLM-based agents. We\npublicly release the benchmark at\nhttps://huggingface.co/ai-safety-institute/AgentHarm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The robustness of LLMs to jailbreak attacks, where users design prompts to\ncircumvent safety measures and misuse model capabilities, has been studied\nprimarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- which\nuse external tools and can execute multi-stage tasks -- may pose a greater risk\nif misused, but their robustness remains underexplored. To facilitate research\non LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark\nincludes a diverse set of 110 explicitly malicious agent tasks (440 with\naugmentations), covering 11 harm categories including fraud, cybercrime, and\nharassment. In addition to measuring whether models refuse harmful agentic\nrequests, scoring well on AgentHarm requires jailbroken agents to maintain\ntheir capabilities following an attack to complete a multi-step task. We\nevaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly\ncompliant with malicious agent requests without jailbreaking, (2) simple\nuniversal jailbreak templates can be adapted to effectively jailbreak agents,\nand (3) these jailbreaks enable coherent and malicious multi-step agent\nbehavior and retain model capabilities. We publicly release AgentHarm to enable\nsimple and reliable evaluation of attacks and defenses for LLM-based agents. We\npublicly release the benchmark at\nhttps://huggingface.co/ai-safety-institute/AgentHarm."
                },
                "authors": [
                    {
                        "name": "Maksym Andriushchenko"
                    },
                    {
                        "name": "Alexandra Souly"
                    },
                    {
                        "name": "Mateusz Dziemian"
                    },
                    {
                        "name": "Derek Duenas"
                    },
                    {
                        "name": "Maxwell Lin"
                    },
                    {
                        "name": "Justin Wang"
                    },
                    {
                        "name": "Dan Hendrycks"
                    },
                    {
                        "name": "Andy Zou"
                    },
                    {
                        "name": "Zico Kolter"
                    },
                    {
                        "name": "Matt Fredrikson"
                    },
                    {
                        "name": "Eric Winsor"
                    },
                    {
                        "name": "Jerome Wynne"
                    },
                    {
                        "name": "Yarin Gal"
                    },
                    {
                        "name": "Xander Davies"
                    }
                ],
                "author_detail": {
                    "name": "Xander Davies"
                },
                "author": "Xander Davies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09016v1",
                "updated": "2024-10-11T17:30:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    30,
                    28,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:30:28Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    30,
                    28,
                    4,
                    285,
                    0
                ],
                "title": "Parameter-Efficient Fine-Tuning of State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-Efficient Fine-Tuning of State Space Models"
                },
                "summary": "Deep State Space Models (SSMs), such as Mamba (Gu & Dao, 2024), have emerged\nas powerful tools for language modeling, offering high performance with\nefficient inference and linear scaling in sequence length. However, the\napplication of parameter-efficient fine-tuning (PEFT) methods to SSM-based\nmodels remains largely unexplored. This paper aims to systematically study two\nkey questions: (i) How do existing PEFT methods perform on SSM-based models?\n(ii) Which modules are most effective for fine-tuning? We conduct an empirical\nbenchmark of four basic PEFT methods on SSM-based models. Our findings reveal\nthat prompt-based methods (e.g., prefix-tuning) are no longer effective, an\nempirical result further supported by theoretical analysis. In contrast, LoRA\nremains effective for SSM-based models. We further investigate the optimal\napplication of LoRA within these models, demonstrating both theoretically and\nexperimentally that applying LoRA to linear projection matrices without\nmodifying SSM modules yields the best results, as LoRA is not effective at\ntuning SSM modules. To further improve performance, we introduce LoRA with\nSelective Dimension tuning (SDLoRA), which selectively updates certain channels\nand states on SSM modules while applying LoRA to linear projection matrices.\nExtensive experimental results show that this approach outperforms standard\nLoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep State Space Models (SSMs), such as Mamba (Gu & Dao, 2024), have emerged\nas powerful tools for language modeling, offering high performance with\nefficient inference and linear scaling in sequence length. However, the\napplication of parameter-efficient fine-tuning (PEFT) methods to SSM-based\nmodels remains largely unexplored. This paper aims to systematically study two\nkey questions: (i) How do existing PEFT methods perform on SSM-based models?\n(ii) Which modules are most effective for fine-tuning? We conduct an empirical\nbenchmark of four basic PEFT methods on SSM-based models. Our findings reveal\nthat prompt-based methods (e.g., prefix-tuning) are no longer effective, an\nempirical result further supported by theoretical analysis. In contrast, LoRA\nremains effective for SSM-based models. We further investigate the optimal\napplication of LoRA within these models, demonstrating both theoretically and\nexperimentally that applying LoRA to linear projection matrices without\nmodifying SSM modules yields the best results, as LoRA is not effective at\ntuning SSM modules. To further improve performance, we introduce LoRA with\nSelective Dimension tuning (SDLoRA), which selectively updates certain channels\nand states on SSM modules while applying LoRA to linear projection matrices.\nExtensive experimental results show that this approach outperforms standard\nLoRA."
                },
                "authors": [
                    {
                        "name": "Kevin Galim"
                    },
                    {
                        "name": "Wonjun Kang"
                    },
                    {
                        "name": "Yuchen Zeng"
                    },
                    {
                        "name": "Hyung Il Koo"
                    },
                    {
                        "name": "Kangwook Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kangwook Lee"
                },
                "author": "Kangwook Lee",
                "arxiv_comment": "Code is available at https://github.com/furiosa-ai/ssm-peft",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09013v1",
                "updated": "2024-10-11T17:30:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    30,
                    2,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:30:02Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    30,
                    2,
                    4,
                    285,
                    0
                ],
                "title": "The Impact of Visual Information in Chinese Characters: Evaluating Large\n  Models' Ability to Recognize and Utilize Radicals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Visual Information in Chinese Characters: Evaluating Large\n  Models' Ability to Recognize and Utilize Radicals"
                },
                "summary": "The glyphic writing system of Chinese incorporates information-rich visual\nfeatures in each character, such as radicals that provide hints about meaning\nor pronunciation. However, there has been no investigation into whether\ncontemporary Large Language Models (LLMs) and Vision-Language Models (VLMs) can\nharness these sub-character features in Chinese through prompting. In this\nstudy, we establish a benchmark to evaluate LLMs' and VLMs' understanding of\nvisual elements in Chinese characters, including radicals, composition\nstructures, strokes, and stroke counts. Our results reveal that models\nsurprisingly exhibit some, but still limited, knowledge of the visual\ninformation, regardless of whether images of characters are provided. To incite\nmodels' ability to use radicals, we further experiment with incorporating\nradicals into the prompts for Chinese language understanding tasks. We observe\nconsistent improvement in Part-Of-Speech tagging when providing additional\ninformation about radicals, suggesting the potential to enhance CLP by\nintegrating sub-character information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The glyphic writing system of Chinese incorporates information-rich visual\nfeatures in each character, such as radicals that provide hints about meaning\nor pronunciation. However, there has been no investigation into whether\ncontemporary Large Language Models (LLMs) and Vision-Language Models (VLMs) can\nharness these sub-character features in Chinese through prompting. In this\nstudy, we establish a benchmark to evaluate LLMs' and VLMs' understanding of\nvisual elements in Chinese characters, including radicals, composition\nstructures, strokes, and stroke counts. Our results reveal that models\nsurprisingly exhibit some, but still limited, knowledge of the visual\ninformation, regardless of whether images of characters are provided. To incite\nmodels' ability to use radicals, we further experiment with incorporating\nradicals into the prompts for Chinese language understanding tasks. We observe\nconsistent improvement in Part-Of-Speech tagging when providing additional\ninformation about radicals, suggesting the potential to enhance CLP by\nintegrating sub-character information."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Wu"
                    },
                    {
                        "name": "Karl Stratos"
                    },
                    {
                        "name": "Wei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xu"
                },
                "author": "Wei Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09012v1",
                "updated": "2024-10-11T17:27:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    27,
                    4,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:27:04Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    27,
                    4,
                    4,
                    285,
                    0
                ],
                "title": "Software Engineering and Foundation Models: Insights from Industry Blogs\n  Using a Jury of Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software Engineering and Foundation Models: Insights from Industry Blogs\n  Using a Jury of Foundation Models"
                },
                "summary": "Foundation models (FMs) such as large language models (LLMs) have\nsignificantly impacted many fields, including software engineering (SE). The\ninteraction between SE and FMs has led to the integration of FMs into SE\npractices (FM4SE) and the application of SE methodologies to FMs (SE4FM). While\nseveral literature surveys exist on academic contributions to these trends, we\nare the first to provide a practitioner's view. We analyze 155 FM4SE and 997\nSE4FM blog posts from leading technology companies, leveraging an FM-powered\nsurveying approach to systematically label and summarize the discussed\nactivities and tasks. We observed that while code generation is the most\nprominent FM4SE task, FMs are leveraged for many other SE activities such as\ncode understanding, summarization, and API recommendation. The majority of blog\nposts on SE4FM are about model deployment & operation, and system architecture\n& orchestration. Although the emphasis is on cloud deployments, there is a\ngrowing interest in compressing FMs and deploying them on smaller devices such\nas edge or mobile devices. We outline eight future research directions inspired\nby our gained insights, aiming to bridge the gap between academic findings and\nreal-world applications. Our study not only enriches the body of knowledge on\npractical applications of FM4SE and SE4FM but also demonstrates the utility of\nFMs as a powerful and efficient approach in conducting literature surveys\nwithin technical and grey literature domains. Our dataset, results, code and\nused prompts can be found in our online replication package at\nhttps://github.com/SAILResearch/fmse-blogs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models (FMs) such as large language models (LLMs) have\nsignificantly impacted many fields, including software engineering (SE). The\ninteraction between SE and FMs has led to the integration of FMs into SE\npractices (FM4SE) and the application of SE methodologies to FMs (SE4FM). While\nseveral literature surveys exist on academic contributions to these trends, we\nare the first to provide a practitioner's view. We analyze 155 FM4SE and 997\nSE4FM blog posts from leading technology companies, leveraging an FM-powered\nsurveying approach to systematically label and summarize the discussed\nactivities and tasks. We observed that while code generation is the most\nprominent FM4SE task, FMs are leveraged for many other SE activities such as\ncode understanding, summarization, and API recommendation. The majority of blog\nposts on SE4FM are about model deployment & operation, and system architecture\n& orchestration. Although the emphasis is on cloud deployments, there is a\ngrowing interest in compressing FMs and deploying them on smaller devices such\nas edge or mobile devices. We outline eight future research directions inspired\nby our gained insights, aiming to bridge the gap between academic findings and\nreal-world applications. Our study not only enriches the body of knowledge on\npractical applications of FM4SE and SE4FM but also demonstrates the utility of\nFMs as a powerful and efficient approach in conducting literature surveys\nwithin technical and grey literature domains. Our dataset, results, code and\nused prompts can be found in our online replication package at\nhttps://github.com/SAILResearch/fmse-blogs."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Cor-Paul Bezemer"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09008v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09008v1",
                "updated": "2024-10-11T17:25:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    25,
                    52,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:25:52Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    25,
                    52,
                    4,
                    285,
                    0
                ],
                "title": "SuperCorrect: Supervising and Correcting Language Models with\n  Error-Driven Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperCorrect: Supervising and Correcting Language Models with\n  Error-Driven Insights"
                },
                "summary": "Large language models (LLMs) like GPT-4, PaLM, and LLaMA have shown\nsignificant improvements in various reasoning tasks. However, smaller models\nsuch as Llama-3-8B and DeepSeekMath-Base still struggle with complex\nmathematical reasoning because they fail to effectively identify and correct\nreasoning errors. Recent reflection-based methods aim to address these issues\nby enabling self-reflection and self-correction, but they still face challenges\nin independently detecting errors in their reasoning steps. To overcome these\nlimitations, we propose SuperCorrect, a novel two-stage framework that uses a\nlarge teacher model to supervise and correct both the reasoning and reflection\nprocesses of a smaller student model. In the first stage, we extract\nhierarchical high-level and detailed thought templates from the teacher model\nto guide the student model in eliciting more fine-grained reasoning thoughts.\nIn the second stage, we introduce cross-model collaborative direct preference\noptimization (DPO) to enhance the self-correction abilities of the student\nmodel by following the teacher's correction traces during training. This\ncross-model DPO approach teaches the student model to effectively locate and\nresolve erroneous thoughts with error-driven insights from the teacher model,\nbreaking the bottleneck of its thoughts and acquiring new skills and knowledge\nto tackle challenging problems. Extensive experiments consistently demonstrate\nour superiority over previous methods. Notably, our SuperCorrect-7B model\nsignificantly surpasses powerful DeepSeekMath-7B by 7.8%/5.3% and\nQwen2.5-Math-7B by 15.1%/6.3% on MATH/GSM8K benchmarks, achieving new SOTA\nperformance among all 7B models. Code:\nhttps://github.com/YangLing0818/SuperCorrect-llm",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) like GPT-4, PaLM, and LLaMA have shown\nsignificant improvements in various reasoning tasks. However, smaller models\nsuch as Llama-3-8B and DeepSeekMath-Base still struggle with complex\nmathematical reasoning because they fail to effectively identify and correct\nreasoning errors. Recent reflection-based methods aim to address these issues\nby enabling self-reflection and self-correction, but they still face challenges\nin independently detecting errors in their reasoning steps. To overcome these\nlimitations, we propose SuperCorrect, a novel two-stage framework that uses a\nlarge teacher model to supervise and correct both the reasoning and reflection\nprocesses of a smaller student model. In the first stage, we extract\nhierarchical high-level and detailed thought templates from the teacher model\nto guide the student model in eliciting more fine-grained reasoning thoughts.\nIn the second stage, we introduce cross-model collaborative direct preference\noptimization (DPO) to enhance the self-correction abilities of the student\nmodel by following the teacher's correction traces during training. This\ncross-model DPO approach teaches the student model to effectively locate and\nresolve erroneous thoughts with error-driven insights from the teacher model,\nbreaking the bottleneck of its thoughts and acquiring new skills and knowledge\nto tackle challenging problems. Extensive experiments consistently demonstrate\nour superiority over previous methods. Notably, our SuperCorrect-7B model\nsignificantly surpasses powerful DeepSeekMath-7B by 7.8%/5.3% and\nQwen2.5-Math-7B by 15.1%/6.3% on MATH/GSM8K benchmarks, achieving new SOTA\nperformance among all 7B models. Code:\nhttps://github.com/YangLing0818/SuperCorrect-llm"
                },
                "authors": [
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Zhaochen Yu"
                    },
                    {
                        "name": "Tianjun Zhang"
                    },
                    {
                        "name": "Minkai Xu"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Shuicheng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Shuicheng Yan"
                },
                "author": "Shuicheng Yan",
                "arxiv_comment": "Project: https://github.com/YangLing0818/SuperCorrect-llm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09008v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09006v1",
                "updated": "2024-10-11T17:24:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    24,
                    0,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:24:00Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    24,
                    0,
                    4,
                    285,
                    0
                ],
                "title": "From Interaction to Impact: Towards Safer AI Agents Through\n  Understanding and Evaluating UI Operation Impacts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Interaction to Impact: Towards Safer AI Agents Through\n  Understanding and Evaluating UI Operation Impacts"
                },
                "summary": "With advances in generative AI, there is increasing work towards creating\nautonomous agents that can manage daily tasks by operating user interfaces\n(UIs). While prior research has studied the mechanics of how AI agents might\nnavigate UIs and understand UI structure, the effects of agents and their\nautonomous actions-particularly those that may be risky or irreversible-remain\nunder-explored. In this work, we investigate the real-world impacts and\nconsequences of UI actions by AI agents. We began by developing a taxonomy of\nthe impacts of UI actions through a series of workshops with domain experts.\nFollowing this, we conducted a data synthesis study to gather realistic UI\nscreen traces and action data that users perceive as impactful. We then used\nour impact categories to annotate our collected data and data repurposed from\nexisting UI navigation datasets. Our quantitative evaluations of different\nlarge language models (LLMs) and variants demonstrate how well different LLMs\ncan understand the impacts of UI actions that might be taken by an agent. We\nshow that our taxonomy enhances the reasoning capabilities of these LLMs for\nunderstanding the impacts of UI actions, but our findings also reveal\nsignificant gaps in their ability to reliably classify more nuanced or complex\ncategories of impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With advances in generative AI, there is increasing work towards creating\nautonomous agents that can manage daily tasks by operating user interfaces\n(UIs). While prior research has studied the mechanics of how AI agents might\nnavigate UIs and understand UI structure, the effects of agents and their\nautonomous actions-particularly those that may be risky or irreversible-remain\nunder-explored. In this work, we investigate the real-world impacts and\nconsequences of UI actions by AI agents. We began by developing a taxonomy of\nthe impacts of UI actions through a series of workshops with domain experts.\nFollowing this, we conducted a data synthesis study to gather realistic UI\nscreen traces and action data that users perceive as impactful. We then used\nour impact categories to annotate our collected data and data repurposed from\nexisting UI navigation datasets. Our quantitative evaluations of different\nlarge language models (LLMs) and variants demonstrate how well different LLMs\ncan understand the impacts of UI actions that might be taken by an agent. We\nshow that our taxonomy enhances the reasoning capabilities of these LLMs for\nunderstanding the impacts of UI actions, but our findings also reveal\nsignificant gaps in their ability to reliably classify more nuanced or complex\ncategories of impact."
                },
                "authors": [
                    {
                        "name": "Zhuohao Jerry Zhang"
                    },
                    {
                        "name": "Eldon Schoop"
                    },
                    {
                        "name": "Jeffrey Nichols"
                    },
                    {
                        "name": "Anuj Mahajan"
                    },
                    {
                        "name": "Amanda Swearngin"
                    }
                ],
                "author_detail": {
                    "name": "Amanda Swearngin"
                },
                "author": "Amanda Swearngin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04482v2",
                "updated": "2024-10-11T17:21:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    21,
                    40,
                    4,
                    285,
                    0
                ],
                "published": "2024-07-05T13:04:31Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    13,
                    4,
                    31,
                    4,
                    187,
                    0
                ],
                "title": "Controlling Whisper: Universal Acoustic Adversarial Attacks to Control\n  Speech Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling Whisper: Universal Acoustic Adversarial Attacks to Control\n  Speech Foundation Models"
                },
                "summary": "Speech enabled foundation models, either in the form of flexible speech\nrecognition based systems or audio-prompted large language models (LLMs), are\nbecoming increasingly popular. One of the interesting aspects of these models\nis their ability to perform tasks other than automatic speech recognition (ASR)\nusing an appropriate prompt. For example, the OpenAI Whisper model can perform\nboth speech transcription and speech translation. With the development of\naudio-prompted LLMs there is the potential for even greater control options. In\nthis work we demonstrate that with this greater flexibility the systems can be\nsusceptible to model-control adversarial attacks. Without any access to the\nmodel prompt it is possible to modify the behaviour of the system by\nappropriately changing the audio input. To illustrate this risk, we demonstrate\nthat it is possible to prepend a short universal adversarial acoustic segment\nto any input speech signal to override the prompt setting of an ASR foundation\nmodel. Specifically, we successfully use a universal adversarial acoustic\nsegment to control Whisper to always perform speech translation, despite being\nset to perform speech transcription. Overall, this work demonstrates a new form\nof adversarial attack on multi-tasking speech enabled foundation models that\nneeds to be considered prior to the deployment of this form of model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech enabled foundation models, either in the form of flexible speech\nrecognition based systems or audio-prompted large language models (LLMs), are\nbecoming increasingly popular. One of the interesting aspects of these models\nis their ability to perform tasks other than automatic speech recognition (ASR)\nusing an appropriate prompt. For example, the OpenAI Whisper model can perform\nboth speech transcription and speech translation. With the development of\naudio-prompted LLMs there is the potential for even greater control options. In\nthis work we demonstrate that with this greater flexibility the systems can be\nsusceptible to model-control adversarial attacks. Without any access to the\nmodel prompt it is possible to modify the behaviour of the system by\nappropriately changing the audio input. To illustrate this risk, we demonstrate\nthat it is possible to prepend a short universal adversarial acoustic segment\nto any input speech signal to override the prompt setting of an ASR foundation\nmodel. Specifically, we successfully use a universal adversarial acoustic\nsegment to control Whisper to always perform speech translation, despite being\nset to perform speech transcription. Overall, this work demonstrates a new form\nof adversarial attack on multi-tasking speech enabled foundation models that\nneeds to be considered prior to the deployment of this form of model."
                },
                "authors": [
                    {
                        "name": "Vyas Raina"
                    },
                    {
                        "name": "Mark Gales"
                    }
                ],
                "author_detail": {
                    "name": "Mark Gales"
                },
                "author": "Mark Gales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00467v2",
                "updated": "2024-10-11T17:21:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    21,
                    34,
                    4,
                    285,
                    0
                ],
                "published": "2024-03-30T20:20:54Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    20,
                    20,
                    54,
                    5,
                    90,
                    0
                ],
                "title": "Characterization of uncertainties in electron-argon collision cross\n  sections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterization of uncertainties in electron-argon collision cross\n  sections"
                },
                "summary": "The predictive capability of a plasma discharge model depends on accurate\nrepresentations of electron-impact collision cross sections, which determine\nthe key reaction rates and transport properties of the plasma. Although many\ncross sections have been identified through experiments and quantum mechanical\nsimulations, their uncertainties are not well-investigated. We characterize the\nuncertainties in electron-argon collision cross sections using a Bayesian\nframework. Six collision processes -- elastic momentum transfer, ionization,\nand four excitations -- are characterized with semi-empirical models, whose\nparametric uncertainties effectively capture the features important to the\nmacroscopic properties of the plasma, namely transport properties and chemical\nreaction rates. The method is designed to capture the effects of systematic\nerrors that lead to large discrepancies between some data sets. Specifically,\nfor the purposes of Bayesian inference, each of the parametric cross section\nmodels is augmented with a Gaussian process representing systematic measurement\nerrors as well as model inadequacies in the parametric form. The results show\nthat the method is able to capture scatter in the data between the\nelectron-beam experiments and ab-initio quantum simulations. The calibrated\ncross section models are further validated against measurements from\nswarm-parameter experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The predictive capability of a plasma discharge model depends on accurate\nrepresentations of electron-impact collision cross sections, which determine\nthe key reaction rates and transport properties of the plasma. Although many\ncross sections have been identified through experiments and quantum mechanical\nsimulations, their uncertainties are not well-investigated. We characterize the\nuncertainties in electron-argon collision cross sections using a Bayesian\nframework. Six collision processes -- elastic momentum transfer, ionization,\nand four excitations -- are characterized with semi-empirical models, whose\nparametric uncertainties effectively capture the features important to the\nmacroscopic properties of the plasma, namely transport properties and chemical\nreaction rates. The method is designed to capture the effects of systematic\nerrors that lead to large discrepancies between some data sets. Specifically,\nfor the purposes of Bayesian inference, each of the parametric cross section\nmodels is augmented with a Gaussian process representing systematic measurement\nerrors as well as model inadequacies in the parametric form. The results show\nthat the method is able to capture scatter in the data between the\nelectron-beam experiments and ab-initio quantum simulations. The calibrated\ncross section models are further validated against measurements from\nswarm-parameter experiments."
                },
                "authors": [
                    {
                        "name": "Seung Whan Chung"
                    },
                    {
                        "name": "Todd A. Oliver"
                    },
                    {
                        "name": "Laxminarayan L. Raja"
                    },
                    {
                        "name": "Robert D. Moser"
                    }
                ],
                "author_detail": {
                    "name": "Robert D. Moser"
                },
                "author": "Robert D. Moser",
                "arxiv_comment": "40 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09004v1",
                "updated": "2024-10-11T17:20:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    20,
                    4,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:20:04Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    20,
                    4,
                    4,
                    285,
                    0
                ],
                "title": "DA-Ada: Learning Domain-Aware Adapter for Domain Adaptive Object\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DA-Ada: Learning Domain-Aware Adapter for Domain Adaptive Object\n  Detection"
                },
                "summary": "Domain adaptive object detection (DAOD) aims to generalize detectors trained\non an annotated source domain to an unlabelled target domain. As the\nvisual-language models (VLMs) can provide essential general knowledge on unseen\nimages, freezing the visual encoder and inserting a domain-agnostic adapter can\nlearn domain-invariant knowledge for DAOD. However, the domain-agnostic adapter\nis inevitably biased to the source domain. It discards some beneficial\nknowledge discriminative on the unlabelled domain, i.e., domain-specific\nknowledge of the target domain. To solve the issue, we propose a novel\nDomain-Aware Adapter (DA-Ada) tailored for the DAOD task. The key point is\nexploiting domain-specific knowledge between the essential general knowledge\nand domain-invariant knowledge. DA-Ada consists of the Domain-Invariant Adapter\n(DIA) for learning domain-invariant knowledge and the Domain-Specific Adapter\n(DSA) for injecting the domain-specific knowledge from the information\ndiscarded by the visual encoder. Comprehensive experiments over multiple DAOD\ntasks show that DA-Ada can efficiently infer a domain-aware visual encoder for\nboosting domain adaptive object detection. Our code is available at\nhttps://github.com/Therock90421/DA-Ada.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain adaptive object detection (DAOD) aims to generalize detectors trained\non an annotated source domain to an unlabelled target domain. As the\nvisual-language models (VLMs) can provide essential general knowledge on unseen\nimages, freezing the visual encoder and inserting a domain-agnostic adapter can\nlearn domain-invariant knowledge for DAOD. However, the domain-agnostic adapter\nis inevitably biased to the source domain. It discards some beneficial\nknowledge discriminative on the unlabelled domain, i.e., domain-specific\nknowledge of the target domain. To solve the issue, we propose a novel\nDomain-Aware Adapter (DA-Ada) tailored for the DAOD task. The key point is\nexploiting domain-specific knowledge between the essential general knowledge\nand domain-invariant knowledge. DA-Ada consists of the Domain-Invariant Adapter\n(DIA) for learning domain-invariant knowledge and the Domain-Specific Adapter\n(DSA) for injecting the domain-specific knowledge from the information\ndiscarded by the visual encoder. Comprehensive experiments over multiple DAOD\ntasks show that DA-Ada can efficiently infer a domain-aware visual encoder for\nboosting domain adaptive object detection. Our code is available at\nhttps://github.com/Therock90421/DA-Ada."
                },
                "authors": [
                    {
                        "name": "Haochen Li"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Hantao Yao"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Yifan Hao"
                    },
                    {
                        "name": "Xinkai Song"
                    },
                    {
                        "name": "Xiaqing Li"
                    },
                    {
                        "name": "Yongwei Zhao"
                    },
                    {
                        "name": "Ling Li"
                    },
                    {
                        "name": "Yunji Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yunji Chen"
                },
                "author": "Yunji Chen",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06800v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06800v3",
                "updated": "2024-10-11T17:16:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    16,
                    30,
                    4,
                    285,
                    0
                ],
                "published": "2024-05-10T20:23:46Z",
                "published_parsed": [
                    2024,
                    5,
                    10,
                    20,
                    23,
                    46,
                    4,
                    131,
                    0
                ],
                "title": "LLM-Generated Black-box Explanations Can Be Adversarially Helpful",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Generated Black-box Explanations Can Be Adversarially Helpful"
                },
                "summary": "Large Language Models (LLMs) are becoming vital tools that help us solve and\nunderstand complex problems by acting as digital assistants. LLMs can generate\nconvincing explanations, even when only given the inputs and outputs of these\nproblems, i.e., in a ``black-box'' approach. However, our research uncovers a\nhidden risk tied to this approach, which we call *adversarial helpfulness*.\nThis happens when an LLM's explanations make a wrong answer look right,\npotentially leading people to trust incorrect solutions. In this paper, we show\nthat this issue affects not just humans, but also LLM evaluators. Digging\ndeeper, we identify and examine key persuasive strategies employed by LLMs. Our\nfindings reveal that these models employ strategies such as reframing the\nquestions, expressing an elevated level of confidence, and cherry-picking\nevidence to paint misleading answers in a credible light. To examine if LLMs\nare able to navigate complex-structured knowledge when generating adversarially\nhelpful explanations, we create a special task based on navigating through\ngraphs. Most LLMs are not able to find alternative paths along simple graphs,\nindicating that their misleading explanations aren't produced by only logical\ndeductions using complex knowledge. These findings shed light on the\nlimitations of the black-box explanation setting and allow us to provide advice\non the safe usage of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are becoming vital tools that help us solve and\nunderstand complex problems by acting as digital assistants. LLMs can generate\nconvincing explanations, even when only given the inputs and outputs of these\nproblems, i.e., in a ``black-box'' approach. However, our research uncovers a\nhidden risk tied to this approach, which we call *adversarial helpfulness*.\nThis happens when an LLM's explanations make a wrong answer look right,\npotentially leading people to trust incorrect solutions. In this paper, we show\nthat this issue affects not just humans, but also LLM evaluators. Digging\ndeeper, we identify and examine key persuasive strategies employed by LLMs. Our\nfindings reveal that these models employ strategies such as reframing the\nquestions, expressing an elevated level of confidence, and cherry-picking\nevidence to paint misleading answers in a credible light. To examine if LLMs\nare able to navigate complex-structured knowledge when generating adversarially\nhelpful explanations, we create a special task based on navigating through\ngraphs. Most LLMs are not able to find alternative paths along simple graphs,\nindicating that their misleading explanations aren't produced by only logical\ndeductions using complex knowledge. These findings shed light on the\nlimitations of the black-box explanation setting and allow us to provide advice\non the safe usage of LLMs."
                },
                "authors": [
                    {
                        "name": "Rohan Ajwani"
                    },
                    {
                        "name": "Shashidhar Reddy Javaji"
                    },
                    {
                        "name": "Frank Rudzicz"
                    },
                    {
                        "name": "Zining Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Zining Zhu"
                },
                "author": "Zining Zhu",
                "arxiv_comment": "NeurIPS Regulatable ML Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06800v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06800v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08996v1",
                "updated": "2024-10-11T17:09:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    9,
                    22,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:09:22Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    9,
                    22,
                    4,
                    285,
                    0
                ],
                "title": "Hypothesis-only Biases in Large Language Model-Elicited Natural Language\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hypothesis-only Biases in Large Language Model-Elicited Natural Language\n  Inference"
                },
                "summary": "We test whether replacing crowdsource workers with LLMs to write Natural\nLanguage Inference (NLI) hypotheses similarly results in annotation artifacts.\nWe recreate a portion of the Stanford NLI corpus using GPT-4, Llama-2 and\nMistral 7b, and train hypothesis-only classifiers to determine whether\nLLM-elicited hypotheses contain annotation artifacts. On our LLM-elicited NLI\ndatasets, BERT-based hypothesis-only classifiers achieve between 86-96%\naccuracy, indicating these datasets contain hypothesis-only artifacts. We also\nfind frequent \"give-aways\" in LLM-generated hypotheses, e.g. the phrase\n\"swimming in a pool\" appears in more than 10,000 contradictions generated by\nGPT-4. Our analysis provides empirical evidence that well-attested biases in\nNLI can persist in LLM-generated data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We test whether replacing crowdsource workers with LLMs to write Natural\nLanguage Inference (NLI) hypotheses similarly results in annotation artifacts.\nWe recreate a portion of the Stanford NLI corpus using GPT-4, Llama-2 and\nMistral 7b, and train hypothesis-only classifiers to determine whether\nLLM-elicited hypotheses contain annotation artifacts. On our LLM-elicited NLI\ndatasets, BERT-based hypothesis-only classifiers achieve between 86-96%\naccuracy, indicating these datasets contain hypothesis-only artifacts. We also\nfind frequent \"give-aways\" in LLM-generated hypotheses, e.g. the phrase\n\"swimming in a pool\" appears in more than 10,000 contradictions generated by\nGPT-4. Our analysis provides empirical evidence that well-attested biases in\nNLI can persist in LLM-generated data."
                },
                "authors": [
                    {
                        "name": "Grace Proebsting"
                    },
                    {
                        "name": "Adam Poliak"
                    }
                ],
                "author_detail": {
                    "name": "Adam Poliak"
                },
                "author": "Adam Poliak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08991v1",
                "updated": "2024-10-11T17:03:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    3,
                    13,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:03:13Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    3,
                    13,
                    4,
                    285,
                    0
                ],
                "title": "Science is Exploration: Computational Frontiers for Conceptual Metaphor\n  Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Science is Exploration: Computational Frontiers for Conceptual Metaphor\n  Theory"
                },
                "summary": "Metaphors are everywhere. They appear extensively across all domains of\nnatural language, from the most sophisticated poetry to seemingly dry academic\nprose. A significant body of research in the cognitive science of language\nargues for the existence of conceptual metaphors, the systematic structuring of\none domain of experience in the language of another. Conceptual metaphors are\nnot simply rhetorical flourishes but are crucial evidence of the role of\nanalogical reasoning in human cognition. In this paper, we ask whether Large\nLanguage Models (LLMs) can accurately identify and explain the presence of such\nconceptual metaphors in natural language data. Using a novel prompting\ntechnique based on metaphor annotation guidelines, we demonstrate that LLMs are\na promising tool for large-scale computational research on conceptual\nmetaphors. Further, we show that LLMs are able to apply procedural guidelines\ndesigned for human annotators, displaying a surprising depth of linguistic\nknowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metaphors are everywhere. They appear extensively across all domains of\nnatural language, from the most sophisticated poetry to seemingly dry academic\nprose. A significant body of research in the cognitive science of language\nargues for the existence of conceptual metaphors, the systematic structuring of\none domain of experience in the language of another. Conceptual metaphors are\nnot simply rhetorical flourishes but are crucial evidence of the role of\nanalogical reasoning in human cognition. In this paper, we ask whether Large\nLanguage Models (LLMs) can accurately identify and explain the presence of such\nconceptual metaphors in natural language data. Using a novel prompting\ntechnique based on metaphor annotation guidelines, we demonstrate that LLMs are\na promising tool for large-scale computational research on conceptual\nmetaphors. Further, we show that LLMs are able to apply procedural guidelines\ndesigned for human annotators, displaying a surprising depth of linguistic\nknowledge."
                },
                "authors": [
                    {
                        "name": "Rebecca M. M. Hicke"
                    },
                    {
                        "name": "Ross Deans Kristensen-McLachlan"
                    }
                ],
                "author_detail": {
                    "name": "Ross Deans Kristensen-McLachlan"
                },
                "author": "Ross Deans Kristensen-McLachlan",
                "arxiv_comment": "Accepted to the 2024 Computational Humanities Research Conference\n  (CHR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08989v1",
                "updated": "2024-10-11T17:01:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    1,
                    43,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:01:43Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    1,
                    43,
                    4,
                    285,
                    0
                ],
                "title": "SubZero: Random Subspace Zeroth-Order Optimization for Memory-Efficient\n  LLM Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SubZero: Random Subspace Zeroth-Order Optimization for Memory-Efficient\n  LLM Fine-Tuning"
                },
                "summary": "Fine-tuning Large Language Models (LLMs) has proven effective for a variety\nof downstream tasks. However, as LLMs grow in size, the memory demands for\nbackpropagation become increasingly prohibitive. Zeroth-order (ZO) optimization\nmethods offer a memory-efficient alternative by using forward passes to\nestimate gradients, but the variance of gradient estimates typically scales\nlinearly with the model's parameter dimension$\\unicode{x2013}$a significant\nissue for LLMs. In this paper, we propose the random Subspace Zeroth-order\n(SubZero) optimization to address the challenges posed by LLMs' high\ndimensionality. We introduce a low-rank perturbation tailored for LLMs that\nsignificantly reduces memory consumption while improving training performance.\nAdditionally, we prove that our gradient estimation closely approximates the\nbackpropagation gradient, exhibits lower variance than traditional ZO methods,\nand ensures convergence when combined with SGD. Experimental results show that\nSubZero enhances fine-tuning performance and achieves faster convergence\ncompared to standard ZO approaches like MeZO across various language modeling\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Large Language Models (LLMs) has proven effective for a variety\nof downstream tasks. However, as LLMs grow in size, the memory demands for\nbackpropagation become increasingly prohibitive. Zeroth-order (ZO) optimization\nmethods offer a memory-efficient alternative by using forward passes to\nestimate gradients, but the variance of gradient estimates typically scales\nlinearly with the model's parameter dimension$\\unicode{x2013}$a significant\nissue for LLMs. In this paper, we propose the random Subspace Zeroth-order\n(SubZero) optimization to address the challenges posed by LLMs' high\ndimensionality. We introduce a low-rank perturbation tailored for LLMs that\nsignificantly reduces memory consumption while improving training performance.\nAdditionally, we prove that our gradient estimation closely approximates the\nbackpropagation gradient, exhibits lower variance than traditional ZO methods,\nand ensures convergence when combined with SGD. Experimental results show that\nSubZero enhances fine-tuning performance and achieves faster convergence\ncompared to standard ZO approaches like MeZO across various language modeling\ntasks."
                },
                "authors": [
                    {
                        "name": "Ziming Yu"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Sike Wang"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Hua Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hua Huang"
                },
                "author": "Hua Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08985v1",
                "updated": "2024-10-11T16:57:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    57,
                    30,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T16:57:30Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    57,
                    30,
                    4,
                    285,
                    0
                ],
                "title": "Towards Trustworthy Knowledge Graph Reasoning: An Uncertainty Aware\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Trustworthy Knowledge Graph Reasoning: An Uncertainty Aware\n  Perspective"
                },
                "summary": "Recently, Knowledge Graphs (KGs) have been successfully coupled with Large\nLanguage Models (LLMs) to mitigate their hallucinations and enhance their\nreasoning capability, such as in KG-based retrieval-augmented frameworks.\nHowever, current KG-LLM frameworks lack rigorous uncertainty estimation,\nlimiting their reliable deployment in high-stakes applications. Directly\nincorporating uncertainty quantification into KG-LLM frameworks presents\nchallenges due to their complex architectures and the intricate interactions\nbetween the knowledge graph and language model components. To address this gap,\nwe propose a new trustworthy KG-LLM framework, Uncertainty Aware\nKnowledge-Graph Reasoning (UAG), which incorporates uncertainty quantification\ninto the KG-LLM framework. We design an uncertainty-aware multi-step reasoning\nframework that leverages conformal prediction to provide a theoretical\nguarantee on the prediction set. To manage the error rate of the multi-step\nprocess, we additionally introduce an error rate control module to adjust the\nerror rate within the individual components. Extensive experiments show that\nour proposed UAG can achieve any pre-defined coverage rate while reducing the\nprediction set/interval size by 40% on average over the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Knowledge Graphs (KGs) have been successfully coupled with Large\nLanguage Models (LLMs) to mitigate their hallucinations and enhance their\nreasoning capability, such as in KG-based retrieval-augmented frameworks.\nHowever, current KG-LLM frameworks lack rigorous uncertainty estimation,\nlimiting their reliable deployment in high-stakes applications. Directly\nincorporating uncertainty quantification into KG-LLM frameworks presents\nchallenges due to their complex architectures and the intricate interactions\nbetween the knowledge graph and language model components. To address this gap,\nwe propose a new trustworthy KG-LLM framework, Uncertainty Aware\nKnowledge-Graph Reasoning (UAG), which incorporates uncertainty quantification\ninto the KG-LLM framework. We design an uncertainty-aware multi-step reasoning\nframework that leverages conformal prediction to provide a theoretical\nguarantee on the prediction set. To manage the error rate of the multi-step\nprocess, we additionally introduce an error rate control module to adjust the\nerror rate within the individual components. Extensive experiments show that\nour proposed UAG can achieve any pre-defined coverage rate while reducing the\nprediction set/interval size by 40% on average over the baselines."
                },
                "authors": [
                    {
                        "name": "Bo Ni"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Lu Cheng"
                    },
                    {
                        "name": "Erik Blasch"
                    },
                    {
                        "name": "Tyler Derr"
                    }
                ],
                "author_detail": {
                    "name": "Tyler Derr"
                },
                "author": "Tyler Derr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08976v1",
                "updated": "2024-10-11T16:48:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    48,
                    32,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T16:48:32Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    48,
                    32,
                    4,
                    285,
                    0
                ],
                "title": "Learning Representations of Instruments for Partial Identification of\n  Treatment Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Representations of Instruments for Partial Identification of\n  Treatment Effects"
                },
                "summary": "Reliable estimation of treatment effects from observational data is important\nin many disciplines such as medicine. However, estimation is challenging when\nunconfoundedness as a standard assumption in the causal inference literature is\nviolated. In this work, we leverage arbitrary (potentially high-dimensional)\ninstruments to estimate bounds on the conditional average treatment effect\n(CATE). Our contributions are three-fold: (1) We propose a novel approach for\npartial identification through a mapping of instruments to a discrete\nrepresentation space so that we yield valid bounds on the CATE. This is crucial\nfor reliable decision-making in real-world applications. (2) We derive a\ntwo-step procedure that learns tight bounds using a tailored neural\npartitioning of the latent instrument space. As a result, we avoid instability\nissues due to numerical approximations or adversarial training. Furthermore,\nour procedure aims to reduce the estimation variance in finite-sample settings\nto yield more reliable estimates. (3) We show theoretically that our procedure\nobtains valid bounds while reducing estimation variance. We further perform\nextensive experiments to demonstrate the effectiveness across various settings.\nOverall, our procedure offers a novel path for practitioners to make use of\npotentially high-dimensional instruments (e.g., as in Mendelian randomization).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable estimation of treatment effects from observational data is important\nin many disciplines such as medicine. However, estimation is challenging when\nunconfoundedness as a standard assumption in the causal inference literature is\nviolated. In this work, we leverage arbitrary (potentially high-dimensional)\ninstruments to estimate bounds on the conditional average treatment effect\n(CATE). Our contributions are three-fold: (1) We propose a novel approach for\npartial identification through a mapping of instruments to a discrete\nrepresentation space so that we yield valid bounds on the CATE. This is crucial\nfor reliable decision-making in real-world applications. (2) We derive a\ntwo-step procedure that learns tight bounds using a tailored neural\npartitioning of the latent instrument space. As a result, we avoid instability\nissues due to numerical approximations or adversarial training. Furthermore,\nour procedure aims to reduce the estimation variance in finite-sample settings\nto yield more reliable estimates. (3) We show theoretically that our procedure\nobtains valid bounds while reducing estimation variance. We further perform\nextensive experiments to demonstrate the effectiveness across various settings.\nOverall, our procedure offers a novel path for practitioners to make use of\npotentially high-dimensional instruments (e.g., as in Mendelian randomization)."
                },
                "authors": [
                    {
                        "name": "Jonas Schweisthal"
                    },
                    {
                        "name": "Dennis Frauen"
                    },
                    {
                        "name": "Maresa Schröder"
                    },
                    {
                        "name": "Konstantin Hess"
                    },
                    {
                        "name": "Niki Kilbertus"
                    },
                    {
                        "name": "Stefan Feuerriegel"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Feuerriegel"
                },
                "author": "Stefan Feuerriegel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08972v1",
                "updated": "2024-10-11T16:44:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    44,
                    39,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T16:44:39Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    44,
                    39,
                    4,
                    285,
                    0
                ],
                "title": "ALVIN: Active Learning Via INterpolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALVIN: Active Learning Via INterpolation"
                },
                "summary": "Active Learning aims to minimize annotation effort by selecting the most\nuseful instances from a pool of unlabeled data. However, typical active\nlearning methods overlook the presence of distinct example groups within a\nclass, whose prevalence may vary, e.g., in occupation classification datasets\ncertain demographics are disproportionately represented in specific classes.\nThis oversight causes models to rely on shortcuts for predictions, i.e.,\nspurious correlations between input attributes and labels occurring in\nwell-represented groups. To address this issue, we propose Active Learning Via\nINterpolation (ALVIN), which conducts intra-class interpolations between\nexamples from under-represented and well-represented groups to create anchors,\ni.e., artificial points situated between the example groups in the\nrepresentation space. By selecting instances close to the anchors for\nannotation, ALVIN identifies informative examples exposing the model to regions\nof the representation space that counteract the influence of shortcuts.\nCrucially, since the model considers these examples to be of high certainty,\nthey are likely to be ignored by typical active learning methods. Experimental\nresults on six datasets encompassing sentiment analysis, natural language\ninference, and paraphrase detection demonstrate that ALVIN outperforms\nstate-of-the-art active learning methods in both in-distribution and\nout-of-distribution generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Learning aims to minimize annotation effort by selecting the most\nuseful instances from a pool of unlabeled data. However, typical active\nlearning methods overlook the presence of distinct example groups within a\nclass, whose prevalence may vary, e.g., in occupation classification datasets\ncertain demographics are disproportionately represented in specific classes.\nThis oversight causes models to rely on shortcuts for predictions, i.e.,\nspurious correlations between input attributes and labels occurring in\nwell-represented groups. To address this issue, we propose Active Learning Via\nINterpolation (ALVIN), which conducts intra-class interpolations between\nexamples from under-represented and well-represented groups to create anchors,\ni.e., artificial points situated between the example groups in the\nrepresentation space. By selecting instances close to the anchors for\nannotation, ALVIN identifies informative examples exposing the model to regions\nof the representation space that counteract the influence of shortcuts.\nCrucially, since the model considers these examples to be of high certainty,\nthey are likely to be ignored by typical active learning methods. Experimental\nresults on six datasets encompassing sentiment analysis, natural language\ninference, and paraphrase detection demonstrate that ALVIN outperforms\nstate-of-the-art active learning methods in both in-distribution and\nout-of-distribution generalization."
                },
                "authors": [
                    {
                        "name": "Michalis Korakakis"
                    },
                    {
                        "name": "Andreas Vlachos"
                    },
                    {
                        "name": "Adrian Weller"
                    }
                ],
                "author_detail": {
                    "name": "Adrian Weller"
                },
                "author": "Adrian Weller",
                "arxiv_comment": "Accepted to EMNLP 2024 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08970v1",
                "updated": "2024-10-11T16:40:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    40,
                    3,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T16:40:03Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    40,
                    3,
                    4,
                    285,
                    0
                ],
                "title": "NoVo: Norm Voting off Hallucinations with Attention Heads in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NoVo: Norm Voting off Hallucinations with Attention Heads in Large\n  Language Models"
                },
                "summary": "Hallucinations in Large Language Models (LLMs) remain a major obstacle,\nparticularly in high-stakes applications where factual accuracy is critical.\nWhile representation editing and reading methods have made strides in reducing\nhallucinations, their heavy reliance on specialised tools and training on\nin-domain samples, makes them difficult to scale and prone to overfitting. This\nlimits their accuracy gains and generalizability to diverse datasets. This\npaper presents a lightweight method, Norm Voting (NoVo), which harnesses the\nuntapped potential of attention head norms to dramatically enhance factual\naccuracy in zero-shot multiple-choice questions (MCQs). NoVo begins by\nautomatically selecting truth-correlated head norms with an efficient,\ninference-only algorithm using only 30 random samples, allowing NoVo to\neffortlessly scale to diverse datasets. Afterwards, selected head norms are\nemployed in a simple voting algorithm, which yields significant gains in\nprediction accuracy. On TruthfulQA MC1, NoVo surpasses the current\nstate-of-the-art and all previous methods by an astounding margin -- at least\n19 accuracy points. NoVo demonstrates exceptional generalization to 20 diverse\ndatasets, with significant gains in over 90\\% of them, far exceeding all\ncurrent representation editing and reading methods. NoVo also reveals promising\ngains to finetuning strategies and building textual adversarial defence. NoVo's\neffectiveness with head norms opens new frontiers in LLM interpretability,\nrobustness and reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations in Large Language Models (LLMs) remain a major obstacle,\nparticularly in high-stakes applications where factual accuracy is critical.\nWhile representation editing and reading methods have made strides in reducing\nhallucinations, their heavy reliance on specialised tools and training on\nin-domain samples, makes them difficult to scale and prone to overfitting. This\nlimits their accuracy gains and generalizability to diverse datasets. This\npaper presents a lightweight method, Norm Voting (NoVo), which harnesses the\nuntapped potential of attention head norms to dramatically enhance factual\naccuracy in zero-shot multiple-choice questions (MCQs). NoVo begins by\nautomatically selecting truth-correlated head norms with an efficient,\ninference-only algorithm using only 30 random samples, allowing NoVo to\neffortlessly scale to diverse datasets. Afterwards, selected head norms are\nemployed in a simple voting algorithm, which yields significant gains in\nprediction accuracy. On TruthfulQA MC1, NoVo surpasses the current\nstate-of-the-art and all previous methods by an astounding margin -- at least\n19 accuracy points. NoVo demonstrates exceptional generalization to 20 diverse\ndatasets, with significant gains in over 90\\% of them, far exceeding all\ncurrent representation editing and reading methods. NoVo also reveals promising\ngains to finetuning strategies and building textual adversarial defence. NoVo's\neffectiveness with head norms opens new frontiers in LLM interpretability,\nrobustness and reliability."
                },
                "authors": [
                    {
                        "name": "Zheng Yi Ho"
                    },
                    {
                        "name": "Siyuan Liang"
                    },
                    {
                        "name": "Sen Zhang"
                    },
                    {
                        "name": "Yibing Zhan"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08968v1",
                "updated": "2024-10-11T16:38:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    38,
                    1,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T16:38:01Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    38,
                    1,
                    4,
                    285,
                    0
                ],
                "title": "Controllable Safety Alignment: Inference-Time Adaptation to Diverse\n  Safety Requirements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllable Safety Alignment: Inference-Time Adaptation to Diverse\n  Safety Requirements"
                },
                "summary": "The current paradigm for safety alignment of large language models (LLMs)\nfollows a one-size-fits-all approach: the model refuses to interact with any\ncontent deemed unsafe by the model provider. This approach lacks flexibility in\nthe face of varying social norms across cultures and regions. In addition,\nusers may have diverse safety needs, making a model with static safety\nstandards too restrictive to be useful, as well as too costly to be re-aligned.\n  We propose Controllable Safety Alignment (CoSA), a framework designed to\nadapt models to diverse safety requirements without re-training. Instead of\naligning a fixed model, we align models to follow safety configs -- free-form\nnatural language descriptions of the desired safety behaviors -- that are\nprovided as part of the system prompt. To adjust model safety behavior,\nauthorized users only need to modify such safety configs at inference time. To\nenable that, we propose CoSAlign, a data-centric method for aligning LLMs to\neasily adapt to diverse safety configs. Furthermore, we devise a novel\ncontrollability evaluation protocol that considers both helpfulness and\nconfigured safety, summarizing them into CoSA-Score, and construct CoSApien, a\nhuman-authored benchmark that consists of real-world LLM use cases with diverse\nsafety requirements and corresponding evaluation prompts.\n  We show that CoSAlign leads to substantial gains of controllability over\nstrong baselines including in-context alignment. Our framework encourages\nbetter representation and adaptation to pluralistic human values in LLMs, and\nthereby increasing their practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current paradigm for safety alignment of large language models (LLMs)\nfollows a one-size-fits-all approach: the model refuses to interact with any\ncontent deemed unsafe by the model provider. This approach lacks flexibility in\nthe face of varying social norms across cultures and regions. In addition,\nusers may have diverse safety needs, making a model with static safety\nstandards too restrictive to be useful, as well as too costly to be re-aligned.\n  We propose Controllable Safety Alignment (CoSA), a framework designed to\nadapt models to diverse safety requirements without re-training. Instead of\naligning a fixed model, we align models to follow safety configs -- free-form\nnatural language descriptions of the desired safety behaviors -- that are\nprovided as part of the system prompt. To adjust model safety behavior,\nauthorized users only need to modify such safety configs at inference time. To\nenable that, we propose CoSAlign, a data-centric method for aligning LLMs to\neasily adapt to diverse safety configs. Furthermore, we devise a novel\ncontrollability evaluation protocol that considers both helpfulness and\nconfigured safety, summarizing them into CoSA-Score, and construct CoSApien, a\nhuman-authored benchmark that consists of real-world LLM use cases with diverse\nsafety requirements and corresponding evaluation prompts.\n  We show that CoSAlign leads to substantial gains of controllability over\nstrong baselines including in-context alignment. Our framework encourages\nbetter representation and adaptation to pluralistic human values in LLMs, and\nthereby increasing their practicality."
                },
                "authors": [
                    {
                        "name": "Jingyu Zhang"
                    },
                    {
                        "name": "Ahmed Elgohary"
                    },
                    {
                        "name": "Ahmed Magooda"
                    },
                    {
                        "name": "Daniel Khashabi"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08964v1",
                "updated": "2024-10-11T16:32:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    32,
                    5,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T16:32:05Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    32,
                    5,
                    4,
                    285,
                    0
                ],
                "title": "Language Imbalance Driven Rewarding for Multilingual Self-improving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Imbalance Driven Rewarding for Multilingual Self-improving"
                },
                "summary": "Large Language Models (LLMs) have achieved state-of-the-art performance\nacross numerous tasks. However, these advancements have predominantly benefited\n\"first-class\" languages such as English and Chinese, leaving many other\nlanguages underrepresented. This imbalance, while limiting broader\napplications, generates a natural preference ranking between languages,\noffering an opportunity to bootstrap the multilingual capabilities of LLM in a\nself-improving manner. Thus, we propose $\\textit{Language Imbalance Driven\nRewarding}$, where the inherent imbalance between dominant and non-dominant\nlanguages within LLMs is leveraged as a reward signal. Iterative DPO training\ndemonstrates that this approach not only enhances LLM performance in\nnon-dominant languages but also improves the dominant language's capacity,\nthereby yielding an iterative reward signal. Fine-tuning\nMeta-Llama-3-8B-Instruct over two iterations of this approach results in\ncontinuous improvements in multilingual performance across\ninstruction-following and arithmetic reasoning tasks, evidenced by an average\nimprovement of 7.46% win rate on the X-AlpacaEval leaderboard and 13.9%\naccuracy on the MGSM benchmark. This work serves as an initial exploration,\npaving the way for multilingual self-improvement of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved state-of-the-art performance\nacross numerous tasks. However, these advancements have predominantly benefited\n\"first-class\" languages such as English and Chinese, leaving many other\nlanguages underrepresented. This imbalance, while limiting broader\napplications, generates a natural preference ranking between languages,\noffering an opportunity to bootstrap the multilingual capabilities of LLM in a\nself-improving manner. Thus, we propose $\\textit{Language Imbalance Driven\nRewarding}$, where the inherent imbalance between dominant and non-dominant\nlanguages within LLMs is leveraged as a reward signal. Iterative DPO training\ndemonstrates that this approach not only enhances LLM performance in\nnon-dominant languages but also improves the dominant language's capacity,\nthereby yielding an iterative reward signal. Fine-tuning\nMeta-Llama-3-8B-Instruct over two iterations of this approach results in\ncontinuous improvements in multilingual performance across\ninstruction-following and arithmetic reasoning tasks, evidenced by an average\nimprovement of 7.46% win rate on the X-AlpacaEval leaderboard and 13.9%\naccuracy on the MGSM benchmark. This work serves as an initial exploration,\npaving the way for multilingual self-improvement of LLMs."
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Junhong Wu"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Chengqing Zong"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01707v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01707v2",
                "updated": "2024-10-11T16:28:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    28,
                    36,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-02T16:15:31Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    15,
                    31,
                    2,
                    276,
                    0
                ],
                "title": "Interpretable Contrastive Monte Carlo Tree Search Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Contrastive Monte Carlo Tree Search Reasoning"
                },
                "summary": "We propose SC-MCTS*: a novel Monte Carlo Tree Search (MCTS) reasoning\nalgorithm for Large Language Models (LLMs), significantly improves both\nreasoning accuracy and speed. Our motivation comes from: 1. Previous MCTS LLM\nreasoning works often overlooked its biggest drawback--slower speed compared to\nCoT; 2. Previous research mainly used MCTS as a tool for LLM reasoning on\nvarious tasks with limited quantitative analysis or ablation studies of its\ncomponents from reasoning interpretability perspective. 3. The reward model is\nthe most crucial component in MCTS, however previous work has rarely conducted\nin-depth study or improvement of MCTS's reward models. Thus, we conducted\nextensive ablation studies and quantitative analysis on components of MCTS,\nrevealing the impact of each component on the MCTS reasoning performance of\nLLMs. Building on this, (i) we designed a highly interpretable reward model\nbased on the principle of contrastive decoding and (ii) achieved an average\nspeed improvement of 51.9% per node using speculative decoding. Additionally,\n(iii) we improved UCT node selection strategy and backpropagation used in\nprevious works, resulting in significant performance improvement. We\noutperformed o1-mini by an average of 17.4% on the Blocksworld multi-step\nreasoning dataset using Llama-3.1-70B with SC-MCTS*. Our code is available at\n\\url{https://github.com/zitian-gao/SC-MCTS}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose SC-MCTS*: a novel Monte Carlo Tree Search (MCTS) reasoning\nalgorithm for Large Language Models (LLMs), significantly improves both\nreasoning accuracy and speed. Our motivation comes from: 1. Previous MCTS LLM\nreasoning works often overlooked its biggest drawback--slower speed compared to\nCoT; 2. Previous research mainly used MCTS as a tool for LLM reasoning on\nvarious tasks with limited quantitative analysis or ablation studies of its\ncomponents from reasoning interpretability perspective. 3. The reward model is\nthe most crucial component in MCTS, however previous work has rarely conducted\nin-depth study or improvement of MCTS's reward models. Thus, we conducted\nextensive ablation studies and quantitative analysis on components of MCTS,\nrevealing the impact of each component on the MCTS reasoning performance of\nLLMs. Building on this, (i) we designed a highly interpretable reward model\nbased on the principle of contrastive decoding and (ii) achieved an average\nspeed improvement of 51.9% per node using speculative decoding. Additionally,\n(iii) we improved UCT node selection strategy and backpropagation used in\nprevious works, resulting in significant performance improvement. We\noutperformed o1-mini by an average of 17.4% on the Blocksworld multi-step\nreasoning dataset using Llama-3.1-70B with SC-MCTS*. Our code is available at\n\\url{https://github.com/zitian-gao/SC-MCTS}."
                },
                "authors": [
                    {
                        "name": "Zitian Gao"
                    },
                    {
                        "name": "Boye Niu"
                    },
                    {
                        "name": "Xuzheng He"
                    },
                    {
                        "name": "Haotian Xu"
                    },
                    {
                        "name": "Hongzhang Liu"
                    },
                    {
                        "name": "Aiwei Liu"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Lijie Wen"
                    }
                ],
                "author_detail": {
                    "name": "Lijie Wen"
                },
                "author": "Lijie Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01707v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01707v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.20086v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.20086v3",
                "updated": "2024-10-11T16:20:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    20,
                    16,
                    4,
                    285,
                    0
                ],
                "published": "2024-06-28T17:54:47Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    17,
                    54,
                    47,
                    4,
                    180,
                    0
                ],
                "title": "Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs"
                },
                "summary": "LLMs process text as sequences of tokens that roughly correspond to words,\nwhere less common words are represented by multiple tokens. However, individual\ntokens are often semantically unrelated to the meanings of the words/concepts\nthey comprise. For example, Llama-2-7b's tokenizer splits the word\n\"northeastern\" into the tokens ['_n', 'ort', 'he', 'astern'], none of which\ncorrespond to semantically meaningful units like \"north\" or \"east.\" Similarly,\nthe overall meanings of named entities like \"Neil Young\" and multi-word\nexpressions like \"break a leg\" cannot be directly inferred from their\nconstituent tokens. Mechanistically, how do LLMs convert such arbitrary groups\nof tokens into useful higher-level representations? In this work, we find that\nlast token representations of named entities and multi-token words exhibit a\npronounced \"erasure\" effect, where information about previous and current\ntokens is rapidly forgotten in early layers. Using this observation, we propose\na method to \"read out\" the implicit vocabulary of an autoregressive LLM by\nexamining differences in token representations across layers, and present\nresults of this method for Llama-2-7b and Llama-3-8B. To our knowledge, this is\nthe first attempt to probe the implicit vocabulary of an LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs process text as sequences of tokens that roughly correspond to words,\nwhere less common words are represented by multiple tokens. However, individual\ntokens are often semantically unrelated to the meanings of the words/concepts\nthey comprise. For example, Llama-2-7b's tokenizer splits the word\n\"northeastern\" into the tokens ['_n', 'ort', 'he', 'astern'], none of which\ncorrespond to semantically meaningful units like \"north\" or \"east.\" Similarly,\nthe overall meanings of named entities like \"Neil Young\" and multi-word\nexpressions like \"break a leg\" cannot be directly inferred from their\nconstituent tokens. Mechanistically, how do LLMs convert such arbitrary groups\nof tokens into useful higher-level representations? In this work, we find that\nlast token representations of named entities and multi-token words exhibit a\npronounced \"erasure\" effect, where information about previous and current\ntokens is rapidly forgotten in early layers. Using this observation, we propose\na method to \"read out\" the implicit vocabulary of an autoregressive LLM by\nexamining differences in token representations across layers, and present\nresults of this method for Llama-2-7b and Llama-3-8B. To our knowledge, this is\nthe first attempt to probe the implicit vocabulary of an LLM."
                },
                "authors": [
                    {
                        "name": "Sheridan Feucht"
                    },
                    {
                        "name": "David Atkinson"
                    },
                    {
                        "name": "Byron Wallace"
                    },
                    {
                        "name": "David Bau"
                    }
                ],
                "author_detail": {
                    "name": "David Bau"
                },
                "author": "David Bau",
                "arxiv_comment": "13 pages, 14 figures. Code and data at\n  https://footprints.baulab.info/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.20086v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.20086v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14517v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14517v2",
                "updated": "2024-10-11T16:19:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    19,
                    55,
                    4,
                    285,
                    0
                ],
                "published": "2024-06-20T17:27:14Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    17,
                    27,
                    14,
                    3,
                    172,
                    0
                ],
                "title": "PostMark: A Robust Blackbox Watermark for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PostMark: A Robust Blackbox Watermark for Large Language Models"
                },
                "summary": "The most effective techniques to detect LLM-generated text rely on inserting\na detectable signature -- or watermark -- during the model's decoding process.\nMost existing watermarking methods require access to the underlying LLM's\nlogits, which LLM API providers are loath to share due to fears of model\ndistillation. As such, these watermarks must be implemented independently by\neach LLM provider. In this paper, we develop PostMark, a modular post-hoc\nwatermarking procedure in which an input-dependent set of words (determined via\na semantic embedding) is inserted into the text after the decoding process has\ncompleted. Critically, PostMark does not require logit access, which means it\ncan be implemented by a third party. We also show that PostMark is more robust\nto paraphrasing attacks than existing watermarking methods: our experiments\ncover eight baseline algorithms, five base LLMs, and three datasets. Finally,\nwe evaluate the impact of PostMark on text quality using both automated and\nhuman assessments, highlighting the trade-off between quality and robustness to\nparaphrasing. We release our code, outputs, and annotations at\nhttps://github.com/lilakk/PostMark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The most effective techniques to detect LLM-generated text rely on inserting\na detectable signature -- or watermark -- during the model's decoding process.\nMost existing watermarking methods require access to the underlying LLM's\nlogits, which LLM API providers are loath to share due to fears of model\ndistillation. As such, these watermarks must be implemented independently by\neach LLM provider. In this paper, we develop PostMark, a modular post-hoc\nwatermarking procedure in which an input-dependent set of words (determined via\na semantic embedding) is inserted into the text after the decoding process has\ncompleted. Critically, PostMark does not require logit access, which means it\ncan be implemented by a third party. We also show that PostMark is more robust\nto paraphrasing attacks than existing watermarking methods: our experiments\ncover eight baseline algorithms, five base LLMs, and three datasets. Finally,\nwe evaluate the impact of PostMark on text quality using both automated and\nhuman assessments, highlighting the trade-off between quality and robustness to\nparaphrasing. We release our code, outputs, and annotations at\nhttps://github.com/lilakk/PostMark."
                },
                "authors": [
                    {
                        "name": "Yapei Chang"
                    },
                    {
                        "name": "Kalpesh Krishna"
                    },
                    {
                        "name": "Amir Houmansadr"
                    },
                    {
                        "name": "John Wieting"
                    },
                    {
                        "name": "Mohit Iyyer"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Iyyer"
                },
                "author": "Mohit Iyyer",
                "arxiv_comment": "EMNLP 2024; 19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14517v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14517v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08950v1",
                "updated": "2024-10-11T16:17:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    17,
                    47,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T16:17:47Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    17,
                    47,
                    4,
                    285,
                    0
                ],
                "title": "On the Adversarial Transferability of Generalized \"Skip Connections\"",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Adversarial Transferability of Generalized \"Skip Connections\""
                },
                "summary": "Skip connection is an essential ingredient for modern deep models to be\ndeeper and more powerful. Despite their huge success in normal scenarios\n(state-of-the-art classification performance on natural examples), we\ninvestigate and identify an interesting property of skip connections under\nadversarial scenarios, namely, the use of skip connections allows easier\ngeneration of highly transferable adversarial examples. Specifically, in\nResNet-like models (with skip connections), we find that using more gradients\nfrom the skip connections rather than the residual modules according to a decay\nfactor during backpropagation allows one to craft adversarial examples with\nhigh transferability. The above method is termed as Skip Gradient Method (SGM).\nAlthough starting from ResNet-like models in vision domains, we further extend\nSGM to more advanced architectures, including Vision Transformers (ViTs) and\nmodels with length-varying paths and other domains, i.e. natural language\nprocessing. We conduct comprehensive transfer attacks against various models\nincluding ResNets, Transformers, Inceptions, Neural Architecture Search, and\nLarge Language Models (LLMs). We show that employing SGM can greatly improve\nthe transferability of crafted attacks in almost all cases. Furthermore,\nconsidering the big complexity for practical use, we further demonstrate that\nSGM can even improve the transferability on ensembles of models or targeted\nattacks and the stealthiness against current defenses. At last, we provide\ntheoretical explanations and empirical insights on how SGM works. Our findings\nnot only motivate new adversarial research into the architectural\ncharacteristics of models but also open up further challenges for secure model\narchitecture design. Our code is available at https://github.com/mo666666/SGM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip connection is an essential ingredient for modern deep models to be\ndeeper and more powerful. Despite their huge success in normal scenarios\n(state-of-the-art classification performance on natural examples), we\ninvestigate and identify an interesting property of skip connections under\nadversarial scenarios, namely, the use of skip connections allows easier\ngeneration of highly transferable adversarial examples. Specifically, in\nResNet-like models (with skip connections), we find that using more gradients\nfrom the skip connections rather than the residual modules according to a decay\nfactor during backpropagation allows one to craft adversarial examples with\nhigh transferability. The above method is termed as Skip Gradient Method (SGM).\nAlthough starting from ResNet-like models in vision domains, we further extend\nSGM to more advanced architectures, including Vision Transformers (ViTs) and\nmodels with length-varying paths and other domains, i.e. natural language\nprocessing. We conduct comprehensive transfer attacks against various models\nincluding ResNets, Transformers, Inceptions, Neural Architecture Search, and\nLarge Language Models (LLMs). We show that employing SGM can greatly improve\nthe transferability of crafted attacks in almost all cases. Furthermore,\nconsidering the big complexity for practical use, we further demonstrate that\nSGM can even improve the transferability on ensembles of models or targeted\nattacks and the stealthiness against current defenses. At last, we provide\ntheoretical explanations and empirical insights on how SGM works. Our findings\nnot only motivate new adversarial research into the architectural\ncharacteristics of models but also open up further challenges for secure model\narchitecture design. Our code is available at https://github.com/mo666666/SGM."
                },
                "authors": [
                    {
                        "name": "Yisen Wang"
                    },
                    {
                        "name": "Yichuan Mo"
                    },
                    {
                        "name": "Dongxian Wu"
                    },
                    {
                        "name": "Mingjie Li"
                    },
                    {
                        "name": "Xingjun Ma"
                    },
                    {
                        "name": "Zhouchen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouchen Lin"
                },
                "author": "Zhouchen Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08948v1",
                "updated": "2024-10-11T16:16:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    16,
                    38,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T16:16:38Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    16,
                    38,
                    4,
                    285,
                    0
                ],
                "title": "The Dynamics of Social Conventions in LLM populations: Spontaneous\n  Emergence, Collective Biases and Tipping Points",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dynamics of Social Conventions in LLM populations: Spontaneous\n  Emergence, Collective Biases and Tipping Points"
                },
                "summary": "Social conventions are the foundation for social and economic life. As\nlegions of AI agents increasingly interact with each other and with humans,\ntheir ability to form shared conventions will determine how effectively they\nwill coordinate behaviors, integrate into society and influence it. Here, we\ninvestigate the dynamics of conventions within populations of Large Language\nModel (LLM) agents using simulated interactions. First, we show that globally\naccepted social conventions can spontaneously arise from local interactions\nbetween communicating LLMs. Second, we demonstrate how strong collective biases\ncan emerge during this process, even when individual agents appear to be\nunbiased. Third, we examine how minority groups of committed LLMs can drive\nsocial change by establishing new social conventions. We show that once these\nminority groups reach a critical size, they can consistently overturn\nestablished behaviors. In all cases, contrasting the experimental results with\npredictions from a minimal multi-agent model allows us to isolate the specific\nrole of LLM agents. Our results clarify how AI systems can autonomously develop\nnorms without explicit programming and have implications for designing AI\nsystems that align with human values and societal goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social conventions are the foundation for social and economic life. As\nlegions of AI agents increasingly interact with each other and with humans,\ntheir ability to form shared conventions will determine how effectively they\nwill coordinate behaviors, integrate into society and influence it. Here, we\ninvestigate the dynamics of conventions within populations of Large Language\nModel (LLM) agents using simulated interactions. First, we show that globally\naccepted social conventions can spontaneously arise from local interactions\nbetween communicating LLMs. Second, we demonstrate how strong collective biases\ncan emerge during this process, even when individual agents appear to be\nunbiased. Third, we examine how minority groups of committed LLMs can drive\nsocial change by establishing new social conventions. We show that once these\nminority groups reach a critical size, they can consistently overturn\nestablished behaviors. In all cases, contrasting the experimental results with\npredictions from a minimal multi-agent model allows us to isolate the specific\nrole of LLM agents. Our results clarify how AI systems can autonomously develop\nnorms without explicit programming and have implications for designing AI\nsystems that align with human values and societal goals."
                },
                "authors": [
                    {
                        "name": "Ariel Flint Ashery"
                    },
                    {
                        "name": "Luca Maria Aiello"
                    },
                    {
                        "name": "Andrea Baronchelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Baronchelli"
                },
                "author": "Andrea Baronchelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08937v1",
                "updated": "2024-10-11T16:03:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    3,
                    10,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T16:03:10Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    3,
                    10,
                    4,
                    285,
                    0
                ],
                "title": "Distributed Quantum Hypothesis Testing under Zero-rate Communication\n  Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Quantum Hypothesis Testing under Zero-rate Communication\n  Constraints"
                },
                "summary": "The trade-offs between error probabilities in quantum hypothesis testing are\nby now well-understood in the centralized setting, but much less is known for\ndistributed settings. Here, we study a distributed binary hypothesis testing\nproblem to infer a bipartite quantum state shared between two remote parties,\nwhere one of these parties communicates classical information to the tester at\nzero-rate (while the other party communicates classical or quantum information\nto the tester at zero-rate or higher). As our main contribution, we derive an\nefficiently computable single-letter formula for the Stein's exponent of this\nproblem, when the state under the alternative is product. For the general case,\nwe show that the Stein's exponent is given by a multi-letter expression\ninvolving max-min optimization of regularized measured relative entropy. While\nthis becomes single-letter for the fully classical case, we further prove that\nthis already does not happen in the same way for classical-quantum states in\ngeneral. As a key tool for proving the converse direction of our results, we\ndevelop a quantum version of the blowing-up lemma which may be of independent\ninterest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The trade-offs between error probabilities in quantum hypothesis testing are\nby now well-understood in the centralized setting, but much less is known for\ndistributed settings. Here, we study a distributed binary hypothesis testing\nproblem to infer a bipartite quantum state shared between two remote parties,\nwhere one of these parties communicates classical information to the tester at\nzero-rate (while the other party communicates classical or quantum information\nto the tester at zero-rate or higher). As our main contribution, we derive an\nefficiently computable single-letter formula for the Stein's exponent of this\nproblem, when the state under the alternative is product. For the general case,\nwe show that the Stein's exponent is given by a multi-letter expression\ninvolving max-min optimization of regularized measured relative entropy. While\nthis becomes single-letter for the fully classical case, we further prove that\nthis already does not happen in the same way for classical-quantum states in\ngeneral. As a key tool for proving the converse direction of our results, we\ndevelop a quantum version of the blowing-up lemma which may be of independent\ninterest."
                },
                "authors": [
                    {
                        "name": "Sreejith Sreekumar"
                    },
                    {
                        "name": "Christoph Hirche"
                    },
                    {
                        "name": "Hao-Chung Cheng"
                    },
                    {
                        "name": "Mario Berta"
                    }
                ],
                "author_detail": {
                    "name": "Mario Berta"
                },
                "author": "Mario Berta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19913v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19913v2",
                "updated": "2024-10-11T15:55:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    55,
                    0,
                    4,
                    285,
                    0
                ],
                "published": "2024-06-28T13:36:08Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    13,
                    36,
                    8,
                    4,
                    180,
                    0
                ],
                "title": "Automated Deep Neural Network Inference Partitioning for Distributed\n  Embedded Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Deep Neural Network Inference Partitioning for Distributed\n  Embedded Systems"
                },
                "summary": "Distributed systems can be found in various applications, e.g., in robotics\nor autonomous driving, to achieve higher flexibility and robustness. Thereby,\ndata flow centric applications such as Deep Neural Network (DNN) inference\nbenefit from partitioning the workload over multiple compute nodes in terms of\nperformance and energy-efficiency. However, mapping large models on distributed\nembedded systems is a complex task, due to low latency and high throughput\nrequirements combined with strict energy and memory constraints. In this paper,\nwe present a novel approach for hardware-aware layer scheduling of DNN\ninference in distributed embedded systems. Therefore, our proposed framework\nuses a graph-based algorithm to automatically find beneficial partitioning\npoints in a given DNN. Each of these is evaluated based on several essential\nsystem metrics such as accuracy and memory utilization, while considering the\nrespective system constraints. We demonstrate our approach in terms of the\nimpact of inference partitioning on various performance metrics of six\ndifferent DNNs. As an example, we can achieve a 47.5 % throughput increase for\nEfficientNet-B0 inference partitioned onto two platforms while observing high\nenergy-efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed systems can be found in various applications, e.g., in robotics\nor autonomous driving, to achieve higher flexibility and robustness. Thereby,\ndata flow centric applications such as Deep Neural Network (DNN) inference\nbenefit from partitioning the workload over multiple compute nodes in terms of\nperformance and energy-efficiency. However, mapping large models on distributed\nembedded systems is a complex task, due to low latency and high throughput\nrequirements combined with strict energy and memory constraints. In this paper,\nwe present a novel approach for hardware-aware layer scheduling of DNN\ninference in distributed embedded systems. Therefore, our proposed framework\nuses a graph-based algorithm to automatically find beneficial partitioning\npoints in a given DNN. Each of these is evaluated based on several essential\nsystem metrics such as accuracy and memory utilization, while considering the\nrespective system constraints. We demonstrate our approach in terms of the\nimpact of inference partitioning on various performance metrics of six\ndifferent DNNs. As an example, we can achieve a 47.5 % throughput increase for\nEfficientNet-B0 inference partitioned onto two platforms while observing high\nenergy-efficiency."
                },
                "authors": [
                    {
                        "name": "Fabian Kreß"
                    },
                    {
                        "name": "El Mahdi El Annabi"
                    },
                    {
                        "name": "Tim Hotfilter"
                    },
                    {
                        "name": "Julian Hoefer"
                    },
                    {
                        "name": "Tanja Harbaum"
                    },
                    {
                        "name": "Juergen Becker"
                    }
                ],
                "author_detail": {
                    "name": "Juergen Becker"
                },
                "author": "Juergen Becker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19913v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19913v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08928v1",
                "updated": "2024-10-11T15:53:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    53,
                    24,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T15:53:24Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    53,
                    24,
                    4,
                    285,
                    0
                ],
                "title": "Towards Cross-Lingual LLM Evaluation for European Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Cross-Lingual LLM Evaluation for European Languages"
                },
                "summary": "The rise of Large Language Models (LLMs) has revolutionized natural language\nprocessing across numerous languages and tasks. However, evaluating LLM\nperformance in a consistent and meaningful way across multiple European\nlanguages remains challenging, especially due to the scarcity of multilingual\nbenchmarks. We introduce a cross-lingual evaluation approach tailored for\nEuropean languages. We employ translated versions of five widely-used\nbenchmarks to assess the capabilities of 40 LLMs across 21 European languages.\nOur contributions include examining the effectiveness of translated benchmarks,\nassessing the impact of different translation services, and offering a\nmultilingual evaluation framework for LLMs that includes newly created\ndatasets: EU20-MMLU, EU20-HellaSwag, EU20-ARC, EU20-TruthfulQA, and EU20-GSM8K.\nThe benchmarks and results are made publicly available to encourage further\nresearch in multilingual LLM evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Large Language Models (LLMs) has revolutionized natural language\nprocessing across numerous languages and tasks. However, evaluating LLM\nperformance in a consistent and meaningful way across multiple European\nlanguages remains challenging, especially due to the scarcity of multilingual\nbenchmarks. We introduce a cross-lingual evaluation approach tailored for\nEuropean languages. We employ translated versions of five widely-used\nbenchmarks to assess the capabilities of 40 LLMs across 21 European languages.\nOur contributions include examining the effectiveness of translated benchmarks,\nassessing the impact of different translation services, and offering a\nmultilingual evaluation framework for LLMs that includes newly created\ndatasets: EU20-MMLU, EU20-HellaSwag, EU20-ARC, EU20-TruthfulQA, and EU20-GSM8K.\nThe benchmarks and results are made publicly available to encourage further\nresearch in multilingual LLM evaluation."
                },
                "authors": [
                    {
                        "name": "Klaudia Thellmann"
                    },
                    {
                        "name": "Bernhard Stadler"
                    },
                    {
                        "name": "Michael Fromm"
                    },
                    {
                        "name": "Jasper Schulze Buschhoff"
                    },
                    {
                        "name": "Alex Jude"
                    },
                    {
                        "name": "Fabio Barth"
                    },
                    {
                        "name": "Johannes Leveling"
                    },
                    {
                        "name": "Nicolas Flores-Herr"
                    },
                    {
                        "name": "Joachim Köhler"
                    },
                    {
                        "name": "René Jäkel"
                    },
                    {
                        "name": "Mehdi Ali"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Ali"
                },
                "author": "Mehdi Ali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08924v1",
                "updated": "2024-10-11T15:50:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    50,
                    17,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T15:50:17Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    50,
                    17,
                    4,
                    285,
                    0
                ],
                "title": "DiffPO: A causal diffusion model for learning distributions of potential\n  outcomes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffPO: A causal diffusion model for learning distributions of potential\n  outcomes"
                },
                "summary": "Predicting potential outcomes of interventions from observational data is\ncrucial for decision-making in medicine, but the task is challenging due to the\nfundamental problem of causal inference. Existing methods are largely limited\nto point estimates of potential outcomes with no uncertain quantification;\nthus, the full information about the distributions of potential outcomes is\ntypically ignored. In this paper, we propose a novel causal diffusion model\ncalled DiffPO, which is carefully designed for reliable inferences in medicine\nby learning the distribution of potential outcomes. In our DiffPO, we leverage\na tailored conditional denoising diffusion model to learn complex\ndistributions, where we address the selection bias through a novel orthogonal\ndiffusion loss. Another strength of our DiffPO method is that it is highly\nflexible (e.g., it can also be used to estimate different causal quantities\nsuch as CATE). Across a wide range of experiments, we show that our method\nachieves state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting potential outcomes of interventions from observational data is\ncrucial for decision-making in medicine, but the task is challenging due to the\nfundamental problem of causal inference. Existing methods are largely limited\nto point estimates of potential outcomes with no uncertain quantification;\nthus, the full information about the distributions of potential outcomes is\ntypically ignored. In this paper, we propose a novel causal diffusion model\ncalled DiffPO, which is carefully designed for reliable inferences in medicine\nby learning the distribution of potential outcomes. In our DiffPO, we leverage\na tailored conditional denoising diffusion model to learn complex\ndistributions, where we address the selection bias through a novel orthogonal\ndiffusion loss. Another strength of our DiffPO method is that it is highly\nflexible (e.g., it can also be used to estimate different causal quantities\nsuch as CATE). Across a wide range of experiments, we show that our method\nachieves state-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Yuchen Ma"
                    },
                    {
                        "name": "Valentyn Melnychuk"
                    },
                    {
                        "name": "Jonas Schweisthal"
                    },
                    {
                        "name": "Stefan Feuerriegel"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Feuerriegel"
                },
                "author": "Stefan Feuerriegel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08923v1",
                "updated": "2024-10-11T15:50:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    50,
                    1,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T15:50:01Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    50,
                    1,
                    4,
                    285,
                    0
                ],
                "title": "Path-minimizing Latent ODEs for improved extrapolation and inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Path-minimizing Latent ODEs for improved extrapolation and inference"
                },
                "summary": "Latent ODE models provide flexible descriptions of dynamic systems, but they\ncan struggle with extrapolation and predicting complicated non-linear dynamics.\nThe latent ODE approach implicitly relies on encoders to identify unknown\nsystem parameters and initial conditions, whereas the evaluation times are\nknown and directly provided to the ODE solver. This dichotomy can be exploited\nby encouraging time-independent latent representations. By replacing the common\nvariational penalty in latent space with an $\\ell_2$ penalty on the path length\nof each system, the models learn data representations that can easily be\ndistinguished from those of systems with different configurations. This results\nin faster training, smaller models, more accurate interpolation and long-time\nextrapolation compared to the baseline ODE models with GRU, RNN, and LSTM\nencoder/decoders on tests with damped harmonic oscillator, self-gravitating\nfluid, and predator-prey systems. We also demonstrate superior results for\nsimulation-based inference of the Lotka-Volterra parameters and initial\nconditions by using the latents as data summaries for a conditional normalizing\nflow. Our change to the training loss is agnostic to the specific recognition\nnetwork used by the decoder and can therefore easily be adopted by other latent\nODE models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent ODE models provide flexible descriptions of dynamic systems, but they\ncan struggle with extrapolation and predicting complicated non-linear dynamics.\nThe latent ODE approach implicitly relies on encoders to identify unknown\nsystem parameters and initial conditions, whereas the evaluation times are\nknown and directly provided to the ODE solver. This dichotomy can be exploited\nby encouraging time-independent latent representations. By replacing the common\nvariational penalty in latent space with an $\\ell_2$ penalty on the path length\nof each system, the models learn data representations that can easily be\ndistinguished from those of systems with different configurations. This results\nin faster training, smaller models, more accurate interpolation and long-time\nextrapolation compared to the baseline ODE models with GRU, RNN, and LSTM\nencoder/decoders on tests with damped harmonic oscillator, self-gravitating\nfluid, and predator-prey systems. We also demonstrate superior results for\nsimulation-based inference of the Lotka-Volterra parameters and initial\nconditions by using the latents as data summaries for a conditional normalizing\nflow. Our change to the training loss is agnostic to the specific recognition\nnetwork used by the decoder and can therefore easily be adopted by other latent\nODE models."
                },
                "authors": [
                    {
                        "name": "Matt L. Sampson"
                    },
                    {
                        "name": "Peter Melchior"
                    }
                ],
                "author_detail": {
                    "name": "Peter Melchior"
                },
                "author": "Peter Melchior",
                "arxiv_comment": "20 pages 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08922v1",
                "updated": "2024-10-11T15:49:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    49,
                    42,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T15:49:42Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    49,
                    42,
                    4,
                    285,
                    0
                ],
                "title": "Exploring the Design Space of Cognitive Engagement Techniques with\n  AI-Generated Code for Enhanced Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Design Space of Cognitive Engagement Techniques with\n  AI-Generated Code for Enhanced Learning"
                },
                "summary": "Novice programmers are increasingly relying on Large Language Models (LLMs)\nto generate code for learning programming concepts. However, this interaction\ncan lead to superficial engagement, giving learners an illusion of learning and\nhindering skill development. To address this issue, we conducted a systematic\ndesign exploration to develop seven cognitive engagement techniques aimed at\npromoting deeper engagement with AI-generated code. In this paper, we describe\nour design process, the initial seven techniques and results from a\nbetween-subjects study (N=82). We then iteratively refined the top techniques\nand further evaluated them through a within-subjects study (N=42). We evaluate\nthe friction each technique introduces, their effectiveness in helping learners\napply concepts to isomorphic tasks without AI assistance, and their success in\naligning learners' perceived and actual coding abilities. Ultimately, our\nresults highlight the most effective technique: guiding learners through the\nstep-by-step problem-solving process, where they engage in an interactive\ndialog with the AI, prompting what needs to be done at each stage before the\ncorresponding code is revealed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novice programmers are increasingly relying on Large Language Models (LLMs)\nto generate code for learning programming concepts. However, this interaction\ncan lead to superficial engagement, giving learners an illusion of learning and\nhindering skill development. To address this issue, we conducted a systematic\ndesign exploration to develop seven cognitive engagement techniques aimed at\npromoting deeper engagement with AI-generated code. In this paper, we describe\nour design process, the initial seven techniques and results from a\nbetween-subjects study (N=82). We then iteratively refined the top techniques\nand further evaluated them through a within-subjects study (N=42). We evaluate\nthe friction each technique introduces, their effectiveness in helping learners\napply concepts to isomorphic tasks without AI assistance, and their success in\naligning learners' perceived and actual coding abilities. Ultimately, our\nresults highlight the most effective technique: guiding learners through the\nstep-by-step problem-solving process, where they engage in an interactive\ndialog with the AI, prompting what needs to be done at each stage before the\ncorresponding code is revealed."
                },
                "authors": [
                    {
                        "name": "Majeed Kazemitabaar"
                    },
                    {
                        "name": "Oliver Huang"
                    },
                    {
                        "name": "Sangho Suh"
                    },
                    {
                        "name": "Austin Z. Henley"
                    },
                    {
                        "name": "Tovi Grossman"
                    }
                ],
                "author_detail": {
                    "name": "Tovi Grossman"
                },
                "author": "Tovi Grossman",
                "arxiv_comment": "19 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08918v1",
                "updated": "2024-10-11T15:46:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    46,
                    9,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T15:46:09Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    46,
                    9,
                    4,
                    285,
                    0
                ],
                "title": "Wikimedia data for AI: a review of Wikimedia datasets for NLP tasks and\n  AI-assisted editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wikimedia data for AI: a review of Wikimedia datasets for NLP tasks and\n  AI-assisted editing"
                },
                "summary": "Wikimedia content is used extensively by the AI community and within the\nlanguage modeling community in particular. In this paper, we provide a review\nof the different ways in which Wikimedia data is curated to use in NLP tasks\nacross pre-training, post-training, and model evaluations. We point to\nopportunities for greater use of Wikimedia content but also identify ways in\nwhich the language modeling community could better center the needs of\nWikimedia editors. In particular, we call for incorporating additional sources\nof Wikimedia data, a greater focus on benchmarks for LLMs that encode Wikimedia\nprinciples, and greater multilingualism in Wikimedia-derived datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wikimedia content is used extensively by the AI community and within the\nlanguage modeling community in particular. In this paper, we provide a review\nof the different ways in which Wikimedia data is curated to use in NLP tasks\nacross pre-training, post-training, and model evaluations. We point to\nopportunities for greater use of Wikimedia content but also identify ways in\nwhich the language modeling community could better center the needs of\nWikimedia editors. In particular, we call for incorporating additional sources\nof Wikimedia data, a greater focus on benchmarks for LLMs that encode Wikimedia\nprinciples, and greater multilingualism in Wikimedia-derived datasets."
                },
                "authors": [
                    {
                        "name": "Isaac Johnson"
                    },
                    {
                        "name": "Lucie-Aimée Kaffee"
                    },
                    {
                        "name": "Miriam Redi"
                    }
                ],
                "author_detail": {
                    "name": "Miriam Redi"
                },
                "author": "Miriam Redi",
                "arxiv_comment": "Accepted to NLP for Wikipedia Workshop at EMNLP '24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01305v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01305v2",
                "updated": "2024-10-11T15:44:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    44,
                    28,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-02T07:57:33Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    7,
                    57,
                    33,
                    2,
                    276,
                    0
                ],
                "title": "Revisiting Hierarchical Text Classification: Inference and Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Hierarchical Text Classification: Inference and Metrics"
                },
                "summary": "Hierarchical text classification (HTC) is the task of assigning labels to a\ntext within a structured space organized as a hierarchy. Recent works treat HTC\nas a conventional multilabel classification problem, therefore evaluating it as\nsuch. We instead propose to evaluate models based on specifically designed\nhierarchical metrics and we demonstrate the intricacy of metric choice and\nprediction inference method. We introduce a new challenging dataset and we\nevaluate fairly, recent sophisticated models, comparing them with a range of\nsimple but strong baselines, including a new theoretically motivated loss.\nFinally, we show that those baselines are very often competitive with the\nlatest models. This highlights the importance of carefully considering the\nevaluation methodology when proposing new methods for HTC. Code implementation\nand dataset are available at \\url{https://github.com/RomanPlaud/revisitingHTC}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical text classification (HTC) is the task of assigning labels to a\ntext within a structured space organized as a hierarchy. Recent works treat HTC\nas a conventional multilabel classification problem, therefore evaluating it as\nsuch. We instead propose to evaluate models based on specifically designed\nhierarchical metrics and we demonstrate the intricacy of metric choice and\nprediction inference method. We introduce a new challenging dataset and we\nevaluate fairly, recent sophisticated models, comparing them with a range of\nsimple but strong baselines, including a new theoretically motivated loss.\nFinally, we show that those baselines are very often competitive with the\nlatest models. This highlights the importance of carefully considering the\nevaluation methodology when proposing new methods for HTC. Code implementation\nand dataset are available at \\url{https://github.com/RomanPlaud/revisitingHTC}."
                },
                "authors": [
                    {
                        "name": "Roman Plaud"
                    },
                    {
                        "name": "Matthieu Labeau"
                    },
                    {
                        "name": "Antoine Saillenfest"
                    },
                    {
                        "name": "Thomas Bonald"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Bonald"
                },
                "author": "Thomas Bonald",
                "arxiv_comment": "Accepted at CoNLL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01305v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01305v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08911v1",
                "updated": "2024-10-11T15:32:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    32,
                    48,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T15:32:48Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    32,
                    48,
                    4,
                    285,
                    0
                ],
                "title": "Test-driven Software Experimentation with LASSO: an LLM Benchmarking\n  Example",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-driven Software Experimentation with LASSO: an LLM Benchmarking\n  Example"
                },
                "summary": "Empirical software engineering faces a critical gap: the lack of standardized\ntools for rapid development and execution of Test-Driven Software Experiments\n(TDSEs) - that is, experiments that involve the execution of software subjects\nand the observation and analysis of their \"de facto\" run-time behavior. In this\npaper we present a general-purpose analysis platform called LASSO that provides\na minimal set of domain-specific languages and data structures to conduct\nTDSEs. By empowering users with an executable scripting language to design and\nexecute TDSEs, LASSO enables efficient evaluation of run-time semantics and\nexecution characteristics in addition to statically determined properties. We\npresent an example TDSE that demonstrates the practical benefits of LASSO's\nscripting capabilities for assessing the reliability of LLMs for code\ngeneration by means of a self-contained, reusable and extensible study script.\nThe LASSO platform is freely available at:\nhttps://softwareobservatorium.github.io/, and a demo video is available on\nYouTube: https://youtu.be/tzY9oNTWXzw",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical software engineering faces a critical gap: the lack of standardized\ntools for rapid development and execution of Test-Driven Software Experiments\n(TDSEs) - that is, experiments that involve the execution of software subjects\nand the observation and analysis of their \"de facto\" run-time behavior. In this\npaper we present a general-purpose analysis platform called LASSO that provides\na minimal set of domain-specific languages and data structures to conduct\nTDSEs. By empowering users with an executable scripting language to design and\nexecute TDSEs, LASSO enables efficient evaluation of run-time semantics and\nexecution characteristics in addition to statically determined properties. We\npresent an example TDSE that demonstrates the practical benefits of LASSO's\nscripting capabilities for assessing the reliability of LLMs for code\ngeneration by means of a self-contained, reusable and extensible study script.\nThe LASSO platform is freely available at:\nhttps://softwareobservatorium.github.io/, and a demo video is available on\nYouTube: https://youtu.be/tzY9oNTWXzw"
                },
                "authors": [
                    {
                        "name": "Marcus Kessel"
                    }
                ],
                "author_detail": {
                    "name": "Marcus Kessel"
                },
                "author": "Marcus Kessel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.1; D.2.4; I.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2203.06617v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2203.06617v3",
                "updated": "2024-10-11T15:32:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    32,
                    7,
                    4,
                    285,
                    0
                ],
                "published": "2022-03-13T10:29:34Z",
                "published_parsed": [
                    2022,
                    3,
                    13,
                    10,
                    29,
                    34,
                    6,
                    72,
                    0
                ],
                "title": "Generalized Median of Means Principle for Bayesian Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized Median of Means Principle for Bayesian Inference"
                },
                "summary": "The topic of robustness is experiencing a resurgence of interest in the\nstatistical and machine learning communities. In particular, robust algorithms\nmaking use of the so-called median of means estimator were shown to satisfy\nstrong performance guarantees for many problems, including estimation of the\nmean, covariance structure as well as linear regression. In this work, we\npropose an extension of the median of means principle to the Bayesian\nframework, leading to the notion of the robust posterior distribution. In\nparticular, we (a) quantify robustness of this posterior to outliers, (b) show\nthat it satisfies a version of the Bernstein-von Mises theorem that connects\nBayesian credible sets to the traditional confidence intervals, and (c)\ndemonstrate that our approach performs well in applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The topic of robustness is experiencing a resurgence of interest in the\nstatistical and machine learning communities. In particular, robust algorithms\nmaking use of the so-called median of means estimator were shown to satisfy\nstrong performance guarantees for many problems, including estimation of the\nmean, covariance structure as well as linear regression. In this work, we\npropose an extension of the median of means principle to the Bayesian\nframework, leading to the notion of the robust posterior distribution. In\nparticular, we (a) quantify robustness of this posterior to outliers, (b) show\nthat it satisfies a version of the Bernstein-von Mises theorem that connects\nBayesian credible sets to the traditional confidence intervals, and (c)\ndemonstrate that our approach performs well in applications."
                },
                "authors": [
                    {
                        "name": "Stanislav Minsker"
                    },
                    {
                        "name": "Shunan Yao"
                    }
                ],
                "author_detail": {
                    "name": "Shunan Yao"
                },
                "author": "Shunan Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2203.06617v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2203.06617v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.09047v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.09047v2",
                "updated": "2024-10-11T15:26:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    26,
                    14,
                    4,
                    285,
                    0
                ],
                "published": "2023-07-18T07:59:37Z",
                "published_parsed": [
                    2023,
                    7,
                    18,
                    7,
                    59,
                    37,
                    1,
                    199,
                    0
                ],
                "title": "Modular Multimodal Machine Learning for Extraction of Theorems and\n  Proofs in Long Scientific Documents (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modular Multimodal Machine Learning for Extraction of Theorems and\n  Proofs in Long Scientific Documents (Extended Version)"
                },
                "summary": "We address the extraction of mathematical statements and their proofs from\nscholarly PDF articles as a multimodal classification problem, utilizing text,\nfont features, and bitmap image renderings of PDFs as distinct modalities. We\npropose a modular sequential multimodal machine learning approach specifically\ndesigned for extracting theorem-like environments and proofs. This is based on\na cross-modal attention mechanism to generate multimodal paragraph embeddings,\nwhich are then fed into our novel multimodal sliding window transformer\narchitecture to capture sequential information across paragraphs. Our document\nAI methodology stands out as it eliminates the need for OCR preprocessing,\nLaTeX sources during inference, or custom pre-training on specialized losses to\nunderstand cross-modality relationships. Unlike many conventional approaches\nthat operate at a single-page level, ours can be directly applied to multi-page\nPDFs and seamlessly handles the page breaks often found in lengthy scientific\nmathematical documents. Our approach demonstrates performance improvements\nobtained by transitioning from unimodality to multimodality, and finally by\nincorporating sequential modeling over paragraphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the extraction of mathematical statements and their proofs from\nscholarly PDF articles as a multimodal classification problem, utilizing text,\nfont features, and bitmap image renderings of PDFs as distinct modalities. We\npropose a modular sequential multimodal machine learning approach specifically\ndesigned for extracting theorem-like environments and proofs. This is based on\na cross-modal attention mechanism to generate multimodal paragraph embeddings,\nwhich are then fed into our novel multimodal sliding window transformer\narchitecture to capture sequential information across paragraphs. Our document\nAI methodology stands out as it eliminates the need for OCR preprocessing,\nLaTeX sources during inference, or custom pre-training on specialized losses to\nunderstand cross-modality relationships. Unlike many conventional approaches\nthat operate at a single-page level, ours can be directly applied to multi-page\nPDFs and seamlessly handles the page breaks often found in lengthy scientific\nmathematical documents. Our approach demonstrates performance improvements\nobtained by transitioning from unimodality to multimodality, and finally by\nincorporating sequential modeling over paragraphs."
                },
                "authors": [
                    {
                        "name": "Shrey Mishra"
                    },
                    {
                        "name": "Antoine Gauquier"
                    },
                    {
                        "name": "Pierre Senellart"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Senellart"
                },
                "author": "Pierre Senellart",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.09047v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.09047v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08899v1",
                "updated": "2024-10-11T15:18:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    18,
                    48,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T15:18:48Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    18,
                    48,
                    4,
                    285,
                    0
                ],
                "title": "Utilizing ChatGPT in a Data Structures and Algorithms Course: A Teaching\n  Assistant's Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing ChatGPT in a Data Structures and Algorithms Course: A Teaching\n  Assistant's Perspective"
                },
                "summary": "Integrating large language models (LLMs) like ChatGPT is revolutionizing the\nfield of computer science education. These models offer new possibilities for\nenriching student learning and supporting teaching assistants (TAs) in\nproviding prompt feedback and supplementary learning resources. This research\ndelves into the use of ChatGPT in a data structures and algorithms (DSA)\ncourse, particularly when combined with TA supervision. The findings\ndemonstrate that incorporating ChatGPT with structured prompts and active TA\nguidance enhances students' understanding of intricate algorithmic concepts,\nboosts engagement, and elevates academic performance. However, challenges exist\nin addressing academic integrity and the limitations of LLMs in tackling\ncomplex problems. The study underscores the importance of active TA involvement\nin reducing students' reliance on AI-generated content and amplifying the\noverall educational impact. The results suggest that while LLMs can be\nadvantageous for education, their successful integration demands continuous\noversight and a thoughtful balance between AI and human guidance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating large language models (LLMs) like ChatGPT is revolutionizing the\nfield of computer science education. These models offer new possibilities for\nenriching student learning and supporting teaching assistants (TAs) in\nproviding prompt feedback and supplementary learning resources. This research\ndelves into the use of ChatGPT in a data structures and algorithms (DSA)\ncourse, particularly when combined with TA supervision. The findings\ndemonstrate that incorporating ChatGPT with structured prompts and active TA\nguidance enhances students' understanding of intricate algorithmic concepts,\nboosts engagement, and elevates academic performance. However, challenges exist\nin addressing academic integrity and the limitations of LLMs in tackling\ncomplex problems. The study underscores the importance of active TA involvement\nin reducing students' reliance on AI-generated content and amplifying the\noverall educational impact. The results suggest that while LLMs can be\nadvantageous for education, their successful integration demands continuous\noversight and a thoughtful balance between AI and human guidance."
                },
                "authors": [
                    {
                        "name": "Pooriya Jamie"
                    },
                    {
                        "name": "Reyhaneh Hajihashemi"
                    },
                    {
                        "name": "Sharareh Alipour"
                    }
                ],
                "author_detail": {
                    "name": "Sharareh Alipour"
                },
                "author": "Sharareh Alipour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.3.2; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11557v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11557v4",
                "updated": "2024-10-11T15:13:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    13,
                    51,
                    4,
                    285,
                    0
                ],
                "published": "2024-08-21T12:09:37Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    12,
                    9,
                    37,
                    2,
                    234,
                    0
                ],
                "title": "A Quick, trustworthy spectral knowledge Q&A system leveraging\n  retrieval-augmented generation on LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Quick, trustworthy spectral knowledge Q&A system leveraging\n  retrieval-augmented generation on LLM"
                },
                "summary": "Large Language Model (LLM) has demonstrated significant success in a range of\nnatural language processing (NLP) tasks within general domain. The emergence of\nLLM has introduced innovative methodologies across diverse fields, including\nthe natural sciences. Researchers aim to implement automated, concurrent\nprocess driven by LLM to supplant conventional manual, repetitive and\nlabor-intensive work. In the domain of spectral analysis and detection, it is\nimperative for researchers to autonomously acquire pertinent knowledge across\nvarious research objects, which encompasses the spectroscopic techniques and\nthe chemometric methods that are employed in experiments and analysis.\nParadoxically, despite the recognition of spectroscopic detection as an\neffective analytical method, the fundamental process of knowledge retrieval\nremains both time-intensive and repetitive. In response to this challenge, we\nfirst introduced the Spectral Detection and Analysis Based Paper(SDAAP)\ndataset, which is the first open-source textual knowledge dataset for spectral\nanalysis and detection and contains annotated literature data as well as\ncorresponding knowledge instruction data. Subsequently, we also designed an\nautomated Q\\&A framework based on the SDAAP dataset, which can retrieve\nrelevant knowledge and generate high-quality responses by extracting entities\nin the input as retrieval parameters. It is worth noting that: within this\nframework, LLM is only used as a tool to provide generalizability, while RAG\ntechnique is used to accurately capture the source of the knowledge.This\napproach not only improves the quality of the generated responses, but also\nensures the traceability of the knowledge. Experimental results show that our\nframework generates responses with more reliable expertise compared to the\nbaseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) has demonstrated significant success in a range of\nnatural language processing (NLP) tasks within general domain. The emergence of\nLLM has introduced innovative methodologies across diverse fields, including\nthe natural sciences. Researchers aim to implement automated, concurrent\nprocess driven by LLM to supplant conventional manual, repetitive and\nlabor-intensive work. In the domain of spectral analysis and detection, it is\nimperative for researchers to autonomously acquire pertinent knowledge across\nvarious research objects, which encompasses the spectroscopic techniques and\nthe chemometric methods that are employed in experiments and analysis.\nParadoxically, despite the recognition of spectroscopic detection as an\neffective analytical method, the fundamental process of knowledge retrieval\nremains both time-intensive and repetitive. In response to this challenge, we\nfirst introduced the Spectral Detection and Analysis Based Paper(SDAAP)\ndataset, which is the first open-source textual knowledge dataset for spectral\nanalysis and detection and contains annotated literature data as well as\ncorresponding knowledge instruction data. Subsequently, we also designed an\nautomated Q\\&A framework based on the SDAAP dataset, which can retrieve\nrelevant knowledge and generate high-quality responses by extracting entities\nin the input as retrieval parameters. It is worth noting that: within this\nframework, LLM is only used as a tool to provide generalizability, while RAG\ntechnique is used to accurately capture the source of the knowledge.This\napproach not only improves the quality of the generated responses, but also\nensures the traceability of the knowledge. Experimental results show that our\nframework generates responses with more reliable expertise compared to the\nbaseline."
                },
                "authors": [
                    {
                        "name": "Jiheng Liang"
                    },
                    {
                        "name": "Ziru Yu"
                    },
                    {
                        "name": "Zujie Xie"
                    },
                    {
                        "name": "Xiangyang Yu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Yu"
                },
                "author": "Xiangyang Yu",
                "arxiv_comment": "16 pages,10 figures,3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11557v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11557v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08892v1",
                "updated": "2024-10-11T15:10:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    10,
                    38,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T15:10:38Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    10,
                    38,
                    4,
                    285,
                    0
                ],
                "title": "Federated Learning in Practice: Reflections and Projections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning in Practice: Reflections and Projections"
                },
                "summary": "Federated Learning (FL) is a machine learning technique that enables multiple\nentities to collaboratively learn a shared model without exchanging their local\ndata. Over the past decade, FL systems have achieved substantial progress,\nscaling to millions of devices across various learning domains while offering\nmeaningful differential privacy (DP) guarantees. Production systems from\norganizations like Google, Apple, and Meta demonstrate the real-world\napplicability of FL. However, key challenges remain, including verifying\nserver-side DP guarantees and coordinating training across heterogeneous\ndevices, limiting broader adoption. Additionally, emerging trends such as large\n(multi-modal) models and blurred lines between training, inference, and\npersonalization challenge traditional FL frameworks. In response, we propose a\nredefined FL framework that prioritizes privacy principles rather than rigid\ndefinitions. We also chart a path forward by leveraging trusted execution\nenvironments and open-source ecosystems to address these challenges and\nfacilitate future advancements in FL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is a machine learning technique that enables multiple\nentities to collaboratively learn a shared model without exchanging their local\ndata. Over the past decade, FL systems have achieved substantial progress,\nscaling to millions of devices across various learning domains while offering\nmeaningful differential privacy (DP) guarantees. Production systems from\norganizations like Google, Apple, and Meta demonstrate the real-world\napplicability of FL. However, key challenges remain, including verifying\nserver-side DP guarantees and coordinating training across heterogeneous\ndevices, limiting broader adoption. Additionally, emerging trends such as large\n(multi-modal) models and blurred lines between training, inference, and\npersonalization challenge traditional FL frameworks. In response, we propose a\nredefined FL framework that prioritizes privacy principles rather than rigid\ndefinitions. We also chart a path forward by leveraging trusted execution\nenvironments and open-source ecosystems to address these challenges and\nfacilitate future advancements in FL."
                },
                "authors": [
                    {
                        "name": "Katharine Daly"
                    },
                    {
                        "name": "Hubert Eichner"
                    },
                    {
                        "name": "Peter Kairouz"
                    },
                    {
                        "name": "H. Brendan McMahan"
                    },
                    {
                        "name": "Daniel Ramage"
                    },
                    {
                        "name": "Zheng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Xu"
                },
                "author": "Zheng Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06964v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06964v2",
                "updated": "2024-10-11T15:09:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    9,
                    44,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-09T15:02:28Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    2,
                    28,
                    2,
                    283,
                    0
                ],
                "title": "Bridge the Points: Graph-based Few-shot Segment Anything Semantically",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridge the Points: Graph-based Few-shot Segment Anything Semantically"
                },
                "summary": "The recent advancements in large-scale pre-training techniques have\nsignificantly enhanced the capabilities of vision foundation models, notably\nthe Segment Anything Model (SAM), which can generate precise masks based on\npoint and box prompts. Recent studies extend SAM to Few-shot Semantic\nSegmentation (FSS), focusing on prompt generation for SAM-based automatic\nsemantic segmentation. However, these methods struggle with selecting suitable\nprompts, require specific hyperparameter settings for different scenarios, and\nexperience prolonged one-shot inference times due to the overuse of SAM,\nresulting in low efficiency and limited automation ability. To address these\nissues, we propose a simple yet effective approach based on graph analysis. In\nparticular, a Positive-Negative Alignment module dynamically selects the point\nprompts for generating masks, especially uncovering the potential of the\nbackground context as the negative reference. Another subsequent Point-Mask\nClustering module aligns the granularity of masks and selected points as a\ndirected graph, based on mask coverage over points. These points are then\naggregated by decomposing the weakly connected components of the directed graph\nin an efficient manner, constructing distinct natural clusters. Finally, the\npositive and overshooting gating, benefiting from graph-based granularity\nalignment, aggregate high-confident masks and filter out the false-positive\nmasks for final prediction, reducing the usage of additional hyperparameters\nand redundant mask generation. Extensive experimental analysis across standard\nFSS, One-shot Part Segmentation, and Cross Domain FSS datasets validate the\neffectiveness and efficiency of the proposed approach, surpassing\nstate-of-the-art generalist models with a mIoU of 58.7% on COCO-20i and 35.2%\non LVIS-92i. The code is available in https://andyzaq.github.io/GF-SAM/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advancements in large-scale pre-training techniques have\nsignificantly enhanced the capabilities of vision foundation models, notably\nthe Segment Anything Model (SAM), which can generate precise masks based on\npoint and box prompts. Recent studies extend SAM to Few-shot Semantic\nSegmentation (FSS), focusing on prompt generation for SAM-based automatic\nsemantic segmentation. However, these methods struggle with selecting suitable\nprompts, require specific hyperparameter settings for different scenarios, and\nexperience prolonged one-shot inference times due to the overuse of SAM,\nresulting in low efficiency and limited automation ability. To address these\nissues, we propose a simple yet effective approach based on graph analysis. In\nparticular, a Positive-Negative Alignment module dynamically selects the point\nprompts for generating masks, especially uncovering the potential of the\nbackground context as the negative reference. Another subsequent Point-Mask\nClustering module aligns the granularity of masks and selected points as a\ndirected graph, based on mask coverage over points. These points are then\naggregated by decomposing the weakly connected components of the directed graph\nin an efficient manner, constructing distinct natural clusters. Finally, the\npositive and overshooting gating, benefiting from graph-based granularity\nalignment, aggregate high-confident masks and filter out the false-positive\nmasks for final prediction, reducing the usage of additional hyperparameters\nand redundant mask generation. Extensive experimental analysis across standard\nFSS, One-shot Part Segmentation, and Cross Domain FSS datasets validate the\neffectiveness and efficiency of the proposed approach, surpassing\nstate-of-the-art generalist models with a mIoU of 58.7% on COCO-20i and 35.2%\non LVIS-92i. The code is available in https://andyzaq.github.io/GF-SAM/."
                },
                "authors": [
                    {
                        "name": "Anqi Zhang"
                    },
                    {
                        "name": "Guangyu Gao"
                    },
                    {
                        "name": "Jianbo Jiao"
                    },
                    {
                        "name": "Chi Harold Liu"
                    },
                    {
                        "name": "Yunchao Wei"
                    }
                ],
                "author_detail": {
                    "name": "Yunchao Wei"
                },
                "author": "Yunchao Wei",
                "arxiv_comment": "Accepted to NeurIPS 2024 as Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06964v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06964v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03659v2",
                "updated": "2024-10-11T15:07:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    7,
                    31,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-04T17:59:28Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    17,
                    59,
                    28,
                    4,
                    278,
                    0
                ],
                "title": "Unraveling Cross-Modality Knowledge Conflicts in Large Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unraveling Cross-Modality Knowledge Conflicts in Large Vision-Language\n  Models"
                },
                "summary": "Large Vision-Language Models (LVLMs) have demonstrated impressive\ncapabilities for capturing and reasoning over multimodal inputs. However, these\nmodels are prone to parametric knowledge conflicts, which arise from\ninconsistencies of represented knowledge between their vision and language\ncomponents. In this paper, we formally define the problem of\n$\\textbf{cross-modality parametric knowledge conflict}$ and present a\nsystematic approach to detect, interpret, and mitigate them. We introduce a\npipeline that identifies conflicts between visual and textual answers, showing\na persistently high conflict rate across modalities in recent LVLMs regardless\nof the model size. We further investigate how these conflicts interfere with\nthe inference process and propose a contrastive metric to discern the\nconflicting samples from the others. Building on these insights, we develop a\nnovel dynamic contrastive decoding method that removes undesirable logits\ninferred from the less confident modality components based on answer\nconfidence. For models that do not provide logits, we also introduce two\nprompt-based strategies to mitigate the conflicts. Our methods achieve\npromising improvements in accuracy on both the ViQuAE and InfoSeek datasets.\nSpecifically, using LLaVA-34B, our proposed dynamic contrastive decoding\nimproves an average accuracy of 2.24%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) have demonstrated impressive\ncapabilities for capturing and reasoning over multimodal inputs. However, these\nmodels are prone to parametric knowledge conflicts, which arise from\ninconsistencies of represented knowledge between their vision and language\ncomponents. In this paper, we formally define the problem of\n$\\textbf{cross-modality parametric knowledge conflict}$ and present a\nsystematic approach to detect, interpret, and mitigate them. We introduce a\npipeline that identifies conflicts between visual and textual answers, showing\na persistently high conflict rate across modalities in recent LVLMs regardless\nof the model size. We further investigate how these conflicts interfere with\nthe inference process and propose a contrastive metric to discern the\nconflicting samples from the others. Building on these insights, we develop a\nnovel dynamic contrastive decoding method that removes undesirable logits\ninferred from the less confident modality components based on answer\nconfidence. For models that do not provide logits, we also introduce two\nprompt-based strategies to mitigate the conflicts. Our methods achieve\npromising improvements in accuracy on both the ViQuAE and InfoSeek datasets.\nSpecifically, using LLaVA-34B, our proposed dynamic contrastive decoding\nimproves an average accuracy of 2.24%."
                },
                "authors": [
                    {
                        "name": "Tinghui Zhu"
                    },
                    {
                        "name": "Qin Liu"
                    },
                    {
                        "name": "Fei Wang"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    },
                    {
                        "name": "Muhao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Muhao Chen"
                },
                "author": "Muhao Chen",
                "arxiv_comment": "Website:\n  https://darthzhu.github.io/cross-modality-knowledge-conflict/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08343v2",
                "updated": "2024-10-11T15:03:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    3,
                    53,
                    4,
                    285,
                    0
                ],
                "published": "2024-07-11T09:47:50Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    9,
                    47,
                    50,
                    3,
                    193,
                    0
                ],
                "title": "Many wrong models approach to localize an odor source in turbulence with\n  static sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many wrong models approach to localize an odor source in turbulence with\n  static sensors"
                },
                "summary": "The problem of locating an odor source in turbulent flows is central to key\napplications such as environmental monitoring and disaster response. We address\nthis challenge by designing an algorithm based on Bayesian inference, which\nuses odor measurements from an ensemble of static sensors to estimate the\nsource position through a stochastic model of the environment. The problem is\nhard because of the multi-scale and out-of-equilibrium properties of turbulent\ntransport, which lacks accurate analytical and phenomenological modeling, thus\npreventing a guaranteed convergence for Bayesian approaches. To overcome the\nrisk of relying on a single unavoidably wrong model approximation, we propose a\nmethod to rank \"many wrong models\" and to blend their predictions. We evaluate\nour weighted Bayesian update algorithm by its ability to estimate the source\nlocation with predefined accuracy and/or within a specified time frame, and\ncompare it to standard Monte Carlo sampling methods. To demonstrate the\nrobustness and potential applications of both approaches under realistic\nenvironmental conditions, we use high-quality direct numerical simulations of\nthe Navier-Stokes equations to mimic the transport of odors in the atmospheric\nboundary layer. Despite minimal prior information about the source and\nenvironmental conditions, our proposed approach consistently proves to be more\naccurate, reliable, and robust than Monte Carlo methods, thus showing promise\nas a new tool for addressing the odor source localization problem in real-world\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The problem of locating an odor source in turbulent flows is central to key\napplications such as environmental monitoring and disaster response. We address\nthis challenge by designing an algorithm based on Bayesian inference, which\nuses odor measurements from an ensemble of static sensors to estimate the\nsource position through a stochastic model of the environment. The problem is\nhard because of the multi-scale and out-of-equilibrium properties of turbulent\ntransport, which lacks accurate analytical and phenomenological modeling, thus\npreventing a guaranteed convergence for Bayesian approaches. To overcome the\nrisk of relying on a single unavoidably wrong model approximation, we propose a\nmethod to rank \"many wrong models\" and to blend their predictions. We evaluate\nour weighted Bayesian update algorithm by its ability to estimate the source\nlocation with predefined accuracy and/or within a specified time frame, and\ncompare it to standard Monte Carlo sampling methods. To demonstrate the\nrobustness and potential applications of both approaches under realistic\nenvironmental conditions, we use high-quality direct numerical simulations of\nthe Navier-Stokes equations to mimic the transport of odors in the atmospheric\nboundary layer. Despite minimal prior information about the source and\nenvironmental conditions, our proposed approach consistently proves to be more\naccurate, reliable, and robust than Monte Carlo methods, thus showing promise\nas a new tool for addressing the odor source localization problem in real-world\nscenarios."
                },
                "authors": [
                    {
                        "name": "Lorenzo Piro"
                    },
                    {
                        "name": "Robin A. Heinonen"
                    },
                    {
                        "name": "Massimo Cencini"
                    },
                    {
                        "name": "Luca Biferale"
                    }
                ],
                "author_detail": {
                    "name": "Luca Biferale"
                },
                "author": "Luca Biferale",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01298v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01298v2",
                "updated": "2024-10-11T14:58:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    58,
                    31,
                    4,
                    285,
                    0
                ],
                "published": "2024-08-02T14:41:03Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    14,
                    41,
                    3,
                    4,
                    215,
                    0
                ],
                "title": "Probabilistic Inversion Modeling of Gas Emissions: A Gradient-Based MCMC\n  Estimation of Gaussian Plume Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Inversion Modeling of Gas Emissions: A Gradient-Based MCMC\n  Estimation of Gaussian Plume Parameters"
                },
                "summary": "In response to global concerns regarding air quality and the environmental\nimpact of greenhouse gas emissions, detecting and quantifying sources of\nemissions has become critical. To understand this impact and target mitigations\neffectively, methods for accurate quantification of greenhouse gas emissions\nare required. In this paper, we focus on the inversion of concentration\nmeasurements to estimate source location and emission rate. In practice, such\nmethods often rely on atmospheric stability class-based Gaussian plume\ndispersion models. However, incorrectly identifying the atmospheric stability\nclass can lead to significant bias in estimates of source characteristics. We\npresent a robust approach that reduces this bias by jointly estimating the\nhorizontal and vertical dispersion parameters of the Gaussian plume model,\ntogether with source location and emission rate, atmospheric background\nconcentration, and sensor measurement error variance. Uncertainty in parameter\nestimation is quantified through probabilistic inversion using gradient-based\nMCMC methods. A simulation study is performed to assess the inversion\nmethodology. We then focus on inference for the published Chilbolton dataset\nwhich contains controlled methane releases and demonstrates the practical\nbenefits of estimating dispersion parameters in source inversion problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In response to global concerns regarding air quality and the environmental\nimpact of greenhouse gas emissions, detecting and quantifying sources of\nemissions has become critical. To understand this impact and target mitigations\neffectively, methods for accurate quantification of greenhouse gas emissions\nare required. In this paper, we focus on the inversion of concentration\nmeasurements to estimate source location and emission rate. In practice, such\nmethods often rely on atmospheric stability class-based Gaussian plume\ndispersion models. However, incorrectly identifying the atmospheric stability\nclass can lead to significant bias in estimates of source characteristics. We\npresent a robust approach that reduces this bias by jointly estimating the\nhorizontal and vertical dispersion parameters of the Gaussian plume model,\ntogether with source location and emission rate, atmospheric background\nconcentration, and sensor measurement error variance. Uncertainty in parameter\nestimation is quantified through probabilistic inversion using gradient-based\nMCMC methods. A simulation study is performed to assess the inversion\nmethodology. We then focus on inference for the published Chilbolton dataset\nwhich contains controlled methane releases and demonstrates the practical\nbenefits of estimating dispersion parameters in source inversion problems."
                },
                "authors": [
                    {
                        "name": "Thomas Newman"
                    },
                    {
                        "name": "Christopher Nemeth"
                    },
                    {
                        "name": "Matthew Jones"
                    },
                    {
                        "name": "Philip Jonathan"
                    }
                ],
                "author_detail": {
                    "name": "Philip Jonathan"
                },
                "author": "Philip Jonathan",
                "arxiv_comment": "21 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01298v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01298v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07225v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07225v3",
                "updated": "2024-10-11T14:55:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    55,
                    44,
                    4,
                    285,
                    0
                ],
                "published": "2023-10-11T06:26:19Z",
                "published_parsed": [
                    2023,
                    10,
                    11,
                    6,
                    26,
                    19,
                    2,
                    284,
                    0
                ],
                "title": "Do Large Language Models have Shared Weaknesses in Medical Question\n  Answering?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models have Shared Weaknesses in Medical Question\n  Answering?"
                },
                "summary": "Large language models (LLMs) have made rapid improvement on medical\nbenchmarks, but their unreliability remains a persistent challenge for safe\nreal-world uses. To design for the use LLMs as a category, rather than for\nspecific models, requires developing an understanding of shared strengths and\nweaknesses which appear across models. To address this challenge, we benchmark\na range of top LLMs and identify consistent patterns across models. We test\n$16$ well-known LLMs on $874$ newly collected questions from Polish medical\nlicensing exams. For each question, we score each model on the top-1 accuracy\nand the distribution of probabilities assigned. We then compare these results\nwith factors such as question difficulty for humans, question length, and the\nscores of the other models. LLM accuracies were positively correlated pairwise\n($0.39$ to $0.58$). Model performance was also correlated with human\nperformance ($0.09$ to $0.13$), but negatively correlated to the difference\nbetween the question-level accuracy of top-scoring and bottom-scoring humans\n($-0.09$ to $-0.14$). The top output probability and question length were\npositive and negative predictors of accuracy respectively (p$< 0.05$). The top\nscoring LLM, GPT-4o Turbo, scored $84\\%$, with Claude Opus, Gemini 1.5 Pro and\nLlama 3/3.1 between $74\\%$ and $79\\%$. We found evidence of similarities\nbetween models in which questions they answer correctly, as well as\nsimilarities with human test takers. Larger models typically performed better,\nbut differences in training, architecture, and data were also highly impactful.\nModel accuracy was positively correlated with confidence, but negatively\ncorrelated with question length. We find similar results with older models, and\nargue that these patterns are likely to persist across future models using\nsimilar training methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made rapid improvement on medical\nbenchmarks, but their unreliability remains a persistent challenge for safe\nreal-world uses. To design for the use LLMs as a category, rather than for\nspecific models, requires developing an understanding of shared strengths and\nweaknesses which appear across models. To address this challenge, we benchmark\na range of top LLMs and identify consistent patterns across models. We test\n$16$ well-known LLMs on $874$ newly collected questions from Polish medical\nlicensing exams. For each question, we score each model on the top-1 accuracy\nand the distribution of probabilities assigned. We then compare these results\nwith factors such as question difficulty for humans, question length, and the\nscores of the other models. LLM accuracies were positively correlated pairwise\n($0.39$ to $0.58$). Model performance was also correlated with human\nperformance ($0.09$ to $0.13$), but negatively correlated to the difference\nbetween the question-level accuracy of top-scoring and bottom-scoring humans\n($-0.09$ to $-0.14$). The top output probability and question length were\npositive and negative predictors of accuracy respectively (p$< 0.05$). The top\nscoring LLM, GPT-4o Turbo, scored $84\\%$, with Claude Opus, Gemini 1.5 Pro and\nLlama 3/3.1 between $74\\%$ and $79\\%$. We found evidence of similarities\nbetween models in which questions they answer correctly, as well as\nsimilarities with human test takers. Larger models typically performed better,\nbut differences in training, architecture, and data were also highly impactful.\nModel accuracy was positively correlated with confidence, but negatively\ncorrelated with question length. We find similar results with older models, and\nargue that these patterns are likely to persist across future models using\nsimilar training methods."
                },
                "authors": [
                    {
                        "name": "Andrew M. Bean"
                    },
                    {
                        "name": "Karolina Korgul"
                    },
                    {
                        "name": "Felix Krones"
                    },
                    {
                        "name": "Robert McCraith"
                    },
                    {
                        "name": "Adam Mahdi"
                    }
                ],
                "author_detail": {
                    "name": "Adam Mahdi"
                },
                "author": "Adam Mahdi",
                "arxiv_comment": "8 pages, 10 figures. To appear in NeurIPS 2024 Advancements in\n  Medical Foundation Models Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.07225v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07225v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08869v1",
                "updated": "2024-10-11T14:46:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    46,
                    49,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T14:46:49Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    46,
                    49,
                    4,
                    285,
                    0
                ],
                "title": "Evolution of SAE Features Across Layers in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolution of SAE Features Across Layers in LLMs"
                },
                "summary": "Sparse Autoencoders for transformer-based language models are typically\ndefined independently per layer. In this work we analyze statistical\nrelationships between features in adjacent layers to understand how features\nevolve through a forward pass. We provide a graph visualization interface for\nfeatures and their most similar next-layer neighbors, and build communities of\nrelated features across layers. We find that a considerable amount of features\nare passed through from a previous layer, some features can be expressed as\nquasi-boolean combinations of previous features, and some features become more\nspecialized in later layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders for transformer-based language models are typically\ndefined independently per layer. In this work we analyze statistical\nrelationships between features in adjacent layers to understand how features\nevolve through a forward pass. We provide a graph visualization interface for\nfeatures and their most similar next-layer neighbors, and build communities of\nrelated features across layers. We find that a considerable amount of features\nare passed through from a previous layer, some features can be expressed as\nquasi-boolean combinations of previous features, and some features become more\nspecialized in later layers."
                },
                "authors": [
                    {
                        "name": "Daniel Balcells"
                    },
                    {
                        "name": "Benjamin Lerner"
                    },
                    {
                        "name": "Michael Oesterle"
                    },
                    {
                        "name": "Ediz Ucar"
                    },
                    {
                        "name": "Stefan Heimersheim"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Heimersheim"
                },
                "author": "Stefan Heimersheim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17263v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17263v3",
                "updated": "2024-10-11T14:45:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    45,
                    41,
                    4,
                    285,
                    0
                ],
                "published": "2024-06-25T04:07:22Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    4,
                    7,
                    22,
                    1,
                    177,
                    0
                ],
                "title": "Efficient, Multimodal, and Derivative-Free Bayesian Inference With\n  Fisher-Rao Gradient Flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient, Multimodal, and Derivative-Free Bayesian Inference With\n  Fisher-Rao Gradient Flows"
                },
                "summary": "In this paper, we study efficient approximate sampling for probability\ndistributions known up to normalization constants. We specifically focus on a\nproblem class arising in Bayesian inference for large-scale inverse problems in\nscience and engineering applications. The computational challenges we address\nwith the proposed methodology are: (i) the need for repeated evaluations of\nexpensive forward models; (ii) the potential existence of multiple modes; and\n(iii) the fact that gradient of, or adjoint solver for, the forward model might\nnot be feasible.\n  While existing Bayesian inference methods meet some of these challenges\nindividually, we propose a framework that tackles all three systematically. Our\napproach builds upon the Fisher-Rao gradient flow in probability space,\nyielding a dynamical system for probability densities that converges towards\nthe target distribution at a uniform exponential rate. This rapid convergence\nis advantageous for the computational burden outlined in (i). We apply Gaussian\nmixture approximations with operator splitting techniques to simulate the flow\nnumerically; the resulting approximation can capture multiple modes thus\naddressing (ii). Furthermore, we employ the Kalman methodology to facilitate a\nderivative-free update of these Gaussian components and their respective\nweights, addressing the issue in (iii).\n  The proposed methodology results in an efficient derivative-free sampler\nflexible enough to handle multi-modal distributions: Gaussian Mixture Kalman\nInversion (GMKI). The effectiveness of GMKI is demonstrated both theoretically\nand numerically in several experiments with multimodal target distributions,\nincluding proof-of-concept and two-dimensional examples, as well as a\nlarge-scale application: recovering the Navier-Stokes initial condition from\nsolution data at positive times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study efficient approximate sampling for probability\ndistributions known up to normalization constants. We specifically focus on a\nproblem class arising in Bayesian inference for large-scale inverse problems in\nscience and engineering applications. The computational challenges we address\nwith the proposed methodology are: (i) the need for repeated evaluations of\nexpensive forward models; (ii) the potential existence of multiple modes; and\n(iii) the fact that gradient of, or adjoint solver for, the forward model might\nnot be feasible.\n  While existing Bayesian inference methods meet some of these challenges\nindividually, we propose a framework that tackles all three systematically. Our\napproach builds upon the Fisher-Rao gradient flow in probability space,\nyielding a dynamical system for probability densities that converges towards\nthe target distribution at a uniform exponential rate. This rapid convergence\nis advantageous for the computational burden outlined in (i). We apply Gaussian\nmixture approximations with operator splitting techniques to simulate the flow\nnumerically; the resulting approximation can capture multiple modes thus\naddressing (ii). Furthermore, we employ the Kalman methodology to facilitate a\nderivative-free update of these Gaussian components and their respective\nweights, addressing the issue in (iii).\n  The proposed methodology results in an efficient derivative-free sampler\nflexible enough to handle multi-modal distributions: Gaussian Mixture Kalman\nInversion (GMKI). The effectiveness of GMKI is demonstrated both theoretically\nand numerically in several experiments with multimodal target distributions,\nincluding proof-of-concept and two-dimensional examples, as well as a\nlarge-scale application: recovering the Navier-Stokes initial condition from\nsolution data at positive times."
                },
                "authors": [
                    {
                        "name": "Yifan Chen"
                    },
                    {
                        "name": "Daniel Zhengyu Huang"
                    },
                    {
                        "name": "Jiaoyang Huang"
                    },
                    {
                        "name": "Sebastian Reich"
                    },
                    {
                        "name": "Andrew M. Stuart"
                    }
                ],
                "author_detail": {
                    "name": "Andrew M. Stuart"
                },
                "author": "Andrew M. Stuart",
                "arxiv_comment": "42 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17263v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17263v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08862v1",
                "updated": "2024-10-11T14:42:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    42,
                    20,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T14:42:20Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    42,
                    20,
                    4,
                    285,
                    0
                ],
                "title": "A test for LISA foreground Gaussianity and stationarity. II. Extreme\n  mass-ratio inspirals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A test for LISA foreground Gaussianity and stationarity. II. Extreme\n  mass-ratio inspirals"
                },
                "summary": "Extreme Mass Ratio Inspirals (EMRIs) are key observational targets for the\nLaser Interferometer Space Antenna (LISA) mission. Unresolvable EMRI signals\ncontribute to forming a gravitational wave background (GWB). Characterizing the\nstatistical features of the GWB from EMRIs is of great importance, as EMRIs\nwill ubiquitously affect large segments of the inference scheme. In this work,\nwe apply a frequentist test for GWB Gaussianity and stationarity, exploring\nthree astrophysically-motivated EMRI populations. We construct the resulting\nsignal by combining state-of-the-art EMRI waveforms and a detailed description\nof the LISA response with time-delay interferometric variables. Depending on\nthe brightness of the GWB, our analysis demonstrates that the resultant EMRI\nforegrounds show varying degrees of departure from the usual statistical\nassumptions that the GWBs are both Gaussian and Stationary. If the GWB is\nnon-stationary with non-Gaussian features, this will challenge the robustness\nof Gaussian-likelihood model, when applied to global inference results, e.g.\nforeground estimation, background detection, and individual-source parameters\nreconstruction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme Mass Ratio Inspirals (EMRIs) are key observational targets for the\nLaser Interferometer Space Antenna (LISA) mission. Unresolvable EMRI signals\ncontribute to forming a gravitational wave background (GWB). Characterizing the\nstatistical features of the GWB from EMRIs is of great importance, as EMRIs\nwill ubiquitously affect large segments of the inference scheme. In this work,\nwe apply a frequentist test for GWB Gaussianity and stationarity, exploring\nthree astrophysically-motivated EMRI populations. We construct the resulting\nsignal by combining state-of-the-art EMRI waveforms and a detailed description\nof the LISA response with time-delay interferometric variables. Depending on\nthe brightness of the GWB, our analysis demonstrates that the resultant EMRI\nforegrounds show varying degrees of departure from the usual statistical\nassumptions that the GWBs are both Gaussian and Stationary. If the GWB is\nnon-stationary with non-Gaussian features, this will challenge the robustness\nof Gaussian-likelihood model, when applied to global inference results, e.g.\nforeground estimation, background detection, and individual-source parameters\nreconstruction."
                },
                "authors": [
                    {
                        "name": "Manuel Piarulli"
                    },
                    {
                        "name": "Riccardo Buscicchio"
                    },
                    {
                        "name": "Federico Pozzoli"
                    },
                    {
                        "name": "Ollie Burke"
                    },
                    {
                        "name": "Matteo Bonetti"
                    },
                    {
                        "name": "Alberto Sesana"
                    }
                ],
                "author_detail": {
                    "name": "Alberto Sesana"
                },
                "author": "Alberto Sesana",
                "arxiv_comment": "12 pages, 5 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08860v1",
                "updated": "2024-10-11T14:40:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    40,
                    51,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T14:40:51Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    40,
                    51,
                    4,
                    285,
                    0
                ],
                "title": "Audio Description Generation in the Era of LLMs and VLMs: A Review of\n  Transferable Generative AI Technologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio Description Generation in the Era of LLMs and VLMs: A Review of\n  Transferable Generative AI Technologies"
                },
                "summary": "Audio descriptions (ADs) function as acoustic commentaries designed to assist\nblind persons and persons with visual impairments in accessing digital media\ncontent on television and in movies, among other settings. As an accessibility\nservice typically provided by trained AD professionals, the generation of ADs\ndemands significant human effort, making the process both time-consuming and\ncostly. Recent advancements in natural language processing (NLP) and computer\nvision (CV), particularly in large language models (LLMs) and vision-language\nmodels (VLMs), have allowed for getting a step closer to automatic AD\ngeneration. This paper reviews the technologies pertinent to AD generation in\nthe era of LLMs and VLMs: we discuss how state-of-the-art NLP and CV\ntechnologies can be applied to generate ADs and identify essential research\ndirections for the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio descriptions (ADs) function as acoustic commentaries designed to assist\nblind persons and persons with visual impairments in accessing digital media\ncontent on television and in movies, among other settings. As an accessibility\nservice typically provided by trained AD professionals, the generation of ADs\ndemands significant human effort, making the process both time-consuming and\ncostly. Recent advancements in natural language processing (NLP) and computer\nvision (CV), particularly in large language models (LLMs) and vision-language\nmodels (VLMs), have allowed for getting a step closer to automatic AD\ngeneration. This paper reviews the technologies pertinent to AD generation in\nthe era of LLMs and VLMs: we discuss how state-of-the-art NLP and CV\ntechnologies can be applied to generate ADs and identify essential research\ndirections for the future."
                },
                "authors": [
                    {
                        "name": "Yingqiang Gao"
                    },
                    {
                        "name": "Lukas Fischer"
                    },
                    {
                        "name": "Alexa Lintner"
                    },
                    {
                        "name": "Sarah Ebling"
                    }
                ],
                "author_detail": {
                    "name": "Sarah Ebling"
                },
                "author": "Sarah Ebling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08858v1",
                "updated": "2024-10-11T14:39:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    39,
                    24,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T14:39:24Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    39,
                    24,
                    4,
                    285,
                    0
                ],
                "title": "Decoding Secret Memorization in Code LLMs Through Token-Level\n  Characterization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding Secret Memorization in Code LLMs Through Token-Level\n  Characterization"
                },
                "summary": "Code Large Language Models (LLMs) have demonstrated remarkable capabilities\nin generating, understanding, and manipulating programming code. However, their\ntraining process inadvertently leads to the memorization of sensitive\ninformation, posing severe privacy risks. Existing studies on memorization in\nLLMs primarily rely on prompt engineering techniques, which suffer from\nlimitations such as widespread hallucination and inefficient extraction of the\ntarget sensitive information. In this paper, we present a novel approach to\ncharacterize real and fake secrets generated by Code LLMs based on token\nprobabilities. We identify four key characteristics that differentiate genuine\nsecrets from hallucinated ones, providing insights into distinguishing real and\nfake secrets. To overcome the limitations of existing works, we propose DESEC,\na two-stage method that leverages token-level features derived from the\nidentified characteristics to guide the token decoding process. DESEC consists\nof constructing an offline token scoring model using a proxy Code LLM and\nemploying the scoring model to guide the decoding process by reassigning token\nlikelihoods. Through extensive experiments on four state-of-the-art Code LLMs\nusing a diverse dataset, we demonstrate the superior performance of DESEC in\nachieving a higher plausible rate and extracting more real secrets compared to\nexisting baselines. Our findings highlight the effectiveness of our token-level\napproach in enabling an extensive assessment of the privacy leakage risks\nassociated with Code LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Large Language Models (LLMs) have demonstrated remarkable capabilities\nin generating, understanding, and manipulating programming code. However, their\ntraining process inadvertently leads to the memorization of sensitive\ninformation, posing severe privacy risks. Existing studies on memorization in\nLLMs primarily rely on prompt engineering techniques, which suffer from\nlimitations such as widespread hallucination and inefficient extraction of the\ntarget sensitive information. In this paper, we present a novel approach to\ncharacterize real and fake secrets generated by Code LLMs based on token\nprobabilities. We identify four key characteristics that differentiate genuine\nsecrets from hallucinated ones, providing insights into distinguishing real and\nfake secrets. To overcome the limitations of existing works, we propose DESEC,\na two-stage method that leverages token-level features derived from the\nidentified characteristics to guide the token decoding process. DESEC consists\nof constructing an offline token scoring model using a proxy Code LLM and\nemploying the scoring model to guide the decoding process by reassigning token\nlikelihoods. Through extensive experiments on four state-of-the-art Code LLMs\nusing a diverse dataset, we demonstrate the superior performance of DESEC in\nachieving a higher plausible rate and extracting more real secrets compared to\nexisting baselines. Our findings highlight the effectiveness of our token-level\napproach in enabling an extensive assessment of the privacy leakage risks\nassociated with Code LLMs."
                },
                "authors": [
                    {
                        "name": "Yuqing Nie"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Kailong Wang"
                    },
                    {
                        "name": "Guoai Xu"
                    },
                    {
                        "name": "Guosheng Xu"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "arxiv_comment": "12 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01326v2",
                "updated": "2024-10-11T14:38:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    38,
                    40,
                    4,
                    285,
                    0
                ],
                "published": "2024-06-03T13:54:05Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    13,
                    54,
                    5,
                    0,
                    155,
                    0
                ],
                "title": "TabPedia: Towards Comprehensive Visual Table Understanding with Concept\n  Synergy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TabPedia: Towards Comprehensive Visual Table Understanding with Concept\n  Synergy"
                },
                "summary": "Tables contain factual and quantitative data accompanied by various\nstructures and contents that pose challenges for machine comprehension.\nPrevious methods generally design task-specific architectures and objectives\nfor individual tasks, resulting in modal isolation and intricate workflows. In\nthis paper, we present a novel large vision-language model, TabPedia, equipped\nwith a concept synergy mechanism. In this mechanism, all the involved diverse\nvisual table understanding (VTU) tasks and multi-source visual embeddings are\nabstracted as concepts. This unified framework allows TabPedia to seamlessly\nintegrate VTU tasks, such as table detection, table structure recognition,\ntable querying, and table question answering, by leveraging the capabilities of\nlarge language models (LLMs). Moreover, the concept synergy mechanism enables\ntable perception-related and comprehension-related tasks to work in harmony, as\nthey can effectively leverage the needed clues from the corresponding source\nperception embeddings. Furthermore, to better evaluate the VTU task in\nreal-world scenarios, we establish a new and comprehensive table VQA benchmark,\nComTQA, featuring approximately 9,000 QA pairs. Extensive quantitative and\nqualitative experiments on both table perception and comprehension tasks,\nconducted across various public benchmarks, validate the effectiveness of our\nTabPedia. The superior performance further confirms the feasibility of using\nLLMs for understanding visual tables when all concepts work in synergy. The\nbenchmark ComTQA has been open-sourced at\nhttps://huggingface.co/datasets/ByteDance/ComTQA. The source code and model\nalso have been released athttps://github.com/zhaowc-ustc/TabPedia.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tables contain factual and quantitative data accompanied by various\nstructures and contents that pose challenges for machine comprehension.\nPrevious methods generally design task-specific architectures and objectives\nfor individual tasks, resulting in modal isolation and intricate workflows. In\nthis paper, we present a novel large vision-language model, TabPedia, equipped\nwith a concept synergy mechanism. In this mechanism, all the involved diverse\nvisual table understanding (VTU) tasks and multi-source visual embeddings are\nabstracted as concepts. This unified framework allows TabPedia to seamlessly\nintegrate VTU tasks, such as table detection, table structure recognition,\ntable querying, and table question answering, by leveraging the capabilities of\nlarge language models (LLMs). Moreover, the concept synergy mechanism enables\ntable perception-related and comprehension-related tasks to work in harmony, as\nthey can effectively leverage the needed clues from the corresponding source\nperception embeddings. Furthermore, to better evaluate the VTU task in\nreal-world scenarios, we establish a new and comprehensive table VQA benchmark,\nComTQA, featuring approximately 9,000 QA pairs. Extensive quantitative and\nqualitative experiments on both table perception and comprehension tasks,\nconducted across various public benchmarks, validate the effectiveness of our\nTabPedia. The superior performance further confirms the feasibility of using\nLLMs for understanding visual tables when all concepts work in synergy. The\nbenchmark ComTQA has been open-sourced at\nhttps://huggingface.co/datasets/ByteDance/ComTQA. The source code and model\nalso have been released athttps://github.com/zhaowc-ustc/TabPedia."
                },
                "authors": [
                    {
                        "name": "Weichao Zhao"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Jingqun Tang"
                    },
                    {
                        "name": "Shu Wei"
                    },
                    {
                        "name": "Binghong Wu"
                    },
                    {
                        "name": "Lei Liao"
                    },
                    {
                        "name": "Yongjie Ye"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Wengang Zhou"
                    },
                    {
                        "name": "Houqiang Li"
                    },
                    {
                        "name": "Can Huang"
                    }
                ],
                "author_detail": {
                    "name": "Can Huang"
                },
                "author": "Can Huang",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08855v1",
                "updated": "2024-10-11T14:32:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    32,
                    6,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T14:32:06Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    32,
                    6,
                    4,
                    285,
                    0
                ],
                "title": "MATCH: Model-Aware TVM-based Compilation for Heterogeneous Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MATCH: Model-Aware TVM-based Compilation for Heterogeneous Edge Devices"
                },
                "summary": "Streamlining the deployment of Deep Neural Networks (DNNs) on heterogeneous\nedge platforms, coupling within the same micro-controller unit (MCU)\ninstruction processors and hardware accelerators for tensor computations, is\nbecoming one of the crucial challenges of the TinyML field.\n  The best-performing DNN compilation toolchains are usually deeply customized\nfor a single MCU family, and porting to a different heterogeneous MCU family\nimplies labor-intensive re-development of almost the entire compiler. On the\nopposite side, retargetable toolchains, such as TVM, fail to exploit the\ncapabilities of custom accelerators, resulting in the generation of general but\nunoptimized code. To overcome this duality, we introduce MATCH, a novel\nTVM-based DNN deployment framework designed for easy agile retargeting across\ndifferent MCU processors and accelerators, thanks to a customizable model-based\nhardware abstraction.\n  We show that a general and retargetable mapping framework enhanced with\nhardware cost models can compete with and even outperform custom toolchains on\ndiverse targets while only needing the definition of an abstract hardware model\nand a SoC-specific API.\n  We tested MATCH on two state-of-the-art heterogeneous MCUs, GAP9 and DIANA.\n  On the four DNN models of the MLPerf Tiny suite MATCH reduces inference\nlatency by up to 60.88 times on DIANA, compared to using the plain TVM, thanks\nto the exploitation of the on-board HW accelerator. Compared to HTVM, a fully\ncustomized toolchain for DIANA, we still reduce the latency by 16.94%. On GAP9,\nusing the same benchmarks, we improve the latency by 2.15 times compared to the\ndedicated DORY compiler, thanks to our heterogeneous DNN mapping approach that\nsynergically exploits the DNN accelerator and the eight-cores cluster available\non board.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streamlining the deployment of Deep Neural Networks (DNNs) on heterogeneous\nedge platforms, coupling within the same micro-controller unit (MCU)\ninstruction processors and hardware accelerators for tensor computations, is\nbecoming one of the crucial challenges of the TinyML field.\n  The best-performing DNN compilation toolchains are usually deeply customized\nfor a single MCU family, and porting to a different heterogeneous MCU family\nimplies labor-intensive re-development of almost the entire compiler. On the\nopposite side, retargetable toolchains, such as TVM, fail to exploit the\ncapabilities of custom accelerators, resulting in the generation of general but\nunoptimized code. To overcome this duality, we introduce MATCH, a novel\nTVM-based DNN deployment framework designed for easy agile retargeting across\ndifferent MCU processors and accelerators, thanks to a customizable model-based\nhardware abstraction.\n  We show that a general and retargetable mapping framework enhanced with\nhardware cost models can compete with and even outperform custom toolchains on\ndiverse targets while only needing the definition of an abstract hardware model\nand a SoC-specific API.\n  We tested MATCH on two state-of-the-art heterogeneous MCUs, GAP9 and DIANA.\n  On the four DNN models of the MLPerf Tiny suite MATCH reduces inference\nlatency by up to 60.88 times on DIANA, compared to using the plain TVM, thanks\nto the exploitation of the on-board HW accelerator. Compared to HTVM, a fully\ncustomized toolchain for DIANA, we still reduce the latency by 16.94%. On GAP9,\nusing the same benchmarks, we improve the latency by 2.15 times compared to the\ndedicated DORY compiler, thanks to our heterogeneous DNN mapping approach that\nsynergically exploits the DNN accelerator and the eight-cores cluster available\non board."
                },
                "authors": [
                    {
                        "name": "Mohamed Amine Hamdi"
                    },
                    {
                        "name": "Francesco Daghero"
                    },
                    {
                        "name": "Giuseppe Maria Sarda"
                    },
                    {
                        "name": "Josse Van Delm"
                    },
                    {
                        "name": "Arne Symons"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Marian Verhelst"
                    },
                    {
                        "name": "Daniele Jahier Pagliari"
                    },
                    {
                        "name": "Alessio Burrello"
                    }
                ],
                "author_detail": {
                    "name": "Alessio Burrello"
                },
                "author": "Alessio Burrello",
                "arxiv_comment": "13 pages, 11 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.2; D.1.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08854v1",
                "updated": "2024-10-11T14:30:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    30,
                    4,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T14:30:04Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    30,
                    4,
                    4,
                    285,
                    0
                ],
                "title": "Hybrid LLM-DDQN based Joint Optimization of V2I Communication and\n  Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid LLM-DDQN based Joint Optimization of V2I Communication and\n  Autonomous Driving"
                },
                "summary": "Large language models (LLMs) have received considerable interest recently due\nto their outstanding reasoning and comprehension capabilities. This work\nexplores applying LLMs to vehicular networks, aiming to jointly optimize\nvehicle-to-infrastructure (V2I) communications and autonomous driving (AD)\npolicies. We deploy LLMs for AD decision-making to maximize traffic flow and\navoid collisions for road safety, and a double deep Q-learning algorithm (DDQN)\nis used for V2I optimization to maximize the received data rate and reduce\nfrequent handovers. In particular, for LLM-enabled AD, we employ the Euclidean\ndistance to identify previously explored AD experiences, and then LLMs can\nlearn from past good and bad decisions for further improvement. Then, LLM-based\nAD decisions will become part of states in V2I problems, and DDQN will optimize\nthe V2I decisions accordingly. After that, the AD and V2I decisions are\niteratively optimized until convergence. Such an iterative optimization\napproach can better explore the interactions between LLMs and conventional\nreinforcement learning techniques, revealing the potential of using LLMs for\nnetwork optimization and management. Finally, the simulations demonstrate that\nour proposed hybrid LLM-DDQN approach outperforms the conventional DDQN\nalgorithm, showing faster convergence and higher average rewards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have received considerable interest recently due\nto their outstanding reasoning and comprehension capabilities. This work\nexplores applying LLMs to vehicular networks, aiming to jointly optimize\nvehicle-to-infrastructure (V2I) communications and autonomous driving (AD)\npolicies. We deploy LLMs for AD decision-making to maximize traffic flow and\navoid collisions for road safety, and a double deep Q-learning algorithm (DDQN)\nis used for V2I optimization to maximize the received data rate and reduce\nfrequent handovers. In particular, for LLM-enabled AD, we employ the Euclidean\ndistance to identify previously explored AD experiences, and then LLMs can\nlearn from past good and bad decisions for further improvement. Then, LLM-based\nAD decisions will become part of states in V2I problems, and DDQN will optimize\nthe V2I decisions accordingly. After that, the AD and V2I decisions are\niteratively optimized until convergence. Such an iterative optimization\napproach can better explore the interactions between LLMs and conventional\nreinforcement learning techniques, revealing the potential of using LLMs for\nnetwork optimization and management. Finally, the simulations demonstrate that\nour proposed hybrid LLM-DDQN approach outperforms the conventional DDQN\nalgorithm, showing faster convergence and higher average rewards."
                },
                "authors": [
                    {
                        "name": "Zijiang Yan"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Hina Tabassum"
                    },
                    {
                        "name": "Xue Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xue Liu"
                },
                "author": "Xue Liu",
                "arxiv_comment": "Submission for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08851v1",
                "updated": "2024-10-11T14:27:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    27,
                    18,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T14:27:18Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    27,
                    18,
                    4,
                    285,
                    0
                ],
                "title": "Measuring the Inconsistency of Large Language Models in Preferential\n  Ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring the Inconsistency of Large Language Models in Preferential\n  Ranking"
                },
                "summary": "Despite large language models' (LLMs) recent advancements, their bias and\nhallucination issues persist, and their ability to offer consistent\npreferential rankings remains underexplored. This study investigates the\ncapacity of LLMs to provide consistent ordinal preferences, a crucial aspect in\nscenarios with dense decision space or lacking absolute answers. We introduce a\nformalization of consistency based on order theory, outlining criteria such as\ntransitivity, asymmetry, reversibility, and independence from irrelevant\nalternatives. Our diagnostic experiments on selected state-of-the-art LLMs\nreveal their inability to meet these criteria, indicating a strong positional\nbias and poor transitivity, with preferences easily swayed by irrelevant\nalternatives. These findings highlight a significant inconsistency in\nLLM-generated preferential rankings, underscoring the need for further research\nto address these limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite large language models' (LLMs) recent advancements, their bias and\nhallucination issues persist, and their ability to offer consistent\npreferential rankings remains underexplored. This study investigates the\ncapacity of LLMs to provide consistent ordinal preferences, a crucial aspect in\nscenarios with dense decision space or lacking absolute answers. We introduce a\nformalization of consistency based on order theory, outlining criteria such as\ntransitivity, asymmetry, reversibility, and independence from irrelevant\nalternatives. Our diagnostic experiments on selected state-of-the-art LLMs\nreveal their inability to meet these criteria, indicating a strong positional\nbias and poor transitivity, with preferences easily swayed by irrelevant\nalternatives. These findings highlight a significant inconsistency in\nLLM-generated preferential rankings, underscoring the need for further research\nto address these limitations."
                },
                "authors": [
                    {
                        "name": "Xiutian Zhao"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Wei Peng"
                    }
                ],
                "author_detail": {
                    "name": "Wei Peng"
                },
                "author": "Wei Peng",
                "arxiv_doi": "10.18653/v1/2024.knowllm-1.14",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.knowllm-1.14",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.08851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 1st Workshop on Towards Knowledgeable Language\n  Models (KnowLLM 2024)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08849v1",
                "updated": "2024-10-11T14:25:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    25,
                    39,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T14:25:39Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    25,
                    39,
                    4,
                    285,
                    0
                ],
                "title": "Causal inference targeting a concentration index for studies of health\n  inequalities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal inference targeting a concentration index for studies of health\n  inequalities"
                },
                "summary": "A concentration index, a standardized covariance between a health variable\nand relative income ranks, is often used to quantify income-related health\ninequalities. There is a lack of formal approach to study the effect of an\nexposure, e.g., education, on such measures of inequality. In this paper we\ncontribute by filling this gap and developing the necessary theory and method.\nThus, we define a counterfactual concentration index for different levels of an\nexposure. We give conditions for their identification, and then deduce their\nefficient influence function. This allows us to propose estimators, which are\nregular asymptotic linear under certain conditions. In particular, these\nestimators are $\\sqrt n$-consistent and asymptotically normal, as well as\nlocally efficient. The implementation of the estimators is based on the fit of\nseveral nuisance functions. The estimators proposed have rate robustness\nproperties allowing for convergence rates slower than $\\sqrt{n}$-rate for some\nof the nuisance function fits. The relevance of the asymptotic results for\nfinite samples is studied with simulation experiments. We also present a case\nstudy of the effect of education on income-related health inequalities for a\nSwedish cohort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A concentration index, a standardized covariance between a health variable\nand relative income ranks, is often used to quantify income-related health\ninequalities. There is a lack of formal approach to study the effect of an\nexposure, e.g., education, on such measures of inequality. In this paper we\ncontribute by filling this gap and developing the necessary theory and method.\nThus, we define a counterfactual concentration index for different levels of an\nexposure. We give conditions for their identification, and then deduce their\nefficient influence function. This allows us to propose estimators, which are\nregular asymptotic linear under certain conditions. In particular, these\nestimators are $\\sqrt n$-consistent and asymptotically normal, as well as\nlocally efficient. The implementation of the estimators is based on the fit of\nseveral nuisance functions. The estimators proposed have rate robustness\nproperties allowing for convergence rates slower than $\\sqrt{n}$-rate for some\nof the nuisance function fits. The relevance of the asymptotic results for\nfinite samples is studied with simulation experiments. We also present a case\nstudy of the effect of education on income-related health inequalities for a\nSwedish cohort."
                },
                "authors": [
                    {
                        "name": "Mohammad Ghasempour"
                    },
                    {
                        "name": "Xavier de Luna"
                    },
                    {
                        "name": "Per E. Gustafsson"
                    }
                ],
                "author_detail": {
                    "name": "Per E. Gustafsson"
                },
                "author": "Per E. Gustafsson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06963v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06963v2",
                "updated": "2024-10-11T14:12:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    12,
                    48,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-09T15:02:08Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    2,
                    8,
                    2,
                    283,
                    0
                ],
                "title": "ELMO: Enhanced Real-time LiDAR Motion Capture through Upsampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELMO: Enhanced Real-time LiDAR Motion Capture through Upsampling"
                },
                "summary": "This paper introduces ELMO, a real-time upsampling motion capture framework\ndesigned for a single LiDAR sensor. Modeled as a conditional autoregressive\ntransformer-based upsampling motion generator, ELMO achieves 60 fps motion\ncapture from a 20 fps LiDAR point cloud sequence. The key feature of ELMO is\nthe coupling of the self-attention mechanism with thoughtfully designed\nembedding modules for motion and point clouds, significantly elevating the\nmotion quality. To facilitate accurate motion capture, we develop a one-time\nskeleton calibration model capable of predicting user skeleton offsets from a\nsingle-frame point cloud. Additionally, we introduce a novel data augmentation\ntechnique utilizing a LiDAR simulator, which enhances global root tracking to\nimprove environmental understanding. To demonstrate the effectiveness of our\nmethod, we compare ELMO with state-of-the-art methods in both image-based and\npoint cloud-based motion capture. We further conduct an ablation study to\nvalidate our design principles. ELMO's fast inference time makes it well-suited\nfor real-time applications, exemplified in our demo video featuring live\nstreaming and interactive gaming scenarios. Furthermore, we contribute a\nhigh-quality LiDAR-mocap synchronized dataset comprising 20 different subjects\nperforming a range of motions, which can serve as a valuable resource for\nfuture research. The dataset and evaluation code are available at {\\blue\n\\url{https://movin3d.github.io/ELMO_SIGASIA2024/}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces ELMO, a real-time upsampling motion capture framework\ndesigned for a single LiDAR sensor. Modeled as a conditional autoregressive\ntransformer-based upsampling motion generator, ELMO achieves 60 fps motion\ncapture from a 20 fps LiDAR point cloud sequence. The key feature of ELMO is\nthe coupling of the self-attention mechanism with thoughtfully designed\nembedding modules for motion and point clouds, significantly elevating the\nmotion quality. To facilitate accurate motion capture, we develop a one-time\nskeleton calibration model capable of predicting user skeleton offsets from a\nsingle-frame point cloud. Additionally, we introduce a novel data augmentation\ntechnique utilizing a LiDAR simulator, which enhances global root tracking to\nimprove environmental understanding. To demonstrate the effectiveness of our\nmethod, we compare ELMO with state-of-the-art methods in both image-based and\npoint cloud-based motion capture. We further conduct an ablation study to\nvalidate our design principles. ELMO's fast inference time makes it well-suited\nfor real-time applications, exemplified in our demo video featuring live\nstreaming and interactive gaming scenarios. Furthermore, we contribute a\nhigh-quality LiDAR-mocap synchronized dataset comprising 20 different subjects\nperforming a range of motions, which can serve as a valuable resource for\nfuture research. The dataset and evaluation code are available at {\\blue\n\\url{https://movin3d.github.io/ELMO_SIGASIA2024/}}"
                },
                "authors": [
                    {
                        "name": "Deok-Kyeong Jang"
                    },
                    {
                        "name": "Dongseok Yang"
                    },
                    {
                        "name": "Deok-Yun Jang"
                    },
                    {
                        "name": "Byeoli Choi"
                    },
                    {
                        "name": "Donghoon Shin"
                    },
                    {
                        "name": "Sung-hee Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sung-hee Lee"
                },
                "author": "Sung-hee Lee",
                "arxiv_comment": "published at ACM Transactions on Graphics (Proc. SIGGRAPH ASIA), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06963v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06963v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08831v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08831v1",
                "updated": "2024-10-11T14:08:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    8,
                    17,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T14:08:17Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    8,
                    17,
                    4,
                    285,
                    0
                ],
                "title": "Distribution-free uncertainty quantification for inverse problems:\n  application to weak lensing mass mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distribution-free uncertainty quantification for inverse problems:\n  application to weak lensing mass mapping"
                },
                "summary": "In inverse problems, distribution-free uncertainty quantification (UQ) aims\nto obtain error bars with coverage guarantees that are independent of any prior\nassumptions about the data distribution. In the context of mass mapping,\nuncertainties could lead to errors that affects our understanding of the\nunderlying mass distribution, or could propagate to cosmological parameter\nestimation, thereby impacting the precision and reliability of cosmological\nmodels. Current surveys, such as Euclid or Rubin, will provide new weak lensing\ndatasets of very high quality. Accurately quantifying uncertainties in mass\nmaps is therefore critical to perform reliable cosmological parameter\ninference. In this paper, we extend the conformalized quantile regression (CQR)\nalgorithm, initially proposed for scalar regression, to inverse problems. We\ncompare our approach with another distribution-free approach based on\nrisk-controlling prediction sets (RCPS). Both methods are based on a\ncalibration dataset, and offer finite-sample coverage guarantees that are\nindependent of the data distribution. Furthermore, they are applicable to any\nmass mapping method, including blackbox predictors. In our experiments, we\napply UQ on three mass-mapping method: the Kaiser-Squires inversion, iterative\nWiener filtering, and the MCALens algorithm. Our experiments reveal that RCPS\ntends to produce overconservative confidence bounds with small calibration\nsets, whereas CQR is designed to avoid this issue. Although the expected\nmiscoverage rate is guaranteed to stay below a user-prescribed threshold\nregardless of the mass mapping method, selecting an appropriate reconstruction\nalgorithm remains crucial for obtaining accurate estimates, especially around\npeak-like structures, which are particularly important for inferring\ncosmological parameters. Additionally, the choice of mass mapping method\ninfluences the size of the error bars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In inverse problems, distribution-free uncertainty quantification (UQ) aims\nto obtain error bars with coverage guarantees that are independent of any prior\nassumptions about the data distribution. In the context of mass mapping,\nuncertainties could lead to errors that affects our understanding of the\nunderlying mass distribution, or could propagate to cosmological parameter\nestimation, thereby impacting the precision and reliability of cosmological\nmodels. Current surveys, such as Euclid or Rubin, will provide new weak lensing\ndatasets of very high quality. Accurately quantifying uncertainties in mass\nmaps is therefore critical to perform reliable cosmological parameter\ninference. In this paper, we extend the conformalized quantile regression (CQR)\nalgorithm, initially proposed for scalar regression, to inverse problems. We\ncompare our approach with another distribution-free approach based on\nrisk-controlling prediction sets (RCPS). Both methods are based on a\ncalibration dataset, and offer finite-sample coverage guarantees that are\nindependent of the data distribution. Furthermore, they are applicable to any\nmass mapping method, including blackbox predictors. In our experiments, we\napply UQ on three mass-mapping method: the Kaiser-Squires inversion, iterative\nWiener filtering, and the MCALens algorithm. Our experiments reveal that RCPS\ntends to produce overconservative confidence bounds with small calibration\nsets, whereas CQR is designed to avoid this issue. Although the expected\nmiscoverage rate is guaranteed to stay below a user-prescribed threshold\nregardless of the mass mapping method, selecting an appropriate reconstruction\nalgorithm remains crucial for obtaining accurate estimates, especially around\npeak-like structures, which are particularly important for inferring\ncosmological parameters. Additionally, the choice of mass mapping method\ninfluences the size of the error bars."
                },
                "authors": [
                    {
                        "name": "Hubert Leterme"
                    },
                    {
                        "name": "Jalal Fadili"
                    },
                    {
                        "name": "Jean-Luc Starck"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Luc Starck"
                },
                "author": "Jean-Luc Starck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08831v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08829v1",
                "updated": "2024-10-11T14:07:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    7,
                    57,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T14:07:57Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    7,
                    57,
                    4,
                    285,
                    0
                ],
                "title": "Unveiling Molecular Secrets: An LLM-Augmented Linear Model for\n  Explainable and Calibratable Molecular Property Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Molecular Secrets: An LLM-Augmented Linear Model for\n  Explainable and Calibratable Molecular Property Prediction"
                },
                "summary": "Explainable molecular property prediction is essential for various scientific\nfields, such as drug discovery and material science. Despite delivering\nintrinsic explainability, linear models struggle with capturing complex,\nnon-linear patterns. Large language models (LLMs), on the other hand, yield\naccurate predictions through powerful inference capabilities yet fail to\nprovide chemically meaningful explanations for their predictions. This work\nproposes a novel framework, called MoleX, which leverages LLM knowledge to\nbuild a simple yet powerful linear model for accurate molecular property\nprediction with faithful explanations. The core of MoleX is to model\ncomplicated molecular structure-property relationships using a simple linear\nmodel, augmented by LLM knowledge and a crafted calibration strategy.\nSpecifically, to extract the maximum amount of task-relevant knowledge from LLM\nembeddings, we employ information bottleneck-inspired fine-tuning and\nsparsity-inducing dimensionality reduction. These informative embeddings are\nthen used to fit a linear model for explainable inference. Moreover, we\nintroduce residual calibration to address prediction errors stemming from\nlinear models' insufficient expressiveness of complex LLM embeddings, thus\nrecovering the LLM's predictive power and boosting overall accuracy.\nTheoretically, we provide a mathematical foundation to justify MoleX's\nexplainability. Extensive experiments demonstrate that MoleX outperforms\nexisting methods in molecular property prediction, establishing a new milestone\nin predictive performance, explainability, and efficiency. In particular, MoleX\nenables CPU inference and accelerates large-scale dataset processing, achieving\ncomparable performance 300x faster with 100,000 fewer parameters than LLMs.\nAdditionally, the calibration improves model performance by up to 12.7% without\ncompromising explainability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable molecular property prediction is essential for various scientific\nfields, such as drug discovery and material science. Despite delivering\nintrinsic explainability, linear models struggle with capturing complex,\nnon-linear patterns. Large language models (LLMs), on the other hand, yield\naccurate predictions through powerful inference capabilities yet fail to\nprovide chemically meaningful explanations for their predictions. This work\nproposes a novel framework, called MoleX, which leverages LLM knowledge to\nbuild a simple yet powerful linear model for accurate molecular property\nprediction with faithful explanations. The core of MoleX is to model\ncomplicated molecular structure-property relationships using a simple linear\nmodel, augmented by LLM knowledge and a crafted calibration strategy.\nSpecifically, to extract the maximum amount of task-relevant knowledge from LLM\nembeddings, we employ information bottleneck-inspired fine-tuning and\nsparsity-inducing dimensionality reduction. These informative embeddings are\nthen used to fit a linear model for explainable inference. Moreover, we\nintroduce residual calibration to address prediction errors stemming from\nlinear models' insufficient expressiveness of complex LLM embeddings, thus\nrecovering the LLM's predictive power and boosting overall accuracy.\nTheoretically, we provide a mathematical foundation to justify MoleX's\nexplainability. Extensive experiments demonstrate that MoleX outperforms\nexisting methods in molecular property prediction, establishing a new milestone\nin predictive performance, explainability, and efficiency. In particular, MoleX\nenables CPU inference and accelerates large-scale dataset processing, achieving\ncomparable performance 300x faster with 100,000 fewer parameters than LLMs.\nAdditionally, the calibration improves model performance by up to 12.7% without\ncompromising explainability."
                },
                "authors": [
                    {
                        "name": "Zhuoran Li"
                    },
                    {
                        "name": "Xu Sun"
                    },
                    {
                        "name": "Wanyu Lin"
                    },
                    {
                        "name": "Jiannong Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jiannong Cao"
                },
                "author": "Jiannong Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08826v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08826v1",
                "updated": "2024-10-11T14:05:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    5,
                    28,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T14:05:28Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    5,
                    28,
                    4,
                    285,
                    0
                ],
                "title": "Towards virtual painting recolouring using Vision Transformer on X-Ray\n  Fluorescence datacubes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards virtual painting recolouring using Vision Transformer on X-Ray\n  Fluorescence datacubes"
                },
                "summary": "In this contribution, we define (and test) a pipeline to perform virtual\npainting recolouring using raw data of X-Ray Fluorescence (XRF) analysis on\npictorial artworks. To circumvent the small dataset size, we generate a\nsynthetic dataset, starting from a database of XRF spectra; furthermore, to\nensure a better generalisation capacity (and to tackle the issue of in-memory\nsize and inference time), we define a Deep Variational Embedding network to\nembed the XRF spectra into a lower dimensional, K-Means friendly, metric space.\n  We thus train a set of models to assign coloured images to embedded XRF\nimages. We report here the devised pipeline performances in terms of visual\nquality metrics, and we close on a discussion on the results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this contribution, we define (and test) a pipeline to perform virtual\npainting recolouring using raw data of X-Ray Fluorescence (XRF) analysis on\npictorial artworks. To circumvent the small dataset size, we generate a\nsynthetic dataset, starting from a database of XRF spectra; furthermore, to\nensure a better generalisation capacity (and to tackle the issue of in-memory\nsize and inference time), we define a Deep Variational Embedding network to\nembed the XRF spectra into a lower dimensional, K-Means friendly, metric space.\n  We thus train a set of models to assign coloured images to embedded XRF\nimages. We report here the devised pipeline performances in terms of visual\nquality metrics, and we close on a discussion on the results."
                },
                "authors": [
                    {
                        "name": "Alessandro Bombini"
                    },
                    {
                        "name": "Fernando García-Avello Bofías"
                    },
                    {
                        "name": "Francesca Giambi"
                    },
                    {
                        "name": "Chiara Ruberto"
                    }
                ],
                "author_detail": {
                    "name": "Chiara Ruberto"
                },
                "author": "Chiara Ruberto",
                "arxiv_comment": "v1: 20 pages, 10 figures; link to code repository",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08826v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.m; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08821v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08821v1",
                "updated": "2024-10-11T14:03:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    3,
                    29,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T14:03:29Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    3,
                    29,
                    4,
                    285,
                    0
                ],
                "title": "Retriever-and-Memory: Towards Adaptive Note-Enhanced Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retriever-and-Memory: Towards Adaptive Note-Enhanced Retrieval-Augmented\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) mitigates issues of the factual errors\nand hallucinated outputs generated by Large Language Models (LLMs) in\nopen-domain question-answering tasks (OpenQA) via introducing external\nknowledge. For complex QA, however, existing RAG methods use LLMs to actively\npredict retrieval timing and directly use the retrieved information for\ngeneration, regardless of whether the retrieval timing accurately reflects the\nactual information needs, or sufficiently considers prior retrieved knowledge,\nwhich may result in insufficient information gathering and interaction,\nyielding low-quality answers. To address these, we propose a generic RAG\napproach called Adaptive Note-Enhanced RAG (Adaptive-Note) for complex QA\ntasks, which includes the iterative information collector, adaptive memory\nreviewer, and task-oriented generator, while following a new\nRetriever-and-Memory paradigm. Specifically, Adaptive-Note introduces an\noverarching view of knowledge growth, iteratively gathering new information in\nthe form of notes and updating them into the existing optimal knowledge\nstructure, enhancing high-quality knowledge interactions. In addition, we\nemploy an adaptive, note-based stop-exploration strategy to decide \"what to\nretrieve and when to stop\" to encourage sufficient knowledge exploration. We\nconduct extensive experiments on five complex QA datasets, and the results\ndemonstrate the superiority and effectiveness of our method and its components.\nThe code and data are at https://github.com/thunlp/Adaptive-Note.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) mitigates issues of the factual errors\nand hallucinated outputs generated by Large Language Models (LLMs) in\nopen-domain question-answering tasks (OpenQA) via introducing external\nknowledge. For complex QA, however, existing RAG methods use LLMs to actively\npredict retrieval timing and directly use the retrieved information for\ngeneration, regardless of whether the retrieval timing accurately reflects the\nactual information needs, or sufficiently considers prior retrieved knowledge,\nwhich may result in insufficient information gathering and interaction,\nyielding low-quality answers. To address these, we propose a generic RAG\napproach called Adaptive Note-Enhanced RAG (Adaptive-Note) for complex QA\ntasks, which includes the iterative information collector, adaptive memory\nreviewer, and task-oriented generator, while following a new\nRetriever-and-Memory paradigm. Specifically, Adaptive-Note introduces an\noverarching view of knowledge growth, iteratively gathering new information in\nthe form of notes and updating them into the existing optimal knowledge\nstructure, enhancing high-quality knowledge interactions. In addition, we\nemploy an adaptive, note-based stop-exploration strategy to decide \"what to\nretrieve and when to stop\" to encourage sufficient knowledge exploration. We\nconduct extensive experiments on five complex QA datasets, and the results\ndemonstrate the superiority and effectiveness of our method and its components.\nThe code and data are at https://github.com/thunlp/Adaptive-Note."
                },
                "authors": [
                    {
                        "name": "Ruobing Wang"
                    },
                    {
                        "name": "Daren Zha"
                    },
                    {
                        "name": "Shi Yu"
                    },
                    {
                        "name": "Qingfei Zhao"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Zhenghao Liu"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "15 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08821v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08821v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08820v1",
                "updated": "2024-10-11T14:02:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    2,
                    42,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T14:02:42Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    2,
                    42,
                    4,
                    285,
                    0
                ],
                "title": "Which Demographics do LLMs Default to During Annotation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Demographics do LLMs Default to During Annotation?"
                },
                "summary": "Demographics and cultural background of annotators influence the labels they\nassign in text annotation -- for instance, an elderly woman might find it\noffensive to read a message addressed to a \"bro\", but a male teenager might\nfind it appropriate. It is therefore important to acknowledge label variations\nto not under-represent members of a society. Two research directions developed\nout of this observation in the context of using large language models (LLM) for\ndata annotations, namely (1) studying biases and inherent knowledge of LLMs and\n(2) injecting diversity in the output by manipulating the prompt with\ndemographic information. We combine these two strands of research and ask the\nquestion to which demographics an LLM resorts to when no demographics is given.\nTo answer this question, we evaluate which attributes of human annotators LLMs\ninherently mimic. Furthermore, we compare non-demographic conditioned prompts\nand placebo-conditioned prompts (e.g., \"you are an annotator who lives in house\nnumber 5\") to demographics-conditioned prompts (\"You are a 45 year old man and\nan expert on politeness annotation. How do you rate {instance}\"). We study\nthese questions for politeness and offensiveness annotations on the POPQUORN\ndata set, a corpus created in a controlled manner to investigate human label\nvariations based on demographics which has not been used for LLM-based analyses\nso far. We observe notable influences related to gender, race, and age in\ndemographic prompting, which contrasts with previous studies that found no such\neffects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demographics and cultural background of annotators influence the labels they\nassign in text annotation -- for instance, an elderly woman might find it\noffensive to read a message addressed to a \"bro\", but a male teenager might\nfind it appropriate. It is therefore important to acknowledge label variations\nto not under-represent members of a society. Two research directions developed\nout of this observation in the context of using large language models (LLM) for\ndata annotations, namely (1) studying biases and inherent knowledge of LLMs and\n(2) injecting diversity in the output by manipulating the prompt with\ndemographic information. We combine these two strands of research and ask the\nquestion to which demographics an LLM resorts to when no demographics is given.\nTo answer this question, we evaluate which attributes of human annotators LLMs\ninherently mimic. Furthermore, we compare non-demographic conditioned prompts\nand placebo-conditioned prompts (e.g., \"you are an annotator who lives in house\nnumber 5\") to demographics-conditioned prompts (\"You are a 45 year old man and\nan expert on politeness annotation. How do you rate {instance}\"). We study\nthese questions for politeness and offensiveness annotations on the POPQUORN\ndata set, a corpus created in a controlled manner to investigate human label\nvariations based on demographics which has not been used for LLM-based analyses\nso far. We observe notable influences related to gender, race, and age in\ndemographic prompting, which contrasts with previous studies that found no such\neffects."
                },
                "authors": [
                    {
                        "name": "Christopher Bagdon"
                    },
                    {
                        "name": "Aidan Combs"
                    },
                    {
                        "name": "Lynn Greschner"
                    },
                    {
                        "name": "Roman Klinger"
                    },
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Sean Papay"
                    },
                    {
                        "name": "Nadine Probol"
                    },
                    {
                        "name": "Yarik Menchaca Resendiz"
                    },
                    {
                        "name": "Johannes Schäfer"
                    },
                    {
                        "name": "Aswathy Velutharambath"
                    },
                    {
                        "name": "Sabine Weber"
                    },
                    {
                        "name": "Amelie Wührl"
                    }
                ],
                "author_detail": {
                    "name": "Amelie Wührl"
                },
                "author": "Amelie Wührl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08815v1",
                "updated": "2024-10-11T13:52:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    52,
                    44,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T13:52:44Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    52,
                    44,
                    4,
                    285,
                    0
                ],
                "title": "StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via\n  Inference-time Hybrid Information Structurization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via\n  Inference-time Hybrid Information Structurization"
                },
                "summary": "Retrieval-augmented generation (RAG) is a key means to effectively enhance\nlarge language models (LLMs) in many knowledge-based tasks. However, existing\nRAG methods struggle with knowledge-intensive reasoning tasks, because useful\ninformation required to these tasks are badly scattered. This characteristic\nmakes it difficult for existing RAG methods to accurately identify key\ninformation and perform global reasoning with such noisy augmentation. In this\npaper, motivated by the cognitive theories that humans convert raw information\ninto various structured knowledge when tackling knowledge-intensive reasoning,\nwe proposes a new framework, StructRAG, which can identify the optimal\nstructure type for the task at hand, reconstruct original documents into this\nstructured format, and infer answers based on the resulting structure.\nExtensive experiments across various knowledge-intensive tasks show that\nStructRAG achieves state-of-the-art performance, particularly excelling in\nchallenging scenarios, demonstrating its potential as an effective solution for\nenhancing LLMs in complex real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) is a key means to effectively enhance\nlarge language models (LLMs) in many knowledge-based tasks. However, existing\nRAG methods struggle with knowledge-intensive reasoning tasks, because useful\ninformation required to these tasks are badly scattered. This characteristic\nmakes it difficult for existing RAG methods to accurately identify key\ninformation and perform global reasoning with such noisy augmentation. In this\npaper, motivated by the cognitive theories that humans convert raw information\ninto various structured knowledge when tackling knowledge-intensive reasoning,\nwe proposes a new framework, StructRAG, which can identify the optimal\nstructure type for the task at hand, reconstruct original documents into this\nstructured format, and infer answers based on the resulting structure.\nExtensive experiments across various knowledge-intensive tasks show that\nStructRAG achieves state-of-the-art performance, particularly excelling in\nchallenging scenarios, demonstrating its potential as an effective solution for\nenhancing LLMs in complex real-world applications."
                },
                "authors": [
                    {
                        "name": "Zhuoqun Li"
                    },
                    {
                        "name": "Xuanang Chen"
                    },
                    {
                        "name": "Haiyang Yu"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Qiaoyu Tang"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Le Sun"
                    },
                    {
                        "name": "Yongbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Li"
                },
                "author": "Yongbin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08811v1",
                "updated": "2024-10-11T13:50:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    50,
                    50,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T13:50:50Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    50,
                    50,
                    4,
                    285,
                    0
                ],
                "title": "PoisonBench: Assessing Large Language Model Vulnerability to Data\n  Poisoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoisonBench: Assessing Large Language Model Vulnerability to Data\n  Poisoning"
                },
                "summary": "Preference learning is a central component for aligning current LLMs, but\nthis process can be vulnerable to data poisoning attacks. To address this\nconcern, we introduce PoisonBench, a benchmark for evaluating large language\nmodels' susceptibility to data poisoning during preference learning. Data\npoisoning attacks can manipulate large language model responses to include\nhidden malicious content or biases, potentially causing the model to generate\nharmful or unintended outputs while appearing to function normally. We deploy\ntwo distinct attack types across eight realistic scenarios, assessing 21\nwidely-used models. Our findings reveal concerning trends: (1) Scaling up\nparameter size does not inherently enhance resilience against poisoning\nattacks; (2) There exists a log-linear relationship between the effects of the\nattack and the data poison ratio; (3) The effect of data poisoning can\ngeneralize to extrapolated triggers that are not included in the poisoned data.\nThese results expose weaknesses in current preference learning techniques,\nhighlighting the urgent need for more robust defenses against malicious models\nand data manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference learning is a central component for aligning current LLMs, but\nthis process can be vulnerable to data poisoning attacks. To address this\nconcern, we introduce PoisonBench, a benchmark for evaluating large language\nmodels' susceptibility to data poisoning during preference learning. Data\npoisoning attacks can manipulate large language model responses to include\nhidden malicious content or biases, potentially causing the model to generate\nharmful or unintended outputs while appearing to function normally. We deploy\ntwo distinct attack types across eight realistic scenarios, assessing 21\nwidely-used models. Our findings reveal concerning trends: (1) Scaling up\nparameter size does not inherently enhance resilience against poisoning\nattacks; (2) There exists a log-linear relationship between the effects of the\nattack and the data poison ratio; (3) The effect of data poisoning can\ngeneralize to extrapolated triggers that are not included in the poisoned data.\nThese results expose weaknesses in current preference learning techniques,\nhighlighting the urgent need for more robust defenses against malicious models\nand data manipulation."
                },
                "authors": [
                    {
                        "name": "Tingchen Fu"
                    },
                    {
                        "name": "Mrinank Sharma"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Shay B. Cohen"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Fazl Barez"
                    }
                ],
                "author_detail": {
                    "name": "Fazl Barez"
                },
                "author": "Fazl Barez",
                "arxiv_comment": "Tingchen Fu and Fazl Barez are core research contributors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07381v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07381v2",
                "updated": "2024-10-11T13:49:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    49,
                    16,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-09T18:59:49Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    18,
                    59,
                    49,
                    2,
                    283,
                    0
                ],
                "title": "Tally: Non-Intrusive Performance Isolation for Concurrent Deep Learning\n  Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tally: Non-Intrusive Performance Isolation for Concurrent Deep Learning\n  Workloads"
                },
                "summary": "GPU underutilization is a significant concern in many production deep\nlearning clusters, leading to prolonged job queues and increased operational\nexpenses. A promising solution to this inefficiency is GPU sharing, which\nimproves resource utilization by allowing multiple workloads to execute\nconcurrently on a single GPU. However, the practical deployment of GPU sharing\nin production settings faces critical obstacles due to the limitations of\nexisting mechanisms, such as high integration costs, inadequate performance\nisolation, and limited application compatibility. To address these issues, we\nintroduce \\emph{Tally}, a non-intrusive GPU sharing mechanism that provides\nrobust performance isolation and comprehensive workload compatibility. Tally\noperates as a virtualization layer between applications and GPUs, transparently\norchestrating the device execution of concurrent workloads. The key to Tally's\nrobust performance isolation capability lies in its fine-grained thread-block\nlevel GPU kernel scheduling strategy, which allows the system to effectively\nmitigate interference caused by workload co-execution. Our evaluation,\nconducted on a diverse set of workload combinations, demonstrates that Tally on\naverage incurs a mere $7.2\\%$ overhead on the $99^{th}$-percentile latency of\nhigh-priority inference tasks when executed concurrently with best-effort\ntraining workloads compared to $188.9\\%$ overhead exhibited by the\nstate-of-the-art GPU sharing systems like TGS, while achieving over $80\\%$ of\nTGS's system throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU underutilization is a significant concern in many production deep\nlearning clusters, leading to prolonged job queues and increased operational\nexpenses. A promising solution to this inefficiency is GPU sharing, which\nimproves resource utilization by allowing multiple workloads to execute\nconcurrently on a single GPU. However, the practical deployment of GPU sharing\nin production settings faces critical obstacles due to the limitations of\nexisting mechanisms, such as high integration costs, inadequate performance\nisolation, and limited application compatibility. To address these issues, we\nintroduce \\emph{Tally}, a non-intrusive GPU sharing mechanism that provides\nrobust performance isolation and comprehensive workload compatibility. Tally\noperates as a virtualization layer between applications and GPUs, transparently\norchestrating the device execution of concurrent workloads. The key to Tally's\nrobust performance isolation capability lies in its fine-grained thread-block\nlevel GPU kernel scheduling strategy, which allows the system to effectively\nmitigate interference caused by workload co-execution. Our evaluation,\nconducted on a diverse set of workload combinations, demonstrates that Tally on\naverage incurs a mere $7.2\\%$ overhead on the $99^{th}$-percentile latency of\nhigh-priority inference tasks when executed concurrently with best-effort\ntraining workloads compared to $188.9\\%$ overhead exhibited by the\nstate-of-the-art GPU sharing systems like TGS, while achieving over $80\\%$ of\nTGS's system throughput."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Anand Jayarajan"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    }
                ],
                "author_detail": {
                    "name": "Gennady Pekhimenko"
                },
                "author": "Gennady Pekhimenko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07381v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07381v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08806v1",
                "updated": "2024-10-11T13:45:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    45,
                    16,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T13:45:16Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    45,
                    16,
                    4,
                    285,
                    0
                ],
                "title": "Don't Transform the Code, Code the Transforms: Towards Precise Code\n  Rewriting using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Transform the Code, Code the Transforms: Towards Precise Code\n  Rewriting using LLMs"
                },
                "summary": "Tools for rewriting, refactoring and optimizing code should be fast and\ncorrect. Large language models (LLMs), by their nature, possess neither of\nthese qualities. Yet, there remains tremendous opportunity in using LLMs to\nimprove code.\n  We explore the use of LLMs not to transform code, but to code transforms. We\npropose a chain-of-thought approach to synthesizing code transformations from a\nsmall number of input/output code examples that incorporates execution and\nfeedback. Unlike the direct rewrite approach, LLM-generated transformations are\neasy to inspect, debug, and validate. The logic of the rewrite is explicitly\ncoded and easy to adapt. The compute required to run code transformations is\nminute compared to that of LLM rewriting.\n  We test our approach on 16 Python code transformations and find that LLM-\ngenerated transforms are perfectly precise for 7 of them and less imprecise\nthan direct LLM rewriting on the others. We hope to encourage further research\nto improving the precision of LLM code rewriting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tools for rewriting, refactoring and optimizing code should be fast and\ncorrect. Large language models (LLMs), by their nature, possess neither of\nthese qualities. Yet, there remains tremendous opportunity in using LLMs to\nimprove code.\n  We explore the use of LLMs not to transform code, but to code transforms. We\npropose a chain-of-thought approach to synthesizing code transformations from a\nsmall number of input/output code examples that incorporates execution and\nfeedback. Unlike the direct rewrite approach, LLM-generated transformations are\neasy to inspect, debug, and validate. The logic of the rewrite is explicitly\ncoded and easy to adapt. The compute required to run code transformations is\nminute compared to that of LLM rewriting.\n  We test our approach on 16 Python code transformations and find that LLM-\ngenerated transforms are perfectly precise for 7 of them and less imprecise\nthan direct LLM rewriting on the others. We hope to encourage further research\nto improving the precision of LLM code rewriting."
                },
                "authors": [
                    {
                        "name": "Chris Cummins"
                    },
                    {
                        "name": "Volker Seeker"
                    },
                    {
                        "name": "Jordi Armengol-Estapé"
                    },
                    {
                        "name": "Aram H. Markosyan"
                    },
                    {
                        "name": "Gabriel Synnaeve"
                    },
                    {
                        "name": "Hugh Leather"
                    }
                ],
                "author_detail": {
                    "name": "Hugh Leather"
                },
                "author": "Hugh Leather",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09656v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09656v3",
                "updated": "2024-10-11T13:42:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    42,
                    12,
                    4,
                    285,
                    0
                ],
                "published": "2024-04-15T10:44:31Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    10,
                    44,
                    31,
                    0,
                    106,
                    0
                ],
                "title": "Learn Your Reference Model for Real Good Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learn Your Reference Model for Real Good Alignment"
                },
                "summary": "Despite the fact that offline methods for Large Language Models (LLMs)\nalignment do not require a direct reward model, they remain susceptible to\noveroptimization. This issue arises when the trained model deviates excessively\nfrom the reference policy, leading to a decrease in sample quality. We propose\na new paradigm of offline alignment methods, called Trust Region (including\nvariants TR-DPO, TR-IPO, TR-KTO), which dynamically updates the reference\npolicy throughout the training process. Our results show that TR alignment\nmethods effectively mitigate overoptimization, enabling models to maintain\nstrong performance even when substantially deviating from the initial reference\npolicy. We demonstrate the efficacy of these approaches not only through toy\nexamples that exhibit reduced overoptimization, but also through direct,\nside-by-side comparisons in specific tasks such as helpful and harmless\ndialogue, as well as summarization, where they surpass conventional methods.\nAdditionally, we report significant improvements in general-purpose assistant\nsetups with the Llama3 model on the AlpacaEval 2 and Arena-Hard benchmarks,\nhighlighting the advantages of Trust Region methods over classical approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the fact that offline methods for Large Language Models (LLMs)\nalignment do not require a direct reward model, they remain susceptible to\noveroptimization. This issue arises when the trained model deviates excessively\nfrom the reference policy, leading to a decrease in sample quality. We propose\na new paradigm of offline alignment methods, called Trust Region (including\nvariants TR-DPO, TR-IPO, TR-KTO), which dynamically updates the reference\npolicy throughout the training process. Our results show that TR alignment\nmethods effectively mitigate overoptimization, enabling models to maintain\nstrong performance even when substantially deviating from the initial reference\npolicy. We demonstrate the efficacy of these approaches not only through toy\nexamples that exhibit reduced overoptimization, but also through direct,\nside-by-side comparisons in specific tasks such as helpful and harmless\ndialogue, as well as summarization, where they surpass conventional methods.\nAdditionally, we report significant improvements in general-purpose assistant\nsetups with the Llama3 model on the AlpacaEval 2 and Arena-Hard benchmarks,\nhighlighting the advantages of Trust Region methods over classical approaches."
                },
                "authors": [
                    {
                        "name": "Alexey Gorbatovski"
                    },
                    {
                        "name": "Boris Shaposhnikov"
                    },
                    {
                        "name": "Alexey Malakhov"
                    },
                    {
                        "name": "Nikita Surnachev"
                    },
                    {
                        "name": "Yaroslav Aksenov"
                    },
                    {
                        "name": "Ian Maksimov"
                    },
                    {
                        "name": "Nikita Balagansky"
                    },
                    {
                        "name": "Daniil Gavrilov"
                    }
                ],
                "author_detail": {
                    "name": "Daniil Gavrilov"
                },
                "author": "Daniil Gavrilov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09656v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09656v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08800v1",
                "updated": "2024-10-11T13:34:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    34,
                    24,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T13:34:24Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    34,
                    24,
                    4,
                    285,
                    0
                ],
                "title": "Data Processing for the OpenGPT-X Model Family",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Processing for the OpenGPT-X Model Family"
                },
                "summary": "This paper presents a comprehensive overview of the data preparation pipeline\ndeveloped for the OpenGPT-X project, a large-scale initiative aimed at creating\nopen and high-performance multilingual large language models (LLMs). The\nproject goal is to deliver models that cover all major European languages, with\na particular focus on real-world applications within the European Union. We\nexplain all data processing steps, starting with the data selection and\nrequirement definition to the preparation of the final datasets for model\ntraining. We distinguish between curated data and web data, as each of these\ncategories is handled by distinct pipelines, with curated data undergoing\nminimal filtering and web data requiring extensive filtering and deduplication.\nThis distinction guided the development of specialized algorithmic solutions\nfor both pipelines. In addition to describing the processing methodologies, we\nprovide an in-depth analysis of the datasets, increasing transparency and\nalignment with European data regulations. Finally, we share key insights and\nchallenges faced during the project, offering recommendations for future\nendeavors in large-scale multilingual data preparation for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive overview of the data preparation pipeline\ndeveloped for the OpenGPT-X project, a large-scale initiative aimed at creating\nopen and high-performance multilingual large language models (LLMs). The\nproject goal is to deliver models that cover all major European languages, with\na particular focus on real-world applications within the European Union. We\nexplain all data processing steps, starting with the data selection and\nrequirement definition to the preparation of the final datasets for model\ntraining. We distinguish between curated data and web data, as each of these\ncategories is handled by distinct pipelines, with curated data undergoing\nminimal filtering and web data requiring extensive filtering and deduplication.\nThis distinction guided the development of specialized algorithmic solutions\nfor both pipelines. In addition to describing the processing methodologies, we\nprovide an in-depth analysis of the datasets, increasing transparency and\nalignment with European data regulations. Finally, we share key insights and\nchallenges faced during the project, offering recommendations for future\nendeavors in large-scale multilingual data preparation for LLMs."
                },
                "authors": [
                    {
                        "name": "Nicolo' Brandizzi"
                    },
                    {
                        "name": "Hammam Abdelwahab"
                    },
                    {
                        "name": "Anirban Bhowmick"
                    },
                    {
                        "name": "Lennard Helmer"
                    },
                    {
                        "name": "Benny Jörg Stein"
                    },
                    {
                        "name": "Pavel Denisov"
                    },
                    {
                        "name": "Qasid Saleem"
                    },
                    {
                        "name": "Michael Fromm"
                    },
                    {
                        "name": "Mehdi Ali"
                    },
                    {
                        "name": "Richard Rutmann"
                    },
                    {
                        "name": "Farzad Naderi"
                    },
                    {
                        "name": "Mohamad Saif Agy"
                    },
                    {
                        "name": "Alexander Schwirjow"
                    },
                    {
                        "name": "Fabian Küch"
                    },
                    {
                        "name": "Luzian Hahn"
                    },
                    {
                        "name": "Malte Ostendorff"
                    },
                    {
                        "name": "Pedro Ortiz Suarez"
                    },
                    {
                        "name": "Georg Rehm"
                    },
                    {
                        "name": "Dennis Wegener"
                    },
                    {
                        "name": "Nicolas Flores-Herr"
                    },
                    {
                        "name": "Joachim Köhler"
                    },
                    {
                        "name": "Johannes Leveling"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Leveling"
                },
                "author": "Johannes Leveling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.1; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05425v2",
                "updated": "2024-10-11T13:32:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    32,
                    32,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-07T18:43:43Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    18,
                    43,
                    43,
                    0,
                    281,
                    0
                ],
                "title": "Designing a Classifier for Active Fire Detection from Multispectral\n  Satellite Imagery Using Neural Architecture Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing a Classifier for Active Fire Detection from Multispectral\n  Satellite Imagery Using Neural Architecture Search"
                },
                "summary": "This paper showcases the use of a reinforcement learning-based Neural\nArchitecture Search (NAS) agent to design a small neural network to perform\nactive fire detection on multispectral satellite imagery. Specifically, we aim\nto design a neural network that can determine if a single multispectral pixel\nis a part of a fire, and do so within the constraints of a Low Earth Orbit\n(LEO) nanosatellite with a limited power budget, to facilitate on-board\nprocessing of sensor data. In order to use reinforcement learning, a reward\nfunction is needed. We supply this reward function in the shape of a regression\nmodel that predicts the F1 score obtained by a particular architecture,\nfollowing quantization to INT8 precision, from purely architectural features.\nThis model is trained by collecting a random sample of neural network\narchitectures, training these architectures, and collecting their\nclassification performance statistics. Besides the F1 score, we also include\nthe total number of trainable parameters in our reward function to limit the\nsize of the designed model and ensure it fits within the resource constraints\nimposed by nanosatellite platforms. Finally, we deployed the best neural\nnetwork to the Google Coral Micro Dev Board and evaluated its inference latency\nand power consumption. This neural network consists of 1,716 trainable\nparameters, takes on average 984{\\mu}s to inference, and consumes around 800mW\nto perform inference. These results show that our reinforcement learning-based\nNAS approach can be successfully applied to novel problems not tackled before.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper showcases the use of a reinforcement learning-based Neural\nArchitecture Search (NAS) agent to design a small neural network to perform\nactive fire detection on multispectral satellite imagery. Specifically, we aim\nto design a neural network that can determine if a single multispectral pixel\nis a part of a fire, and do so within the constraints of a Low Earth Orbit\n(LEO) nanosatellite with a limited power budget, to facilitate on-board\nprocessing of sensor data. In order to use reinforcement learning, a reward\nfunction is needed. We supply this reward function in the shape of a regression\nmodel that predicts the F1 score obtained by a particular architecture,\nfollowing quantization to INT8 precision, from purely architectural features.\nThis model is trained by collecting a random sample of neural network\narchitectures, training these architectures, and collecting their\nclassification performance statistics. Besides the F1 score, we also include\nthe total number of trainable parameters in our reward function to limit the\nsize of the designed model and ensure it fits within the resource constraints\nimposed by nanosatellite platforms. Finally, we deployed the best neural\nnetwork to the Google Coral Micro Dev Board and evaluated its inference latency\nand power consumption. This neural network consists of 1,716 trainable\nparameters, takes on average 984{\\mu}s to inference, and consumes around 800mW\nto perform inference. These results show that our reinforcement learning-based\nNAS approach can be successfully applied to novel problems not tackled before."
                },
                "authors": [
                    {
                        "name": "Amber Cassimon"
                    },
                    {
                        "name": "Phil Reiter"
                    },
                    {
                        "name": "Siegfried Mercelis"
                    },
                    {
                        "name": "Kevin Mets"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Mets"
                },
                "author": "Kevin Mets",
                "arxiv_comment": "Added IEEE Submission Notice",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08791v1",
                "updated": "2024-10-11T13:17:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    17,
                    5,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T13:17:05Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    17,
                    5,
                    4,
                    285,
                    0
                ],
                "title": "Superpipeline: A Universal Approach for Reducing GPU Memory Usage in\n  Large Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superpipeline: A Universal Approach for Reducing GPU Memory Usage in\n  Large Models"
                },
                "summary": "The rapid growth in machine learning models, especially in natural language\nprocessing and computer vision, has led to challenges when running these models\non hardware with limited resources. This paper introduces Superpipeline, a new\nframework designed to optimize the execution of large AI models on constrained\nhardware during both training and inference. Our approach involves dynamically\nmanaging model execution by dividing models into individual layers and\nefficiently transferring these layers between GPU and CPU memory. Superpipeline\nreduces GPU memory usage by up to 60% in our experiments while maintaining\nmodel accuracy and acceptable processing speeds. This allows models that would\notherwise exceed available GPU memory to run effectively. Unlike existing\nsolutions that focus mainly on inference or specific model types, Superpipeline\ncan be applied to large language models (LLMs), vision-language models (VLMs),\nand vision-based models. We tested Superpipeline's performance across various\nmodels and hardware setups. The method includes two key parameters that allow\nfine-tuning the balance between GPU memory use and processing speed.\nImportantly, Superpipeline does not require retraining or changing model\nparameters, ensuring that the original model's output remains unchanged.\nSuperpipeline's simplicity and flexibility make it useful for researchers and\nprofessionals working with advanced AI models on limited hardware. It enables\nthe use of larger models or bigger batch sizes on existing hardware,\npotentially speeding up innovation across many machine learning applications.\nThis work marks an important step toward making advanced AI models more\naccessible and optimizing their deployment in resource-limited environments.\nThe code for Superpipeline is available at\nhttps://github.com/abbasiReza/super-pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth in machine learning models, especially in natural language\nprocessing and computer vision, has led to challenges when running these models\non hardware with limited resources. This paper introduces Superpipeline, a new\nframework designed to optimize the execution of large AI models on constrained\nhardware during both training and inference. Our approach involves dynamically\nmanaging model execution by dividing models into individual layers and\nefficiently transferring these layers between GPU and CPU memory. Superpipeline\nreduces GPU memory usage by up to 60% in our experiments while maintaining\nmodel accuracy and acceptable processing speeds. This allows models that would\notherwise exceed available GPU memory to run effectively. Unlike existing\nsolutions that focus mainly on inference or specific model types, Superpipeline\ncan be applied to large language models (LLMs), vision-language models (VLMs),\nand vision-based models. We tested Superpipeline's performance across various\nmodels and hardware setups. The method includes two key parameters that allow\nfine-tuning the balance between GPU memory use and processing speed.\nImportantly, Superpipeline does not require retraining or changing model\nparameters, ensuring that the original model's output remains unchanged.\nSuperpipeline's simplicity and flexibility make it useful for researchers and\nprofessionals working with advanced AI models on limited hardware. It enables\nthe use of larger models or bigger batch sizes on existing hardware,\npotentially speeding up innovation across many machine learning applications.\nThis work marks an important step toward making advanced AI models more\naccessible and optimizing their deployment in resource-limited environments.\nThe code for Superpipeline is available at\nhttps://github.com/abbasiReza/super-pipeline."
                },
                "authors": [
                    {
                        "name": "Reza Abbasi"
                    },
                    {
                        "name": "Sernam Lim"
                    }
                ],
                "author_detail": {
                    "name": "Sernam Lim"
                },
                "author": "Sernam Lim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08776v1",
                "updated": "2024-10-11T12:49:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    49,
                    5,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T12:49:05Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    49,
                    5,
                    4,
                    285,
                    0
                ],
                "title": "F2A: An Innovative Approach for Prompt Injection by Utilizing Feign\n  Security Detection Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F2A: An Innovative Approach for Prompt Injection by Utilizing Feign\n  Security Detection Agents"
                },
                "summary": "With the rapid development of Large Language Models (LLMs), numerous mature\napplications of LLMs have emerged in the field of content safety detection.\nHowever, we have found that LLMs exhibit blind trust in safety detection\nagents. The general LLMs can be compromised by hackers with this vulnerability.\nHence, this paper proposed an attack named Feign Agent Attack (F2A).Through\nsuch malicious forgery methods, adding fake safety detection results into the\nprompt, the defense mechanism of LLMs can be bypassed, thereby obtaining\nharmful content and hijacking the normal conversation.Continually, a series of\nexperiments were conducted. In these experiments, the hijacking capability of\nF2A on LLMs was analyzed and demonstrated, exploring the fundamental reasons\nwhy LLMs blindly trust safety detection results. The experiments involved\nvarious scenarios where fake safety detection results were injected into\nprompts, and the responses were closely monitored to understand the extent of\nthe vulnerability. Also, this paper provided a reasonable solution to this\nattack, emphasizing that it is important for LLMs to critically evaluate the\nresults of augmented agents to prevent the generating harmful content. By doing\nso, the reliability and security can be significantly improved, protecting the\nLLMs from F2A.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of Large Language Models (LLMs), numerous mature\napplications of LLMs have emerged in the field of content safety detection.\nHowever, we have found that LLMs exhibit blind trust in safety detection\nagents. The general LLMs can be compromised by hackers with this vulnerability.\nHence, this paper proposed an attack named Feign Agent Attack (F2A).Through\nsuch malicious forgery methods, adding fake safety detection results into the\nprompt, the defense mechanism of LLMs can be bypassed, thereby obtaining\nharmful content and hijacking the normal conversation.Continually, a series of\nexperiments were conducted. In these experiments, the hijacking capability of\nF2A on LLMs was analyzed and demonstrated, exploring the fundamental reasons\nwhy LLMs blindly trust safety detection results. The experiments involved\nvarious scenarios where fake safety detection results were injected into\nprompts, and the responses were closely monitored to understand the extent of\nthe vulnerability. Also, this paper provided a reasonable solution to this\nattack, emphasizing that it is important for LLMs to critically evaluate the\nresults of augmented agents to prevent the generating harmful content. By doing\nso, the reliability and security can be significantly improved, protecting the\nLLMs from F2A."
                },
                "authors": [
                    {
                        "name": "Yupeng Ren"
                    }
                ],
                "author_detail": {
                    "name": "Yupeng Ren"
                },
                "author": "Yupeng Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01595v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01595v2",
                "updated": "2024-10-11T12:47:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    47,
                    48,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-02T14:33:12Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    14,
                    33,
                    12,
                    2,
                    276,
                    0
                ],
                "title": "KnobGen: Controlling the Sophistication of Artwork in Sketch-Based\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KnobGen: Controlling the Sophistication of Artwork in Sketch-Based\n  Diffusion Models"
                },
                "summary": "Recent advances in diffusion models have significantly improved text-to-image\n(T2I) generation, but they often struggle to balance fine-grained precision\nwith high-level control. Methods like ControlNet and T2I-Adapter excel at\nfollowing sketches by seasoned artists but tend to be overly rigid, replicating\nunintentional flaws in sketches from novice users. Meanwhile, coarse-grained\nmethods, such as sketch-based abstraction frameworks, offer more accessible\ninput handling but lack the precise control needed for detailed, professional\nuse. To address these limitations, we propose KnobGen, a dual-pathway framework\nthat democratizes sketch-based image generation by seamlessly adapting to\nvarying levels of sketch complexity and user skill. KnobGen uses a\nCoarse-Grained Controller (CGC) module for high-level semantics and a\nFine-Grained Controller (FGC) module for detailed refinement. The relative\nstrength of these two modules can be adjusted through our knob inference\nmechanism to align with the user's specific needs. These mechanisms ensure that\nKnobGen can flexibly generate images from both novice sketches and those drawn\nby seasoned artists. This maintains control over the final output while\npreserving the natural appearance of the image, as evidenced on the\nMultiGen-20M dataset and a newly collected sketch dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion models have significantly improved text-to-image\n(T2I) generation, but they often struggle to balance fine-grained precision\nwith high-level control. Methods like ControlNet and T2I-Adapter excel at\nfollowing sketches by seasoned artists but tend to be overly rigid, replicating\nunintentional flaws in sketches from novice users. Meanwhile, coarse-grained\nmethods, such as sketch-based abstraction frameworks, offer more accessible\ninput handling but lack the precise control needed for detailed, professional\nuse. To address these limitations, we propose KnobGen, a dual-pathway framework\nthat democratizes sketch-based image generation by seamlessly adapting to\nvarying levels of sketch complexity and user skill. KnobGen uses a\nCoarse-Grained Controller (CGC) module for high-level semantics and a\nFine-Grained Controller (FGC) module for detailed refinement. The relative\nstrength of these two modules can be adjusted through our knob inference\nmechanism to align with the user's specific needs. These mechanisms ensure that\nKnobGen can flexibly generate images from both novice sketches and those drawn\nby seasoned artists. This maintains control over the final output while\npreserving the natural appearance of the image, as evidenced on the\nMultiGen-20M dataset and a newly collected sketch dataset."
                },
                "authors": [
                    {
                        "name": "Pouyan Navard"
                    },
                    {
                        "name": "Amin Karimi Monsefi"
                    },
                    {
                        "name": "Mengxi Zhou"
                    },
                    {
                        "name": "Wei-Lun Chao"
                    },
                    {
                        "name": "Alper Yilmaz"
                    },
                    {
                        "name": "Rajiv Ramnath"
                    }
                ],
                "author_detail": {
                    "name": "Rajiv Ramnath"
                },
                "author": "Rajiv Ramnath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01595v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01595v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14038v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14038v2",
                "updated": "2024-10-11T12:40:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    40,
                    50,
                    4,
                    285,
                    0
                ],
                "published": "2024-09-21T06:49:34Z",
                "published_parsed": [
                    2024,
                    9,
                    21,
                    6,
                    49,
                    34,
                    5,
                    265,
                    0
                ],
                "title": "OAEI-LLM: A Benchmark Dataset for Understanding Large Language Model\n  Hallucinations in Ontology Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OAEI-LLM: A Benchmark Dataset for Understanding Large Language Model\n  Hallucinations in Ontology Matching"
                },
                "summary": "Hallucinations of large language models (LLMs) commonly occur in\ndomain-specific downstream tasks, with no exception in ontology matching (OM).\nThe prevalence of using LLMs for OM raises the need for benchmarks to better\nunderstand LLM hallucinations. The OAEI-LLM dataset is an extended version of\nthe Ontology Alignment Evaluation Initiative (OAEI) datasets that evaluate\nLLM-specific hallucinations in OM tasks. We outline the methodology used in\ndataset construction and schema extension, and provide examples of potential\nuse cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations of large language models (LLMs) commonly occur in\ndomain-specific downstream tasks, with no exception in ontology matching (OM).\nThe prevalence of using LLMs for OM raises the need for benchmarks to better\nunderstand LLM hallucinations. The OAEI-LLM dataset is an extended version of\nthe Ontology Alignment Evaluation Initiative (OAEI) datasets that evaluate\nLLM-specific hallucinations in OM tasks. We outline the methodology used in\ndataset construction and schema extension, and provide examples of potential\nuse cases."
                },
                "authors": [
                    {
                        "name": "Zhangcheng Qiang"
                    },
                    {
                        "name": "Kerry Taylor"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Jing Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Jiang"
                },
                "author": "Jing Jiang",
                "arxiv_comment": "4 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14038v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14038v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08764v1",
                "updated": "2024-10-11T12:23:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    23,
                    45,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T12:23:45Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    23,
                    45,
                    4,
                    285,
                    0
                ],
                "title": "Measuring the Groundedness of Legal Question-Answering Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring the Groundedness of Legal Question-Answering Systems"
                },
                "summary": "In high-stakes domains like legal question-answering, the accuracy and\ntrustworthiness of generative AI systems are of paramount importance. This work\npresents a comprehensive benchmark of various methods to assess the\ngroundedness of AI-generated responses, aiming to significantly enhance their\nreliability. Our experiments include similarity-based metrics and natural\nlanguage inference models to evaluate whether responses are well-founded in the\ngiven contexts. We also explore different prompting strategies for large\nlanguage models to improve the detection of ungrounded responses. We validated\nthe effectiveness of these methods using a newly created grounding\nclassification corpus, designed specifically for legal queries and\ncorresponding responses from retrieval-augmented prompting, focusing on their\nalignment with source material. Our results indicate potential in groundedness\nclassification of generated responses, with the best method achieving a\nmacro-F1 score of 0.8. Additionally, we evaluated the methods in terms of their\nlatency to determine their suitability for real-world applications, as this\nstep typically follows the generation process. This capability is essential for\nprocesses that may trigger additional manual verification or automated response\nregeneration. In summary, this study demonstrates the potential of various\ndetection methods to improve the trustworthiness of generative AI in legal\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In high-stakes domains like legal question-answering, the accuracy and\ntrustworthiness of generative AI systems are of paramount importance. This work\npresents a comprehensive benchmark of various methods to assess the\ngroundedness of AI-generated responses, aiming to significantly enhance their\nreliability. Our experiments include similarity-based metrics and natural\nlanguage inference models to evaluate whether responses are well-founded in the\ngiven contexts. We also explore different prompting strategies for large\nlanguage models to improve the detection of ungrounded responses. We validated\nthe effectiveness of these methods using a newly created grounding\nclassification corpus, designed specifically for legal queries and\ncorresponding responses from retrieval-augmented prompting, focusing on their\nalignment with source material. Our results indicate potential in groundedness\nclassification of generated responses, with the best method achieving a\nmacro-F1 score of 0.8. Additionally, we evaluated the methods in terms of their\nlatency to determine their suitability for real-world applications, as this\nstep typically follows the generation process. This capability is essential for\nprocesses that may trigger additional manual verification or automated response\nregeneration. In summary, this study demonstrates the potential of various\ndetection methods to improve the trustworthiness of generative AI in legal\nsettings."
                },
                "authors": [
                    {
                        "name": "Dietrich Trautmann"
                    },
                    {
                        "name": "Natalia Ostapuk"
                    },
                    {
                        "name": "Quentin Grail"
                    },
                    {
                        "name": "Adrian Alan Pol"
                    },
                    {
                        "name": "Guglielmo Bonifazi"
                    },
                    {
                        "name": "Shang Gao"
                    },
                    {
                        "name": "Martin Gajek"
                    }
                ],
                "author_detail": {
                    "name": "Martin Gajek"
                },
                "author": "Martin Gajek",
                "arxiv_comment": "to appear NLLP @ EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20135v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20135v4",
                "updated": "2024-10-11T12:19:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    57,
                    4,
                    285,
                    0
                ],
                "published": "2024-09-30T09:34:31Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    34,
                    31,
                    0,
                    274,
                    0
                ],
                "title": "Federated Instruction Tuning of LLMs with Domain Coverage Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Instruction Tuning of LLMs with Domain Coverage Augmentation"
                },
                "summary": "Federated Domain-specific Instruction Tuning (FedDIT) utilizes limited\ncross-client private data together with server-side public data for instruction\naugmentation, ultimately boosting model performance within specific domains. To\ndate, the factors affecting FedDIT remain unclear, and existing instruction\naugmentation methods primarily focus on the centralized setting without\nconsidering distributed environments. Our experiments reveal that the\ncross-client domain coverage, rather than data heterogeneity, drives model\nperformance in FedDIT. In response, we propose FedDCA, which optimizes domain\ncoverage through greedy client center selection and retrieval-based\naugmentation. For client-side computational efficiency and system scalability,\nFedDCA$^*$, the variant of FedDCA, utilizes heterogeneous encoders with\nserver-side feature alignment. Extensive experiments across four distinct\ndomains (code, medical, financial, and mathematical) substantiate the\neffectiveness of both methods. Additionally, we investigate privacy\npreservation against memory extraction attacks utilizing various amounts of\npublic data. Results show that there is no significant correlation between the\nvolume of public data and the privacy-preserving capability. However, as the\nfine-tuning rounds increase, the risk of privacy leakage reduces or converges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Domain-specific Instruction Tuning (FedDIT) utilizes limited\ncross-client private data together with server-side public data for instruction\naugmentation, ultimately boosting model performance within specific domains. To\ndate, the factors affecting FedDIT remain unclear, and existing instruction\naugmentation methods primarily focus on the centralized setting without\nconsidering distributed environments. Our experiments reveal that the\ncross-client domain coverage, rather than data heterogeneity, drives model\nperformance in FedDIT. In response, we propose FedDCA, which optimizes domain\ncoverage through greedy client center selection and retrieval-based\naugmentation. For client-side computational efficiency and system scalability,\nFedDCA$^*$, the variant of FedDCA, utilizes heterogeneous encoders with\nserver-side feature alignment. Extensive experiments across four distinct\ndomains (code, medical, financial, and mathematical) substantiate the\neffectiveness of both methods. Additionally, we investigate privacy\npreservation against memory extraction attacks utilizing various amounts of\npublic data. Results show that there is no significant correlation between the\nvolume of public data and the privacy-preserving capability. However, as the\nfine-tuning rounds increase, the risk of privacy leakage reduces or converges."
                },
                "authors": [
                    {
                        "name": "Zezhou Wang"
                    },
                    {
                        "name": "Yaxin Du"
                    },
                    {
                        "name": "Zhuzhong Qian"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20135v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20135v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08755v1",
                "updated": "2024-10-11T12:13:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    13,
                    3,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T12:13:03Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    13,
                    3,
                    4,
                    285,
                    0
                ],
                "title": "PILLAR: an AI-Powered Privacy Threat Modeling Tool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PILLAR: an AI-Powered Privacy Threat Modeling Tool"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has unlocked new\npossibilities for applying artificial intelligence across a wide range of\nfields, including privacy engineering. As modern applications increasingly\nhandle sensitive user data, safeguarding privacy has become more critical than\never. To protect privacy effectively, potential threats need to be identified\nand addressed early in the system development process. Frameworks like LINDDUN\noffer structured approaches for uncovering these risks, but despite their\nvalue, they often demand substantial manual effort, expert input, and detailed\nsystem knowledge. This makes the process time-consuming and prone to errors.\nCurrent privacy threat modeling methods, such as LINDDUN, typically rely on\ncreating and analyzing complex data flow diagrams (DFDs) and system\ndescriptions to pinpoint potential privacy issues. While these approaches are\nthorough, they can be cumbersome, relying heavily on the precision of the data\nprovided by users. Moreover, they often generate a long list of threats without\nclear guidance on how to prioritize them, leaving developers unsure of where to\nfocus their efforts. In response to these challenges, we introduce PILLAR\n(Privacy risk Identification with LINDDUN and LLM Analysis Report), a new tool\nthat integrates LLMs with the LINDDUN framework to streamline and enhance\nprivacy threat modeling. PILLAR automates key parts of the LINDDUN process,\nsuch as generating DFDs, classifying threats, and prioritizing risks. By\nleveraging the capabilities of LLMs, PILLAR can take natural language\ndescriptions of systems and transform them into comprehensive threat models\nwith minimal input from users, reducing the workload on developers and privacy\nexperts while improving the efficiency and accuracy of the process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has unlocked new\npossibilities for applying artificial intelligence across a wide range of\nfields, including privacy engineering. As modern applications increasingly\nhandle sensitive user data, safeguarding privacy has become more critical than\never. To protect privacy effectively, potential threats need to be identified\nand addressed early in the system development process. Frameworks like LINDDUN\noffer structured approaches for uncovering these risks, but despite their\nvalue, they often demand substantial manual effort, expert input, and detailed\nsystem knowledge. This makes the process time-consuming and prone to errors.\nCurrent privacy threat modeling methods, such as LINDDUN, typically rely on\ncreating and analyzing complex data flow diagrams (DFDs) and system\ndescriptions to pinpoint potential privacy issues. While these approaches are\nthorough, they can be cumbersome, relying heavily on the precision of the data\nprovided by users. Moreover, they often generate a long list of threats without\nclear guidance on how to prioritize them, leaving developers unsure of where to\nfocus their efforts. In response to these challenges, we introduce PILLAR\n(Privacy risk Identification with LINDDUN and LLM Analysis Report), a new tool\nthat integrates LLMs with the LINDDUN framework to streamline and enhance\nprivacy threat modeling. PILLAR automates key parts of the LINDDUN process,\nsuch as generating DFDs, classifying threats, and prioritizing risks. By\nleveraging the capabilities of LLMs, PILLAR can take natural language\ndescriptions of systems and transform them into comprehensive threat models\nwith minimal input from users, reducing the workload on developers and privacy\nexperts while improving the efficiency and accuracy of the process."
                },
                "authors": [
                    {
                        "name": "Majid Mollaeefar"
                    },
                    {
                        "name": "Andrea Bissoli"
                    },
                    {
                        "name": "Silvio Ranise"
                    }
                ],
                "author_detail": {
                    "name": "Silvio Ranise"
                },
                "author": "Silvio Ranise",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04827v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04827v3",
                "updated": "2024-10-11T12:06:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    6,
                    32,
                    4,
                    285,
                    0
                ],
                "published": "2024-06-07T10:52:15Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    10,
                    52,
                    15,
                    4,
                    159,
                    0
                ],
                "title": "Auditing Differential Privacy Guarantees Using Density Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing Differential Privacy Guarantees Using Density Estimation"
                },
                "summary": "We present a novel method for accurately auditing the differential privacy\n(DP) guarantees of DP mechanisms. In particular, our solution is applicable to\nauditing DP guarantees of machine learning (ML) models. Previous auditing\nmethods tightly capture the privacy guarantees of DP-SGD trained models in the\nwhite-box setting where the auditor has access to all intermediate models;\nhowever, the success of these methods depends on a priori information about the\nparametric form of the noise and the subsampling ratio used for sampling the\ngradients. We present a method that does not require such information and is\nagnostic to the randomization used for the underlying mechanism. Similarly to\nseveral previous DP auditing methods, we assume that the auditor has access to\na set of independent observations from two one-dimensional distributions\ncorresponding to outputs from two neighbouring datasets. Furthermore, our\nsolution is based on a simple histogram-based density estimation technique to\nfind lower bounds for the statistical distance between these distributions when\nmeasured using the hockey-stick divergence. We show that our approach also\nnaturally generalizes the previously considered class of threshold membership\ninference auditing methods. We improve upon accurate auditing methods such as\nthe $f$-DP auditing. Moreover, we address an open problem on how to accurately\naudit the subsampled Gaussian mechanism without any knowledge of the parameters\nof the underlying mechanism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel method for accurately auditing the differential privacy\n(DP) guarantees of DP mechanisms. In particular, our solution is applicable to\nauditing DP guarantees of machine learning (ML) models. Previous auditing\nmethods tightly capture the privacy guarantees of DP-SGD trained models in the\nwhite-box setting where the auditor has access to all intermediate models;\nhowever, the success of these methods depends on a priori information about the\nparametric form of the noise and the subsampling ratio used for sampling the\ngradients. We present a method that does not require such information and is\nagnostic to the randomization used for the underlying mechanism. Similarly to\nseveral previous DP auditing methods, we assume that the auditor has access to\na set of independent observations from two one-dimensional distributions\ncorresponding to outputs from two neighbouring datasets. Furthermore, our\nsolution is based on a simple histogram-based density estimation technique to\nfind lower bounds for the statistical distance between these distributions when\nmeasured using the hockey-stick divergence. We show that our approach also\nnaturally generalizes the previously considered class of threshold membership\ninference auditing methods. We improve upon accurate auditing methods such as\nthe $f$-DP auditing. Moreover, we address an open problem on how to accurately\naudit the subsampled Gaussian mechanism without any knowledge of the parameters\nof the underlying mechanism."
                },
                "authors": [
                    {
                        "name": "Antti Koskela"
                    },
                    {
                        "name": "Jafar Mohammadi"
                    }
                ],
                "author_detail": {
                    "name": "Jafar Mohammadi"
                },
                "author": "Jafar Mohammadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04827v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04827v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08782v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08782v2",
                "updated": "2024-10-11T12:04:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    4,
                    11,
                    4,
                    285,
                    0
                ],
                "published": "2024-08-16T14:54:41Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    54,
                    41,
                    4,
                    229,
                    0
                ],
                "title": "EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling\n  MiXed Emotions and Discourse Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling\n  MiXed Emotions and Discourse Dynamics"
                },
                "summary": "Designing emotionally intelligent conversational systems to provide comfort\nand advice to people experiencing distress is a compelling area of research.\nRecently, with advancements in large language models (LLMs), end-to-end\ndialogue agents without explicit strategy prediction steps have become\nprevalent. However, implicit strategy planning lacks transparency, and recent\nstudies show that LLMs' inherent preference bias towards certain\nsocio-emotional strategies hinders the delivery of high-quality emotional\nsupport. To address this challenge, we propose decoupling strategy prediction\nfrom language generation, and introduce a novel dialogue strategy prediction\nframework, EmoDynamiX, which models the discourse dynamics between user\nfine-grained emotions and system strategies using a heterogeneous graph for\nbetter performance and transparency. Experimental results on two ESC datasets\nshow EmoDynamiX outperforms previous state-of-the-art methods with a\nsignificant margin (better proficiency and lower preference bias). Our approach\nalso exhibits better transparency by allowing backtracing of decision making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing emotionally intelligent conversational systems to provide comfort\nand advice to people experiencing distress is a compelling area of research.\nRecently, with advancements in large language models (LLMs), end-to-end\ndialogue agents without explicit strategy prediction steps have become\nprevalent. However, implicit strategy planning lacks transparency, and recent\nstudies show that LLMs' inherent preference bias towards certain\nsocio-emotional strategies hinders the delivery of high-quality emotional\nsupport. To address this challenge, we propose decoupling strategy prediction\nfrom language generation, and introduce a novel dialogue strategy prediction\nframework, EmoDynamiX, which models the discourse dynamics between user\nfine-grained emotions and system strategies using a heterogeneous graph for\nbetter performance and transparency. Experimental results on two ESC datasets\nshow EmoDynamiX outperforms previous state-of-the-art methods with a\nsignificant margin (better proficiency and lower preference bias). Our approach\nalso exhibits better transparency by allowing backtracing of decision making."
                },
                "authors": [
                    {
                        "name": "Chenwei Wan"
                    },
                    {
                        "name": "Matthieu Labeau"
                    },
                    {
                        "name": "Chloé Clavel"
                    }
                ],
                "author_detail": {
                    "name": "Chloé Clavel"
                },
                "author": "Chloé Clavel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08782v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08782v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08740v1",
                "updated": "2024-10-11T11:59:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    11,
                    59,
                    40,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T11:59:40Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    11,
                    59,
                    40,
                    4,
                    285,
                    0
                ],
                "title": "Hespi: A pipeline for automatically detecting information from hebarium\n  specimen sheets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hespi: A pipeline for automatically detecting information from hebarium\n  specimen sheets"
                },
                "summary": "Specimen associated biodiversity data are sought after for biological,\nenvironmental, climate, and conservation sciences. A rate shift is required for\nthe extraction of data from specimen images to eliminate the bottleneck that\nthe reliance on human-mediated transcription of these data represents. We\napplied advanced computer vision techniques to develop the `Hespi' (HErbarium\nSpecimen sheet PIpeline), which extracts a pre-catalogue subset of collection\ndata on the institutional labels on herbarium specimens from their digital\nimages. The pipeline integrates two object detection models; the first detects\nbounding boxes around text-based labels and the second detects bounding boxes\naround text-based data fields on the primary institutional label. The pipeline\nclassifies text-based institutional labels as printed, typed, handwritten, or a\ncombination and applies Optical Character Recognition (OCR) and Handwritten\nText Recognition (HTR) for data extraction. The recognized text is then\ncorrected against authoritative databases of taxon names. The extracted text is\nalso corrected with the aide of a multimodal Large Language Model (LLM). Hespi\naccurately detects and extracts text for test datasets including specimen sheet\nimages from international herbaria. The components of the pipeline are modular\nand users can train their own models with their own data and use them in place\nof the models provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Specimen associated biodiversity data are sought after for biological,\nenvironmental, climate, and conservation sciences. A rate shift is required for\nthe extraction of data from specimen images to eliminate the bottleneck that\nthe reliance on human-mediated transcription of these data represents. We\napplied advanced computer vision techniques to develop the `Hespi' (HErbarium\nSpecimen sheet PIpeline), which extracts a pre-catalogue subset of collection\ndata on the institutional labels on herbarium specimens from their digital\nimages. The pipeline integrates two object detection models; the first detects\nbounding boxes around text-based labels and the second detects bounding boxes\naround text-based data fields on the primary institutional label. The pipeline\nclassifies text-based institutional labels as printed, typed, handwritten, or a\ncombination and applies Optical Character Recognition (OCR) and Handwritten\nText Recognition (HTR) for data extraction. The recognized text is then\ncorrected against authoritative databases of taxon names. The extracted text is\nalso corrected with the aide of a multimodal Large Language Model (LLM). Hespi\naccurately detects and extracts text for test datasets including specimen sheet\nimages from international herbaria. The components of the pipeline are modular\nand users can train their own models with their own data and use them in place\nof the models provided."
                },
                "authors": [
                    {
                        "name": "Robert Turnbull"
                    },
                    {
                        "name": "Emily Fitzgerald"
                    },
                    {
                        "name": "Karen Thompson"
                    },
                    {
                        "name": "Joanne L. Birch"
                    }
                ],
                "author_detail": {
                    "name": "Joanne L. Birch"
                },
                "author": "Joanne L. Birch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01379v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01379v4",
                "updated": "2024-10-11T11:54:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    11,
                    54,
                    35,
                    4,
                    285,
                    0
                ],
                "published": "2024-05-02T15:20:01Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    15,
                    20,
                    1,
                    3,
                    123,
                    0
                ],
                "title": "Verification and Refinement of Natural Language Explanations through\n  LLM-Symbolic Theorem Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verification and Refinement of Natural Language Explanations through\n  LLM-Symbolic Theorem Proving"
                },
                "summary": "Natural language explanations represent a proxy for evaluating\nexplanation-based and multi-step Natural Language Inference (NLI) models.\nHowever, assessing the validity of explanations for NLI is challenging as it\ntypically involves the crowd-sourcing of apposite datasets, a process that is\ntime-consuming and prone to logical errors. To address existing limitations,\nthis paper investigates the verification and refinement of natural language\nexplanations through the integration of Large Language Models (LLMs) and\nTheorem Provers (TPs). Specifically, we present a neuro-symbolic framework,\nnamed Explanation-Refiner, that integrates TPs with LLMs to generate and\nformalise explanatory sentences and suggest potential inference strategies for\nNLI. In turn, the TP is employed to provide formal guarantees on the logical\nvalidity of the explanations and to generate feedback for subsequent\nimprovements. We demonstrate how Explanation-Refiner can be jointly used to\nevaluate explanatory reasoning, autoformalisation, and error correction\nmechanisms of state-of-the-art LLMs as well as to automatically enhance the\nquality of explanations of variable complexity in different domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language explanations represent a proxy for evaluating\nexplanation-based and multi-step Natural Language Inference (NLI) models.\nHowever, assessing the validity of explanations for NLI is challenging as it\ntypically involves the crowd-sourcing of apposite datasets, a process that is\ntime-consuming and prone to logical errors. To address existing limitations,\nthis paper investigates the verification and refinement of natural language\nexplanations through the integration of Large Language Models (LLMs) and\nTheorem Provers (TPs). Specifically, we present a neuro-symbolic framework,\nnamed Explanation-Refiner, that integrates TPs with LLMs to generate and\nformalise explanatory sentences and suggest potential inference strategies for\nNLI. In turn, the TP is employed to provide formal guarantees on the logical\nvalidity of the explanations and to generate feedback for subsequent\nimprovements. We demonstrate how Explanation-Refiner can be jointly used to\nevaluate explanatory reasoning, autoformalisation, and error correction\nmechanisms of state-of-the-art LLMs as well as to automatically enhance the\nquality of explanations of variable complexity in different domains."
                },
                "authors": [
                    {
                        "name": "Xin Quan"
                    },
                    {
                        "name": "Marco Valentino"
                    },
                    {
                        "name": "Louise A. Dennis"
                    },
                    {
                        "name": "André Freitas"
                    }
                ],
                "author_detail": {
                    "name": "André Freitas"
                },
                "author": "André Freitas",
                "arxiv_comment": "Camera-ready for EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01379v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01379v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08731v1",
                "updated": "2024-10-11T11:41:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    11,
                    41,
                    2,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T11:41:02Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    11,
                    41,
                    2,
                    4,
                    285,
                    0
                ],
                "title": "Developing a Pragmatic Benchmark for Assessing Korean Legal Language\n  Understanding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing a Pragmatic Benchmark for Assessing Korean Legal Language\n  Understanding in Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance in the\nlegal domain, with GPT-4 even passing the Uniform Bar Exam in the U.S. However\ntheir efficacy remains limited for non-standardized tasks and tasks in\nlanguages other than English. This underscores the need for careful evaluation\nof LLMs within each legal system before application. Here, we introduce KBL, a\nbenchmark for assessing the Korean legal language understanding of LLMs,\nconsisting of (1) 7 legal knowledge tasks (510 examples), (2) 4 legal reasoning\ntasks (288 examples), and (3) the Korean bar exam (4 domains, 53 tasks, 2,510\nexamples). First two datasets were developed in close collaboration with\nlawyers to evaluate LLMs in practical scenarios in a certified manner.\nFurthermore, considering legal practitioners' frequent use of extensive legal\ndocuments for research, we assess LLMs in both a closed book setting, where\nthey rely solely on internal knowledge, and a retrieval-augmented generation\n(RAG) setting, using a corpus of Korean statutes and precedents. The results\nindicate substantial room and opportunities for improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance in the\nlegal domain, with GPT-4 even passing the Uniform Bar Exam in the U.S. However\ntheir efficacy remains limited for non-standardized tasks and tasks in\nlanguages other than English. This underscores the need for careful evaluation\nof LLMs within each legal system before application. Here, we introduce KBL, a\nbenchmark for assessing the Korean legal language understanding of LLMs,\nconsisting of (1) 7 legal knowledge tasks (510 examples), (2) 4 legal reasoning\ntasks (288 examples), and (3) the Korean bar exam (4 domains, 53 tasks, 2,510\nexamples). First two datasets were developed in close collaboration with\nlawyers to evaluate LLMs in practical scenarios in a certified manner.\nFurthermore, considering legal practitioners' frequent use of extensive legal\ndocuments for research, we assess LLMs in both a closed book setting, where\nthey rely solely on internal knowledge, and a retrieval-augmented generation\n(RAG) setting, using a corpus of Korean statutes and precedents. The results\nindicate substantial room and opportunities for improvement."
                },
                "authors": [
                    {
                        "name": "Yeeun Kim"
                    },
                    {
                        "name": "Young Rok Choi"
                    },
                    {
                        "name": "Eunkyung Choi"
                    },
                    {
                        "name": "Jinhwan Choi"
                    },
                    {
                        "name": "Hai Jin Park"
                    },
                    {
                        "name": "Wonseok Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Wonseok Hwang"
                },
                "author": "Wonseok Hwang",
                "arxiv_comment": "EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07041v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07041v3",
                "updated": "2024-10-11T11:36:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    11,
                    36,
                    39,
                    4,
                    285,
                    0
                ],
                "published": "2024-03-11T16:26:06Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    16,
                    26,
                    6,
                    0,
                    71,
                    0
                ],
                "title": "Ant Colony Sampling with GFlowNets for Combinatorial Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ant Colony Sampling with GFlowNets for Combinatorial Optimization"
                },
                "summary": "We present the Generative Flow Ant Colony Sampler (GFACS), a novel\nmeta-heuristic method that hierarchically combines amortized inference and\nparallel stochastic search. Our method first leverages Generative Flow Networks\n(GFlowNets) to amortize a multi-modal prior distribution over combinatorial\nsolution space that encompasses both high-reward and diversified solutions.\nThis prior is iteratively updated via parallel stochastic search in the spirit\nof Ant Colony Optimization (ACO), leading to the posterior distribution that\ngenerates near-optimal solutions. Extensive experiments across seven\ncombinatorial optimization problems demonstrate GFACS's promising performances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the Generative Flow Ant Colony Sampler (GFACS), a novel\nmeta-heuristic method that hierarchically combines amortized inference and\nparallel stochastic search. Our method first leverages Generative Flow Networks\n(GFlowNets) to amortize a multi-modal prior distribution over combinatorial\nsolution space that encompasses both high-reward and diversified solutions.\nThis prior is iteratively updated via parallel stochastic search in the spirit\nof Ant Colony Optimization (ACO), leading to the posterior distribution that\ngenerates near-optimal solutions. Extensive experiments across seven\ncombinatorial optimization problems demonstrate GFACS's promising performances."
                },
                "authors": [
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Sanghyeok Choi"
                    },
                    {
                        "name": "Hyeonah Kim"
                    },
                    {
                        "name": "Jiwoo Son"
                    },
                    {
                        "name": "Jinkyoo Park"
                    },
                    {
                        "name": "Yoshua Bengio"
                    }
                ],
                "author_detail": {
                    "name": "Yoshua Bengio"
                },
                "author": "Yoshua Bengio",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07041v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07041v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20222v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20222v2",
                "updated": "2024-10-11T11:32:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    11,
                    32,
                    16,
                    4,
                    285,
                    0
                ],
                "published": "2024-09-30T12:01:29Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    12,
                    1,
                    29,
                    0,
                    274,
                    0
                ],
                "title": "Beyond Prompts: Dynamic Conversational Benchmarking of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Prompts: Dynamic Conversational Benchmarking of Large Language\n  Models"
                },
                "summary": "We introduce a dynamic benchmarking system for conversational agents that\nevaluates their performance through a single, simulated, and lengthy\nuser$\\leftrightarrow$agent interaction. The interaction is a conversation\nbetween the user and agent, where multiple tasks are introduced and then\nundertaken concurrently. We context switch regularly to interleave the tasks,\nwhich constructs a realistic testing scenario in which we assess the Long-Term\nMemory, Continual Learning, and Information Integration capabilities of the\nagents. Results from both proprietary and open-source Large-Language Models\nshow that LLMs in general perform well on single-task interactions, but they\nstruggle on the same tasks when they are interleaved. Notably, short-context\nLLMs supplemented with an LTM system perform as well as or better than those\nwith larger contexts. Our benchmark suggests that there are other challenges\nfor LLMs responding to more natural interactions that contemporary benchmarks\nhave heretofore not been able to capture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a dynamic benchmarking system for conversational agents that\nevaluates their performance through a single, simulated, and lengthy\nuser$\\leftrightarrow$agent interaction. The interaction is a conversation\nbetween the user and agent, where multiple tasks are introduced and then\nundertaken concurrently. We context switch regularly to interleave the tasks,\nwhich constructs a realistic testing scenario in which we assess the Long-Term\nMemory, Continual Learning, and Information Integration capabilities of the\nagents. Results from both proprietary and open-source Large-Language Models\nshow that LLMs in general perform well on single-task interactions, but they\nstruggle on the same tasks when they are interleaved. Notably, short-context\nLLMs supplemented with an LTM system perform as well as or better than those\nwith larger contexts. Our benchmark suggests that there are other challenges\nfor LLMs responding to more natural interactions that contemporary benchmarks\nhave heretofore not been able to capture."
                },
                "authors": [
                    {
                        "name": "David Castillo-Bolado"
                    },
                    {
                        "name": "Joseph Davidson"
                    },
                    {
                        "name": "Finlay Gray"
                    },
                    {
                        "name": "Marek Rosa"
                    }
                ],
                "author_detail": {
                    "name": "Marek Rosa"
                },
                "author": "Marek Rosa",
                "arxiv_comment": "Accepted as a poster at NeurIPS D&B Track 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20222v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20222v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08723v1",
                "updated": "2024-10-11T11:23:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    11,
                    23,
                    26,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T11:23:26Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    11,
                    23,
                    26,
                    4,
                    285,
                    0
                ],
                "title": "Investigating Human-Computer Interaction and Visual Comprehension in\n  Text Generation Process of Natural Language Generation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Human-Computer Interaction and Visual Comprehension in\n  Text Generation Process of Natural Language Generation Models"
                },
                "summary": "Natural language generation (NLG) models are becoming a highly sought-after\nresearch focus in the field of natural language processing (NLP), demonstrating\nstrong capabilities in text generation tasks such as writing and dialogue\ngeneration. Despite the impressive performance of NLG models, their complex\narchitecture and extensive model weights result in a lack of interpretability.\nThis limitation hampers their adoption in many critical decision-making\nscenarios. Fortunately, the intervention of human-computer interaction and\nvisual comprehension provides users with the possibility of opening the \"black\nbox\". In this paper, we conduct a investigation addressing the roles and\nlimitations of human-computer interactive and visual comprehension in text\ngeneration process of NLG models. We present a taxonomy of interaction methods\nand visualization techniques, providing a structured overview of the three main\nresearch subjects and their corresponding six tasks within the application\nprocess of large language models (LLMs). Finally, we summarize the shortcomings\nin the existing work and investigate the key challenges and emerging\nopportunities in the era of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language generation (NLG) models are becoming a highly sought-after\nresearch focus in the field of natural language processing (NLP), demonstrating\nstrong capabilities in text generation tasks such as writing and dialogue\ngeneration. Despite the impressive performance of NLG models, their complex\narchitecture and extensive model weights result in a lack of interpretability.\nThis limitation hampers their adoption in many critical decision-making\nscenarios. Fortunately, the intervention of human-computer interaction and\nvisual comprehension provides users with the possibility of opening the \"black\nbox\". In this paper, we conduct a investigation addressing the roles and\nlimitations of human-computer interactive and visual comprehension in text\ngeneration process of NLG models. We present a taxonomy of interaction methods\nand visualization techniques, providing a structured overview of the three main\nresearch subjects and their corresponding six tasks within the application\nprocess of large language models (LLMs). Finally, we summarize the shortcomings\nin the existing work and investigate the key challenges and emerging\nopportunities in the era of LLMs."
                },
                "authors": [
                    {
                        "name": "Yunchao Wang"
                    },
                    {
                        "name": "Zihang Fu"
                    },
                    {
                        "name": "Chaoqing Xu"
                    },
                    {
                        "name": "Guodao Sun"
                    },
                    {
                        "name": "Ronghua Liang"
                    }
                ],
                "author_detail": {
                    "name": "Ronghua Liang"
                },
                "author": "Ronghua Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08711v1",
                "updated": "2024-10-11T10:54:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    10,
                    54,
                    9,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T10:54:09Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    10,
                    54,
                    9,
                    4,
                    285,
                    0
                ],
                "title": "On-Chip Learning via Transformer In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-Chip Learning via Transformer In-Context Learning"
                },
                "summary": "Autoregressive decoder-only transformers have become key components for\nscalable sequence processing and generation models. However, the transformer's\nself-attention mechanism requires transferring prior token projections from the\nmain memory at each time step (token), thus severely limiting their performance\non conventional processors. Self-attention can be viewed as a dynamic\nfeed-forward layer, whose matrix is input sequence-dependent similarly to the\nresult of local synaptic plasticity. Using this insight, we present a\nneuromorphic decoder-only transformer model that utilizes an on-chip plasticity\nprocessor to compute self-attention. Interestingly, the training of\ntransformers enables them to ``learn'' the input context during inference. We\ndemonstrate this in-context learning ability of transformers on the Loihi 2\nprocessor by solving a few-shot classification problem. With this we emphasize\nthe importance of pretrained models especially their ability to find simple,\nlocal, backpropagation free, learning rules enabling on-chip learning and\nadaptation in a hardware friendly manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive decoder-only transformers have become key components for\nscalable sequence processing and generation models. However, the transformer's\nself-attention mechanism requires transferring prior token projections from the\nmain memory at each time step (token), thus severely limiting their performance\non conventional processors. Self-attention can be viewed as a dynamic\nfeed-forward layer, whose matrix is input sequence-dependent similarly to the\nresult of local synaptic plasticity. Using this insight, we present a\nneuromorphic decoder-only transformer model that utilizes an on-chip plasticity\nprocessor to compute self-attention. Interestingly, the training of\ntransformers enables them to ``learn'' the input context during inference. We\ndemonstrate this in-context learning ability of transformers on the Loihi 2\nprocessor by solving a few-shot classification problem. With this we emphasize\nthe importance of pretrained models especially their ability to find simple,\nlocal, backpropagation free, learning rules enabling on-chip learning and\nadaptation in a hardware friendly manner."
                },
                "authors": [
                    {
                        "name": "Jan Finkbeiner"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08710v1",
                "updated": "2024-10-11T10:53:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    10,
                    53,
                    38,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T10:53:38Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    10,
                    53,
                    38,
                    4,
                    285,
                    0
                ],
                "title": "Preferential Normalizing Flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preferential Normalizing Flows"
                },
                "summary": "Eliciting a high-dimensional probability distribution from an expert via\nnoisy judgments is notoriously challenging, yet useful for many applications,\nsuch as prior elicitation and reward modeling. We introduce a method for\neliciting the expert's belief density as a normalizing flow based solely on\npreferential questions such as comparing or ranking alternatives. This allows\neliciting in principle arbitrarily flexible densities, but flow estimation is\nsusceptible to the challenge of collapsing or diverging probability mass that\nmakes it difficult in practice. We tackle this problem by introducing a novel\nfunctional prior for the flow, motivated by a decision-theoretic argument, and\nshow empirically that the belief density can be inferred as the function-space\nmaximum a posteriori estimate. We demonstrate our method by eliciting\nmultivariate belief densities of simulated experts, including the prior belief\nof a general-purpose large language model over a real-world dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eliciting a high-dimensional probability distribution from an expert via\nnoisy judgments is notoriously challenging, yet useful for many applications,\nsuch as prior elicitation and reward modeling. We introduce a method for\neliciting the expert's belief density as a normalizing flow based solely on\npreferential questions such as comparing or ranking alternatives. This allows\neliciting in principle arbitrarily flexible densities, but flow estimation is\nsusceptible to the challenge of collapsing or diverging probability mass that\nmakes it difficult in practice. We tackle this problem by introducing a novel\nfunctional prior for the flow, motivated by a decision-theoretic argument, and\nshow empirically that the belief density can be inferred as the function-space\nmaximum a posteriori estimate. We demonstrate our method by eliciting\nmultivariate belief densities of simulated experts, including the prior belief\nof a general-purpose large language model over a real-world dataset."
                },
                "authors": [
                    {
                        "name": "Petrus Mikkola"
                    },
                    {
                        "name": "Luigi Acerbi"
                    },
                    {
                        "name": "Arto Klami"
                    }
                ],
                "author_detail": {
                    "name": "Arto Klami"
                },
                "author": "Arto Klami",
                "arxiv_comment": "29 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08708v1",
                "updated": "2024-10-11T10:53:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    10,
                    53,
                    0,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T10:53:00Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    10,
                    53,
                    0,
                    4,
                    285,
                    0
                ],
                "title": "Koopman correlations underlie linear response and causality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Koopman correlations underlie linear response and causality"
                },
                "summary": "The inference of causal relationships among observed variables is a pivotal,\nlongstanding problem in the scientific community. An intuitive method for\nquantifying these causal links involves examining the response of one variable\nto perturbations in another. The fluctuation-dissipation theorem elegantly\nconnects this response to the correlation functions of the unperturbed system,\nthereby bridging the concepts of causality and correlation. However, this\nrelationship becomes intricate in nonlinear systems, where knowledge of the\ninvariant measure is required but elusive, especially in high-dimensional\nspaces. In this study, we establish a novel link between the Koopman operator\nof nonlinear stochastic systems and the response function. This connection\nprovides an alternative method for computing the response function using\ngeneralized correlation functions, even when the invariant measure is unknown.\nWe validate our theoretical framework by applying it to a nonlinear\nhigh-dimensional system amenable to exact solutions, demonstrating convergence\nand consistency with established results. Finally, we discuss a significant\ninterplay between the resulting causal network and the relevant time scales of\nthe system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference of causal relationships among observed variables is a pivotal,\nlongstanding problem in the scientific community. An intuitive method for\nquantifying these causal links involves examining the response of one variable\nto perturbations in another. The fluctuation-dissipation theorem elegantly\nconnects this response to the correlation functions of the unperturbed system,\nthereby bridging the concepts of causality and correlation. However, this\nrelationship becomes intricate in nonlinear systems, where knowledge of the\ninvariant measure is required but elusive, especially in high-dimensional\nspaces. In this study, we establish a novel link between the Koopman operator\nof nonlinear stochastic systems and the response function. This connection\nprovides an alternative method for computing the response function using\ngeneralized correlation functions, even when the invariant measure is unknown.\nWe validate our theoretical framework by applying it to a nonlinear\nhigh-dimensional system amenable to exact solutions, demonstrating convergence\nand consistency with established results. Finally, we discuss a significant\ninterplay between the resulting causal network and the relevant time scales of\nthe system."
                },
                "authors": [
                    {
                        "name": "Gabriele Di Antonio"
                    },
                    {
                        "name": "Gianni Valerio Vinci"
                    }
                ],
                "author_detail": {
                    "name": "Gianni Valerio Vinci"
                },
                "author": "Gianni Valerio Vinci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08706v1",
                "updated": "2024-10-11T10:48:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    10,
                    48,
                    51,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T10:48:51Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    10,
                    48,
                    51,
                    4,
                    285,
                    0
                ],
                "title": "Goal-Oriented Communications for Real-time Inference with Two-Way Delay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Goal-Oriented Communications for Real-time Inference with Two-Way Delay"
                },
                "summary": "We design a goal-oriented communication strategy for remote inference, where\nan intelligent model (e.g., a pre-trained neural network) at the receiver side\npredicts the real-time value of a target signal based on data packets\ntransmitted from a remote location. The inference error depends on both the Age\nof Information (AoI) and the length of the data packets. Previous formulations\nof this problem either assumed IID transmission delays with immediate feedback\nor focused only on monotonic relations where inference performance degrades as\nthe input data ages. In contrast, we consider a possibly non-monotonic\nrelationship between the inference error and AoI. We show how to minimize the\nexpected time-average inference error under two-way delay, where the delay\nprocess can have memory. Simulation results highlight the significant benefits\nof adopting such a goal-oriented communication strategy for remote inference,\nespecially under highly variable delay scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We design a goal-oriented communication strategy for remote inference, where\nan intelligent model (e.g., a pre-trained neural network) at the receiver side\npredicts the real-time value of a target signal based on data packets\ntransmitted from a remote location. The inference error depends on both the Age\nof Information (AoI) and the length of the data packets. Previous formulations\nof this problem either assumed IID transmission delays with immediate feedback\nor focused only on monotonic relations where inference performance degrades as\nthe input data ages. In contrast, we consider a possibly non-monotonic\nrelationship between the inference error and AoI. We show how to minimize the\nexpected time-average inference error under two-way delay, where the delay\nprocess can have memory. Simulation results highlight the significant benefits\nof adopting such a goal-oriented communication strategy for remote inference,\nespecially under highly variable delay scenarios."
                },
                "authors": [
                    {
                        "name": "Cagri Ari"
                    },
                    {
                        "name": "Md Kamran Chowdhury Shisher"
                    },
                    {
                        "name": "Yin Sun"
                    },
                    {
                        "name": "Elif Uysal"
                    }
                ],
                "author_detail": {
                    "name": "Elif Uysal"
                },
                "author": "Elif Uysal",
                "arxiv_comment": "12 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.06468v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.06468v4",
                "updated": "2024-10-11T10:48:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    10,
                    48,
                    15,
                    4,
                    285,
                    0
                ],
                "published": "2024-01-12T09:29:13Z",
                "published_parsed": [
                    2024,
                    1,
                    12,
                    9,
                    29,
                    13,
                    4,
                    12,
                    0
                ],
                "title": "Adapting Large Language Models for Document-Level Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Large Language Models for Document-Level Machine Translation"
                },
                "summary": "Large language models (LLMs) have significantly advanced various natural\nlanguage processing (NLP) tasks. Recent research indicates that\nmoderately-sized LLMs often outperform larger ones after task-specific\nfine-tuning. This study focuses on adapting LLMs for document-level machine\ntranslation (DocMT) for specific language pairs. We first investigate the\nimpact of prompt strategies on translation performance and then conduct\nextensive experiments using two fine-tuning methods, three LLM backbones, and\n18 translation tasks across nine language pairs. Our results show that\nspecialized models can sometimes surpass GPT-4 in translation performance but\nstill face issues like off-target translation due to error propagation in\ndecoding. We provide an in-depth analysis of these LLMs tailored for DocMT,\nexamining translation errors, discourse phenomena, strategies for training and\ninference, the data efficiency of parallel documents, recent test set\nevaluations, and zero-shot crosslingual transfer. Our findings highlight the\nstrengths and limitations of LLM-based DocMT models and provide a foundation\nfor future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly advanced various natural\nlanguage processing (NLP) tasks. Recent research indicates that\nmoderately-sized LLMs often outperform larger ones after task-specific\nfine-tuning. This study focuses on adapting LLMs for document-level machine\ntranslation (DocMT) for specific language pairs. We first investigate the\nimpact of prompt strategies on translation performance and then conduct\nextensive experiments using two fine-tuning methods, three LLM backbones, and\n18 translation tasks across nine language pairs. Our results show that\nspecialized models can sometimes surpass GPT-4 in translation performance but\nstill face issues like off-target translation due to error propagation in\ndecoding. We provide an in-depth analysis of these LLMs tailored for DocMT,\nexamining translation errors, discourse phenomena, strategies for training and\ninference, the data efficiency of parallel documents, recent test set\nevaluations, and zero-shot crosslingual transfer. Our findings highlight the\nstrengths and limitations of LLM-based DocMT models and provide a foundation\nfor future research."
                },
                "authors": [
                    {
                        "name": "Minghao Wu"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Lizhen Qu"
                    },
                    {
                        "name": "George Foster"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    }
                ],
                "author_detail": {
                    "name": "Gholamreza Haffari"
                },
                "author": "Gholamreza Haffari",
                "arxiv_comment": "25 pages, 18 tables, 7 figures; ARR Feb 2024, 4/3/2, meta 2, rejected\n  by ACL2024; ARR June 2024, 4.5/3/2, meta 3, rejected by EMNLP2024;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.06468v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.06468v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08698v1",
                "updated": "2024-10-11T10:35:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    10,
                    35,
                    58,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T10:35:58Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    10,
                    35,
                    58,
                    4,
                    285,
                    0
                ],
                "title": "SocialGaze: Improving the Integration of Human Social Norms in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SocialGaze: Improving the Integration of Human Social Norms in Large\n  Language Models"
                },
                "summary": "While much research has explored enhancing the reasoning capabilities of\nlarge language models (LLMs) in the last few years, there is a gap in\nunderstanding the alignment of these models with social values and norms. We\nintroduce the task of judging social acceptance. Social acceptance requires\nmodels to judge and rationalize the acceptability of people's actions in social\nsituations. For example, is it socially acceptable for a neighbor to ask others\nin the community to keep their pets indoors at night? We find that LLMs'\nunderstanding of social acceptance is often misaligned with human consensus. To\nalleviate this, we introduce SocialGaze, a multi-step prompting framework, in\nwhich a language model verbalizes a social situation from multiple perspectives\nbefore forming a judgment. Our experiments demonstrate that the SocialGaze\napproach improves the alignment with human judgments by up to 11 F1 points with\nthe GPT-3.5 model. We also identify biases and correlations in LLMs in\nassigning blame that is related to features such as the gender (males are\nsignificantly more likely to be judged unfairly) and age (LLMs are more aligned\nwith humans for older narrators).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While much research has explored enhancing the reasoning capabilities of\nlarge language models (LLMs) in the last few years, there is a gap in\nunderstanding the alignment of these models with social values and norms. We\nintroduce the task of judging social acceptance. Social acceptance requires\nmodels to judge and rationalize the acceptability of people's actions in social\nsituations. For example, is it socially acceptable for a neighbor to ask others\nin the community to keep their pets indoors at night? We find that LLMs'\nunderstanding of social acceptance is often misaligned with human consensus. To\nalleviate this, we introduce SocialGaze, a multi-step prompting framework, in\nwhich a language model verbalizes a social situation from multiple perspectives\nbefore forming a judgment. Our experiments demonstrate that the SocialGaze\napproach improves the alignment with human judgments by up to 11 F1 points with\nthe GPT-3.5 model. We also identify biases and correlations in LLMs in\nassigning blame that is related to features such as the gender (males are\nsignificantly more likely to be judged unfairly) and age (LLMs are more aligned\nwith humans for older narrators)."
                },
                "authors": [
                    {
                        "name": "Anvesh Rao Vijjini"
                    },
                    {
                        "name": "Rakesh R. Menon"
                    },
                    {
                        "name": "Jiayi Fu"
                    },
                    {
                        "name": "Shashank Srivastava"
                    },
                    {
                        "name": "Snigdha Chaturvedi"
                    }
                ],
                "author_detail": {
                    "name": "Snigdha Chaturvedi"
                },
                "author": "Snigdha Chaturvedi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08696v1",
                "updated": "2024-10-11T10:34:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    10,
                    34,
                    28,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T10:34:28Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    10,
                    34,
                    28,
                    4,
                    285,
                    0
                ],
                "title": "AMPO: Automatic Multi-Branched Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AMPO: Automatic Multi-Branched Prompt Optimization"
                },
                "summary": "Prompt engineering is very important to enhance the performance of large\nlanguage models (LLMs). When dealing with complex issues, prompt engineers tend\nto distill multiple patterns from examples and inject relevant solutions to\noptimize the prompts, achieving satisfying results. However, existing automatic\nprompt optimization techniques are only limited to producing single flow\ninstructions, struggling with handling diverse patterns. In this paper, we\npresent AMPO, an automatic prompt optimization method that can iteratively\ndevelop a multi-branched prompt using failure cases as feedback. Our goal is to\nexplore a novel way of structuring prompts with multi-branches to better handle\nmultiple patterns in complex tasks, for which we introduce three modules:\nPattern Recognition, Branch Adjustment, and Branch Pruning. In experiments\nacross five tasks, AMPO consistently achieves the best results. Additionally,\nour approach demonstrates significant optimization efficiency due to our\nadoption of a minimal search strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt engineering is very important to enhance the performance of large\nlanguage models (LLMs). When dealing with complex issues, prompt engineers tend\nto distill multiple patterns from examples and inject relevant solutions to\noptimize the prompts, achieving satisfying results. However, existing automatic\nprompt optimization techniques are only limited to producing single flow\ninstructions, struggling with handling diverse patterns. In this paper, we\npresent AMPO, an automatic prompt optimization method that can iteratively\ndevelop a multi-branched prompt using failure cases as feedback. Our goal is to\nexplore a novel way of structuring prompts with multi-branches to better handle\nmultiple patterns in complex tasks, for which we introduce three modules:\nPattern Recognition, Branch Adjustment, and Branch Pruning. In experiments\nacross five tasks, AMPO consistently achieves the best results. Additionally,\nour approach demonstrates significant optimization efficiency due to our\nadoption of a minimal search strategy."
                },
                "authors": [
                    {
                        "name": "Sheng Yang"
                    },
                    {
                        "name": "Yurong Wu"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Zineng Zhou"
                    },
                    {
                        "name": "Bin Benjamin Zhu"
                    },
                    {
                        "name": "Xiaodi Sun"
                    },
                    {
                        "name": "Jian-Guang Lou"
                    },
                    {
                        "name": "Zhiming Ding"
                    },
                    {
                        "name": "Anbang Hu"
                    },
                    {
                        "name": "Yuan Fang"
                    },
                    {
                        "name": "Yunsong Li"
                    },
                    {
                        "name": "Junyan Chen"
                    },
                    {
                        "name": "Linjun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Linjun Yang"
                },
                "author": "Linjun Yang",
                "arxiv_comment": "13 pages, 7 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12902v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12902v2",
                "updated": "2024-10-11T10:27:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    10,
                    27,
                    16,
                    4,
                    285,
                    0
                ],
                "published": "2024-06-10T06:43:25Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    6,
                    43,
                    25,
                    0,
                    162,
                    0
                ],
                "title": "JavaBench: A Benchmark of Object-Oriented Code Generation for Evaluating\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JavaBench: A Benchmark of Object-Oriented Code Generation for Evaluating\n  Large Language Models"
                },
                "summary": "Code generation benchmarks such as HumanEval are widely adopted to evaluate\nLLMs' capabilities. However, after consolidating the latest 24 benchmarks, we\nnoticed three significant imbalances. First, imbalanced programming language.\n95.8% of benchmarks involve Python, while only 5 benchmarks involve Java.\nSecond, imbalanced code granularity. Function-/statement-level benchmarks\naccount for over 83.3% of benchmarks. Only a mere handful extends to\nclass-/project-levels, and all are limited to Python. Third, lacking advanced\nfeatures. Existing benchmarks primarily assess basic coding skills, while\noverlooking advanced Object-Oriented Programming (OOP) features (i.e.,\nencapsulation, inheritance, and polymorphism).\n  To fill these gaps, we propose JavaBench, a project-level Java benchmark that\nexercises OOP features. It comprises four Java projects with 389 methods in 106\nJava classes. The test coverage is up to 92%, and JavaBench is attested by 282\nundergraduate students, reaching a 90.93/100 average score (i.e., pass rate\nagainst the test suite), ensuring the quality of documentation, code skeleton,\nand tests. To better evaluate LLM's capability against JavaBench, we introduce\na systematic evaluation design covering three context settings and five\nsynthesis strategies at two granularities using three hierarchical metrics. Our\nextensive experiment yields several interesting findings. First, we noticed\nthat regarding project-level Java programming, LLMs are far behind\nundergraduate students (no project can be correctly completed by any studied\nLLMs, and at most 41.17% Pass@5 in a more relaxed evaluation). Second, using\nmethod signature as prompt context may strike an ideal balance for\nproject-level code generation. JavaBench is publicly available at\nhttps://github.com/java-bench/JavaBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation benchmarks such as HumanEval are widely adopted to evaluate\nLLMs' capabilities. However, after consolidating the latest 24 benchmarks, we\nnoticed three significant imbalances. First, imbalanced programming language.\n95.8% of benchmarks involve Python, while only 5 benchmarks involve Java.\nSecond, imbalanced code granularity. Function-/statement-level benchmarks\naccount for over 83.3% of benchmarks. Only a mere handful extends to\nclass-/project-levels, and all are limited to Python. Third, lacking advanced\nfeatures. Existing benchmarks primarily assess basic coding skills, while\noverlooking advanced Object-Oriented Programming (OOP) features (i.e.,\nencapsulation, inheritance, and polymorphism).\n  To fill these gaps, we propose JavaBench, a project-level Java benchmark that\nexercises OOP features. It comprises four Java projects with 389 methods in 106\nJava classes. The test coverage is up to 92%, and JavaBench is attested by 282\nundergraduate students, reaching a 90.93/100 average score (i.e., pass rate\nagainst the test suite), ensuring the quality of documentation, code skeleton,\nand tests. To better evaluate LLM's capability against JavaBench, we introduce\na systematic evaluation design covering three context settings and five\nsynthesis strategies at two granularities using three hierarchical metrics. Our\nextensive experiment yields several interesting findings. First, we noticed\nthat regarding project-level Java programming, LLMs are far behind\nundergraduate students (no project can be correctly completed by any studied\nLLMs, and at most 41.17% Pass@5 in a more relaxed evaluation). Second, using\nmethod signature as prompt context may strike an ideal balance for\nproject-level code generation. JavaBench is publicly available at\nhttps://github.com/java-bench/JavaBench."
                },
                "authors": [
                    {
                        "name": "Jialun Cao"
                    },
                    {
                        "name": "Zhiyong Chen"
                    },
                    {
                        "name": "Jiarong Wu"
                    },
                    {
                        "name": "Shing-chi Cheung"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "arxiv_comment": "Accepted by ASE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12902v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12902v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.09048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09048v1",
                "updated": "2024-10-11T17:59:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    59,
                    58,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:59:58Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    59,
                    58,
                    4,
                    285,
                    0
                ],
                "title": "Towards Trustworthy LLMs for Code: A Data-Centric Synergistic Auditing\n  Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Trustworthy LLMs for Code: A Data-Centric Synergistic Auditing\n  Framework"
                },
                "summary": "LLM-powered coding and development assistants have become prevalent to\nprogrammers' workflows. However, concerns about the trustworthiness of LLMs for\ncode persist despite their widespread use. Much of the existing research\nfocused on either training or evaluation, raising questions about whether\nstakeholders in training and evaluation align in their understanding of model\ntrustworthiness and whether they can move toward a unified direction. In this\npaper, we propose a vision for a unified trustworthiness auditing framework,\nDataTrust, which adopts a data-centric approach that synergistically emphasizes\nboth training and evaluation data and their correlations. DataTrust aims to\nconnect model trustworthiness indicators in evaluation with data quality\nindicators in training. It autonomously inspects training data and evaluates\nmodel trustworthiness using synthesized data, attributing potential causes from\nspecific evaluation data to corresponding training data and refining indicator\nconnections. Additionally, a trustworthiness arena powered by DataTrust will\nengage crowdsourced input and deliver quantitative outcomes. We outline the\nbenefits that various stakeholders can gain from DataTrust and discuss the\nchallenges and opportunities it presents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-powered coding and development assistants have become prevalent to\nprogrammers' workflows. However, concerns about the trustworthiness of LLMs for\ncode persist despite their widespread use. Much of the existing research\nfocused on either training or evaluation, raising questions about whether\nstakeholders in training and evaluation align in their understanding of model\ntrustworthiness and whether they can move toward a unified direction. In this\npaper, we propose a vision for a unified trustworthiness auditing framework,\nDataTrust, which adopts a data-centric approach that synergistically emphasizes\nboth training and evaluation data and their correlations. DataTrust aims to\nconnect model trustworthiness indicators in evaluation with data quality\nindicators in training. It autonomously inspects training data and evaluates\nmodel trustworthiness using synthesized data, attributing potential causes from\nspecific evaluation data to corresponding training data and refining indicator\nconnections. Additionally, a trustworthiness arena powered by DataTrust will\nengage crowdsourced input and deliver quantitative outcomes. We outline the\nbenefits that various stakeholders can gain from DataTrust and discuss the\nchallenges and opportunities it presents."
                },
                "authors": [
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Zhenpeng Chen"
                    },
                    {
                        "name": "Tianlin Li"
                    },
                    {
                        "name": "Yilun Zhao"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "Short Vision Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15240v2",
                "updated": "2024-10-11T17:59:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    59,
                    32,
                    4,
                    285,
                    0
                ],
                "published": "2024-08-27T17:57:45Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    57,
                    45,
                    1,
                    240,
                    0
                ],
                "title": "Generative Verifiers: Reward Modeling as Next-Token Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Verifiers: Reward Modeling as Next-Token Prediction"
                },
                "summary": "Verifiers or reward models are often used to enhance the reasoning\nperformance of large language models (LLMs). A common approach is the Best-of-N\nmethod, where N candidate solutions generated by the LLM are ranked by a\nverifier, and the best one is selected. While LLM-based verifiers are typically\ntrained as discriminative classifiers to score solutions, they do not utilize\nthe text generation capabilities of pretrained LLMs. To overcome this\nlimitation, we instead propose training verifiers using the ubiquitous\nnext-token prediction objective, jointly on verification and solution\ngeneration. Compared to standard verifiers, such generative verifiers (GenRM)\ncan benefit from several advantages of LLMs: they integrate seamlessly with\ninstruction tuning, enable chain-of-thought reasoning, and can utilize\nadditional test-time compute via majority voting for better verification. We\ndemonstrate that GenRM outperforms discriminative, DPO verifiers, and\nLLM-as-a-Judge, resulting in a 16-40% improvement in the number of problems\nsolved with Best-of-N on algorithmic and math reasoning tasks. Furthermore, we\nfind that training GenRM with synthetic verification rationales is sufficient\nto pick out subtle errors on math problems. Finally, we demonstrate that\ngenerative verifiers scale favorably with model size and inference-time\ncompute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verifiers or reward models are often used to enhance the reasoning\nperformance of large language models (LLMs). A common approach is the Best-of-N\nmethod, where N candidate solutions generated by the LLM are ranked by a\nverifier, and the best one is selected. While LLM-based verifiers are typically\ntrained as discriminative classifiers to score solutions, they do not utilize\nthe text generation capabilities of pretrained LLMs. To overcome this\nlimitation, we instead propose training verifiers using the ubiquitous\nnext-token prediction objective, jointly on verification and solution\ngeneration. Compared to standard verifiers, such generative verifiers (GenRM)\ncan benefit from several advantages of LLMs: they integrate seamlessly with\ninstruction tuning, enable chain-of-thought reasoning, and can utilize\nadditional test-time compute via majority voting for better verification. We\ndemonstrate that GenRM outperforms discriminative, DPO verifiers, and\nLLM-as-a-Judge, resulting in a 16-40% improvement in the number of problems\nsolved with Best-of-N on algorithmic and math reasoning tasks. Furthermore, we\nfind that training GenRM with synthetic verification rationales is sufficient\nto pick out subtle errors on math problems. Finally, we demonstrate that\ngenerative verifiers scale favorably with model size and inference-time\ncompute."
                },
                "authors": [
                    {
                        "name": "Lunjun Zhang"
                    },
                    {
                        "name": "Arian Hosseini"
                    },
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "Mehran Kazemi"
                    },
                    {
                        "name": "Aviral Kumar"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    }
                ],
                "author_detail": {
                    "name": "Rishabh Agarwal"
                },
                "author": "Rishabh Agarwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09047v1",
                "updated": "2024-10-11T17:59:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    59,
                    31,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:59:31Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    59,
                    31,
                    4,
                    285,
                    0
                ],
                "title": "Unraveling and Mitigating Safety Alignment Degradation of\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unraveling and Mitigating Safety Alignment Degradation of\n  Vision-Language Models"
                },
                "summary": "The safety alignment ability of Vision-Language Models (VLMs) is prone to be\ndegraded by the integration of the vision module compared to its LLM backbone.\nWe investigate this phenomenon, dubbed as ''safety alignment degradation'' in\nthis paper, and show that the challenge arises from the representation gap that\nemerges when introducing vision modality to VLMs. In particular, we show that\nthe representations of multi-modal inputs shift away from that of text-only\ninputs which represent the distribution that the LLM backbone is optimized for.\nAt the same time, the safety alignment capabilities, initially developed within\nthe textual embedding space, do not successfully transfer to this new\nmulti-modal representation space. To reduce safety alignment degradation, we\nintroduce Cross-Modality Representation Manipulation (CMRM), an inference time\nrepresentation intervention method for recovering the safety alignment ability\nthat is inherent in the LLM backbone of VLMs, while simultaneously preserving\nthe functional capabilities of VLMs. The empirical results show that our\nframework significantly recovers the alignment ability that is inherited from\nthe LLM backbone with minimal impact on the fluency and linguistic capabilities\nof pre-trained VLMs even without additional training. Specifically, the unsafe\nrate of LLaVA-7B on multi-modal input can be reduced from 61.53% to as low as\n3.15% with only inference-time intervention.\n  WARNING: This paper contains examples of toxic or harmful language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The safety alignment ability of Vision-Language Models (VLMs) is prone to be\ndegraded by the integration of the vision module compared to its LLM backbone.\nWe investigate this phenomenon, dubbed as ''safety alignment degradation'' in\nthis paper, and show that the challenge arises from the representation gap that\nemerges when introducing vision modality to VLMs. In particular, we show that\nthe representations of multi-modal inputs shift away from that of text-only\ninputs which represent the distribution that the LLM backbone is optimized for.\nAt the same time, the safety alignment capabilities, initially developed within\nthe textual embedding space, do not successfully transfer to this new\nmulti-modal representation space. To reduce safety alignment degradation, we\nintroduce Cross-Modality Representation Manipulation (CMRM), an inference time\nrepresentation intervention method for recovering the safety alignment ability\nthat is inherent in the LLM backbone of VLMs, while simultaneously preserving\nthe functional capabilities of VLMs. The empirical results show that our\nframework significantly recovers the alignment ability that is inherited from\nthe LLM backbone with minimal impact on the fluency and linguistic capabilities\nof pre-trained VLMs even without additional training. Specifically, the unsafe\nrate of LLaVA-7B on multi-modal input can be reduced from 61.53% to as low as\n3.15% with only inference-time intervention.\n  WARNING: This paper contains examples of toxic or harmful language."
                },
                "authors": [
                    {
                        "name": "Qin Liu"
                    },
                    {
                        "name": "Chao Shang"
                    },
                    {
                        "name": "Ling Liu"
                    },
                    {
                        "name": "Nikolaos Pappas"
                    },
                    {
                        "name": "Jie Ma"
                    },
                    {
                        "name": "Neha Anna John"
                    },
                    {
                        "name": "Srikanth Doss"
                    },
                    {
                        "name": "Lluis Marquez"
                    },
                    {
                        "name": "Miguel Ballesteros"
                    },
                    {
                        "name": "Yassine Benajiba"
                    }
                ],
                "author_detail": {
                    "name": "Yassine Benajiba"
                },
                "author": "Yassine Benajiba",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09045v1",
                "updated": "2024-10-11T17:58:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    58,
                    2,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:58:02Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    58,
                    2,
                    4,
                    285,
                    0
                ],
                "title": "MiRAGeNews: Multimodal Realistic AI-Generated News Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiRAGeNews: Multimodal Realistic AI-Generated News Detection"
                },
                "summary": "The proliferation of inflammatory or misleading \"fake\" news content has\nbecome increasingly common in recent years. Simultaneously, it has become\neasier than ever to use AI tools to generate photorealistic images depicting\nany scene imaginable. Combining these two -- AI-generated fake news content --\nis particularly potent and dangerous. To combat the spread of AI-generated fake\nnews, we propose the MiRAGeNews Dataset, a dataset of 12,500 high-quality real\nand AI-generated image-caption pairs from state-of-the-art generators. We find\nthat our dataset poses a significant challenge to humans (60% F-1) and\nstate-of-the-art multi-modal LLMs (< 24% F-1). Using our dataset we train a\nmulti-modal detector (MiRAGe) that improves by +5.1% F-1 over state-of-the-art\nbaselines on image-caption pairs from out-of-domain image generators and news\npublishers. We release our code and data to aid future work on detecting\nAI-generated content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of inflammatory or misleading \"fake\" news content has\nbecome increasingly common in recent years. Simultaneously, it has become\neasier than ever to use AI tools to generate photorealistic images depicting\nany scene imaginable. Combining these two -- AI-generated fake news content --\nis particularly potent and dangerous. To combat the spread of AI-generated fake\nnews, we propose the MiRAGeNews Dataset, a dataset of 12,500 high-quality real\nand AI-generated image-caption pairs from state-of-the-art generators. We find\nthat our dataset poses a significant challenge to humans (60% F-1) and\nstate-of-the-art multi-modal LLMs (< 24% F-1). Using our dataset we train a\nmulti-modal detector (MiRAGe) that improves by +5.1% F-1 over state-of-the-art\nbaselines on image-caption pairs from out-of-domain image generators and news\npublishers. We release our code and data to aid future work on detecting\nAI-generated content."
                },
                "authors": [
                    {
                        "name": "Runsheng Huang"
                    },
                    {
                        "name": "Liam Dugan"
                    },
                    {
                        "name": "Yue Yang"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    }
                ],
                "author_detail": {
                    "name": "Chris Callison-Burch"
                },
                "author": "Chris Callison-Burch",
                "arxiv_comment": "EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09043v1",
                "updated": "2024-10-11T17:57:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    57,
                    16,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:57:16Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    57,
                    16,
                    4,
                    285,
                    0
                ],
                "title": "Transforming In-Vehicle Network Intrusion Detection: VAE-based Knowledge\n  Distillation Meets Explainable AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transforming In-Vehicle Network Intrusion Detection: VAE-based Knowledge\n  Distillation Meets Explainable AI"
                },
                "summary": "In the evolving landscape of autonomous vehicles, ensuring robust in-vehicle\nnetwork (IVN) security is paramount. This paper introduces an advanced\nintrusion detection system (IDS) called KD-XVAE that uses a Variational\nAutoencoder (VAE)-based knowledge distillation approach to enhance both\nperformance and efficiency. Our model significantly reduces complexity,\noperating with just 1669 parameters and achieving an inference time of 0.3 ms\nper batch, making it highly suitable for resource-constrained automotive\nenvironments. Evaluations in the HCRL Car-Hacking dataset demonstrate\nexceptional capabilities, attaining perfect scores (Recall, Precision, F1 Score\nof 100%, and FNR of 0%) under multiple attack types, including DoS, Fuzzing,\nGear Spoofing, and RPM Spoofing. Comparative analysis on the CICIoV2024 dataset\nfurther underscores its superiority over traditional machine learning models,\nachieving perfect detection metrics. We furthermore integrate Explainable AI\n(XAI) techniques to ensure transparency in the model's decisions. The VAE\ncompresses the original feature space into a latent space, on which the\ndistilled model is trained. SHAP(SHapley Additive exPlanations) values provide\ninsights into the importance of each latent dimension, mapped back to original\nfeatures for intuitive understanding. Our paper advances the field by\nintegrating state-of-the-art techniques, addressing critical challenges in the\ndeployment of efficient, trustworthy, and reliable IDSes for autonomous\nvehicles, ensuring enhanced protection against emerging cyber threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the evolving landscape of autonomous vehicles, ensuring robust in-vehicle\nnetwork (IVN) security is paramount. This paper introduces an advanced\nintrusion detection system (IDS) called KD-XVAE that uses a Variational\nAutoencoder (VAE)-based knowledge distillation approach to enhance both\nperformance and efficiency. Our model significantly reduces complexity,\noperating with just 1669 parameters and achieving an inference time of 0.3 ms\nper batch, making it highly suitable for resource-constrained automotive\nenvironments. Evaluations in the HCRL Car-Hacking dataset demonstrate\nexceptional capabilities, attaining perfect scores (Recall, Precision, F1 Score\nof 100%, and FNR of 0%) under multiple attack types, including DoS, Fuzzing,\nGear Spoofing, and RPM Spoofing. Comparative analysis on the CICIoV2024 dataset\nfurther underscores its superiority over traditional machine learning models,\nachieving perfect detection metrics. We furthermore integrate Explainable AI\n(XAI) techniques to ensure transparency in the model's decisions. The VAE\ncompresses the original feature space into a latent space, on which the\ndistilled model is trained. SHAP(SHapley Additive exPlanations) values provide\ninsights into the importance of each latent dimension, mapped back to original\nfeatures for intuitive understanding. Our paper advances the field by\nintegrating state-of-the-art techniques, addressing critical challenges in the\ndeployment of efficient, trustworthy, and reliable IDSes for autonomous\nvehicles, ensuring enhanced protection against emerging cyber threats."
                },
                "authors": [
                    {
                        "name": "Muhammet Anil Yagiz"
                    },
                    {
                        "name": "Pedram MohajerAnsari"
                    },
                    {
                        "name": "Mert D. Pese"
                    },
                    {
                        "name": "Polat Goktas"
                    }
                ],
                "author_detail": {
                    "name": "Polat Goktas"
                },
                "author": "Polat Goktas",
                "arxiv_comment": "In Proceedings of the Sixth Workshop on CPSIoT Security and Privacy\n  (CPSIoTSec 24), October 14-18, 2024, Salt Lake City, UT, USA. ACM, New York,\n  NY, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09040v1",
                "updated": "2024-10-11T17:55:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    55,
                    9,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:55:09Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    55,
                    9,
                    4,
                    285,
                    0
                ],
                "title": "AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention\n  Manipulation"
                },
                "summary": "This paper studies the vulnerabilities of transformer-based Large Language\nModels (LLMs) to jailbreaking attacks, focusing specifically on the\noptimization-based Greedy Coordinate Gradient (GCG) strategy. We first observe\na positive correlation between the effectiveness of attacks and the internal\nbehaviors of the models. For instance, attacks tend to be less effective when\nmodels pay more attention to system prompts designed to ensure LLM safety\nalignment. Building on this discovery, we introduce an enhanced method that\nmanipulates models' attention scores to facilitate LLM jailbreaking, which we\nterm AttnGCG. Empirically, AttnGCG shows consistent improvements in attack\nefficacy across diverse LLMs, achieving an average increase of ~7% in the\nLlama-2 series and ~10% in the Gemma series. Our strategy also demonstrates\nrobust attack transferability against both unseen harmful goals and black-box\nLLMs like GPT-3.5 and GPT-4. Moreover, we note our attention-score\nvisualization is more interpretable, allowing us to gain better insights into\nhow our targeted attention manipulation facilitates more effective\njailbreaking. We release the code at\nhttps://github.com/UCSC-VLAA/AttnGCG-attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies the vulnerabilities of transformer-based Large Language\nModels (LLMs) to jailbreaking attacks, focusing specifically on the\noptimization-based Greedy Coordinate Gradient (GCG) strategy. We first observe\na positive correlation between the effectiveness of attacks and the internal\nbehaviors of the models. For instance, attacks tend to be less effective when\nmodels pay more attention to system prompts designed to ensure LLM safety\nalignment. Building on this discovery, we introduce an enhanced method that\nmanipulates models' attention scores to facilitate LLM jailbreaking, which we\nterm AttnGCG. Empirically, AttnGCG shows consistent improvements in attack\nefficacy across diverse LLMs, achieving an average increase of ~7% in the\nLlama-2 series and ~10% in the Gemma series. Our strategy also demonstrates\nrobust attack transferability against both unseen harmful goals and black-box\nLLMs like GPT-3.5 and GPT-4. Moreover, we note our attention-score\nvisualization is more interpretable, allowing us to gain better insights into\nhow our targeted attention manipulation facilitates more effective\njailbreaking. We release the code at\nhttps://github.com/UCSC-VLAA/AttnGCG-attack."
                },
                "authors": [
                    {
                        "name": "Zijun Wang"
                    },
                    {
                        "name": "Haoqin Tu"
                    },
                    {
                        "name": "Jieru Mei"
                    },
                    {
                        "name": "Bingchen Zhao"
                    },
                    {
                        "name": "Yisen Wang"
                    },
                    {
                        "name": "Cihang Xie"
                    }
                ],
                "author_detail": {
                    "name": "Cihang Xie"
                },
                "author": "Cihang Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09038v1",
                "updated": "2024-10-11T17:54:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    54,
                    14,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:54:14Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    54,
                    14,
                    4,
                    285,
                    0
                ],
                "title": "SimpleStrat: Diversifying Language Model Generation with Stratification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimpleStrat: Diversifying Language Model Generation with Stratification"
                },
                "summary": "Generating diverse responses from large language models (LLMs) is crucial for\napplications such as planning/search and synthetic data generation, where\ndiversity provides distinct answers across generations. Prior approaches rely\non increasing temperature to increase diversity. However, contrary to popular\nbelief, we show not only does this approach produce lower quality individual\ngenerations as temperature increases, but it depends on model's next-token\nprobabilities being similar to the true distribution of answers. We propose\n\\method{}, an alternative approach that uses the language model itself to\npartition the space into strata. At inference, a random stratum is selected and\na sample drawn from within the strata. To measure diversity, we introduce\nCoverageQA, a dataset of underspecified questions with multiple equally\nplausible answers, and assess diversity by measuring KL Divergence between the\noutput distribution and uniform distribution over valid ground truth answers.\nAs computing probability per response/solution for proprietary models is\ninfeasible, we measure recall on ground truth solutions. Our evaluation show\nusing SimpleStrat achieves higher recall by 0.05 compared to GPT-4o and 0.36\naverage reduction in KL Divergence compared to Llama 3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating diverse responses from large language models (LLMs) is crucial for\napplications such as planning/search and synthetic data generation, where\ndiversity provides distinct answers across generations. Prior approaches rely\non increasing temperature to increase diversity. However, contrary to popular\nbelief, we show not only does this approach produce lower quality individual\ngenerations as temperature increases, but it depends on model's next-token\nprobabilities being similar to the true distribution of answers. We propose\n\\method{}, an alternative approach that uses the language model itself to\npartition the space into strata. At inference, a random stratum is selected and\na sample drawn from within the strata. To measure diversity, we introduce\nCoverageQA, a dataset of underspecified questions with multiple equally\nplausible answers, and assess diversity by measuring KL Divergence between the\noutput distribution and uniform distribution over valid ground truth answers.\nAs computing probability per response/solution for proprietary models is\ninfeasible, we measure recall on ground truth solutions. Our evaluation show\nusing SimpleStrat achieves higher recall by 0.05 compared to GPT-4o and 0.36\naverage reduction in KL Divergence compared to Llama 3."
                },
                "authors": [
                    {
                        "name": "Justin Wong"
                    },
                    {
                        "name": "Yury Orlovskiy"
                    },
                    {
                        "name": "Michael Luo"
                    },
                    {
                        "name": "Sanjit A. Seshia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09037v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09037v1",
                "updated": "2024-10-11T17:53:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    53,
                    27,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:53:27Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    53,
                    27,
                    4,
                    285,
                    0
                ],
                "title": "Mentor-KD: Making Small Language Models Better Multi-step Reasoners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mentor-KD: Making Small Language Models Better Multi-step Reasoners"
                },
                "summary": "Large Language Models (LLMs) have displayed remarkable performances across\nvarious complex tasks by leveraging Chain-of-Thought (CoT) prompting. Recently,\nstudies have proposed a Knowledge Distillation (KD) approach, reasoning\ndistillation, which transfers such reasoning ability of LLMs through\nfine-tuning language models of multi-step rationales generated by LLM teachers.\nHowever, they have inadequately considered two challenges regarding\ninsufficient distillation sets from the LLM teacher model, in terms of 1) data\nquality and 2) soft label provision. In this paper, we propose Mentor-KD, which\neffectively distills the multi-step reasoning capability of LLMs to smaller LMs\nwhile addressing the aforementioned challenges. Specifically, we exploit a\nmentor, intermediate-sized task-specific fine-tuned model, to augment\nadditional CoT annotations and provide soft labels for the student model during\nreasoning distillation. We conduct extensive experiments and confirm\nMentor-KD's effectiveness across various models and complex reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have displayed remarkable performances across\nvarious complex tasks by leveraging Chain-of-Thought (CoT) prompting. Recently,\nstudies have proposed a Knowledge Distillation (KD) approach, reasoning\ndistillation, which transfers such reasoning ability of LLMs through\nfine-tuning language models of multi-step rationales generated by LLM teachers.\nHowever, they have inadequately considered two challenges regarding\ninsufficient distillation sets from the LLM teacher model, in terms of 1) data\nquality and 2) soft label provision. In this paper, we propose Mentor-KD, which\neffectively distills the multi-step reasoning capability of LLMs to smaller LMs\nwhile addressing the aforementioned challenges. Specifically, we exploit a\nmentor, intermediate-sized task-specific fine-tuned model, to augment\nadditional CoT annotations and provide soft labels for the student model during\nreasoning distillation. We conduct extensive experiments and confirm\nMentor-KD's effectiveness across various models and complex reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Hojae Lee"
                    },
                    {
                        "name": "Junho Kim"
                    },
                    {
                        "name": "SangKeun Lee"
                    }
                ],
                "author_detail": {
                    "name": "SangKeun Lee"
                },
                "author": "SangKeun Lee",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09037v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09037v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09034v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09034v1",
                "updated": "2024-10-11T17:50:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    50,
                    59,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:50:59Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    50,
                    59,
                    4,
                    285,
                    0
                ],
                "title": "PEAR: A Robust and Flexible Automation Framework for Ptychography\n  Enabled by Multiple Large Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PEAR: A Robust and Flexible Automation Framework for Ptychography\n  Enabled by Multiple Large Language Model Agents"
                },
                "summary": "Ptychography is an advanced computational imaging technique in X-ray and\nelectron microscopy. It has been widely adopted across scientific research\nfields, including physics, chemistry, biology, and materials science, as well\nas in industrial applications such as semiconductor characterization. In\npractice, obtaining high-quality ptychographic images requires simultaneous\noptimization of numerous experimental and algorithmic parameters.\nTraditionally, parameter selection often relies on trial and error, leading to\nlow-throughput workflows and potential human bias. In this work, we develop the\n\"Ptychographic Experiment and Analysis Robot\" (PEAR), a framework that\nleverages large language models (LLMs) to automate data analysis in\nptychography. To ensure high robustness and accuracy, PEAR employs multiple LLM\nagents for tasks including knowledge retrieval, code generation, parameter\nrecommendation, and image reasoning. Our study demonstrates that PEAR's\nmulti-agent design significantly improves the workflow success rate, even with\nsmaller open-weight models such as LLaMA 3.1 8B. PEAR also supports various\nautomation levels and is designed to work with customized local knowledge\nbases, ensuring flexibility and adaptability across different research\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ptychography is an advanced computational imaging technique in X-ray and\nelectron microscopy. It has been widely adopted across scientific research\nfields, including physics, chemistry, biology, and materials science, as well\nas in industrial applications such as semiconductor characterization. In\npractice, obtaining high-quality ptychographic images requires simultaneous\noptimization of numerous experimental and algorithmic parameters.\nTraditionally, parameter selection often relies on trial and error, leading to\nlow-throughput workflows and potential human bias. In this work, we develop the\n\"Ptychographic Experiment and Analysis Robot\" (PEAR), a framework that\nleverages large language models (LLMs) to automate data analysis in\nptychography. To ensure high robustness and accuracy, PEAR employs multiple LLM\nagents for tasks including knowledge retrieval, code generation, parameter\nrecommendation, and image reasoning. Our study demonstrates that PEAR's\nmulti-agent design significantly improves the workflow success rate, even with\nsmaller open-weight models such as LLaMA 3.1 8B. PEAR also supports various\nautomation levels and is designed to work with customized local knowledge\nbases, ensuring flexibility and adaptability across different research\nenvironments."
                },
                "authors": [
                    {
                        "name": "Xiangyu Yin"
                    },
                    {
                        "name": "Chuqiao Shi"
                    },
                    {
                        "name": "Yimo Han"
                    },
                    {
                        "name": "Yi Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Jiang"
                },
                "author": "Yi Jiang",
                "arxiv_comment": "18 pages, 5 figures, technical preview report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09034v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09034v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02392v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02392v3",
                "updated": "2024-10-11T17:43:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    43,
                    48,
                    4,
                    285,
                    0
                ],
                "published": "2024-02-04T08:11:45Z",
                "published_parsed": [
                    2024,
                    2,
                    4,
                    8,
                    11,
                    45,
                    6,
                    35,
                    0
                ],
                "title": "DeLLMa: Decision Making Under Uncertainty with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeLLMa: Decision Making Under Uncertainty with Large Language Models"
                },
                "summary": "The potential of large language models (LLMs) as decision support tools is\nincreasingly being explored in fields such as business, engineering, and\nmedicine, which often face challenging tasks of decision-making under\nuncertainty. In this paper, we show that directly prompting LLMs on these types\nof decision-making problems can yield poor results, especially as the problem\ncomplexity increases. To aid in these tasks, we propose DeLLMa (Decision-making\nLarge Language Model assistant), a framework designed to enhance\ndecision-making accuracy in uncertain environments. DeLLMa involves a\nmulti-step reasoning procedure that integrates recent best practices in scaling\ninference-time reasoning, drawing upon principles from decision theory and\nutility theory, to provide an accurate and human-auditable decision-making\nprocess. We validate our procedure on multiple realistic decision-making\nenvironments, demonstrating that DeLLMa can consistently enhance the\ndecision-making performance of leading language models, and achieve up to a 40%\nincrease in accuracy over competing methods. Additionally, we show how\nperformance improves when scaling compute at test time, and carry out human\nevaluations to benchmark components of DeLLMa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The potential of large language models (LLMs) as decision support tools is\nincreasingly being explored in fields such as business, engineering, and\nmedicine, which often face challenging tasks of decision-making under\nuncertainty. In this paper, we show that directly prompting LLMs on these types\nof decision-making problems can yield poor results, especially as the problem\ncomplexity increases. To aid in these tasks, we propose DeLLMa (Decision-making\nLarge Language Model assistant), a framework designed to enhance\ndecision-making accuracy in uncertain environments. DeLLMa involves a\nmulti-step reasoning procedure that integrates recent best practices in scaling\ninference-time reasoning, drawing upon principles from decision theory and\nutility theory, to provide an accurate and human-auditable decision-making\nprocess. We validate our procedure on multiple realistic decision-making\nenvironments, demonstrating that DeLLMa can consistently enhance the\ndecision-making performance of leading language models, and achieve up to a 40%\nincrease in accuracy over competing methods. Additionally, we show how\nperformance improves when scaling compute at test time, and carry out human\nevaluations to benchmark components of DeLLMa."
                },
                "authors": [
                    {
                        "name": "Ollie Liu"
                    },
                    {
                        "name": "Deqing Fu"
                    },
                    {
                        "name": "Dani Yogatama"
                    },
                    {
                        "name": "Willie Neiswanger"
                    }
                ],
                "author_detail": {
                    "name": "Willie Neiswanger"
                },
                "author": "Willie Neiswanger",
                "arxiv_comment": "37 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.02392v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02392v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09024v1",
                "updated": "2024-10-11T17:39:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    39,
                    22,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:39:22Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    39,
                    22,
                    4,
                    285,
                    0
                ],
                "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents"
                },
                "summary": "The robustness of LLMs to jailbreak attacks, where users design prompts to\ncircumvent safety measures and misuse model capabilities, has been studied\nprimarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- which\nuse external tools and can execute multi-stage tasks -- may pose a greater risk\nif misused, but their robustness remains underexplored. To facilitate research\non LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark\nincludes a diverse set of 110 explicitly malicious agent tasks (440 with\naugmentations), covering 11 harm categories including fraud, cybercrime, and\nharassment. In addition to measuring whether models refuse harmful agentic\nrequests, scoring well on AgentHarm requires jailbroken agents to maintain\ntheir capabilities following an attack to complete a multi-step task. We\nevaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly\ncompliant with malicious agent requests without jailbreaking, (2) simple\nuniversal jailbreak templates can be adapted to effectively jailbreak agents,\nand (3) these jailbreaks enable coherent and malicious multi-step agent\nbehavior and retain model capabilities. We publicly release AgentHarm to enable\nsimple and reliable evaluation of attacks and defenses for LLM-based agents. We\npublicly release the benchmark at\nhttps://huggingface.co/ai-safety-institute/AgentHarm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The robustness of LLMs to jailbreak attacks, where users design prompts to\ncircumvent safety measures and misuse model capabilities, has been studied\nprimarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- which\nuse external tools and can execute multi-stage tasks -- may pose a greater risk\nif misused, but their robustness remains underexplored. To facilitate research\non LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark\nincludes a diverse set of 110 explicitly malicious agent tasks (440 with\naugmentations), covering 11 harm categories including fraud, cybercrime, and\nharassment. In addition to measuring whether models refuse harmful agentic\nrequests, scoring well on AgentHarm requires jailbroken agents to maintain\ntheir capabilities following an attack to complete a multi-step task. We\nevaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly\ncompliant with malicious agent requests without jailbreaking, (2) simple\nuniversal jailbreak templates can be adapted to effectively jailbreak agents,\nand (3) these jailbreaks enable coherent and malicious multi-step agent\nbehavior and retain model capabilities. We publicly release AgentHarm to enable\nsimple and reliable evaluation of attacks and defenses for LLM-based agents. We\npublicly release the benchmark at\nhttps://huggingface.co/ai-safety-institute/AgentHarm."
                },
                "authors": [
                    {
                        "name": "Maksym Andriushchenko"
                    },
                    {
                        "name": "Alexandra Souly"
                    },
                    {
                        "name": "Mateusz Dziemian"
                    },
                    {
                        "name": "Derek Duenas"
                    },
                    {
                        "name": "Maxwell Lin"
                    },
                    {
                        "name": "Justin Wang"
                    },
                    {
                        "name": "Dan Hendrycks"
                    },
                    {
                        "name": "Andy Zou"
                    },
                    {
                        "name": "Zico Kolter"
                    },
                    {
                        "name": "Matt Fredrikson"
                    },
                    {
                        "name": "Eric Winsor"
                    },
                    {
                        "name": "Jerome Wynne"
                    },
                    {
                        "name": "Yarin Gal"
                    },
                    {
                        "name": "Xander Davies"
                    }
                ],
                "author_detail": {
                    "name": "Xander Davies"
                },
                "author": "Xander Davies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09013v1",
                "updated": "2024-10-11T17:30:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    30,
                    2,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:30:02Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    30,
                    2,
                    4,
                    285,
                    0
                ],
                "title": "The Impact of Visual Information in Chinese Characters: Evaluating Large\n  Models' Ability to Recognize and Utilize Radicals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Visual Information in Chinese Characters: Evaluating Large\n  Models' Ability to Recognize and Utilize Radicals"
                },
                "summary": "The glyphic writing system of Chinese incorporates information-rich visual\nfeatures in each character, such as radicals that provide hints about meaning\nor pronunciation. However, there has been no investigation into whether\ncontemporary Large Language Models (LLMs) and Vision-Language Models (VLMs) can\nharness these sub-character features in Chinese through prompting. In this\nstudy, we establish a benchmark to evaluate LLMs' and VLMs' understanding of\nvisual elements in Chinese characters, including radicals, composition\nstructures, strokes, and stroke counts. Our results reveal that models\nsurprisingly exhibit some, but still limited, knowledge of the visual\ninformation, regardless of whether images of characters are provided. To incite\nmodels' ability to use radicals, we further experiment with incorporating\nradicals into the prompts for Chinese language understanding tasks. We observe\nconsistent improvement in Part-Of-Speech tagging when providing additional\ninformation about radicals, suggesting the potential to enhance CLP by\nintegrating sub-character information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The glyphic writing system of Chinese incorporates information-rich visual\nfeatures in each character, such as radicals that provide hints about meaning\nor pronunciation. However, there has been no investigation into whether\ncontemporary Large Language Models (LLMs) and Vision-Language Models (VLMs) can\nharness these sub-character features in Chinese through prompting. In this\nstudy, we establish a benchmark to evaluate LLMs' and VLMs' understanding of\nvisual elements in Chinese characters, including radicals, composition\nstructures, strokes, and stroke counts. Our results reveal that models\nsurprisingly exhibit some, but still limited, knowledge of the visual\ninformation, regardless of whether images of characters are provided. To incite\nmodels' ability to use radicals, we further experiment with incorporating\nradicals into the prompts for Chinese language understanding tasks. We observe\nconsistent improvement in Part-Of-Speech tagging when providing additional\ninformation about radicals, suggesting the potential to enhance CLP by\nintegrating sub-character information."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Wu"
                    },
                    {
                        "name": "Karl Stratos"
                    },
                    {
                        "name": "Wei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xu"
                },
                "author": "Wei Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09012v1",
                "updated": "2024-10-11T17:27:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    27,
                    4,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:27:04Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    27,
                    4,
                    4,
                    285,
                    0
                ],
                "title": "Software Engineering and Foundation Models: Insights from Industry Blogs\n  Using a Jury of Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software Engineering and Foundation Models: Insights from Industry Blogs\n  Using a Jury of Foundation Models"
                },
                "summary": "Foundation models (FMs) such as large language models (LLMs) have\nsignificantly impacted many fields, including software engineering (SE). The\ninteraction between SE and FMs has led to the integration of FMs into SE\npractices (FM4SE) and the application of SE methodologies to FMs (SE4FM). While\nseveral literature surveys exist on academic contributions to these trends, we\nare the first to provide a practitioner's view. We analyze 155 FM4SE and 997\nSE4FM blog posts from leading technology companies, leveraging an FM-powered\nsurveying approach to systematically label and summarize the discussed\nactivities and tasks. We observed that while code generation is the most\nprominent FM4SE task, FMs are leveraged for many other SE activities such as\ncode understanding, summarization, and API recommendation. The majority of blog\nposts on SE4FM are about model deployment & operation, and system architecture\n& orchestration. Although the emphasis is on cloud deployments, there is a\ngrowing interest in compressing FMs and deploying them on smaller devices such\nas edge or mobile devices. We outline eight future research directions inspired\nby our gained insights, aiming to bridge the gap between academic findings and\nreal-world applications. Our study not only enriches the body of knowledge on\npractical applications of FM4SE and SE4FM but also demonstrates the utility of\nFMs as a powerful and efficient approach in conducting literature surveys\nwithin technical and grey literature domains. Our dataset, results, code and\nused prompts can be found in our online replication package at\nhttps://github.com/SAILResearch/fmse-blogs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models (FMs) such as large language models (LLMs) have\nsignificantly impacted many fields, including software engineering (SE). The\ninteraction between SE and FMs has led to the integration of FMs into SE\npractices (FM4SE) and the application of SE methodologies to FMs (SE4FM). While\nseveral literature surveys exist on academic contributions to these trends, we\nare the first to provide a practitioner's view. We analyze 155 FM4SE and 997\nSE4FM blog posts from leading technology companies, leveraging an FM-powered\nsurveying approach to systematically label and summarize the discussed\nactivities and tasks. We observed that while code generation is the most\nprominent FM4SE task, FMs are leveraged for many other SE activities such as\ncode understanding, summarization, and API recommendation. The majority of blog\nposts on SE4FM are about model deployment & operation, and system architecture\n& orchestration. Although the emphasis is on cloud deployments, there is a\ngrowing interest in compressing FMs and deploying them on smaller devices such\nas edge or mobile devices. We outline eight future research directions inspired\nby our gained insights, aiming to bridge the gap between academic findings and\nreal-world applications. Our study not only enriches the body of knowledge on\npractical applications of FM4SE and SE4FM but also demonstrates the utility of\nFMs as a powerful and efficient approach in conducting literature surveys\nwithin technical and grey literature domains. Our dataset, results, code and\nused prompts can be found in our online replication package at\nhttps://github.com/SAILResearch/fmse-blogs."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Cor-Paul Bezemer"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09008v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09008v1",
                "updated": "2024-10-11T17:25:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    25,
                    52,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:25:52Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    25,
                    52,
                    4,
                    285,
                    0
                ],
                "title": "SuperCorrect: Supervising and Correcting Language Models with\n  Error-Driven Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperCorrect: Supervising and Correcting Language Models with\n  Error-Driven Insights"
                },
                "summary": "Large language models (LLMs) like GPT-4, PaLM, and LLaMA have shown\nsignificant improvements in various reasoning tasks. However, smaller models\nsuch as Llama-3-8B and DeepSeekMath-Base still struggle with complex\nmathematical reasoning because they fail to effectively identify and correct\nreasoning errors. Recent reflection-based methods aim to address these issues\nby enabling self-reflection and self-correction, but they still face challenges\nin independently detecting errors in their reasoning steps. To overcome these\nlimitations, we propose SuperCorrect, a novel two-stage framework that uses a\nlarge teacher model to supervise and correct both the reasoning and reflection\nprocesses of a smaller student model. In the first stage, we extract\nhierarchical high-level and detailed thought templates from the teacher model\nto guide the student model in eliciting more fine-grained reasoning thoughts.\nIn the second stage, we introduce cross-model collaborative direct preference\noptimization (DPO) to enhance the self-correction abilities of the student\nmodel by following the teacher's correction traces during training. This\ncross-model DPO approach teaches the student model to effectively locate and\nresolve erroneous thoughts with error-driven insights from the teacher model,\nbreaking the bottleneck of its thoughts and acquiring new skills and knowledge\nto tackle challenging problems. Extensive experiments consistently demonstrate\nour superiority over previous methods. Notably, our SuperCorrect-7B model\nsignificantly surpasses powerful DeepSeekMath-7B by 7.8%/5.3% and\nQwen2.5-Math-7B by 15.1%/6.3% on MATH/GSM8K benchmarks, achieving new SOTA\nperformance among all 7B models. Code:\nhttps://github.com/YangLing0818/SuperCorrect-llm",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) like GPT-4, PaLM, and LLaMA have shown\nsignificant improvements in various reasoning tasks. However, smaller models\nsuch as Llama-3-8B and DeepSeekMath-Base still struggle with complex\nmathematical reasoning because they fail to effectively identify and correct\nreasoning errors. Recent reflection-based methods aim to address these issues\nby enabling self-reflection and self-correction, but they still face challenges\nin independently detecting errors in their reasoning steps. To overcome these\nlimitations, we propose SuperCorrect, a novel two-stage framework that uses a\nlarge teacher model to supervise and correct both the reasoning and reflection\nprocesses of a smaller student model. In the first stage, we extract\nhierarchical high-level and detailed thought templates from the teacher model\nto guide the student model in eliciting more fine-grained reasoning thoughts.\nIn the second stage, we introduce cross-model collaborative direct preference\noptimization (DPO) to enhance the self-correction abilities of the student\nmodel by following the teacher's correction traces during training. This\ncross-model DPO approach teaches the student model to effectively locate and\nresolve erroneous thoughts with error-driven insights from the teacher model,\nbreaking the bottleneck of its thoughts and acquiring new skills and knowledge\nto tackle challenging problems. Extensive experiments consistently demonstrate\nour superiority over previous methods. Notably, our SuperCorrect-7B model\nsignificantly surpasses powerful DeepSeekMath-7B by 7.8%/5.3% and\nQwen2.5-Math-7B by 15.1%/6.3% on MATH/GSM8K benchmarks, achieving new SOTA\nperformance among all 7B models. Code:\nhttps://github.com/YangLing0818/SuperCorrect-llm"
                },
                "authors": [
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Zhaochen Yu"
                    },
                    {
                        "name": "Tianjun Zhang"
                    },
                    {
                        "name": "Minkai Xu"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Shuicheng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Shuicheng Yan"
                },
                "author": "Shuicheng Yan",
                "arxiv_comment": "Project: https://github.com/YangLing0818/SuperCorrect-llm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09008v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09006v1",
                "updated": "2024-10-11T17:24:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    24,
                    0,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:24:00Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    24,
                    0,
                    4,
                    285,
                    0
                ],
                "title": "From Interaction to Impact: Towards Safer AI Agents Through\n  Understanding and Evaluating UI Operation Impacts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Interaction to Impact: Towards Safer AI Agents Through\n  Understanding and Evaluating UI Operation Impacts"
                },
                "summary": "With advances in generative AI, there is increasing work towards creating\nautonomous agents that can manage daily tasks by operating user interfaces\n(UIs). While prior research has studied the mechanics of how AI agents might\nnavigate UIs and understand UI structure, the effects of agents and their\nautonomous actions-particularly those that may be risky or irreversible-remain\nunder-explored. In this work, we investigate the real-world impacts and\nconsequences of UI actions by AI agents. We began by developing a taxonomy of\nthe impacts of UI actions through a series of workshops with domain experts.\nFollowing this, we conducted a data synthesis study to gather realistic UI\nscreen traces and action data that users perceive as impactful. We then used\nour impact categories to annotate our collected data and data repurposed from\nexisting UI navigation datasets. Our quantitative evaluations of different\nlarge language models (LLMs) and variants demonstrate how well different LLMs\ncan understand the impacts of UI actions that might be taken by an agent. We\nshow that our taxonomy enhances the reasoning capabilities of these LLMs for\nunderstanding the impacts of UI actions, but our findings also reveal\nsignificant gaps in their ability to reliably classify more nuanced or complex\ncategories of impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With advances in generative AI, there is increasing work towards creating\nautonomous agents that can manage daily tasks by operating user interfaces\n(UIs). While prior research has studied the mechanics of how AI agents might\nnavigate UIs and understand UI structure, the effects of agents and their\nautonomous actions-particularly those that may be risky or irreversible-remain\nunder-explored. In this work, we investigate the real-world impacts and\nconsequences of UI actions by AI agents. We began by developing a taxonomy of\nthe impacts of UI actions through a series of workshops with domain experts.\nFollowing this, we conducted a data synthesis study to gather realistic UI\nscreen traces and action data that users perceive as impactful. We then used\nour impact categories to annotate our collected data and data repurposed from\nexisting UI navigation datasets. Our quantitative evaluations of different\nlarge language models (LLMs) and variants demonstrate how well different LLMs\ncan understand the impacts of UI actions that might be taken by an agent. We\nshow that our taxonomy enhances the reasoning capabilities of these LLMs for\nunderstanding the impacts of UI actions, but our findings also reveal\nsignificant gaps in their ability to reliably classify more nuanced or complex\ncategories of impact."
                },
                "authors": [
                    {
                        "name": "Zhuohao Jerry Zhang"
                    },
                    {
                        "name": "Eldon Schoop"
                    },
                    {
                        "name": "Jeffrey Nichols"
                    },
                    {
                        "name": "Anuj Mahajan"
                    },
                    {
                        "name": "Amanda Swearngin"
                    }
                ],
                "author_detail": {
                    "name": "Amanda Swearngin"
                },
                "author": "Amanda Swearngin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04482v2",
                "updated": "2024-10-11T17:21:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    21,
                    40,
                    4,
                    285,
                    0
                ],
                "published": "2024-07-05T13:04:31Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    13,
                    4,
                    31,
                    4,
                    187,
                    0
                ],
                "title": "Controlling Whisper: Universal Acoustic Adversarial Attacks to Control\n  Speech Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling Whisper: Universal Acoustic Adversarial Attacks to Control\n  Speech Foundation Models"
                },
                "summary": "Speech enabled foundation models, either in the form of flexible speech\nrecognition based systems or audio-prompted large language models (LLMs), are\nbecoming increasingly popular. One of the interesting aspects of these models\nis their ability to perform tasks other than automatic speech recognition (ASR)\nusing an appropriate prompt. For example, the OpenAI Whisper model can perform\nboth speech transcription and speech translation. With the development of\naudio-prompted LLMs there is the potential for even greater control options. In\nthis work we demonstrate that with this greater flexibility the systems can be\nsusceptible to model-control adversarial attacks. Without any access to the\nmodel prompt it is possible to modify the behaviour of the system by\nappropriately changing the audio input. To illustrate this risk, we demonstrate\nthat it is possible to prepend a short universal adversarial acoustic segment\nto any input speech signal to override the prompt setting of an ASR foundation\nmodel. Specifically, we successfully use a universal adversarial acoustic\nsegment to control Whisper to always perform speech translation, despite being\nset to perform speech transcription. Overall, this work demonstrates a new form\nof adversarial attack on multi-tasking speech enabled foundation models that\nneeds to be considered prior to the deployment of this form of model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech enabled foundation models, either in the form of flexible speech\nrecognition based systems or audio-prompted large language models (LLMs), are\nbecoming increasingly popular. One of the interesting aspects of these models\nis their ability to perform tasks other than automatic speech recognition (ASR)\nusing an appropriate prompt. For example, the OpenAI Whisper model can perform\nboth speech transcription and speech translation. With the development of\naudio-prompted LLMs there is the potential for even greater control options. In\nthis work we demonstrate that with this greater flexibility the systems can be\nsusceptible to model-control adversarial attacks. Without any access to the\nmodel prompt it is possible to modify the behaviour of the system by\nappropriately changing the audio input. To illustrate this risk, we demonstrate\nthat it is possible to prepend a short universal adversarial acoustic segment\nto any input speech signal to override the prompt setting of an ASR foundation\nmodel. Specifically, we successfully use a universal adversarial acoustic\nsegment to control Whisper to always perform speech translation, despite being\nset to perform speech transcription. Overall, this work demonstrates a new form\nof adversarial attack on multi-tasking speech enabled foundation models that\nneeds to be considered prior to the deployment of this form of model."
                },
                "authors": [
                    {
                        "name": "Vyas Raina"
                    },
                    {
                        "name": "Mark Gales"
                    }
                ],
                "author_detail": {
                    "name": "Mark Gales"
                },
                "author": "Mark Gales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06800v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06800v3",
                "updated": "2024-10-11T17:16:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    16,
                    30,
                    4,
                    285,
                    0
                ],
                "published": "2024-05-10T20:23:46Z",
                "published_parsed": [
                    2024,
                    5,
                    10,
                    20,
                    23,
                    46,
                    4,
                    131,
                    0
                ],
                "title": "LLM-Generated Black-box Explanations Can Be Adversarially Helpful",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Generated Black-box Explanations Can Be Adversarially Helpful"
                },
                "summary": "Large Language Models (LLMs) are becoming vital tools that help us solve and\nunderstand complex problems by acting as digital assistants. LLMs can generate\nconvincing explanations, even when only given the inputs and outputs of these\nproblems, i.e., in a ``black-box'' approach. However, our research uncovers a\nhidden risk tied to this approach, which we call *adversarial helpfulness*.\nThis happens when an LLM's explanations make a wrong answer look right,\npotentially leading people to trust incorrect solutions. In this paper, we show\nthat this issue affects not just humans, but also LLM evaluators. Digging\ndeeper, we identify and examine key persuasive strategies employed by LLMs. Our\nfindings reveal that these models employ strategies such as reframing the\nquestions, expressing an elevated level of confidence, and cherry-picking\nevidence to paint misleading answers in a credible light. To examine if LLMs\nare able to navigate complex-structured knowledge when generating adversarially\nhelpful explanations, we create a special task based on navigating through\ngraphs. Most LLMs are not able to find alternative paths along simple graphs,\nindicating that their misleading explanations aren't produced by only logical\ndeductions using complex knowledge. These findings shed light on the\nlimitations of the black-box explanation setting and allow us to provide advice\non the safe usage of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are becoming vital tools that help us solve and\nunderstand complex problems by acting as digital assistants. LLMs can generate\nconvincing explanations, even when only given the inputs and outputs of these\nproblems, i.e., in a ``black-box'' approach. However, our research uncovers a\nhidden risk tied to this approach, which we call *adversarial helpfulness*.\nThis happens when an LLM's explanations make a wrong answer look right,\npotentially leading people to trust incorrect solutions. In this paper, we show\nthat this issue affects not just humans, but also LLM evaluators. Digging\ndeeper, we identify and examine key persuasive strategies employed by LLMs. Our\nfindings reveal that these models employ strategies such as reframing the\nquestions, expressing an elevated level of confidence, and cherry-picking\nevidence to paint misleading answers in a credible light. To examine if LLMs\nare able to navigate complex-structured knowledge when generating adversarially\nhelpful explanations, we create a special task based on navigating through\ngraphs. Most LLMs are not able to find alternative paths along simple graphs,\nindicating that their misleading explanations aren't produced by only logical\ndeductions using complex knowledge. These findings shed light on the\nlimitations of the black-box explanation setting and allow us to provide advice\non the safe usage of LLMs."
                },
                "authors": [
                    {
                        "name": "Rohan Ajwani"
                    },
                    {
                        "name": "Shashidhar Reddy Javaji"
                    },
                    {
                        "name": "Frank Rudzicz"
                    },
                    {
                        "name": "Zining Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Zining Zhu"
                },
                "author": "Zining Zhu",
                "arxiv_comment": "NeurIPS Regulatable ML Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06800v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06800v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08996v1",
                "updated": "2024-10-11T17:09:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    9,
                    22,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:09:22Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    9,
                    22,
                    4,
                    285,
                    0
                ],
                "title": "Hypothesis-only Biases in Large Language Model-Elicited Natural Language\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hypothesis-only Biases in Large Language Model-Elicited Natural Language\n  Inference"
                },
                "summary": "We test whether replacing crowdsource workers with LLMs to write Natural\nLanguage Inference (NLI) hypotheses similarly results in annotation artifacts.\nWe recreate a portion of the Stanford NLI corpus using GPT-4, Llama-2 and\nMistral 7b, and train hypothesis-only classifiers to determine whether\nLLM-elicited hypotheses contain annotation artifacts. On our LLM-elicited NLI\ndatasets, BERT-based hypothesis-only classifiers achieve between 86-96%\naccuracy, indicating these datasets contain hypothesis-only artifacts. We also\nfind frequent \"give-aways\" in LLM-generated hypotheses, e.g. the phrase\n\"swimming in a pool\" appears in more than 10,000 contradictions generated by\nGPT-4. Our analysis provides empirical evidence that well-attested biases in\nNLI can persist in LLM-generated data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We test whether replacing crowdsource workers with LLMs to write Natural\nLanguage Inference (NLI) hypotheses similarly results in annotation artifacts.\nWe recreate a portion of the Stanford NLI corpus using GPT-4, Llama-2 and\nMistral 7b, and train hypothesis-only classifiers to determine whether\nLLM-elicited hypotheses contain annotation artifacts. On our LLM-elicited NLI\ndatasets, BERT-based hypothesis-only classifiers achieve between 86-96%\naccuracy, indicating these datasets contain hypothesis-only artifacts. We also\nfind frequent \"give-aways\" in LLM-generated hypotheses, e.g. the phrase\n\"swimming in a pool\" appears in more than 10,000 contradictions generated by\nGPT-4. Our analysis provides empirical evidence that well-attested biases in\nNLI can persist in LLM-generated data."
                },
                "authors": [
                    {
                        "name": "Grace Proebsting"
                    },
                    {
                        "name": "Adam Poliak"
                    }
                ],
                "author_detail": {
                    "name": "Adam Poliak"
                },
                "author": "Adam Poliak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08991v1",
                "updated": "2024-10-11T17:03:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    3,
                    13,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:03:13Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    3,
                    13,
                    4,
                    285,
                    0
                ],
                "title": "Science is Exploration: Computational Frontiers for Conceptual Metaphor\n  Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Science is Exploration: Computational Frontiers for Conceptual Metaphor\n  Theory"
                },
                "summary": "Metaphors are everywhere. They appear extensively across all domains of\nnatural language, from the most sophisticated poetry to seemingly dry academic\nprose. A significant body of research in the cognitive science of language\nargues for the existence of conceptual metaphors, the systematic structuring of\none domain of experience in the language of another. Conceptual metaphors are\nnot simply rhetorical flourishes but are crucial evidence of the role of\nanalogical reasoning in human cognition. In this paper, we ask whether Large\nLanguage Models (LLMs) can accurately identify and explain the presence of such\nconceptual metaphors in natural language data. Using a novel prompting\ntechnique based on metaphor annotation guidelines, we demonstrate that LLMs are\na promising tool for large-scale computational research on conceptual\nmetaphors. Further, we show that LLMs are able to apply procedural guidelines\ndesigned for human annotators, displaying a surprising depth of linguistic\nknowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metaphors are everywhere. They appear extensively across all domains of\nnatural language, from the most sophisticated poetry to seemingly dry academic\nprose. A significant body of research in the cognitive science of language\nargues for the existence of conceptual metaphors, the systematic structuring of\none domain of experience in the language of another. Conceptual metaphors are\nnot simply rhetorical flourishes but are crucial evidence of the role of\nanalogical reasoning in human cognition. In this paper, we ask whether Large\nLanguage Models (LLMs) can accurately identify and explain the presence of such\nconceptual metaphors in natural language data. Using a novel prompting\ntechnique based on metaphor annotation guidelines, we demonstrate that LLMs are\na promising tool for large-scale computational research on conceptual\nmetaphors. Further, we show that LLMs are able to apply procedural guidelines\ndesigned for human annotators, displaying a surprising depth of linguistic\nknowledge."
                },
                "authors": [
                    {
                        "name": "Rebecca M. M. Hicke"
                    },
                    {
                        "name": "Ross Deans Kristensen-McLachlan"
                    }
                ],
                "author_detail": {
                    "name": "Ross Deans Kristensen-McLachlan"
                },
                "author": "Ross Deans Kristensen-McLachlan",
                "arxiv_comment": "Accepted to the 2024 Computational Humanities Research Conference\n  (CHR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08989v1",
                "updated": "2024-10-11T17:01:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    1,
                    43,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T17:01:43Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    1,
                    43,
                    4,
                    285,
                    0
                ],
                "title": "SubZero: Random Subspace Zeroth-Order Optimization for Memory-Efficient\n  LLM Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SubZero: Random Subspace Zeroth-Order Optimization for Memory-Efficient\n  LLM Fine-Tuning"
                },
                "summary": "Fine-tuning Large Language Models (LLMs) has proven effective for a variety\nof downstream tasks. However, as LLMs grow in size, the memory demands for\nbackpropagation become increasingly prohibitive. Zeroth-order (ZO) optimization\nmethods offer a memory-efficient alternative by using forward passes to\nestimate gradients, but the variance of gradient estimates typically scales\nlinearly with the model's parameter dimension$\\unicode{x2013}$a significant\nissue for LLMs. In this paper, we propose the random Subspace Zeroth-order\n(SubZero) optimization to address the challenges posed by LLMs' high\ndimensionality. We introduce a low-rank perturbation tailored for LLMs that\nsignificantly reduces memory consumption while improving training performance.\nAdditionally, we prove that our gradient estimation closely approximates the\nbackpropagation gradient, exhibits lower variance than traditional ZO methods,\nand ensures convergence when combined with SGD. Experimental results show that\nSubZero enhances fine-tuning performance and achieves faster convergence\ncompared to standard ZO approaches like MeZO across various language modeling\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Large Language Models (LLMs) has proven effective for a variety\nof downstream tasks. However, as LLMs grow in size, the memory demands for\nbackpropagation become increasingly prohibitive. Zeroth-order (ZO) optimization\nmethods offer a memory-efficient alternative by using forward passes to\nestimate gradients, but the variance of gradient estimates typically scales\nlinearly with the model's parameter dimension$\\unicode{x2013}$a significant\nissue for LLMs. In this paper, we propose the random Subspace Zeroth-order\n(SubZero) optimization to address the challenges posed by LLMs' high\ndimensionality. We introduce a low-rank perturbation tailored for LLMs that\nsignificantly reduces memory consumption while improving training performance.\nAdditionally, we prove that our gradient estimation closely approximates the\nbackpropagation gradient, exhibits lower variance than traditional ZO methods,\nand ensures convergence when combined with SGD. Experimental results show that\nSubZero enhances fine-tuning performance and achieves faster convergence\ncompared to standard ZO approaches like MeZO across various language modeling\ntasks."
                },
                "authors": [
                    {
                        "name": "Ziming Yu"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Sike Wang"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Hua Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hua Huang"
                },
                "author": "Hua Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08985v1",
                "updated": "2024-10-11T16:57:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    57,
                    30,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T16:57:30Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    57,
                    30,
                    4,
                    285,
                    0
                ],
                "title": "Towards Trustworthy Knowledge Graph Reasoning: An Uncertainty Aware\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Trustworthy Knowledge Graph Reasoning: An Uncertainty Aware\n  Perspective"
                },
                "summary": "Recently, Knowledge Graphs (KGs) have been successfully coupled with Large\nLanguage Models (LLMs) to mitigate their hallucinations and enhance their\nreasoning capability, such as in KG-based retrieval-augmented frameworks.\nHowever, current KG-LLM frameworks lack rigorous uncertainty estimation,\nlimiting their reliable deployment in high-stakes applications. Directly\nincorporating uncertainty quantification into KG-LLM frameworks presents\nchallenges due to their complex architectures and the intricate interactions\nbetween the knowledge graph and language model components. To address this gap,\nwe propose a new trustworthy KG-LLM framework, Uncertainty Aware\nKnowledge-Graph Reasoning (UAG), which incorporates uncertainty quantification\ninto the KG-LLM framework. We design an uncertainty-aware multi-step reasoning\nframework that leverages conformal prediction to provide a theoretical\nguarantee on the prediction set. To manage the error rate of the multi-step\nprocess, we additionally introduce an error rate control module to adjust the\nerror rate within the individual components. Extensive experiments show that\nour proposed UAG can achieve any pre-defined coverage rate while reducing the\nprediction set/interval size by 40% on average over the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Knowledge Graphs (KGs) have been successfully coupled with Large\nLanguage Models (LLMs) to mitigate their hallucinations and enhance their\nreasoning capability, such as in KG-based retrieval-augmented frameworks.\nHowever, current KG-LLM frameworks lack rigorous uncertainty estimation,\nlimiting their reliable deployment in high-stakes applications. Directly\nincorporating uncertainty quantification into KG-LLM frameworks presents\nchallenges due to their complex architectures and the intricate interactions\nbetween the knowledge graph and language model components. To address this gap,\nwe propose a new trustworthy KG-LLM framework, Uncertainty Aware\nKnowledge-Graph Reasoning (UAG), which incorporates uncertainty quantification\ninto the KG-LLM framework. We design an uncertainty-aware multi-step reasoning\nframework that leverages conformal prediction to provide a theoretical\nguarantee on the prediction set. To manage the error rate of the multi-step\nprocess, we additionally introduce an error rate control module to adjust the\nerror rate within the individual components. Extensive experiments show that\nour proposed UAG can achieve any pre-defined coverage rate while reducing the\nprediction set/interval size by 40% on average over the baselines."
                },
                "authors": [
                    {
                        "name": "Bo Ni"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Lu Cheng"
                    },
                    {
                        "name": "Erik Blasch"
                    },
                    {
                        "name": "Tyler Derr"
                    }
                ],
                "author_detail": {
                    "name": "Tyler Derr"
                },
                "author": "Tyler Derr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08970v1",
                "updated": "2024-10-11T16:40:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    40,
                    3,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T16:40:03Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    40,
                    3,
                    4,
                    285,
                    0
                ],
                "title": "NoVo: Norm Voting off Hallucinations with Attention Heads in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NoVo: Norm Voting off Hallucinations with Attention Heads in Large\n  Language Models"
                },
                "summary": "Hallucinations in Large Language Models (LLMs) remain a major obstacle,\nparticularly in high-stakes applications where factual accuracy is critical.\nWhile representation editing and reading methods have made strides in reducing\nhallucinations, their heavy reliance on specialised tools and training on\nin-domain samples, makes them difficult to scale and prone to overfitting. This\nlimits their accuracy gains and generalizability to diverse datasets. This\npaper presents a lightweight method, Norm Voting (NoVo), which harnesses the\nuntapped potential of attention head norms to dramatically enhance factual\naccuracy in zero-shot multiple-choice questions (MCQs). NoVo begins by\nautomatically selecting truth-correlated head norms with an efficient,\ninference-only algorithm using only 30 random samples, allowing NoVo to\neffortlessly scale to diverse datasets. Afterwards, selected head norms are\nemployed in a simple voting algorithm, which yields significant gains in\nprediction accuracy. On TruthfulQA MC1, NoVo surpasses the current\nstate-of-the-art and all previous methods by an astounding margin -- at least\n19 accuracy points. NoVo demonstrates exceptional generalization to 20 diverse\ndatasets, with significant gains in over 90\\% of them, far exceeding all\ncurrent representation editing and reading methods. NoVo also reveals promising\ngains to finetuning strategies and building textual adversarial defence. NoVo's\neffectiveness with head norms opens new frontiers in LLM interpretability,\nrobustness and reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations in Large Language Models (LLMs) remain a major obstacle,\nparticularly in high-stakes applications where factual accuracy is critical.\nWhile representation editing and reading methods have made strides in reducing\nhallucinations, their heavy reliance on specialised tools and training on\nin-domain samples, makes them difficult to scale and prone to overfitting. This\nlimits their accuracy gains and generalizability to diverse datasets. This\npaper presents a lightweight method, Norm Voting (NoVo), which harnesses the\nuntapped potential of attention head norms to dramatically enhance factual\naccuracy in zero-shot multiple-choice questions (MCQs). NoVo begins by\nautomatically selecting truth-correlated head norms with an efficient,\ninference-only algorithm using only 30 random samples, allowing NoVo to\neffortlessly scale to diverse datasets. Afterwards, selected head norms are\nemployed in a simple voting algorithm, which yields significant gains in\nprediction accuracy. On TruthfulQA MC1, NoVo surpasses the current\nstate-of-the-art and all previous methods by an astounding margin -- at least\n19 accuracy points. NoVo demonstrates exceptional generalization to 20 diverse\ndatasets, with significant gains in over 90\\% of them, far exceeding all\ncurrent representation editing and reading methods. NoVo also reveals promising\ngains to finetuning strategies and building textual adversarial defence. NoVo's\neffectiveness with head norms opens new frontiers in LLM interpretability,\nrobustness and reliability."
                },
                "authors": [
                    {
                        "name": "Zheng Yi Ho"
                    },
                    {
                        "name": "Siyuan Liang"
                    },
                    {
                        "name": "Sen Zhang"
                    },
                    {
                        "name": "Yibing Zhan"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08968v1",
                "updated": "2024-10-11T16:38:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    38,
                    1,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T16:38:01Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    38,
                    1,
                    4,
                    285,
                    0
                ],
                "title": "Controllable Safety Alignment: Inference-Time Adaptation to Diverse\n  Safety Requirements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllable Safety Alignment: Inference-Time Adaptation to Diverse\n  Safety Requirements"
                },
                "summary": "The current paradigm for safety alignment of large language models (LLMs)\nfollows a one-size-fits-all approach: the model refuses to interact with any\ncontent deemed unsafe by the model provider. This approach lacks flexibility in\nthe face of varying social norms across cultures and regions. In addition,\nusers may have diverse safety needs, making a model with static safety\nstandards too restrictive to be useful, as well as too costly to be re-aligned.\n  We propose Controllable Safety Alignment (CoSA), a framework designed to\nadapt models to diverse safety requirements without re-training. Instead of\naligning a fixed model, we align models to follow safety configs -- free-form\nnatural language descriptions of the desired safety behaviors -- that are\nprovided as part of the system prompt. To adjust model safety behavior,\nauthorized users only need to modify such safety configs at inference time. To\nenable that, we propose CoSAlign, a data-centric method for aligning LLMs to\neasily adapt to diverse safety configs. Furthermore, we devise a novel\ncontrollability evaluation protocol that considers both helpfulness and\nconfigured safety, summarizing them into CoSA-Score, and construct CoSApien, a\nhuman-authored benchmark that consists of real-world LLM use cases with diverse\nsafety requirements and corresponding evaluation prompts.\n  We show that CoSAlign leads to substantial gains of controllability over\nstrong baselines including in-context alignment. Our framework encourages\nbetter representation and adaptation to pluralistic human values in LLMs, and\nthereby increasing their practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current paradigm for safety alignment of large language models (LLMs)\nfollows a one-size-fits-all approach: the model refuses to interact with any\ncontent deemed unsafe by the model provider. This approach lacks flexibility in\nthe face of varying social norms across cultures and regions. In addition,\nusers may have diverse safety needs, making a model with static safety\nstandards too restrictive to be useful, as well as too costly to be re-aligned.\n  We propose Controllable Safety Alignment (CoSA), a framework designed to\nadapt models to diverse safety requirements without re-training. Instead of\naligning a fixed model, we align models to follow safety configs -- free-form\nnatural language descriptions of the desired safety behaviors -- that are\nprovided as part of the system prompt. To adjust model safety behavior,\nauthorized users only need to modify such safety configs at inference time. To\nenable that, we propose CoSAlign, a data-centric method for aligning LLMs to\neasily adapt to diverse safety configs. Furthermore, we devise a novel\ncontrollability evaluation protocol that considers both helpfulness and\nconfigured safety, summarizing them into CoSA-Score, and construct CoSApien, a\nhuman-authored benchmark that consists of real-world LLM use cases with diverse\nsafety requirements and corresponding evaluation prompts.\n  We show that CoSAlign leads to substantial gains of controllability over\nstrong baselines including in-context alignment. Our framework encourages\nbetter representation and adaptation to pluralistic human values in LLMs, and\nthereby increasing their practicality."
                },
                "authors": [
                    {
                        "name": "Jingyu Zhang"
                    },
                    {
                        "name": "Ahmed Elgohary"
                    },
                    {
                        "name": "Ahmed Magooda"
                    },
                    {
                        "name": "Daniel Khashabi"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08964v1",
                "updated": "2024-10-11T16:32:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    32,
                    5,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T16:32:05Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    32,
                    5,
                    4,
                    285,
                    0
                ],
                "title": "Language Imbalance Driven Rewarding for Multilingual Self-improving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Imbalance Driven Rewarding for Multilingual Self-improving"
                },
                "summary": "Large Language Models (LLMs) have achieved state-of-the-art performance\nacross numerous tasks. However, these advancements have predominantly benefited\n\"first-class\" languages such as English and Chinese, leaving many other\nlanguages underrepresented. This imbalance, while limiting broader\napplications, generates a natural preference ranking between languages,\noffering an opportunity to bootstrap the multilingual capabilities of LLM in a\nself-improving manner. Thus, we propose $\\textit{Language Imbalance Driven\nRewarding}$, where the inherent imbalance between dominant and non-dominant\nlanguages within LLMs is leveraged as a reward signal. Iterative DPO training\ndemonstrates that this approach not only enhances LLM performance in\nnon-dominant languages but also improves the dominant language's capacity,\nthereby yielding an iterative reward signal. Fine-tuning\nMeta-Llama-3-8B-Instruct over two iterations of this approach results in\ncontinuous improvements in multilingual performance across\ninstruction-following and arithmetic reasoning tasks, evidenced by an average\nimprovement of 7.46% win rate on the X-AlpacaEval leaderboard and 13.9%\naccuracy on the MGSM benchmark. This work serves as an initial exploration,\npaving the way for multilingual self-improvement of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved state-of-the-art performance\nacross numerous tasks. However, these advancements have predominantly benefited\n\"first-class\" languages such as English and Chinese, leaving many other\nlanguages underrepresented. This imbalance, while limiting broader\napplications, generates a natural preference ranking between languages,\noffering an opportunity to bootstrap the multilingual capabilities of LLM in a\nself-improving manner. Thus, we propose $\\textit{Language Imbalance Driven\nRewarding}$, where the inherent imbalance between dominant and non-dominant\nlanguages within LLMs is leveraged as a reward signal. Iterative DPO training\ndemonstrates that this approach not only enhances LLM performance in\nnon-dominant languages but also improves the dominant language's capacity,\nthereby yielding an iterative reward signal. Fine-tuning\nMeta-Llama-3-8B-Instruct over two iterations of this approach results in\ncontinuous improvements in multilingual performance across\ninstruction-following and arithmetic reasoning tasks, evidenced by an average\nimprovement of 7.46% win rate on the X-AlpacaEval leaderboard and 13.9%\naccuracy on the MGSM benchmark. This work serves as an initial exploration,\npaving the way for multilingual self-improvement of LLMs."
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Junhong Wu"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Chengqing Zong"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01707v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01707v2",
                "updated": "2024-10-11T16:28:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    28,
                    36,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-02T16:15:31Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    15,
                    31,
                    2,
                    276,
                    0
                ],
                "title": "Interpretable Contrastive Monte Carlo Tree Search Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Contrastive Monte Carlo Tree Search Reasoning"
                },
                "summary": "We propose SC-MCTS*: a novel Monte Carlo Tree Search (MCTS) reasoning\nalgorithm for Large Language Models (LLMs), significantly improves both\nreasoning accuracy and speed. Our motivation comes from: 1. Previous MCTS LLM\nreasoning works often overlooked its biggest drawback--slower speed compared to\nCoT; 2. Previous research mainly used MCTS as a tool for LLM reasoning on\nvarious tasks with limited quantitative analysis or ablation studies of its\ncomponents from reasoning interpretability perspective. 3. The reward model is\nthe most crucial component in MCTS, however previous work has rarely conducted\nin-depth study or improvement of MCTS's reward models. Thus, we conducted\nextensive ablation studies and quantitative analysis on components of MCTS,\nrevealing the impact of each component on the MCTS reasoning performance of\nLLMs. Building on this, (i) we designed a highly interpretable reward model\nbased on the principle of contrastive decoding and (ii) achieved an average\nspeed improvement of 51.9% per node using speculative decoding. Additionally,\n(iii) we improved UCT node selection strategy and backpropagation used in\nprevious works, resulting in significant performance improvement. We\noutperformed o1-mini by an average of 17.4% on the Blocksworld multi-step\nreasoning dataset using Llama-3.1-70B with SC-MCTS*. Our code is available at\n\\url{https://github.com/zitian-gao/SC-MCTS}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose SC-MCTS*: a novel Monte Carlo Tree Search (MCTS) reasoning\nalgorithm for Large Language Models (LLMs), significantly improves both\nreasoning accuracy and speed. Our motivation comes from: 1. Previous MCTS LLM\nreasoning works often overlooked its biggest drawback--slower speed compared to\nCoT; 2. Previous research mainly used MCTS as a tool for LLM reasoning on\nvarious tasks with limited quantitative analysis or ablation studies of its\ncomponents from reasoning interpretability perspective. 3. The reward model is\nthe most crucial component in MCTS, however previous work has rarely conducted\nin-depth study or improvement of MCTS's reward models. Thus, we conducted\nextensive ablation studies and quantitative analysis on components of MCTS,\nrevealing the impact of each component on the MCTS reasoning performance of\nLLMs. Building on this, (i) we designed a highly interpretable reward model\nbased on the principle of contrastive decoding and (ii) achieved an average\nspeed improvement of 51.9% per node using speculative decoding. Additionally,\n(iii) we improved UCT node selection strategy and backpropagation used in\nprevious works, resulting in significant performance improvement. We\noutperformed o1-mini by an average of 17.4% on the Blocksworld multi-step\nreasoning dataset using Llama-3.1-70B with SC-MCTS*. Our code is available at\n\\url{https://github.com/zitian-gao/SC-MCTS}."
                },
                "authors": [
                    {
                        "name": "Zitian Gao"
                    },
                    {
                        "name": "Boye Niu"
                    },
                    {
                        "name": "Xuzheng He"
                    },
                    {
                        "name": "Haotian Xu"
                    },
                    {
                        "name": "Hongzhang Liu"
                    },
                    {
                        "name": "Aiwei Liu"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Lijie Wen"
                    }
                ],
                "author_detail": {
                    "name": "Lijie Wen"
                },
                "author": "Lijie Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01707v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01707v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.20086v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.20086v3",
                "updated": "2024-10-11T16:20:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    20,
                    16,
                    4,
                    285,
                    0
                ],
                "published": "2024-06-28T17:54:47Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    17,
                    54,
                    47,
                    4,
                    180,
                    0
                ],
                "title": "Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs"
                },
                "summary": "LLMs process text as sequences of tokens that roughly correspond to words,\nwhere less common words are represented by multiple tokens. However, individual\ntokens are often semantically unrelated to the meanings of the words/concepts\nthey comprise. For example, Llama-2-7b's tokenizer splits the word\n\"northeastern\" into the tokens ['_n', 'ort', 'he', 'astern'], none of which\ncorrespond to semantically meaningful units like \"north\" or \"east.\" Similarly,\nthe overall meanings of named entities like \"Neil Young\" and multi-word\nexpressions like \"break a leg\" cannot be directly inferred from their\nconstituent tokens. Mechanistically, how do LLMs convert such arbitrary groups\nof tokens into useful higher-level representations? In this work, we find that\nlast token representations of named entities and multi-token words exhibit a\npronounced \"erasure\" effect, where information about previous and current\ntokens is rapidly forgotten in early layers. Using this observation, we propose\na method to \"read out\" the implicit vocabulary of an autoregressive LLM by\nexamining differences in token representations across layers, and present\nresults of this method for Llama-2-7b and Llama-3-8B. To our knowledge, this is\nthe first attempt to probe the implicit vocabulary of an LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs process text as sequences of tokens that roughly correspond to words,\nwhere less common words are represented by multiple tokens. However, individual\ntokens are often semantically unrelated to the meanings of the words/concepts\nthey comprise. For example, Llama-2-7b's tokenizer splits the word\n\"northeastern\" into the tokens ['_n', 'ort', 'he', 'astern'], none of which\ncorrespond to semantically meaningful units like \"north\" or \"east.\" Similarly,\nthe overall meanings of named entities like \"Neil Young\" and multi-word\nexpressions like \"break a leg\" cannot be directly inferred from their\nconstituent tokens. Mechanistically, how do LLMs convert such arbitrary groups\nof tokens into useful higher-level representations? In this work, we find that\nlast token representations of named entities and multi-token words exhibit a\npronounced \"erasure\" effect, where information about previous and current\ntokens is rapidly forgotten in early layers. Using this observation, we propose\na method to \"read out\" the implicit vocabulary of an autoregressive LLM by\nexamining differences in token representations across layers, and present\nresults of this method for Llama-2-7b and Llama-3-8B. To our knowledge, this is\nthe first attempt to probe the implicit vocabulary of an LLM."
                },
                "authors": [
                    {
                        "name": "Sheridan Feucht"
                    },
                    {
                        "name": "David Atkinson"
                    },
                    {
                        "name": "Byron Wallace"
                    },
                    {
                        "name": "David Bau"
                    }
                ],
                "author_detail": {
                    "name": "David Bau"
                },
                "author": "David Bau",
                "arxiv_comment": "13 pages, 14 figures. Code and data at\n  https://footprints.baulab.info/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.20086v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.20086v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14517v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14517v2",
                "updated": "2024-10-11T16:19:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    19,
                    55,
                    4,
                    285,
                    0
                ],
                "published": "2024-06-20T17:27:14Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    17,
                    27,
                    14,
                    3,
                    172,
                    0
                ],
                "title": "PostMark: A Robust Blackbox Watermark for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PostMark: A Robust Blackbox Watermark for Large Language Models"
                },
                "summary": "The most effective techniques to detect LLM-generated text rely on inserting\na detectable signature -- or watermark -- during the model's decoding process.\nMost existing watermarking methods require access to the underlying LLM's\nlogits, which LLM API providers are loath to share due to fears of model\ndistillation. As such, these watermarks must be implemented independently by\neach LLM provider. In this paper, we develop PostMark, a modular post-hoc\nwatermarking procedure in which an input-dependent set of words (determined via\na semantic embedding) is inserted into the text after the decoding process has\ncompleted. Critically, PostMark does not require logit access, which means it\ncan be implemented by a third party. We also show that PostMark is more robust\nto paraphrasing attacks than existing watermarking methods: our experiments\ncover eight baseline algorithms, five base LLMs, and three datasets. Finally,\nwe evaluate the impact of PostMark on text quality using both automated and\nhuman assessments, highlighting the trade-off between quality and robustness to\nparaphrasing. We release our code, outputs, and annotations at\nhttps://github.com/lilakk/PostMark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The most effective techniques to detect LLM-generated text rely on inserting\na detectable signature -- or watermark -- during the model's decoding process.\nMost existing watermarking methods require access to the underlying LLM's\nlogits, which LLM API providers are loath to share due to fears of model\ndistillation. As such, these watermarks must be implemented independently by\neach LLM provider. In this paper, we develop PostMark, a modular post-hoc\nwatermarking procedure in which an input-dependent set of words (determined via\na semantic embedding) is inserted into the text after the decoding process has\ncompleted. Critically, PostMark does not require logit access, which means it\ncan be implemented by a third party. We also show that PostMark is more robust\nto paraphrasing attacks than existing watermarking methods: our experiments\ncover eight baseline algorithms, five base LLMs, and three datasets. Finally,\nwe evaluate the impact of PostMark on text quality using both automated and\nhuman assessments, highlighting the trade-off between quality and robustness to\nparaphrasing. We release our code, outputs, and annotations at\nhttps://github.com/lilakk/PostMark."
                },
                "authors": [
                    {
                        "name": "Yapei Chang"
                    },
                    {
                        "name": "Kalpesh Krishna"
                    },
                    {
                        "name": "Amir Houmansadr"
                    },
                    {
                        "name": "John Wieting"
                    },
                    {
                        "name": "Mohit Iyyer"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Iyyer"
                },
                "author": "Mohit Iyyer",
                "arxiv_comment": "EMNLP 2024; 19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14517v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14517v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08950v1",
                "updated": "2024-10-11T16:17:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    17,
                    47,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T16:17:47Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    17,
                    47,
                    4,
                    285,
                    0
                ],
                "title": "On the Adversarial Transferability of Generalized \"Skip Connections\"",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Adversarial Transferability of Generalized \"Skip Connections\""
                },
                "summary": "Skip connection is an essential ingredient for modern deep models to be\ndeeper and more powerful. Despite their huge success in normal scenarios\n(state-of-the-art classification performance on natural examples), we\ninvestigate and identify an interesting property of skip connections under\nadversarial scenarios, namely, the use of skip connections allows easier\ngeneration of highly transferable adversarial examples. Specifically, in\nResNet-like models (with skip connections), we find that using more gradients\nfrom the skip connections rather than the residual modules according to a decay\nfactor during backpropagation allows one to craft adversarial examples with\nhigh transferability. The above method is termed as Skip Gradient Method (SGM).\nAlthough starting from ResNet-like models in vision domains, we further extend\nSGM to more advanced architectures, including Vision Transformers (ViTs) and\nmodels with length-varying paths and other domains, i.e. natural language\nprocessing. We conduct comprehensive transfer attacks against various models\nincluding ResNets, Transformers, Inceptions, Neural Architecture Search, and\nLarge Language Models (LLMs). We show that employing SGM can greatly improve\nthe transferability of crafted attacks in almost all cases. Furthermore,\nconsidering the big complexity for practical use, we further demonstrate that\nSGM can even improve the transferability on ensembles of models or targeted\nattacks and the stealthiness against current defenses. At last, we provide\ntheoretical explanations and empirical insights on how SGM works. Our findings\nnot only motivate new adversarial research into the architectural\ncharacteristics of models but also open up further challenges for secure model\narchitecture design. Our code is available at https://github.com/mo666666/SGM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip connection is an essential ingredient for modern deep models to be\ndeeper and more powerful. Despite their huge success in normal scenarios\n(state-of-the-art classification performance on natural examples), we\ninvestigate and identify an interesting property of skip connections under\nadversarial scenarios, namely, the use of skip connections allows easier\ngeneration of highly transferable adversarial examples. Specifically, in\nResNet-like models (with skip connections), we find that using more gradients\nfrom the skip connections rather than the residual modules according to a decay\nfactor during backpropagation allows one to craft adversarial examples with\nhigh transferability. The above method is termed as Skip Gradient Method (SGM).\nAlthough starting from ResNet-like models in vision domains, we further extend\nSGM to more advanced architectures, including Vision Transformers (ViTs) and\nmodels with length-varying paths and other domains, i.e. natural language\nprocessing. We conduct comprehensive transfer attacks against various models\nincluding ResNets, Transformers, Inceptions, Neural Architecture Search, and\nLarge Language Models (LLMs). We show that employing SGM can greatly improve\nthe transferability of crafted attacks in almost all cases. Furthermore,\nconsidering the big complexity for practical use, we further demonstrate that\nSGM can even improve the transferability on ensembles of models or targeted\nattacks and the stealthiness against current defenses. At last, we provide\ntheoretical explanations and empirical insights on how SGM works. Our findings\nnot only motivate new adversarial research into the architectural\ncharacteristics of models but also open up further challenges for secure model\narchitecture design. Our code is available at https://github.com/mo666666/SGM."
                },
                "authors": [
                    {
                        "name": "Yisen Wang"
                    },
                    {
                        "name": "Yichuan Mo"
                    },
                    {
                        "name": "Dongxian Wu"
                    },
                    {
                        "name": "Mingjie Li"
                    },
                    {
                        "name": "Xingjun Ma"
                    },
                    {
                        "name": "Zhouchen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouchen Lin"
                },
                "author": "Zhouchen Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08948v1",
                "updated": "2024-10-11T16:16:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    16,
                    38,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T16:16:38Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    16,
                    38,
                    4,
                    285,
                    0
                ],
                "title": "The Dynamics of Social Conventions in LLM populations: Spontaneous\n  Emergence, Collective Biases and Tipping Points",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dynamics of Social Conventions in LLM populations: Spontaneous\n  Emergence, Collective Biases and Tipping Points"
                },
                "summary": "Social conventions are the foundation for social and economic life. As\nlegions of AI agents increasingly interact with each other and with humans,\ntheir ability to form shared conventions will determine how effectively they\nwill coordinate behaviors, integrate into society and influence it. Here, we\ninvestigate the dynamics of conventions within populations of Large Language\nModel (LLM) agents using simulated interactions. First, we show that globally\naccepted social conventions can spontaneously arise from local interactions\nbetween communicating LLMs. Second, we demonstrate how strong collective biases\ncan emerge during this process, even when individual agents appear to be\nunbiased. Third, we examine how minority groups of committed LLMs can drive\nsocial change by establishing new social conventions. We show that once these\nminority groups reach a critical size, they can consistently overturn\nestablished behaviors. In all cases, contrasting the experimental results with\npredictions from a minimal multi-agent model allows us to isolate the specific\nrole of LLM agents. Our results clarify how AI systems can autonomously develop\nnorms without explicit programming and have implications for designing AI\nsystems that align with human values and societal goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social conventions are the foundation for social and economic life. As\nlegions of AI agents increasingly interact with each other and with humans,\ntheir ability to form shared conventions will determine how effectively they\nwill coordinate behaviors, integrate into society and influence it. Here, we\ninvestigate the dynamics of conventions within populations of Large Language\nModel (LLM) agents using simulated interactions. First, we show that globally\naccepted social conventions can spontaneously arise from local interactions\nbetween communicating LLMs. Second, we demonstrate how strong collective biases\ncan emerge during this process, even when individual agents appear to be\nunbiased. Third, we examine how minority groups of committed LLMs can drive\nsocial change by establishing new social conventions. We show that once these\nminority groups reach a critical size, they can consistently overturn\nestablished behaviors. In all cases, contrasting the experimental results with\npredictions from a minimal multi-agent model allows us to isolate the specific\nrole of LLM agents. Our results clarify how AI systems can autonomously develop\nnorms without explicit programming and have implications for designing AI\nsystems that align with human values and societal goals."
                },
                "authors": [
                    {
                        "name": "Ariel Flint Ashery"
                    },
                    {
                        "name": "Luca Maria Aiello"
                    },
                    {
                        "name": "Andrea Baronchelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Baronchelli"
                },
                "author": "Andrea Baronchelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08928v1",
                "updated": "2024-10-11T15:53:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    53,
                    24,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T15:53:24Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    53,
                    24,
                    4,
                    285,
                    0
                ],
                "title": "Towards Cross-Lingual LLM Evaluation for European Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Cross-Lingual LLM Evaluation for European Languages"
                },
                "summary": "The rise of Large Language Models (LLMs) has revolutionized natural language\nprocessing across numerous languages and tasks. However, evaluating LLM\nperformance in a consistent and meaningful way across multiple European\nlanguages remains challenging, especially due to the scarcity of multilingual\nbenchmarks. We introduce a cross-lingual evaluation approach tailored for\nEuropean languages. We employ translated versions of five widely-used\nbenchmarks to assess the capabilities of 40 LLMs across 21 European languages.\nOur contributions include examining the effectiveness of translated benchmarks,\nassessing the impact of different translation services, and offering a\nmultilingual evaluation framework for LLMs that includes newly created\ndatasets: EU20-MMLU, EU20-HellaSwag, EU20-ARC, EU20-TruthfulQA, and EU20-GSM8K.\nThe benchmarks and results are made publicly available to encourage further\nresearch in multilingual LLM evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Large Language Models (LLMs) has revolutionized natural language\nprocessing across numerous languages and tasks. However, evaluating LLM\nperformance in a consistent and meaningful way across multiple European\nlanguages remains challenging, especially due to the scarcity of multilingual\nbenchmarks. We introduce a cross-lingual evaluation approach tailored for\nEuropean languages. We employ translated versions of five widely-used\nbenchmarks to assess the capabilities of 40 LLMs across 21 European languages.\nOur contributions include examining the effectiveness of translated benchmarks,\nassessing the impact of different translation services, and offering a\nmultilingual evaluation framework for LLMs that includes newly created\ndatasets: EU20-MMLU, EU20-HellaSwag, EU20-ARC, EU20-TruthfulQA, and EU20-GSM8K.\nThe benchmarks and results are made publicly available to encourage further\nresearch in multilingual LLM evaluation."
                },
                "authors": [
                    {
                        "name": "Klaudia Thellmann"
                    },
                    {
                        "name": "Bernhard Stadler"
                    },
                    {
                        "name": "Michael Fromm"
                    },
                    {
                        "name": "Jasper Schulze Buschhoff"
                    },
                    {
                        "name": "Alex Jude"
                    },
                    {
                        "name": "Fabio Barth"
                    },
                    {
                        "name": "Johannes Leveling"
                    },
                    {
                        "name": "Nicolas Flores-Herr"
                    },
                    {
                        "name": "Joachim Köhler"
                    },
                    {
                        "name": "René Jäkel"
                    },
                    {
                        "name": "Mehdi Ali"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Ali"
                },
                "author": "Mehdi Ali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08926v1",
                "updated": "2024-10-11T15:50:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    50,
                    53,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T15:50:53Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    50,
                    53,
                    4,
                    285,
                    0
                ],
                "title": "Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million\n  Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million\n  Images"
                },
                "summary": "We explore the transformative potential of SAM 2, a vision foundation model,\nin advancing gaze estimation and eye tracking technologies. By significantly\nreducing annotation time, lowering technical barriers through its ease of\ndeployment, and enhancing segmentation accuracy, SAM 2 addresses critical\nchallenges faced by researchers and practitioners. Utilizing its zero-shot\nsegmentation capabilities with minimal user input-a single click per video-we\ntested SAM 2 on over 14 million eye images from diverse datasets, including\nvirtual reality setups and the world's largest unified dataset recorded using\nwearable eye trackers. Remarkably, in pupil segmentation tasks, SAM 2 matches\nthe performance of domain-specific models trained solely on eye images,\nachieving competitive mean Intersection over Union (mIoU) scores of up to 93%\nwithout fine-tuning. Additionally, we provide our code and segmentation masks\nfor these widely used datasets to promote further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the transformative potential of SAM 2, a vision foundation model,\nin advancing gaze estimation and eye tracking technologies. By significantly\nreducing annotation time, lowering technical barriers through its ease of\ndeployment, and enhancing segmentation accuracy, SAM 2 addresses critical\nchallenges faced by researchers and practitioners. Utilizing its zero-shot\nsegmentation capabilities with minimal user input-a single click per video-we\ntested SAM 2 on over 14 million eye images from diverse datasets, including\nvirtual reality setups and the world's largest unified dataset recorded using\nwearable eye trackers. Remarkably, in pupil segmentation tasks, SAM 2 matches\nthe performance of domain-specific models trained solely on eye images,\nachieving competitive mean Intersection over Union (mIoU) scores of up to 93%\nwithout fine-tuning. Additionally, we provide our code and segmentation masks\nfor these widely used datasets to promote further research."
                },
                "authors": [
                    {
                        "name": "Virmarie Maquiling"
                    },
                    {
                        "name": "Sean Anthony Byrne"
                    },
                    {
                        "name": "Diederick C. Niehorster"
                    },
                    {
                        "name": "Marco Carminati"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Enkelejda Kasneci"
                },
                "author": "Enkelejda Kasneci",
                "arxiv_comment": "Virmarie Maquiling and Sean Anthony Byrne contributed equally to this\n  paper, 8 pages, 3 figures, CHI Case Study, pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08922v1",
                "updated": "2024-10-11T15:49:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    49,
                    42,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T15:49:42Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    49,
                    42,
                    4,
                    285,
                    0
                ],
                "title": "Exploring the Design Space of Cognitive Engagement Techniques with\n  AI-Generated Code for Enhanced Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Design Space of Cognitive Engagement Techniques with\n  AI-Generated Code for Enhanced Learning"
                },
                "summary": "Novice programmers are increasingly relying on Large Language Models (LLMs)\nto generate code for learning programming concepts. However, this interaction\ncan lead to superficial engagement, giving learners an illusion of learning and\nhindering skill development. To address this issue, we conducted a systematic\ndesign exploration to develop seven cognitive engagement techniques aimed at\npromoting deeper engagement with AI-generated code. In this paper, we describe\nour design process, the initial seven techniques and results from a\nbetween-subjects study (N=82). We then iteratively refined the top techniques\nand further evaluated them through a within-subjects study (N=42). We evaluate\nthe friction each technique introduces, their effectiveness in helping learners\napply concepts to isomorphic tasks without AI assistance, and their success in\naligning learners' perceived and actual coding abilities. Ultimately, our\nresults highlight the most effective technique: guiding learners through the\nstep-by-step problem-solving process, where they engage in an interactive\ndialog with the AI, prompting what needs to be done at each stage before the\ncorresponding code is revealed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novice programmers are increasingly relying on Large Language Models (LLMs)\nto generate code for learning programming concepts. However, this interaction\ncan lead to superficial engagement, giving learners an illusion of learning and\nhindering skill development. To address this issue, we conducted a systematic\ndesign exploration to develop seven cognitive engagement techniques aimed at\npromoting deeper engagement with AI-generated code. In this paper, we describe\nour design process, the initial seven techniques and results from a\nbetween-subjects study (N=82). We then iteratively refined the top techniques\nand further evaluated them through a within-subjects study (N=42). We evaluate\nthe friction each technique introduces, their effectiveness in helping learners\napply concepts to isomorphic tasks without AI assistance, and their success in\naligning learners' perceived and actual coding abilities. Ultimately, our\nresults highlight the most effective technique: guiding learners through the\nstep-by-step problem-solving process, where they engage in an interactive\ndialog with the AI, prompting what needs to be done at each stage before the\ncorresponding code is revealed."
                },
                "authors": [
                    {
                        "name": "Majeed Kazemitabaar"
                    },
                    {
                        "name": "Oliver Huang"
                    },
                    {
                        "name": "Sangho Suh"
                    },
                    {
                        "name": "Austin Z. Henley"
                    },
                    {
                        "name": "Tovi Grossman"
                    }
                ],
                "author_detail": {
                    "name": "Tovi Grossman"
                },
                "author": "Tovi Grossman",
                "arxiv_comment": "19 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08918v1",
                "updated": "2024-10-11T15:46:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    46,
                    9,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T15:46:09Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    46,
                    9,
                    4,
                    285,
                    0
                ],
                "title": "Wikimedia data for AI: a review of Wikimedia datasets for NLP tasks and\n  AI-assisted editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wikimedia data for AI: a review of Wikimedia datasets for NLP tasks and\n  AI-assisted editing"
                },
                "summary": "Wikimedia content is used extensively by the AI community and within the\nlanguage modeling community in particular. In this paper, we provide a review\nof the different ways in which Wikimedia data is curated to use in NLP tasks\nacross pre-training, post-training, and model evaluations. We point to\nopportunities for greater use of Wikimedia content but also identify ways in\nwhich the language modeling community could better center the needs of\nWikimedia editors. In particular, we call for incorporating additional sources\nof Wikimedia data, a greater focus on benchmarks for LLMs that encode Wikimedia\nprinciples, and greater multilingualism in Wikimedia-derived datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wikimedia content is used extensively by the AI community and within the\nlanguage modeling community in particular. In this paper, we provide a review\nof the different ways in which Wikimedia data is curated to use in NLP tasks\nacross pre-training, post-training, and model evaluations. We point to\nopportunities for greater use of Wikimedia content but also identify ways in\nwhich the language modeling community could better center the needs of\nWikimedia editors. In particular, we call for incorporating additional sources\nof Wikimedia data, a greater focus on benchmarks for LLMs that encode Wikimedia\nprinciples, and greater multilingualism in Wikimedia-derived datasets."
                },
                "authors": [
                    {
                        "name": "Isaac Johnson"
                    },
                    {
                        "name": "Lucie-Aimée Kaffee"
                    },
                    {
                        "name": "Miriam Redi"
                    }
                ],
                "author_detail": {
                    "name": "Miriam Redi"
                },
                "author": "Miriam Redi",
                "arxiv_comment": "Accepted to NLP for Wikipedia Workshop at EMNLP '24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08911v1",
                "updated": "2024-10-11T15:32:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    32,
                    48,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T15:32:48Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    32,
                    48,
                    4,
                    285,
                    0
                ],
                "title": "Test-driven Software Experimentation with LASSO: an LLM Benchmarking\n  Example",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-driven Software Experimentation with LASSO: an LLM Benchmarking\n  Example"
                },
                "summary": "Empirical software engineering faces a critical gap: the lack of standardized\ntools for rapid development and execution of Test-Driven Software Experiments\n(TDSEs) - that is, experiments that involve the execution of software subjects\nand the observation and analysis of their \"de facto\" run-time behavior. In this\npaper we present a general-purpose analysis platform called LASSO that provides\na minimal set of domain-specific languages and data structures to conduct\nTDSEs. By empowering users with an executable scripting language to design and\nexecute TDSEs, LASSO enables efficient evaluation of run-time semantics and\nexecution characteristics in addition to statically determined properties. We\npresent an example TDSE that demonstrates the practical benefits of LASSO's\nscripting capabilities for assessing the reliability of LLMs for code\ngeneration by means of a self-contained, reusable and extensible study script.\nThe LASSO platform is freely available at:\nhttps://softwareobservatorium.github.io/, and a demo video is available on\nYouTube: https://youtu.be/tzY9oNTWXzw",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical software engineering faces a critical gap: the lack of standardized\ntools for rapid development and execution of Test-Driven Software Experiments\n(TDSEs) - that is, experiments that involve the execution of software subjects\nand the observation and analysis of their \"de facto\" run-time behavior. In this\npaper we present a general-purpose analysis platform called LASSO that provides\na minimal set of domain-specific languages and data structures to conduct\nTDSEs. By empowering users with an executable scripting language to design and\nexecute TDSEs, LASSO enables efficient evaluation of run-time semantics and\nexecution characteristics in addition to statically determined properties. We\npresent an example TDSE that demonstrates the practical benefits of LASSO's\nscripting capabilities for assessing the reliability of LLMs for code\ngeneration by means of a self-contained, reusable and extensible study script.\nThe LASSO platform is freely available at:\nhttps://softwareobservatorium.github.io/, and a demo video is available on\nYouTube: https://youtu.be/tzY9oNTWXzw"
                },
                "authors": [
                    {
                        "name": "Marcus Kessel"
                    }
                ],
                "author_detail": {
                    "name": "Marcus Kessel"
                },
                "author": "Marcus Kessel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.1; D.2.4; I.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08899v1",
                "updated": "2024-10-11T15:18:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    18,
                    48,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T15:18:48Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    18,
                    48,
                    4,
                    285,
                    0
                ],
                "title": "Utilizing ChatGPT in a Data Structures and Algorithms Course: A Teaching\n  Assistant's Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing ChatGPT in a Data Structures and Algorithms Course: A Teaching\n  Assistant's Perspective"
                },
                "summary": "Integrating large language models (LLMs) like ChatGPT is revolutionizing the\nfield of computer science education. These models offer new possibilities for\nenriching student learning and supporting teaching assistants (TAs) in\nproviding prompt feedback and supplementary learning resources. This research\ndelves into the use of ChatGPT in a data structures and algorithms (DSA)\ncourse, particularly when combined with TA supervision. The findings\ndemonstrate that incorporating ChatGPT with structured prompts and active TA\nguidance enhances students' understanding of intricate algorithmic concepts,\nboosts engagement, and elevates academic performance. However, challenges exist\nin addressing academic integrity and the limitations of LLMs in tackling\ncomplex problems. The study underscores the importance of active TA involvement\nin reducing students' reliance on AI-generated content and amplifying the\noverall educational impact. The results suggest that while LLMs can be\nadvantageous for education, their successful integration demands continuous\noversight and a thoughtful balance between AI and human guidance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating large language models (LLMs) like ChatGPT is revolutionizing the\nfield of computer science education. These models offer new possibilities for\nenriching student learning and supporting teaching assistants (TAs) in\nproviding prompt feedback and supplementary learning resources. This research\ndelves into the use of ChatGPT in a data structures and algorithms (DSA)\ncourse, particularly when combined with TA supervision. The findings\ndemonstrate that incorporating ChatGPT with structured prompts and active TA\nguidance enhances students' understanding of intricate algorithmic concepts,\nboosts engagement, and elevates academic performance. However, challenges exist\nin addressing academic integrity and the limitations of LLMs in tackling\ncomplex problems. The study underscores the importance of active TA involvement\nin reducing students' reliance on AI-generated content and amplifying the\noverall educational impact. The results suggest that while LLMs can be\nadvantageous for education, their successful integration demands continuous\noversight and a thoughtful balance between AI and human guidance."
                },
                "authors": [
                    {
                        "name": "Pooriya Jamie"
                    },
                    {
                        "name": "Reyhaneh Hajihashemi"
                    },
                    {
                        "name": "Sharareh Alipour"
                    }
                ],
                "author_detail": {
                    "name": "Sharareh Alipour"
                },
                "author": "Sharareh Alipour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.3.2; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11557v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11557v4",
                "updated": "2024-10-11T15:13:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    13,
                    51,
                    4,
                    285,
                    0
                ],
                "published": "2024-08-21T12:09:37Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    12,
                    9,
                    37,
                    2,
                    234,
                    0
                ],
                "title": "A Quick, trustworthy spectral knowledge Q&A system leveraging\n  retrieval-augmented generation on LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Quick, trustworthy spectral knowledge Q&A system leveraging\n  retrieval-augmented generation on LLM"
                },
                "summary": "Large Language Model (LLM) has demonstrated significant success in a range of\nnatural language processing (NLP) tasks within general domain. The emergence of\nLLM has introduced innovative methodologies across diverse fields, including\nthe natural sciences. Researchers aim to implement automated, concurrent\nprocess driven by LLM to supplant conventional manual, repetitive and\nlabor-intensive work. In the domain of spectral analysis and detection, it is\nimperative for researchers to autonomously acquire pertinent knowledge across\nvarious research objects, which encompasses the spectroscopic techniques and\nthe chemometric methods that are employed in experiments and analysis.\nParadoxically, despite the recognition of spectroscopic detection as an\neffective analytical method, the fundamental process of knowledge retrieval\nremains both time-intensive and repetitive. In response to this challenge, we\nfirst introduced the Spectral Detection and Analysis Based Paper(SDAAP)\ndataset, which is the first open-source textual knowledge dataset for spectral\nanalysis and detection and contains annotated literature data as well as\ncorresponding knowledge instruction data. Subsequently, we also designed an\nautomated Q\\&A framework based on the SDAAP dataset, which can retrieve\nrelevant knowledge and generate high-quality responses by extracting entities\nin the input as retrieval parameters. It is worth noting that: within this\nframework, LLM is only used as a tool to provide generalizability, while RAG\ntechnique is used to accurately capture the source of the knowledge.This\napproach not only improves the quality of the generated responses, but also\nensures the traceability of the knowledge. Experimental results show that our\nframework generates responses with more reliable expertise compared to the\nbaseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) has demonstrated significant success in a range of\nnatural language processing (NLP) tasks within general domain. The emergence of\nLLM has introduced innovative methodologies across diverse fields, including\nthe natural sciences. Researchers aim to implement automated, concurrent\nprocess driven by LLM to supplant conventional manual, repetitive and\nlabor-intensive work. In the domain of spectral analysis and detection, it is\nimperative for researchers to autonomously acquire pertinent knowledge across\nvarious research objects, which encompasses the spectroscopic techniques and\nthe chemometric methods that are employed in experiments and analysis.\nParadoxically, despite the recognition of spectroscopic detection as an\neffective analytical method, the fundamental process of knowledge retrieval\nremains both time-intensive and repetitive. In response to this challenge, we\nfirst introduced the Spectral Detection and Analysis Based Paper(SDAAP)\ndataset, which is the first open-source textual knowledge dataset for spectral\nanalysis and detection and contains annotated literature data as well as\ncorresponding knowledge instruction data. Subsequently, we also designed an\nautomated Q\\&A framework based on the SDAAP dataset, which can retrieve\nrelevant knowledge and generate high-quality responses by extracting entities\nin the input as retrieval parameters. It is worth noting that: within this\nframework, LLM is only used as a tool to provide generalizability, while RAG\ntechnique is used to accurately capture the source of the knowledge.This\napproach not only improves the quality of the generated responses, but also\nensures the traceability of the knowledge. Experimental results show that our\nframework generates responses with more reliable expertise compared to the\nbaseline."
                },
                "authors": [
                    {
                        "name": "Jiheng Liang"
                    },
                    {
                        "name": "Ziru Yu"
                    },
                    {
                        "name": "Zujie Xie"
                    },
                    {
                        "name": "Xiangyang Yu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Yu"
                },
                "author": "Xiangyang Yu",
                "arxiv_comment": "16 pages,10 figures,3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11557v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11557v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07225v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07225v3",
                "updated": "2024-10-11T14:55:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    55,
                    44,
                    4,
                    285,
                    0
                ],
                "published": "2023-10-11T06:26:19Z",
                "published_parsed": [
                    2023,
                    10,
                    11,
                    6,
                    26,
                    19,
                    2,
                    284,
                    0
                ],
                "title": "Do Large Language Models have Shared Weaknesses in Medical Question\n  Answering?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models have Shared Weaknesses in Medical Question\n  Answering?"
                },
                "summary": "Large language models (LLMs) have made rapid improvement on medical\nbenchmarks, but their unreliability remains a persistent challenge for safe\nreal-world uses. To design for the use LLMs as a category, rather than for\nspecific models, requires developing an understanding of shared strengths and\nweaknesses which appear across models. To address this challenge, we benchmark\na range of top LLMs and identify consistent patterns across models. We test\n$16$ well-known LLMs on $874$ newly collected questions from Polish medical\nlicensing exams. For each question, we score each model on the top-1 accuracy\nand the distribution of probabilities assigned. We then compare these results\nwith factors such as question difficulty for humans, question length, and the\nscores of the other models. LLM accuracies were positively correlated pairwise\n($0.39$ to $0.58$). Model performance was also correlated with human\nperformance ($0.09$ to $0.13$), but negatively correlated to the difference\nbetween the question-level accuracy of top-scoring and bottom-scoring humans\n($-0.09$ to $-0.14$). The top output probability and question length were\npositive and negative predictors of accuracy respectively (p$< 0.05$). The top\nscoring LLM, GPT-4o Turbo, scored $84\\%$, with Claude Opus, Gemini 1.5 Pro and\nLlama 3/3.1 between $74\\%$ and $79\\%$. We found evidence of similarities\nbetween models in which questions they answer correctly, as well as\nsimilarities with human test takers. Larger models typically performed better,\nbut differences in training, architecture, and data were also highly impactful.\nModel accuracy was positively correlated with confidence, but negatively\ncorrelated with question length. We find similar results with older models, and\nargue that these patterns are likely to persist across future models using\nsimilar training methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made rapid improvement on medical\nbenchmarks, but their unreliability remains a persistent challenge for safe\nreal-world uses. To design for the use LLMs as a category, rather than for\nspecific models, requires developing an understanding of shared strengths and\nweaknesses which appear across models. To address this challenge, we benchmark\na range of top LLMs and identify consistent patterns across models. We test\n$16$ well-known LLMs on $874$ newly collected questions from Polish medical\nlicensing exams. For each question, we score each model on the top-1 accuracy\nand the distribution of probabilities assigned. We then compare these results\nwith factors such as question difficulty for humans, question length, and the\nscores of the other models. LLM accuracies were positively correlated pairwise\n($0.39$ to $0.58$). Model performance was also correlated with human\nperformance ($0.09$ to $0.13$), but negatively correlated to the difference\nbetween the question-level accuracy of top-scoring and bottom-scoring humans\n($-0.09$ to $-0.14$). The top output probability and question length were\npositive and negative predictors of accuracy respectively (p$< 0.05$). The top\nscoring LLM, GPT-4o Turbo, scored $84\\%$, with Claude Opus, Gemini 1.5 Pro and\nLlama 3/3.1 between $74\\%$ and $79\\%$. We found evidence of similarities\nbetween models in which questions they answer correctly, as well as\nsimilarities with human test takers. Larger models typically performed better,\nbut differences in training, architecture, and data were also highly impactful.\nModel accuracy was positively correlated with confidence, but negatively\ncorrelated with question length. We find similar results with older models, and\nargue that these patterns are likely to persist across future models using\nsimilar training methods."
                },
                "authors": [
                    {
                        "name": "Andrew M. Bean"
                    },
                    {
                        "name": "Karolina Korgul"
                    },
                    {
                        "name": "Felix Krones"
                    },
                    {
                        "name": "Robert McCraith"
                    },
                    {
                        "name": "Adam Mahdi"
                    }
                ],
                "author_detail": {
                    "name": "Adam Mahdi"
                },
                "author": "Adam Mahdi",
                "arxiv_comment": "8 pages, 10 figures. To appear in NeurIPS 2024 Advancements in\n  Medical Foundation Models Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.07225v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07225v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08869v1",
                "updated": "2024-10-11T14:46:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    46,
                    49,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T14:46:49Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    46,
                    49,
                    4,
                    285,
                    0
                ],
                "title": "Evolution of SAE Features Across Layers in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolution of SAE Features Across Layers in LLMs"
                },
                "summary": "Sparse Autoencoders for transformer-based language models are typically\ndefined independently per layer. In this work we analyze statistical\nrelationships between features in adjacent layers to understand how features\nevolve through a forward pass. We provide a graph visualization interface for\nfeatures and their most similar next-layer neighbors, and build communities of\nrelated features across layers. We find that a considerable amount of features\nare passed through from a previous layer, some features can be expressed as\nquasi-boolean combinations of previous features, and some features become more\nspecialized in later layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders for transformer-based language models are typically\ndefined independently per layer. In this work we analyze statistical\nrelationships between features in adjacent layers to understand how features\nevolve through a forward pass. We provide a graph visualization interface for\nfeatures and their most similar next-layer neighbors, and build communities of\nrelated features across layers. We find that a considerable amount of features\nare passed through from a previous layer, some features can be expressed as\nquasi-boolean combinations of previous features, and some features become more\nspecialized in later layers."
                },
                "authors": [
                    {
                        "name": "Daniel Balcells"
                    },
                    {
                        "name": "Benjamin Lerner"
                    },
                    {
                        "name": "Michael Oesterle"
                    },
                    {
                        "name": "Ediz Ucar"
                    },
                    {
                        "name": "Stefan Heimersheim"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Heimersheim"
                },
                "author": "Stefan Heimersheim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08860v1",
                "updated": "2024-10-11T14:40:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    40,
                    51,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T14:40:51Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    40,
                    51,
                    4,
                    285,
                    0
                ],
                "title": "Audio Description Generation in the Era of LLMs and VLMs: A Review of\n  Transferable Generative AI Technologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio Description Generation in the Era of LLMs and VLMs: A Review of\n  Transferable Generative AI Technologies"
                },
                "summary": "Audio descriptions (ADs) function as acoustic commentaries designed to assist\nblind persons and persons with visual impairments in accessing digital media\ncontent on television and in movies, among other settings. As an accessibility\nservice typically provided by trained AD professionals, the generation of ADs\ndemands significant human effort, making the process both time-consuming and\ncostly. Recent advancements in natural language processing (NLP) and computer\nvision (CV), particularly in large language models (LLMs) and vision-language\nmodels (VLMs), have allowed for getting a step closer to automatic AD\ngeneration. This paper reviews the technologies pertinent to AD generation in\nthe era of LLMs and VLMs: we discuss how state-of-the-art NLP and CV\ntechnologies can be applied to generate ADs and identify essential research\ndirections for the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio descriptions (ADs) function as acoustic commentaries designed to assist\nblind persons and persons with visual impairments in accessing digital media\ncontent on television and in movies, among other settings. As an accessibility\nservice typically provided by trained AD professionals, the generation of ADs\ndemands significant human effort, making the process both time-consuming and\ncostly. Recent advancements in natural language processing (NLP) and computer\nvision (CV), particularly in large language models (LLMs) and vision-language\nmodels (VLMs), have allowed for getting a step closer to automatic AD\ngeneration. This paper reviews the technologies pertinent to AD generation in\nthe era of LLMs and VLMs: we discuss how state-of-the-art NLP and CV\ntechnologies can be applied to generate ADs and identify essential research\ndirections for the future."
                },
                "authors": [
                    {
                        "name": "Yingqiang Gao"
                    },
                    {
                        "name": "Lukas Fischer"
                    },
                    {
                        "name": "Alexa Lintner"
                    },
                    {
                        "name": "Sarah Ebling"
                    }
                ],
                "author_detail": {
                    "name": "Sarah Ebling"
                },
                "author": "Sarah Ebling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08858v1",
                "updated": "2024-10-11T14:39:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    39,
                    24,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T14:39:24Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    39,
                    24,
                    4,
                    285,
                    0
                ],
                "title": "Decoding Secret Memorization in Code LLMs Through Token-Level\n  Characterization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding Secret Memorization in Code LLMs Through Token-Level\n  Characterization"
                },
                "summary": "Code Large Language Models (LLMs) have demonstrated remarkable capabilities\nin generating, understanding, and manipulating programming code. However, their\ntraining process inadvertently leads to the memorization of sensitive\ninformation, posing severe privacy risks. Existing studies on memorization in\nLLMs primarily rely on prompt engineering techniques, which suffer from\nlimitations such as widespread hallucination and inefficient extraction of the\ntarget sensitive information. In this paper, we present a novel approach to\ncharacterize real and fake secrets generated by Code LLMs based on token\nprobabilities. We identify four key characteristics that differentiate genuine\nsecrets from hallucinated ones, providing insights into distinguishing real and\nfake secrets. To overcome the limitations of existing works, we propose DESEC,\na two-stage method that leverages token-level features derived from the\nidentified characteristics to guide the token decoding process. DESEC consists\nof constructing an offline token scoring model using a proxy Code LLM and\nemploying the scoring model to guide the decoding process by reassigning token\nlikelihoods. Through extensive experiments on four state-of-the-art Code LLMs\nusing a diverse dataset, we demonstrate the superior performance of DESEC in\nachieving a higher plausible rate and extracting more real secrets compared to\nexisting baselines. Our findings highlight the effectiveness of our token-level\napproach in enabling an extensive assessment of the privacy leakage risks\nassociated with Code LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Large Language Models (LLMs) have demonstrated remarkable capabilities\nin generating, understanding, and manipulating programming code. However, their\ntraining process inadvertently leads to the memorization of sensitive\ninformation, posing severe privacy risks. Existing studies on memorization in\nLLMs primarily rely on prompt engineering techniques, which suffer from\nlimitations such as widespread hallucination and inefficient extraction of the\ntarget sensitive information. In this paper, we present a novel approach to\ncharacterize real and fake secrets generated by Code LLMs based on token\nprobabilities. We identify four key characteristics that differentiate genuine\nsecrets from hallucinated ones, providing insights into distinguishing real and\nfake secrets. To overcome the limitations of existing works, we propose DESEC,\na two-stage method that leverages token-level features derived from the\nidentified characteristics to guide the token decoding process. DESEC consists\nof constructing an offline token scoring model using a proxy Code LLM and\nemploying the scoring model to guide the decoding process by reassigning token\nlikelihoods. Through extensive experiments on four state-of-the-art Code LLMs\nusing a diverse dataset, we demonstrate the superior performance of DESEC in\nachieving a higher plausible rate and extracting more real secrets compared to\nexisting baselines. Our findings highlight the effectiveness of our token-level\napproach in enabling an extensive assessment of the privacy leakage risks\nassociated with Code LLMs."
                },
                "authors": [
                    {
                        "name": "Yuqing Nie"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Kailong Wang"
                    },
                    {
                        "name": "Guoai Xu"
                    },
                    {
                        "name": "Guosheng Xu"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "arxiv_comment": "12 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01326v2",
                "updated": "2024-10-11T14:38:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    38,
                    40,
                    4,
                    285,
                    0
                ],
                "published": "2024-06-03T13:54:05Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    13,
                    54,
                    5,
                    0,
                    155,
                    0
                ],
                "title": "TabPedia: Towards Comprehensive Visual Table Understanding with Concept\n  Synergy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TabPedia: Towards Comprehensive Visual Table Understanding with Concept\n  Synergy"
                },
                "summary": "Tables contain factual and quantitative data accompanied by various\nstructures and contents that pose challenges for machine comprehension.\nPrevious methods generally design task-specific architectures and objectives\nfor individual tasks, resulting in modal isolation and intricate workflows. In\nthis paper, we present a novel large vision-language model, TabPedia, equipped\nwith a concept synergy mechanism. In this mechanism, all the involved diverse\nvisual table understanding (VTU) tasks and multi-source visual embeddings are\nabstracted as concepts. This unified framework allows TabPedia to seamlessly\nintegrate VTU tasks, such as table detection, table structure recognition,\ntable querying, and table question answering, by leveraging the capabilities of\nlarge language models (LLMs). Moreover, the concept synergy mechanism enables\ntable perception-related and comprehension-related tasks to work in harmony, as\nthey can effectively leverage the needed clues from the corresponding source\nperception embeddings. Furthermore, to better evaluate the VTU task in\nreal-world scenarios, we establish a new and comprehensive table VQA benchmark,\nComTQA, featuring approximately 9,000 QA pairs. Extensive quantitative and\nqualitative experiments on both table perception and comprehension tasks,\nconducted across various public benchmarks, validate the effectiveness of our\nTabPedia. The superior performance further confirms the feasibility of using\nLLMs for understanding visual tables when all concepts work in synergy. The\nbenchmark ComTQA has been open-sourced at\nhttps://huggingface.co/datasets/ByteDance/ComTQA. The source code and model\nalso have been released athttps://github.com/zhaowc-ustc/TabPedia.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tables contain factual and quantitative data accompanied by various\nstructures and contents that pose challenges for machine comprehension.\nPrevious methods generally design task-specific architectures and objectives\nfor individual tasks, resulting in modal isolation and intricate workflows. In\nthis paper, we present a novel large vision-language model, TabPedia, equipped\nwith a concept synergy mechanism. In this mechanism, all the involved diverse\nvisual table understanding (VTU) tasks and multi-source visual embeddings are\nabstracted as concepts. This unified framework allows TabPedia to seamlessly\nintegrate VTU tasks, such as table detection, table structure recognition,\ntable querying, and table question answering, by leveraging the capabilities of\nlarge language models (LLMs). Moreover, the concept synergy mechanism enables\ntable perception-related and comprehension-related tasks to work in harmony, as\nthey can effectively leverage the needed clues from the corresponding source\nperception embeddings. Furthermore, to better evaluate the VTU task in\nreal-world scenarios, we establish a new and comprehensive table VQA benchmark,\nComTQA, featuring approximately 9,000 QA pairs. Extensive quantitative and\nqualitative experiments on both table perception and comprehension tasks,\nconducted across various public benchmarks, validate the effectiveness of our\nTabPedia. The superior performance further confirms the feasibility of using\nLLMs for understanding visual tables when all concepts work in synergy. The\nbenchmark ComTQA has been open-sourced at\nhttps://huggingface.co/datasets/ByteDance/ComTQA. The source code and model\nalso have been released athttps://github.com/zhaowc-ustc/TabPedia."
                },
                "authors": [
                    {
                        "name": "Weichao Zhao"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Jingqun Tang"
                    },
                    {
                        "name": "Shu Wei"
                    },
                    {
                        "name": "Binghong Wu"
                    },
                    {
                        "name": "Lei Liao"
                    },
                    {
                        "name": "Yongjie Ye"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Wengang Zhou"
                    },
                    {
                        "name": "Houqiang Li"
                    },
                    {
                        "name": "Can Huang"
                    }
                ],
                "author_detail": {
                    "name": "Can Huang"
                },
                "author": "Can Huang",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08855v1",
                "updated": "2024-10-11T14:32:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    32,
                    6,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T14:32:06Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    32,
                    6,
                    4,
                    285,
                    0
                ],
                "title": "MATCH: Model-Aware TVM-based Compilation for Heterogeneous Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MATCH: Model-Aware TVM-based Compilation for Heterogeneous Edge Devices"
                },
                "summary": "Streamlining the deployment of Deep Neural Networks (DNNs) on heterogeneous\nedge platforms, coupling within the same micro-controller unit (MCU)\ninstruction processors and hardware accelerators for tensor computations, is\nbecoming one of the crucial challenges of the TinyML field.\n  The best-performing DNN compilation toolchains are usually deeply customized\nfor a single MCU family, and porting to a different heterogeneous MCU family\nimplies labor-intensive re-development of almost the entire compiler. On the\nopposite side, retargetable toolchains, such as TVM, fail to exploit the\ncapabilities of custom accelerators, resulting in the generation of general but\nunoptimized code. To overcome this duality, we introduce MATCH, a novel\nTVM-based DNN deployment framework designed for easy agile retargeting across\ndifferent MCU processors and accelerators, thanks to a customizable model-based\nhardware abstraction.\n  We show that a general and retargetable mapping framework enhanced with\nhardware cost models can compete with and even outperform custom toolchains on\ndiverse targets while only needing the definition of an abstract hardware model\nand a SoC-specific API.\n  We tested MATCH on two state-of-the-art heterogeneous MCUs, GAP9 and DIANA.\n  On the four DNN models of the MLPerf Tiny suite MATCH reduces inference\nlatency by up to 60.88 times on DIANA, compared to using the plain TVM, thanks\nto the exploitation of the on-board HW accelerator. Compared to HTVM, a fully\ncustomized toolchain for DIANA, we still reduce the latency by 16.94%. On GAP9,\nusing the same benchmarks, we improve the latency by 2.15 times compared to the\ndedicated DORY compiler, thanks to our heterogeneous DNN mapping approach that\nsynergically exploits the DNN accelerator and the eight-cores cluster available\non board.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streamlining the deployment of Deep Neural Networks (DNNs) on heterogeneous\nedge platforms, coupling within the same micro-controller unit (MCU)\ninstruction processors and hardware accelerators for tensor computations, is\nbecoming one of the crucial challenges of the TinyML field.\n  The best-performing DNN compilation toolchains are usually deeply customized\nfor a single MCU family, and porting to a different heterogeneous MCU family\nimplies labor-intensive re-development of almost the entire compiler. On the\nopposite side, retargetable toolchains, such as TVM, fail to exploit the\ncapabilities of custom accelerators, resulting in the generation of general but\nunoptimized code. To overcome this duality, we introduce MATCH, a novel\nTVM-based DNN deployment framework designed for easy agile retargeting across\ndifferent MCU processors and accelerators, thanks to a customizable model-based\nhardware abstraction.\n  We show that a general and retargetable mapping framework enhanced with\nhardware cost models can compete with and even outperform custom toolchains on\ndiverse targets while only needing the definition of an abstract hardware model\nand a SoC-specific API.\n  We tested MATCH on two state-of-the-art heterogeneous MCUs, GAP9 and DIANA.\n  On the four DNN models of the MLPerf Tiny suite MATCH reduces inference\nlatency by up to 60.88 times on DIANA, compared to using the plain TVM, thanks\nto the exploitation of the on-board HW accelerator. Compared to HTVM, a fully\ncustomized toolchain for DIANA, we still reduce the latency by 16.94%. On GAP9,\nusing the same benchmarks, we improve the latency by 2.15 times compared to the\ndedicated DORY compiler, thanks to our heterogeneous DNN mapping approach that\nsynergically exploits the DNN accelerator and the eight-cores cluster available\non board."
                },
                "authors": [
                    {
                        "name": "Mohamed Amine Hamdi"
                    },
                    {
                        "name": "Francesco Daghero"
                    },
                    {
                        "name": "Giuseppe Maria Sarda"
                    },
                    {
                        "name": "Josse Van Delm"
                    },
                    {
                        "name": "Arne Symons"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Marian Verhelst"
                    },
                    {
                        "name": "Daniele Jahier Pagliari"
                    },
                    {
                        "name": "Alessio Burrello"
                    }
                ],
                "author_detail": {
                    "name": "Alessio Burrello"
                },
                "author": "Alessio Burrello",
                "arxiv_comment": "13 pages, 11 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.2; D.1.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08854v1",
                "updated": "2024-10-11T14:30:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    30,
                    4,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T14:30:04Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    30,
                    4,
                    4,
                    285,
                    0
                ],
                "title": "Hybrid LLM-DDQN based Joint Optimization of V2I Communication and\n  Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid LLM-DDQN based Joint Optimization of V2I Communication and\n  Autonomous Driving"
                },
                "summary": "Large language models (LLMs) have received considerable interest recently due\nto their outstanding reasoning and comprehension capabilities. This work\nexplores applying LLMs to vehicular networks, aiming to jointly optimize\nvehicle-to-infrastructure (V2I) communications and autonomous driving (AD)\npolicies. We deploy LLMs for AD decision-making to maximize traffic flow and\navoid collisions for road safety, and a double deep Q-learning algorithm (DDQN)\nis used for V2I optimization to maximize the received data rate and reduce\nfrequent handovers. In particular, for LLM-enabled AD, we employ the Euclidean\ndistance to identify previously explored AD experiences, and then LLMs can\nlearn from past good and bad decisions for further improvement. Then, LLM-based\nAD decisions will become part of states in V2I problems, and DDQN will optimize\nthe V2I decisions accordingly. After that, the AD and V2I decisions are\niteratively optimized until convergence. Such an iterative optimization\napproach can better explore the interactions between LLMs and conventional\nreinforcement learning techniques, revealing the potential of using LLMs for\nnetwork optimization and management. Finally, the simulations demonstrate that\nour proposed hybrid LLM-DDQN approach outperforms the conventional DDQN\nalgorithm, showing faster convergence and higher average rewards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have received considerable interest recently due\nto their outstanding reasoning and comprehension capabilities. This work\nexplores applying LLMs to vehicular networks, aiming to jointly optimize\nvehicle-to-infrastructure (V2I) communications and autonomous driving (AD)\npolicies. We deploy LLMs for AD decision-making to maximize traffic flow and\navoid collisions for road safety, and a double deep Q-learning algorithm (DDQN)\nis used for V2I optimization to maximize the received data rate and reduce\nfrequent handovers. In particular, for LLM-enabled AD, we employ the Euclidean\ndistance to identify previously explored AD experiences, and then LLMs can\nlearn from past good and bad decisions for further improvement. Then, LLM-based\nAD decisions will become part of states in V2I problems, and DDQN will optimize\nthe V2I decisions accordingly. After that, the AD and V2I decisions are\niteratively optimized until convergence. Such an iterative optimization\napproach can better explore the interactions between LLMs and conventional\nreinforcement learning techniques, revealing the potential of using LLMs for\nnetwork optimization and management. Finally, the simulations demonstrate that\nour proposed hybrid LLM-DDQN approach outperforms the conventional DDQN\nalgorithm, showing faster convergence and higher average rewards."
                },
                "authors": [
                    {
                        "name": "Zijiang Yan"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Hina Tabassum"
                    },
                    {
                        "name": "Xue Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xue Liu"
                },
                "author": "Xue Liu",
                "arxiv_comment": "Submission for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08852v1",
                "updated": "2024-10-11T14:27:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    27,
                    56,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T14:27:56Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    27,
                    56,
                    4,
                    285,
                    0
                ],
                "title": "Conformalized Interactive Imitation Learning: Handling Expert Shift and\n  Intermittent Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformalized Interactive Imitation Learning: Handling Expert Shift and\n  Intermittent Feedback"
                },
                "summary": "In interactive imitation learning (IL), uncertainty quantification offers a\nway for the learner (i.e. robot) to contend with distribution shifts\nencountered during deployment by actively seeking additional feedback from an\nexpert (i.e. human) online. Prior works use mechanisms like ensemble\ndisagreement or Monte Carlo dropout to quantify when black-box IL policies are\nuncertain; however, these approaches can lead to overconfident estimates when\nfaced with deployment-time distribution shifts. Instead, we contend that we\nneed uncertainty quantification algorithms that can leverage the expert human\nfeedback received during deployment time to adapt the robot's uncertainty\nonline. To tackle this, we draw upon online conformal prediction, a\ndistribution-free method for constructing prediction intervals online given a\nstream of ground-truth labels. Human labels, however, are intermittent in the\ninteractive IL setting. Thus, from the conformal prediction side, we introduce\na novel uncertainty quantification algorithm called intermittent quantile\ntracking (IQT) that leverages a probabilistic model of intermittent labels,\nmaintains asymptotic coverage guarantees, and empirically achieves desired\ncoverage levels. From the interactive IL side, we develop ConformalDAgger, a\nnew approach wherein the robot uses prediction intervals calibrated by IQT as a\nreliable measure of deployment-time uncertainty to actively query for more\nexpert feedback. We compare ConformalDAgger to prior uncertainty-aware DAgger\nmethods in scenarios where the distribution shift is (and isn't) present\nbecause of changes in the expert's policy. We find that in simulated and\nhardware deployments on a 7DOF robotic manipulator, ConformalDAgger detects\nhigh uncertainty when the expert shifts and increases the number of\ninterventions compared to baselines, allowing the robot to more quickly learn\nthe new behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In interactive imitation learning (IL), uncertainty quantification offers a\nway for the learner (i.e. robot) to contend with distribution shifts\nencountered during deployment by actively seeking additional feedback from an\nexpert (i.e. human) online. Prior works use mechanisms like ensemble\ndisagreement or Monte Carlo dropout to quantify when black-box IL policies are\nuncertain; however, these approaches can lead to overconfident estimates when\nfaced with deployment-time distribution shifts. Instead, we contend that we\nneed uncertainty quantification algorithms that can leverage the expert human\nfeedback received during deployment time to adapt the robot's uncertainty\nonline. To tackle this, we draw upon online conformal prediction, a\ndistribution-free method for constructing prediction intervals online given a\nstream of ground-truth labels. Human labels, however, are intermittent in the\ninteractive IL setting. Thus, from the conformal prediction side, we introduce\na novel uncertainty quantification algorithm called intermittent quantile\ntracking (IQT) that leverages a probabilistic model of intermittent labels,\nmaintains asymptotic coverage guarantees, and empirically achieves desired\ncoverage levels. From the interactive IL side, we develop ConformalDAgger, a\nnew approach wherein the robot uses prediction intervals calibrated by IQT as a\nreliable measure of deployment-time uncertainty to actively query for more\nexpert feedback. We compare ConformalDAgger to prior uncertainty-aware DAgger\nmethods in scenarios where the distribution shift is (and isn't) present\nbecause of changes in the expert's policy. We find that in simulated and\nhardware deployments on a 7DOF robotic manipulator, ConformalDAgger detects\nhigh uncertainty when the expert shifts and increases the number of\ninterventions compared to baselines, allowing the robot to more quickly learn\nthe new behavior."
                },
                "authors": [
                    {
                        "name": "Michelle Zhao"
                    },
                    {
                        "name": "Reid Simmons"
                    },
                    {
                        "name": "Henny Admoni"
                    },
                    {
                        "name": "Aaditya Ramdas"
                    },
                    {
                        "name": "Andrea Bajcsy"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Bajcsy"
                },
                "author": "Andrea Bajcsy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08851v1",
                "updated": "2024-10-11T14:27:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    27,
                    18,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T14:27:18Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    27,
                    18,
                    4,
                    285,
                    0
                ],
                "title": "Measuring the Inconsistency of Large Language Models in Preferential\n  Ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring the Inconsistency of Large Language Models in Preferential\n  Ranking"
                },
                "summary": "Despite large language models' (LLMs) recent advancements, their bias and\nhallucination issues persist, and their ability to offer consistent\npreferential rankings remains underexplored. This study investigates the\ncapacity of LLMs to provide consistent ordinal preferences, a crucial aspect in\nscenarios with dense decision space or lacking absolute answers. We introduce a\nformalization of consistency based on order theory, outlining criteria such as\ntransitivity, asymmetry, reversibility, and independence from irrelevant\nalternatives. Our diagnostic experiments on selected state-of-the-art LLMs\nreveal their inability to meet these criteria, indicating a strong positional\nbias and poor transitivity, with preferences easily swayed by irrelevant\nalternatives. These findings highlight a significant inconsistency in\nLLM-generated preferential rankings, underscoring the need for further research\nto address these limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite large language models' (LLMs) recent advancements, their bias and\nhallucination issues persist, and their ability to offer consistent\npreferential rankings remains underexplored. This study investigates the\ncapacity of LLMs to provide consistent ordinal preferences, a crucial aspect in\nscenarios with dense decision space or lacking absolute answers. We introduce a\nformalization of consistency based on order theory, outlining criteria such as\ntransitivity, asymmetry, reversibility, and independence from irrelevant\nalternatives. Our diagnostic experiments on selected state-of-the-art LLMs\nreveal their inability to meet these criteria, indicating a strong positional\nbias and poor transitivity, with preferences easily swayed by irrelevant\nalternatives. These findings highlight a significant inconsistency in\nLLM-generated preferential rankings, underscoring the need for further research\nto address these limitations."
                },
                "authors": [
                    {
                        "name": "Xiutian Zhao"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Wei Peng"
                    }
                ],
                "author_detail": {
                    "name": "Wei Peng"
                },
                "author": "Wei Peng",
                "arxiv_doi": "10.18653/v1/2024.knowllm-1.14",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.knowllm-1.14",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.08851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 1st Workshop on Towards Knowledgeable Language\n  Models (KnowLLM 2024)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08829v1",
                "updated": "2024-10-11T14:07:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    7,
                    57,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T14:07:57Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    7,
                    57,
                    4,
                    285,
                    0
                ],
                "title": "Unveiling Molecular Secrets: An LLM-Augmented Linear Model for\n  Explainable and Calibratable Molecular Property Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Molecular Secrets: An LLM-Augmented Linear Model for\n  Explainable and Calibratable Molecular Property Prediction"
                },
                "summary": "Explainable molecular property prediction is essential for various scientific\nfields, such as drug discovery and material science. Despite delivering\nintrinsic explainability, linear models struggle with capturing complex,\nnon-linear patterns. Large language models (LLMs), on the other hand, yield\naccurate predictions through powerful inference capabilities yet fail to\nprovide chemically meaningful explanations for their predictions. This work\nproposes a novel framework, called MoleX, which leverages LLM knowledge to\nbuild a simple yet powerful linear model for accurate molecular property\nprediction with faithful explanations. The core of MoleX is to model\ncomplicated molecular structure-property relationships using a simple linear\nmodel, augmented by LLM knowledge and a crafted calibration strategy.\nSpecifically, to extract the maximum amount of task-relevant knowledge from LLM\nembeddings, we employ information bottleneck-inspired fine-tuning and\nsparsity-inducing dimensionality reduction. These informative embeddings are\nthen used to fit a linear model for explainable inference. Moreover, we\nintroduce residual calibration to address prediction errors stemming from\nlinear models' insufficient expressiveness of complex LLM embeddings, thus\nrecovering the LLM's predictive power and boosting overall accuracy.\nTheoretically, we provide a mathematical foundation to justify MoleX's\nexplainability. Extensive experiments demonstrate that MoleX outperforms\nexisting methods in molecular property prediction, establishing a new milestone\nin predictive performance, explainability, and efficiency. In particular, MoleX\nenables CPU inference and accelerates large-scale dataset processing, achieving\ncomparable performance 300x faster with 100,000 fewer parameters than LLMs.\nAdditionally, the calibration improves model performance by up to 12.7% without\ncompromising explainability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable molecular property prediction is essential for various scientific\nfields, such as drug discovery and material science. Despite delivering\nintrinsic explainability, linear models struggle with capturing complex,\nnon-linear patterns. Large language models (LLMs), on the other hand, yield\naccurate predictions through powerful inference capabilities yet fail to\nprovide chemically meaningful explanations for their predictions. This work\nproposes a novel framework, called MoleX, which leverages LLM knowledge to\nbuild a simple yet powerful linear model for accurate molecular property\nprediction with faithful explanations. The core of MoleX is to model\ncomplicated molecular structure-property relationships using a simple linear\nmodel, augmented by LLM knowledge and a crafted calibration strategy.\nSpecifically, to extract the maximum amount of task-relevant knowledge from LLM\nembeddings, we employ information bottleneck-inspired fine-tuning and\nsparsity-inducing dimensionality reduction. These informative embeddings are\nthen used to fit a linear model for explainable inference. Moreover, we\nintroduce residual calibration to address prediction errors stemming from\nlinear models' insufficient expressiveness of complex LLM embeddings, thus\nrecovering the LLM's predictive power and boosting overall accuracy.\nTheoretically, we provide a mathematical foundation to justify MoleX's\nexplainability. Extensive experiments demonstrate that MoleX outperforms\nexisting methods in molecular property prediction, establishing a new milestone\nin predictive performance, explainability, and efficiency. In particular, MoleX\nenables CPU inference and accelerates large-scale dataset processing, achieving\ncomparable performance 300x faster with 100,000 fewer parameters than LLMs.\nAdditionally, the calibration improves model performance by up to 12.7% without\ncompromising explainability."
                },
                "authors": [
                    {
                        "name": "Zhuoran Li"
                    },
                    {
                        "name": "Xu Sun"
                    },
                    {
                        "name": "Wanyu Lin"
                    },
                    {
                        "name": "Jiannong Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jiannong Cao"
                },
                "author": "Jiannong Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08821v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08821v1",
                "updated": "2024-10-11T14:03:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    3,
                    29,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T14:03:29Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    3,
                    29,
                    4,
                    285,
                    0
                ],
                "title": "Retriever-and-Memory: Towards Adaptive Note-Enhanced Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retriever-and-Memory: Towards Adaptive Note-Enhanced Retrieval-Augmented\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) mitigates issues of the factual errors\nand hallucinated outputs generated by Large Language Models (LLMs) in\nopen-domain question-answering tasks (OpenQA) via introducing external\nknowledge. For complex QA, however, existing RAG methods use LLMs to actively\npredict retrieval timing and directly use the retrieved information for\ngeneration, regardless of whether the retrieval timing accurately reflects the\nactual information needs, or sufficiently considers prior retrieved knowledge,\nwhich may result in insufficient information gathering and interaction,\nyielding low-quality answers. To address these, we propose a generic RAG\napproach called Adaptive Note-Enhanced RAG (Adaptive-Note) for complex QA\ntasks, which includes the iterative information collector, adaptive memory\nreviewer, and task-oriented generator, while following a new\nRetriever-and-Memory paradigm. Specifically, Adaptive-Note introduces an\noverarching view of knowledge growth, iteratively gathering new information in\nthe form of notes and updating them into the existing optimal knowledge\nstructure, enhancing high-quality knowledge interactions. In addition, we\nemploy an adaptive, note-based stop-exploration strategy to decide \"what to\nretrieve and when to stop\" to encourage sufficient knowledge exploration. We\nconduct extensive experiments on five complex QA datasets, and the results\ndemonstrate the superiority and effectiveness of our method and its components.\nThe code and data are at https://github.com/thunlp/Adaptive-Note.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) mitigates issues of the factual errors\nand hallucinated outputs generated by Large Language Models (LLMs) in\nopen-domain question-answering tasks (OpenQA) via introducing external\nknowledge. For complex QA, however, existing RAG methods use LLMs to actively\npredict retrieval timing and directly use the retrieved information for\ngeneration, regardless of whether the retrieval timing accurately reflects the\nactual information needs, or sufficiently considers prior retrieved knowledge,\nwhich may result in insufficient information gathering and interaction,\nyielding low-quality answers. To address these, we propose a generic RAG\napproach called Adaptive Note-Enhanced RAG (Adaptive-Note) for complex QA\ntasks, which includes the iterative information collector, adaptive memory\nreviewer, and task-oriented generator, while following a new\nRetriever-and-Memory paradigm. Specifically, Adaptive-Note introduces an\noverarching view of knowledge growth, iteratively gathering new information in\nthe form of notes and updating them into the existing optimal knowledge\nstructure, enhancing high-quality knowledge interactions. In addition, we\nemploy an adaptive, note-based stop-exploration strategy to decide \"what to\nretrieve and when to stop\" to encourage sufficient knowledge exploration. We\nconduct extensive experiments on five complex QA datasets, and the results\ndemonstrate the superiority and effectiveness of our method and its components.\nThe code and data are at https://github.com/thunlp/Adaptive-Note."
                },
                "authors": [
                    {
                        "name": "Ruobing Wang"
                    },
                    {
                        "name": "Daren Zha"
                    },
                    {
                        "name": "Shi Yu"
                    },
                    {
                        "name": "Qingfei Zhao"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Zhenghao Liu"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "15 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08821v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08821v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08820v1",
                "updated": "2024-10-11T14:02:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    2,
                    42,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T14:02:42Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    2,
                    42,
                    4,
                    285,
                    0
                ],
                "title": "Which Demographics do LLMs Default to During Annotation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Demographics do LLMs Default to During Annotation?"
                },
                "summary": "Demographics and cultural background of annotators influence the labels they\nassign in text annotation -- for instance, an elderly woman might find it\noffensive to read a message addressed to a \"bro\", but a male teenager might\nfind it appropriate. It is therefore important to acknowledge label variations\nto not under-represent members of a society. Two research directions developed\nout of this observation in the context of using large language models (LLM) for\ndata annotations, namely (1) studying biases and inherent knowledge of LLMs and\n(2) injecting diversity in the output by manipulating the prompt with\ndemographic information. We combine these two strands of research and ask the\nquestion to which demographics an LLM resorts to when no demographics is given.\nTo answer this question, we evaluate which attributes of human annotators LLMs\ninherently mimic. Furthermore, we compare non-demographic conditioned prompts\nand placebo-conditioned prompts (e.g., \"you are an annotator who lives in house\nnumber 5\") to demographics-conditioned prompts (\"You are a 45 year old man and\nan expert on politeness annotation. How do you rate {instance}\"). We study\nthese questions for politeness and offensiveness annotations on the POPQUORN\ndata set, a corpus created in a controlled manner to investigate human label\nvariations based on demographics which has not been used for LLM-based analyses\nso far. We observe notable influences related to gender, race, and age in\ndemographic prompting, which contrasts with previous studies that found no such\neffects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demographics and cultural background of annotators influence the labels they\nassign in text annotation -- for instance, an elderly woman might find it\noffensive to read a message addressed to a \"bro\", but a male teenager might\nfind it appropriate. It is therefore important to acknowledge label variations\nto not under-represent members of a society. Two research directions developed\nout of this observation in the context of using large language models (LLM) for\ndata annotations, namely (1) studying biases and inherent knowledge of LLMs and\n(2) injecting diversity in the output by manipulating the prompt with\ndemographic information. We combine these two strands of research and ask the\nquestion to which demographics an LLM resorts to when no demographics is given.\nTo answer this question, we evaluate which attributes of human annotators LLMs\ninherently mimic. Furthermore, we compare non-demographic conditioned prompts\nand placebo-conditioned prompts (e.g., \"you are an annotator who lives in house\nnumber 5\") to demographics-conditioned prompts (\"You are a 45 year old man and\nan expert on politeness annotation. How do you rate {instance}\"). We study\nthese questions for politeness and offensiveness annotations on the POPQUORN\ndata set, a corpus created in a controlled manner to investigate human label\nvariations based on demographics which has not been used for LLM-based analyses\nso far. We observe notable influences related to gender, race, and age in\ndemographic prompting, which contrasts with previous studies that found no such\neffects."
                },
                "authors": [
                    {
                        "name": "Christopher Bagdon"
                    },
                    {
                        "name": "Aidan Combs"
                    },
                    {
                        "name": "Lynn Greschner"
                    },
                    {
                        "name": "Roman Klinger"
                    },
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Sean Papay"
                    },
                    {
                        "name": "Nadine Probol"
                    },
                    {
                        "name": "Yarik Menchaca Resendiz"
                    },
                    {
                        "name": "Johannes Schäfer"
                    },
                    {
                        "name": "Aswathy Velutharambath"
                    },
                    {
                        "name": "Sabine Weber"
                    },
                    {
                        "name": "Amelie Wührl"
                    }
                ],
                "author_detail": {
                    "name": "Amelie Wührl"
                },
                "author": "Amelie Wührl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08815v1",
                "updated": "2024-10-11T13:52:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    52,
                    44,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T13:52:44Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    52,
                    44,
                    4,
                    285,
                    0
                ],
                "title": "StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via\n  Inference-time Hybrid Information Structurization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via\n  Inference-time Hybrid Information Structurization"
                },
                "summary": "Retrieval-augmented generation (RAG) is a key means to effectively enhance\nlarge language models (LLMs) in many knowledge-based tasks. However, existing\nRAG methods struggle with knowledge-intensive reasoning tasks, because useful\ninformation required to these tasks are badly scattered. This characteristic\nmakes it difficult for existing RAG methods to accurately identify key\ninformation and perform global reasoning with such noisy augmentation. In this\npaper, motivated by the cognitive theories that humans convert raw information\ninto various structured knowledge when tackling knowledge-intensive reasoning,\nwe proposes a new framework, StructRAG, which can identify the optimal\nstructure type for the task at hand, reconstruct original documents into this\nstructured format, and infer answers based on the resulting structure.\nExtensive experiments across various knowledge-intensive tasks show that\nStructRAG achieves state-of-the-art performance, particularly excelling in\nchallenging scenarios, demonstrating its potential as an effective solution for\nenhancing LLMs in complex real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) is a key means to effectively enhance\nlarge language models (LLMs) in many knowledge-based tasks. However, existing\nRAG methods struggle with knowledge-intensive reasoning tasks, because useful\ninformation required to these tasks are badly scattered. This characteristic\nmakes it difficult for existing RAG methods to accurately identify key\ninformation and perform global reasoning with such noisy augmentation. In this\npaper, motivated by the cognitive theories that humans convert raw information\ninto various structured knowledge when tackling knowledge-intensive reasoning,\nwe proposes a new framework, StructRAG, which can identify the optimal\nstructure type for the task at hand, reconstruct original documents into this\nstructured format, and infer answers based on the resulting structure.\nExtensive experiments across various knowledge-intensive tasks show that\nStructRAG achieves state-of-the-art performance, particularly excelling in\nchallenging scenarios, demonstrating its potential as an effective solution for\nenhancing LLMs in complex real-world applications."
                },
                "authors": [
                    {
                        "name": "Zhuoqun Li"
                    },
                    {
                        "name": "Xuanang Chen"
                    },
                    {
                        "name": "Haiyang Yu"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Qiaoyu Tang"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Le Sun"
                    },
                    {
                        "name": "Yongbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Li"
                },
                "author": "Yongbin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08811v1",
                "updated": "2024-10-11T13:50:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    50,
                    50,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T13:50:50Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    50,
                    50,
                    4,
                    285,
                    0
                ],
                "title": "PoisonBench: Assessing Large Language Model Vulnerability to Data\n  Poisoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoisonBench: Assessing Large Language Model Vulnerability to Data\n  Poisoning"
                },
                "summary": "Preference learning is a central component for aligning current LLMs, but\nthis process can be vulnerable to data poisoning attacks. To address this\nconcern, we introduce PoisonBench, a benchmark for evaluating large language\nmodels' susceptibility to data poisoning during preference learning. Data\npoisoning attacks can manipulate large language model responses to include\nhidden malicious content or biases, potentially causing the model to generate\nharmful or unintended outputs while appearing to function normally. We deploy\ntwo distinct attack types across eight realistic scenarios, assessing 21\nwidely-used models. Our findings reveal concerning trends: (1) Scaling up\nparameter size does not inherently enhance resilience against poisoning\nattacks; (2) There exists a log-linear relationship between the effects of the\nattack and the data poison ratio; (3) The effect of data poisoning can\ngeneralize to extrapolated triggers that are not included in the poisoned data.\nThese results expose weaknesses in current preference learning techniques,\nhighlighting the urgent need for more robust defenses against malicious models\nand data manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference learning is a central component for aligning current LLMs, but\nthis process can be vulnerable to data poisoning attacks. To address this\nconcern, we introduce PoisonBench, a benchmark for evaluating large language\nmodels' susceptibility to data poisoning during preference learning. Data\npoisoning attacks can manipulate large language model responses to include\nhidden malicious content or biases, potentially causing the model to generate\nharmful or unintended outputs while appearing to function normally. We deploy\ntwo distinct attack types across eight realistic scenarios, assessing 21\nwidely-used models. Our findings reveal concerning trends: (1) Scaling up\nparameter size does not inherently enhance resilience against poisoning\nattacks; (2) There exists a log-linear relationship between the effects of the\nattack and the data poison ratio; (3) The effect of data poisoning can\ngeneralize to extrapolated triggers that are not included in the poisoned data.\nThese results expose weaknesses in current preference learning techniques,\nhighlighting the urgent need for more robust defenses against malicious models\nand data manipulation."
                },
                "authors": [
                    {
                        "name": "Tingchen Fu"
                    },
                    {
                        "name": "Mrinank Sharma"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Shay B. Cohen"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Fazl Barez"
                    }
                ],
                "author_detail": {
                    "name": "Fazl Barez"
                },
                "author": "Fazl Barez",
                "arxiv_comment": "Tingchen Fu and Fazl Barez are core research contributors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07381v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07381v2",
                "updated": "2024-10-11T13:49:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    49,
                    16,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-09T18:59:49Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    18,
                    59,
                    49,
                    2,
                    283,
                    0
                ],
                "title": "Tally: Non-Intrusive Performance Isolation for Concurrent Deep Learning\n  Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tally: Non-Intrusive Performance Isolation for Concurrent Deep Learning\n  Workloads"
                },
                "summary": "GPU underutilization is a significant concern in many production deep\nlearning clusters, leading to prolonged job queues and increased operational\nexpenses. A promising solution to this inefficiency is GPU sharing, which\nimproves resource utilization by allowing multiple workloads to execute\nconcurrently on a single GPU. However, the practical deployment of GPU sharing\nin production settings faces critical obstacles due to the limitations of\nexisting mechanisms, such as high integration costs, inadequate performance\nisolation, and limited application compatibility. To address these issues, we\nintroduce \\emph{Tally}, a non-intrusive GPU sharing mechanism that provides\nrobust performance isolation and comprehensive workload compatibility. Tally\noperates as a virtualization layer between applications and GPUs, transparently\norchestrating the device execution of concurrent workloads. The key to Tally's\nrobust performance isolation capability lies in its fine-grained thread-block\nlevel GPU kernel scheduling strategy, which allows the system to effectively\nmitigate interference caused by workload co-execution. Our evaluation,\nconducted on a diverse set of workload combinations, demonstrates that Tally on\naverage incurs a mere $7.2\\%$ overhead on the $99^{th}$-percentile latency of\nhigh-priority inference tasks when executed concurrently with best-effort\ntraining workloads compared to $188.9\\%$ overhead exhibited by the\nstate-of-the-art GPU sharing systems like TGS, while achieving over $80\\%$ of\nTGS's system throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU underutilization is a significant concern in many production deep\nlearning clusters, leading to prolonged job queues and increased operational\nexpenses. A promising solution to this inefficiency is GPU sharing, which\nimproves resource utilization by allowing multiple workloads to execute\nconcurrently on a single GPU. However, the practical deployment of GPU sharing\nin production settings faces critical obstacles due to the limitations of\nexisting mechanisms, such as high integration costs, inadequate performance\nisolation, and limited application compatibility. To address these issues, we\nintroduce \\emph{Tally}, a non-intrusive GPU sharing mechanism that provides\nrobust performance isolation and comprehensive workload compatibility. Tally\noperates as a virtualization layer between applications and GPUs, transparently\norchestrating the device execution of concurrent workloads. The key to Tally's\nrobust performance isolation capability lies in its fine-grained thread-block\nlevel GPU kernel scheduling strategy, which allows the system to effectively\nmitigate interference caused by workload co-execution. Our evaluation,\nconducted on a diverse set of workload combinations, demonstrates that Tally on\naverage incurs a mere $7.2\\%$ overhead on the $99^{th}$-percentile latency of\nhigh-priority inference tasks when executed concurrently with best-effort\ntraining workloads compared to $188.9\\%$ overhead exhibited by the\nstate-of-the-art GPU sharing systems like TGS, while achieving over $80\\%$ of\nTGS's system throughput."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Anand Jayarajan"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    }
                ],
                "author_detail": {
                    "name": "Gennady Pekhimenko"
                },
                "author": "Gennady Pekhimenko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07381v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07381v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08806v1",
                "updated": "2024-10-11T13:45:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    45,
                    16,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T13:45:16Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    45,
                    16,
                    4,
                    285,
                    0
                ],
                "title": "Don't Transform the Code, Code the Transforms: Towards Precise Code\n  Rewriting using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Transform the Code, Code the Transforms: Towards Precise Code\n  Rewriting using LLMs"
                },
                "summary": "Tools for rewriting, refactoring and optimizing code should be fast and\ncorrect. Large language models (LLMs), by their nature, possess neither of\nthese qualities. Yet, there remains tremendous opportunity in using LLMs to\nimprove code.\n  We explore the use of LLMs not to transform code, but to code transforms. We\npropose a chain-of-thought approach to synthesizing code transformations from a\nsmall number of input/output code examples that incorporates execution and\nfeedback. Unlike the direct rewrite approach, LLM-generated transformations are\neasy to inspect, debug, and validate. The logic of the rewrite is explicitly\ncoded and easy to adapt. The compute required to run code transformations is\nminute compared to that of LLM rewriting.\n  We test our approach on 16 Python code transformations and find that LLM-\ngenerated transforms are perfectly precise for 7 of them and less imprecise\nthan direct LLM rewriting on the others. We hope to encourage further research\nto improving the precision of LLM code rewriting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tools for rewriting, refactoring and optimizing code should be fast and\ncorrect. Large language models (LLMs), by their nature, possess neither of\nthese qualities. Yet, there remains tremendous opportunity in using LLMs to\nimprove code.\n  We explore the use of LLMs not to transform code, but to code transforms. We\npropose a chain-of-thought approach to synthesizing code transformations from a\nsmall number of input/output code examples that incorporates execution and\nfeedback. Unlike the direct rewrite approach, LLM-generated transformations are\neasy to inspect, debug, and validate. The logic of the rewrite is explicitly\ncoded and easy to adapt. The compute required to run code transformations is\nminute compared to that of LLM rewriting.\n  We test our approach on 16 Python code transformations and find that LLM-\ngenerated transforms are perfectly precise for 7 of them and less imprecise\nthan direct LLM rewriting on the others. We hope to encourage further research\nto improving the precision of LLM code rewriting."
                },
                "authors": [
                    {
                        "name": "Chris Cummins"
                    },
                    {
                        "name": "Volker Seeker"
                    },
                    {
                        "name": "Jordi Armengol-Estapé"
                    },
                    {
                        "name": "Aram H. Markosyan"
                    },
                    {
                        "name": "Gabriel Synnaeve"
                    },
                    {
                        "name": "Hugh Leather"
                    }
                ],
                "author_detail": {
                    "name": "Hugh Leather"
                },
                "author": "Hugh Leather",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09656v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09656v3",
                "updated": "2024-10-11T13:42:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    42,
                    12,
                    4,
                    285,
                    0
                ],
                "published": "2024-04-15T10:44:31Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    10,
                    44,
                    31,
                    0,
                    106,
                    0
                ],
                "title": "Learn Your Reference Model for Real Good Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learn Your Reference Model for Real Good Alignment"
                },
                "summary": "Despite the fact that offline methods for Large Language Models (LLMs)\nalignment do not require a direct reward model, they remain susceptible to\noveroptimization. This issue arises when the trained model deviates excessively\nfrom the reference policy, leading to a decrease in sample quality. We propose\na new paradigm of offline alignment methods, called Trust Region (including\nvariants TR-DPO, TR-IPO, TR-KTO), which dynamically updates the reference\npolicy throughout the training process. Our results show that TR alignment\nmethods effectively mitigate overoptimization, enabling models to maintain\nstrong performance even when substantially deviating from the initial reference\npolicy. We demonstrate the efficacy of these approaches not only through toy\nexamples that exhibit reduced overoptimization, but also through direct,\nside-by-side comparisons in specific tasks such as helpful and harmless\ndialogue, as well as summarization, where they surpass conventional methods.\nAdditionally, we report significant improvements in general-purpose assistant\nsetups with the Llama3 model on the AlpacaEval 2 and Arena-Hard benchmarks,\nhighlighting the advantages of Trust Region methods over classical approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the fact that offline methods for Large Language Models (LLMs)\nalignment do not require a direct reward model, they remain susceptible to\noveroptimization. This issue arises when the trained model deviates excessively\nfrom the reference policy, leading to a decrease in sample quality. We propose\na new paradigm of offline alignment methods, called Trust Region (including\nvariants TR-DPO, TR-IPO, TR-KTO), which dynamically updates the reference\npolicy throughout the training process. Our results show that TR alignment\nmethods effectively mitigate overoptimization, enabling models to maintain\nstrong performance even when substantially deviating from the initial reference\npolicy. We demonstrate the efficacy of these approaches not only through toy\nexamples that exhibit reduced overoptimization, but also through direct,\nside-by-side comparisons in specific tasks such as helpful and harmless\ndialogue, as well as summarization, where they surpass conventional methods.\nAdditionally, we report significant improvements in general-purpose assistant\nsetups with the Llama3 model on the AlpacaEval 2 and Arena-Hard benchmarks,\nhighlighting the advantages of Trust Region methods over classical approaches."
                },
                "authors": [
                    {
                        "name": "Alexey Gorbatovski"
                    },
                    {
                        "name": "Boris Shaposhnikov"
                    },
                    {
                        "name": "Alexey Malakhov"
                    },
                    {
                        "name": "Nikita Surnachev"
                    },
                    {
                        "name": "Yaroslav Aksenov"
                    },
                    {
                        "name": "Ian Maksimov"
                    },
                    {
                        "name": "Nikita Balagansky"
                    },
                    {
                        "name": "Daniil Gavrilov"
                    }
                ],
                "author_detail": {
                    "name": "Daniil Gavrilov"
                },
                "author": "Daniil Gavrilov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09656v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09656v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08800v1",
                "updated": "2024-10-11T13:34:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    34,
                    24,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T13:34:24Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    34,
                    24,
                    4,
                    285,
                    0
                ],
                "title": "Data Processing for the OpenGPT-X Model Family",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Processing for the OpenGPT-X Model Family"
                },
                "summary": "This paper presents a comprehensive overview of the data preparation pipeline\ndeveloped for the OpenGPT-X project, a large-scale initiative aimed at creating\nopen and high-performance multilingual large language models (LLMs). The\nproject goal is to deliver models that cover all major European languages, with\na particular focus on real-world applications within the European Union. We\nexplain all data processing steps, starting with the data selection and\nrequirement definition to the preparation of the final datasets for model\ntraining. We distinguish between curated data and web data, as each of these\ncategories is handled by distinct pipelines, with curated data undergoing\nminimal filtering and web data requiring extensive filtering and deduplication.\nThis distinction guided the development of specialized algorithmic solutions\nfor both pipelines. In addition to describing the processing methodologies, we\nprovide an in-depth analysis of the datasets, increasing transparency and\nalignment with European data regulations. Finally, we share key insights and\nchallenges faced during the project, offering recommendations for future\nendeavors in large-scale multilingual data preparation for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive overview of the data preparation pipeline\ndeveloped for the OpenGPT-X project, a large-scale initiative aimed at creating\nopen and high-performance multilingual large language models (LLMs). The\nproject goal is to deliver models that cover all major European languages, with\na particular focus on real-world applications within the European Union. We\nexplain all data processing steps, starting with the data selection and\nrequirement definition to the preparation of the final datasets for model\ntraining. We distinguish between curated data and web data, as each of these\ncategories is handled by distinct pipelines, with curated data undergoing\nminimal filtering and web data requiring extensive filtering and deduplication.\nThis distinction guided the development of specialized algorithmic solutions\nfor both pipelines. In addition to describing the processing methodologies, we\nprovide an in-depth analysis of the datasets, increasing transparency and\nalignment with European data regulations. Finally, we share key insights and\nchallenges faced during the project, offering recommendations for future\nendeavors in large-scale multilingual data preparation for LLMs."
                },
                "authors": [
                    {
                        "name": "Nicolo' Brandizzi"
                    },
                    {
                        "name": "Hammam Abdelwahab"
                    },
                    {
                        "name": "Anirban Bhowmick"
                    },
                    {
                        "name": "Lennard Helmer"
                    },
                    {
                        "name": "Benny Jörg Stein"
                    },
                    {
                        "name": "Pavel Denisov"
                    },
                    {
                        "name": "Qasid Saleem"
                    },
                    {
                        "name": "Michael Fromm"
                    },
                    {
                        "name": "Mehdi Ali"
                    },
                    {
                        "name": "Richard Rutmann"
                    },
                    {
                        "name": "Farzad Naderi"
                    },
                    {
                        "name": "Mohamad Saif Agy"
                    },
                    {
                        "name": "Alexander Schwirjow"
                    },
                    {
                        "name": "Fabian Küch"
                    },
                    {
                        "name": "Luzian Hahn"
                    },
                    {
                        "name": "Malte Ostendorff"
                    },
                    {
                        "name": "Pedro Ortiz Suarez"
                    },
                    {
                        "name": "Georg Rehm"
                    },
                    {
                        "name": "Dennis Wegener"
                    },
                    {
                        "name": "Nicolas Flores-Herr"
                    },
                    {
                        "name": "Joachim Köhler"
                    },
                    {
                        "name": "Johannes Leveling"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Leveling"
                },
                "author": "Johannes Leveling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.1; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08799v1",
                "updated": "2024-10-11T13:33:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    33,
                    53,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T13:33:53Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    33,
                    53,
                    4,
                    285,
                    0
                ],
                "title": "Online Learning for Intelligent Thermal Management of\n  Interference-coupled and Passively Cooled Base Stations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Learning for Intelligent Thermal Management of\n  Interference-coupled and Passively Cooled Base Stations"
                },
                "summary": "Passively cooled base stations (PCBSs) have emerged to deliver better cost\nand energy efficiency. However, passive cooling necessitates intelligent\nthermal control via traffic management, i.e., the instantaneous data traffic or\nthroughput of a PCBS directly impacts its thermal performance. This is\nparticularly challenging for outdoor deployment of PCBSs because the heat\ndissipation efficiency is uncertain and fluctuates over time. What is more, the\nPCBSs are interference-coupled in multi-cell scenarios. Thus, a\nhigher-throughput PCBS leads to higher interference to the other PCBSs, which,\nin turn, would require more resource consumption to meet their respective\nthroughput targets. In this paper, we address online decision-making for\nmaximizing the total downlink throughput for a multi-PCBS system subject to\nconstraints related on operating temperature. We demonstrate that a\nreinforcement learning (RL) approach, specifically soft actor-critic (SAC), can\nsuccessfully perform throughput maximization while keeping the PCBSs cool, by\nadapting the throughput to time-varying heat dissipation conditions.\nFurthermore, we design a denial and reward mechanism that effectively mitigates\nthe risk of overheating during the exploration phase of RL. Simulation results\nshow that our approach achieves up to 88.6% of the global optimum. This is very\npromising, as our approach operates without prior knowledge of future heat\ndissipation efficiency, which is required by the global optimum.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Passively cooled base stations (PCBSs) have emerged to deliver better cost\nand energy efficiency. However, passive cooling necessitates intelligent\nthermal control via traffic management, i.e., the instantaneous data traffic or\nthroughput of a PCBS directly impacts its thermal performance. This is\nparticularly challenging for outdoor deployment of PCBSs because the heat\ndissipation efficiency is uncertain and fluctuates over time. What is more, the\nPCBSs are interference-coupled in multi-cell scenarios. Thus, a\nhigher-throughput PCBS leads to higher interference to the other PCBSs, which,\nin turn, would require more resource consumption to meet their respective\nthroughput targets. In this paper, we address online decision-making for\nmaximizing the total downlink throughput for a multi-PCBS system subject to\nconstraints related on operating temperature. We demonstrate that a\nreinforcement learning (RL) approach, specifically soft actor-critic (SAC), can\nsuccessfully perform throughput maximization while keeping the PCBSs cool, by\nadapting the throughput to time-varying heat dissipation conditions.\nFurthermore, we design a denial and reward mechanism that effectively mitigates\nthe risk of overheating during the exploration phase of RL. Simulation results\nshow that our approach achieves up to 88.6% of the global optimum. This is very\npromising, as our approach operates without prior knowledge of future heat\ndissipation efficiency, which is required by the global optimum."
                },
                "authors": [
                    {
                        "name": "Zhanwei Yu"
                    },
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Xiaoli Chu"
                    },
                    {
                        "name": "Di Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Di Yuan"
                },
                "author": "Di Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08791v1",
                "updated": "2024-10-11T13:17:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    17,
                    5,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T13:17:05Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    17,
                    5,
                    4,
                    285,
                    0
                ],
                "title": "Superpipeline: A Universal Approach for Reducing GPU Memory Usage in\n  Large Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superpipeline: A Universal Approach for Reducing GPU Memory Usage in\n  Large Models"
                },
                "summary": "The rapid growth in machine learning models, especially in natural language\nprocessing and computer vision, has led to challenges when running these models\non hardware with limited resources. This paper introduces Superpipeline, a new\nframework designed to optimize the execution of large AI models on constrained\nhardware during both training and inference. Our approach involves dynamically\nmanaging model execution by dividing models into individual layers and\nefficiently transferring these layers between GPU and CPU memory. Superpipeline\nreduces GPU memory usage by up to 60% in our experiments while maintaining\nmodel accuracy and acceptable processing speeds. This allows models that would\notherwise exceed available GPU memory to run effectively. Unlike existing\nsolutions that focus mainly on inference or specific model types, Superpipeline\ncan be applied to large language models (LLMs), vision-language models (VLMs),\nand vision-based models. We tested Superpipeline's performance across various\nmodels and hardware setups. The method includes two key parameters that allow\nfine-tuning the balance between GPU memory use and processing speed.\nImportantly, Superpipeline does not require retraining or changing model\nparameters, ensuring that the original model's output remains unchanged.\nSuperpipeline's simplicity and flexibility make it useful for researchers and\nprofessionals working with advanced AI models on limited hardware. It enables\nthe use of larger models or bigger batch sizes on existing hardware,\npotentially speeding up innovation across many machine learning applications.\nThis work marks an important step toward making advanced AI models more\naccessible and optimizing their deployment in resource-limited environments.\nThe code for Superpipeline is available at\nhttps://github.com/abbasiReza/super-pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth in machine learning models, especially in natural language\nprocessing and computer vision, has led to challenges when running these models\non hardware with limited resources. This paper introduces Superpipeline, a new\nframework designed to optimize the execution of large AI models on constrained\nhardware during both training and inference. Our approach involves dynamically\nmanaging model execution by dividing models into individual layers and\nefficiently transferring these layers between GPU and CPU memory. Superpipeline\nreduces GPU memory usage by up to 60% in our experiments while maintaining\nmodel accuracy and acceptable processing speeds. This allows models that would\notherwise exceed available GPU memory to run effectively. Unlike existing\nsolutions that focus mainly on inference or specific model types, Superpipeline\ncan be applied to large language models (LLMs), vision-language models (VLMs),\nand vision-based models. We tested Superpipeline's performance across various\nmodels and hardware setups. The method includes two key parameters that allow\nfine-tuning the balance between GPU memory use and processing speed.\nImportantly, Superpipeline does not require retraining or changing model\nparameters, ensuring that the original model's output remains unchanged.\nSuperpipeline's simplicity and flexibility make it useful for researchers and\nprofessionals working with advanced AI models on limited hardware. It enables\nthe use of larger models or bigger batch sizes on existing hardware,\npotentially speeding up innovation across many machine learning applications.\nThis work marks an important step toward making advanced AI models more\naccessible and optimizing their deployment in resource-limited environments.\nThe code for Superpipeline is available at\nhttps://github.com/abbasiReza/super-pipeline."
                },
                "authors": [
                    {
                        "name": "Reza Abbasi"
                    },
                    {
                        "name": "Sernam Lim"
                    }
                ],
                "author_detail": {
                    "name": "Sernam Lim"
                },
                "author": "Sernam Lim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08776v1",
                "updated": "2024-10-11T12:49:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    49,
                    5,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T12:49:05Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    49,
                    5,
                    4,
                    285,
                    0
                ],
                "title": "F2A: An Innovative Approach for Prompt Injection by Utilizing Feign\n  Security Detection Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F2A: An Innovative Approach for Prompt Injection by Utilizing Feign\n  Security Detection Agents"
                },
                "summary": "With the rapid development of Large Language Models (LLMs), numerous mature\napplications of LLMs have emerged in the field of content safety detection.\nHowever, we have found that LLMs exhibit blind trust in safety detection\nagents. The general LLMs can be compromised by hackers with this vulnerability.\nHence, this paper proposed an attack named Feign Agent Attack (F2A).Through\nsuch malicious forgery methods, adding fake safety detection results into the\nprompt, the defense mechanism of LLMs can be bypassed, thereby obtaining\nharmful content and hijacking the normal conversation.Continually, a series of\nexperiments were conducted. In these experiments, the hijacking capability of\nF2A on LLMs was analyzed and demonstrated, exploring the fundamental reasons\nwhy LLMs blindly trust safety detection results. The experiments involved\nvarious scenarios where fake safety detection results were injected into\nprompts, and the responses were closely monitored to understand the extent of\nthe vulnerability. Also, this paper provided a reasonable solution to this\nattack, emphasizing that it is important for LLMs to critically evaluate the\nresults of augmented agents to prevent the generating harmful content. By doing\nso, the reliability and security can be significantly improved, protecting the\nLLMs from F2A.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of Large Language Models (LLMs), numerous mature\napplications of LLMs have emerged in the field of content safety detection.\nHowever, we have found that LLMs exhibit blind trust in safety detection\nagents. The general LLMs can be compromised by hackers with this vulnerability.\nHence, this paper proposed an attack named Feign Agent Attack (F2A).Through\nsuch malicious forgery methods, adding fake safety detection results into the\nprompt, the defense mechanism of LLMs can be bypassed, thereby obtaining\nharmful content and hijacking the normal conversation.Continually, a series of\nexperiments were conducted. In these experiments, the hijacking capability of\nF2A on LLMs was analyzed and demonstrated, exploring the fundamental reasons\nwhy LLMs blindly trust safety detection results. The experiments involved\nvarious scenarios where fake safety detection results were injected into\nprompts, and the responses were closely monitored to understand the extent of\nthe vulnerability. Also, this paper provided a reasonable solution to this\nattack, emphasizing that it is important for LLMs to critically evaluate the\nresults of augmented agents to prevent the generating harmful content. By doing\nso, the reliability and security can be significantly improved, protecting the\nLLMs from F2A."
                },
                "authors": [
                    {
                        "name": "Yupeng Ren"
                    }
                ],
                "author_detail": {
                    "name": "Yupeng Ren"
                },
                "author": "Yupeng Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14038v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14038v2",
                "updated": "2024-10-11T12:40:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    40,
                    50,
                    4,
                    285,
                    0
                ],
                "published": "2024-09-21T06:49:34Z",
                "published_parsed": [
                    2024,
                    9,
                    21,
                    6,
                    49,
                    34,
                    5,
                    265,
                    0
                ],
                "title": "OAEI-LLM: A Benchmark Dataset for Understanding Large Language Model\n  Hallucinations in Ontology Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OAEI-LLM: A Benchmark Dataset for Understanding Large Language Model\n  Hallucinations in Ontology Matching"
                },
                "summary": "Hallucinations of large language models (LLMs) commonly occur in\ndomain-specific downstream tasks, with no exception in ontology matching (OM).\nThe prevalence of using LLMs for OM raises the need for benchmarks to better\nunderstand LLM hallucinations. The OAEI-LLM dataset is an extended version of\nthe Ontology Alignment Evaluation Initiative (OAEI) datasets that evaluate\nLLM-specific hallucinations in OM tasks. We outline the methodology used in\ndataset construction and schema extension, and provide examples of potential\nuse cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations of large language models (LLMs) commonly occur in\ndomain-specific downstream tasks, with no exception in ontology matching (OM).\nThe prevalence of using LLMs for OM raises the need for benchmarks to better\nunderstand LLM hallucinations. The OAEI-LLM dataset is an extended version of\nthe Ontology Alignment Evaluation Initiative (OAEI) datasets that evaluate\nLLM-specific hallucinations in OM tasks. We outline the methodology used in\ndataset construction and schema extension, and provide examples of potential\nuse cases."
                },
                "authors": [
                    {
                        "name": "Zhangcheng Qiang"
                    },
                    {
                        "name": "Kerry Taylor"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Jing Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Jiang"
                },
                "author": "Jing Jiang",
                "arxiv_comment": "4 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14038v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14038v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20135v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20135v4",
                "updated": "2024-10-11T12:19:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    57,
                    4,
                    285,
                    0
                ],
                "published": "2024-09-30T09:34:31Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    34,
                    31,
                    0,
                    274,
                    0
                ],
                "title": "Federated Instruction Tuning of LLMs with Domain Coverage Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Instruction Tuning of LLMs with Domain Coverage Augmentation"
                },
                "summary": "Federated Domain-specific Instruction Tuning (FedDIT) utilizes limited\ncross-client private data together with server-side public data for instruction\naugmentation, ultimately boosting model performance within specific domains. To\ndate, the factors affecting FedDIT remain unclear, and existing instruction\naugmentation methods primarily focus on the centralized setting without\nconsidering distributed environments. Our experiments reveal that the\ncross-client domain coverage, rather than data heterogeneity, drives model\nperformance in FedDIT. In response, we propose FedDCA, which optimizes domain\ncoverage through greedy client center selection and retrieval-based\naugmentation. For client-side computational efficiency and system scalability,\nFedDCA$^*$, the variant of FedDCA, utilizes heterogeneous encoders with\nserver-side feature alignment. Extensive experiments across four distinct\ndomains (code, medical, financial, and mathematical) substantiate the\neffectiveness of both methods. Additionally, we investigate privacy\npreservation against memory extraction attacks utilizing various amounts of\npublic data. Results show that there is no significant correlation between the\nvolume of public data and the privacy-preserving capability. However, as the\nfine-tuning rounds increase, the risk of privacy leakage reduces or converges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Domain-specific Instruction Tuning (FedDIT) utilizes limited\ncross-client private data together with server-side public data for instruction\naugmentation, ultimately boosting model performance within specific domains. To\ndate, the factors affecting FedDIT remain unclear, and existing instruction\naugmentation methods primarily focus on the centralized setting without\nconsidering distributed environments. Our experiments reveal that the\ncross-client domain coverage, rather than data heterogeneity, drives model\nperformance in FedDIT. In response, we propose FedDCA, which optimizes domain\ncoverage through greedy client center selection and retrieval-based\naugmentation. For client-side computational efficiency and system scalability,\nFedDCA$^*$, the variant of FedDCA, utilizes heterogeneous encoders with\nserver-side feature alignment. Extensive experiments across four distinct\ndomains (code, medical, financial, and mathematical) substantiate the\neffectiveness of both methods. Additionally, we investigate privacy\npreservation against memory extraction attacks utilizing various amounts of\npublic data. Results show that there is no significant correlation between the\nvolume of public data and the privacy-preserving capability. However, as the\nfine-tuning rounds increase, the risk of privacy leakage reduces or converges."
                },
                "authors": [
                    {
                        "name": "Zezhou Wang"
                    },
                    {
                        "name": "Yaxin Du"
                    },
                    {
                        "name": "Zhuzhong Qian"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20135v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20135v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08755v1",
                "updated": "2024-10-11T12:13:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    13,
                    3,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T12:13:03Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    13,
                    3,
                    4,
                    285,
                    0
                ],
                "title": "PILLAR: an AI-Powered Privacy Threat Modeling Tool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PILLAR: an AI-Powered Privacy Threat Modeling Tool"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has unlocked new\npossibilities for applying artificial intelligence across a wide range of\nfields, including privacy engineering. As modern applications increasingly\nhandle sensitive user data, safeguarding privacy has become more critical than\never. To protect privacy effectively, potential threats need to be identified\nand addressed early in the system development process. Frameworks like LINDDUN\noffer structured approaches for uncovering these risks, but despite their\nvalue, they often demand substantial manual effort, expert input, and detailed\nsystem knowledge. This makes the process time-consuming and prone to errors.\nCurrent privacy threat modeling methods, such as LINDDUN, typically rely on\ncreating and analyzing complex data flow diagrams (DFDs) and system\ndescriptions to pinpoint potential privacy issues. While these approaches are\nthorough, they can be cumbersome, relying heavily on the precision of the data\nprovided by users. Moreover, they often generate a long list of threats without\nclear guidance on how to prioritize them, leaving developers unsure of where to\nfocus their efforts. In response to these challenges, we introduce PILLAR\n(Privacy risk Identification with LINDDUN and LLM Analysis Report), a new tool\nthat integrates LLMs with the LINDDUN framework to streamline and enhance\nprivacy threat modeling. PILLAR automates key parts of the LINDDUN process,\nsuch as generating DFDs, classifying threats, and prioritizing risks. By\nleveraging the capabilities of LLMs, PILLAR can take natural language\ndescriptions of systems and transform them into comprehensive threat models\nwith minimal input from users, reducing the workload on developers and privacy\nexperts while improving the efficiency and accuracy of the process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has unlocked new\npossibilities for applying artificial intelligence across a wide range of\nfields, including privacy engineering. As modern applications increasingly\nhandle sensitive user data, safeguarding privacy has become more critical than\never. To protect privacy effectively, potential threats need to be identified\nand addressed early in the system development process. Frameworks like LINDDUN\noffer structured approaches for uncovering these risks, but despite their\nvalue, they often demand substantial manual effort, expert input, and detailed\nsystem knowledge. This makes the process time-consuming and prone to errors.\nCurrent privacy threat modeling methods, such as LINDDUN, typically rely on\ncreating and analyzing complex data flow diagrams (DFDs) and system\ndescriptions to pinpoint potential privacy issues. While these approaches are\nthorough, they can be cumbersome, relying heavily on the precision of the data\nprovided by users. Moreover, they often generate a long list of threats without\nclear guidance on how to prioritize them, leaving developers unsure of where to\nfocus their efforts. In response to these challenges, we introduce PILLAR\n(Privacy risk Identification with LINDDUN and LLM Analysis Report), a new tool\nthat integrates LLMs with the LINDDUN framework to streamline and enhance\nprivacy threat modeling. PILLAR automates key parts of the LINDDUN process,\nsuch as generating DFDs, classifying threats, and prioritizing risks. By\nleveraging the capabilities of LLMs, PILLAR can take natural language\ndescriptions of systems and transform them into comprehensive threat models\nwith minimal input from users, reducing the workload on developers and privacy\nexperts while improving the efficiency and accuracy of the process."
                },
                "authors": [
                    {
                        "name": "Majid Mollaeefar"
                    },
                    {
                        "name": "Andrea Bissoli"
                    },
                    {
                        "name": "Silvio Ranise"
                    }
                ],
                "author_detail": {
                    "name": "Silvio Ranise"
                },
                "author": "Silvio Ranise",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08782v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08782v2",
                "updated": "2024-10-11T12:04:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    4,
                    11,
                    4,
                    285,
                    0
                ],
                "published": "2024-08-16T14:54:41Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    54,
                    41,
                    4,
                    229,
                    0
                ],
                "title": "EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling\n  MiXed Emotions and Discourse Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling\n  MiXed Emotions and Discourse Dynamics"
                },
                "summary": "Designing emotionally intelligent conversational systems to provide comfort\nand advice to people experiencing distress is a compelling area of research.\nRecently, with advancements in large language models (LLMs), end-to-end\ndialogue agents without explicit strategy prediction steps have become\nprevalent. However, implicit strategy planning lacks transparency, and recent\nstudies show that LLMs' inherent preference bias towards certain\nsocio-emotional strategies hinders the delivery of high-quality emotional\nsupport. To address this challenge, we propose decoupling strategy prediction\nfrom language generation, and introduce a novel dialogue strategy prediction\nframework, EmoDynamiX, which models the discourse dynamics between user\nfine-grained emotions and system strategies using a heterogeneous graph for\nbetter performance and transparency. Experimental results on two ESC datasets\nshow EmoDynamiX outperforms previous state-of-the-art methods with a\nsignificant margin (better proficiency and lower preference bias). Our approach\nalso exhibits better transparency by allowing backtracing of decision making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing emotionally intelligent conversational systems to provide comfort\nand advice to people experiencing distress is a compelling area of research.\nRecently, with advancements in large language models (LLMs), end-to-end\ndialogue agents without explicit strategy prediction steps have become\nprevalent. However, implicit strategy planning lacks transparency, and recent\nstudies show that LLMs' inherent preference bias towards certain\nsocio-emotional strategies hinders the delivery of high-quality emotional\nsupport. To address this challenge, we propose decoupling strategy prediction\nfrom language generation, and introduce a novel dialogue strategy prediction\nframework, EmoDynamiX, which models the discourse dynamics between user\nfine-grained emotions and system strategies using a heterogeneous graph for\nbetter performance and transparency. Experimental results on two ESC datasets\nshow EmoDynamiX outperforms previous state-of-the-art methods with a\nsignificant margin (better proficiency and lower preference bias). Our approach\nalso exhibits better transparency by allowing backtracing of decision making."
                },
                "authors": [
                    {
                        "name": "Chenwei Wan"
                    },
                    {
                        "name": "Matthieu Labeau"
                    },
                    {
                        "name": "Chloé Clavel"
                    }
                ],
                "author_detail": {
                    "name": "Chloé Clavel"
                },
                "author": "Chloé Clavel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08782v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08782v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08740v1",
                "updated": "2024-10-11T11:59:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    11,
                    59,
                    40,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T11:59:40Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    11,
                    59,
                    40,
                    4,
                    285,
                    0
                ],
                "title": "Hespi: A pipeline for automatically detecting information from hebarium\n  specimen sheets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hespi: A pipeline for automatically detecting information from hebarium\n  specimen sheets"
                },
                "summary": "Specimen associated biodiversity data are sought after for biological,\nenvironmental, climate, and conservation sciences. A rate shift is required for\nthe extraction of data from specimen images to eliminate the bottleneck that\nthe reliance on human-mediated transcription of these data represents. We\napplied advanced computer vision techniques to develop the `Hespi' (HErbarium\nSpecimen sheet PIpeline), which extracts a pre-catalogue subset of collection\ndata on the institutional labels on herbarium specimens from their digital\nimages. The pipeline integrates two object detection models; the first detects\nbounding boxes around text-based labels and the second detects bounding boxes\naround text-based data fields on the primary institutional label. The pipeline\nclassifies text-based institutional labels as printed, typed, handwritten, or a\ncombination and applies Optical Character Recognition (OCR) and Handwritten\nText Recognition (HTR) for data extraction. The recognized text is then\ncorrected against authoritative databases of taxon names. The extracted text is\nalso corrected with the aide of a multimodal Large Language Model (LLM). Hespi\naccurately detects and extracts text for test datasets including specimen sheet\nimages from international herbaria. The components of the pipeline are modular\nand users can train their own models with their own data and use them in place\nof the models provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Specimen associated biodiversity data are sought after for biological,\nenvironmental, climate, and conservation sciences. A rate shift is required for\nthe extraction of data from specimen images to eliminate the bottleneck that\nthe reliance on human-mediated transcription of these data represents. We\napplied advanced computer vision techniques to develop the `Hespi' (HErbarium\nSpecimen sheet PIpeline), which extracts a pre-catalogue subset of collection\ndata on the institutional labels on herbarium specimens from their digital\nimages. The pipeline integrates two object detection models; the first detects\nbounding boxes around text-based labels and the second detects bounding boxes\naround text-based data fields on the primary institutional label. The pipeline\nclassifies text-based institutional labels as printed, typed, handwritten, or a\ncombination and applies Optical Character Recognition (OCR) and Handwritten\nText Recognition (HTR) for data extraction. The recognized text is then\ncorrected against authoritative databases of taxon names. The extracted text is\nalso corrected with the aide of a multimodal Large Language Model (LLM). Hespi\naccurately detects and extracts text for test datasets including specimen sheet\nimages from international herbaria. The components of the pipeline are modular\nand users can train their own models with their own data and use them in place\nof the models provided."
                },
                "authors": [
                    {
                        "name": "Robert Turnbull"
                    },
                    {
                        "name": "Emily Fitzgerald"
                    },
                    {
                        "name": "Karen Thompson"
                    },
                    {
                        "name": "Joanne L. Birch"
                    }
                ],
                "author_detail": {
                    "name": "Joanne L. Birch"
                },
                "author": "Joanne L. Birch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01379v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01379v4",
                "updated": "2024-10-11T11:54:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    11,
                    54,
                    35,
                    4,
                    285,
                    0
                ],
                "published": "2024-05-02T15:20:01Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    15,
                    20,
                    1,
                    3,
                    123,
                    0
                ],
                "title": "Verification and Refinement of Natural Language Explanations through\n  LLM-Symbolic Theorem Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verification and Refinement of Natural Language Explanations through\n  LLM-Symbolic Theorem Proving"
                },
                "summary": "Natural language explanations represent a proxy for evaluating\nexplanation-based and multi-step Natural Language Inference (NLI) models.\nHowever, assessing the validity of explanations for NLI is challenging as it\ntypically involves the crowd-sourcing of apposite datasets, a process that is\ntime-consuming and prone to logical errors. To address existing limitations,\nthis paper investigates the verification and refinement of natural language\nexplanations through the integration of Large Language Models (LLMs) and\nTheorem Provers (TPs). Specifically, we present a neuro-symbolic framework,\nnamed Explanation-Refiner, that integrates TPs with LLMs to generate and\nformalise explanatory sentences and suggest potential inference strategies for\nNLI. In turn, the TP is employed to provide formal guarantees on the logical\nvalidity of the explanations and to generate feedback for subsequent\nimprovements. We demonstrate how Explanation-Refiner can be jointly used to\nevaluate explanatory reasoning, autoformalisation, and error correction\nmechanisms of state-of-the-art LLMs as well as to automatically enhance the\nquality of explanations of variable complexity in different domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language explanations represent a proxy for evaluating\nexplanation-based and multi-step Natural Language Inference (NLI) models.\nHowever, assessing the validity of explanations for NLI is challenging as it\ntypically involves the crowd-sourcing of apposite datasets, a process that is\ntime-consuming and prone to logical errors. To address existing limitations,\nthis paper investigates the verification and refinement of natural language\nexplanations through the integration of Large Language Models (LLMs) and\nTheorem Provers (TPs). Specifically, we present a neuro-symbolic framework,\nnamed Explanation-Refiner, that integrates TPs with LLMs to generate and\nformalise explanatory sentences and suggest potential inference strategies for\nNLI. In turn, the TP is employed to provide formal guarantees on the logical\nvalidity of the explanations and to generate feedback for subsequent\nimprovements. We demonstrate how Explanation-Refiner can be jointly used to\nevaluate explanatory reasoning, autoformalisation, and error correction\nmechanisms of state-of-the-art LLMs as well as to automatically enhance the\nquality of explanations of variable complexity in different domains."
                },
                "authors": [
                    {
                        "name": "Xin Quan"
                    },
                    {
                        "name": "Marco Valentino"
                    },
                    {
                        "name": "Louise A. Dennis"
                    },
                    {
                        "name": "André Freitas"
                    }
                ],
                "author_detail": {
                    "name": "André Freitas"
                },
                "author": "André Freitas",
                "arxiv_comment": "Camera-ready for EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01379v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01379v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08731v1",
                "updated": "2024-10-11T11:41:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    11,
                    41,
                    2,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T11:41:02Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    11,
                    41,
                    2,
                    4,
                    285,
                    0
                ],
                "title": "Developing a Pragmatic Benchmark for Assessing Korean Legal Language\n  Understanding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing a Pragmatic Benchmark for Assessing Korean Legal Language\n  Understanding in Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance in the\nlegal domain, with GPT-4 even passing the Uniform Bar Exam in the U.S. However\ntheir efficacy remains limited for non-standardized tasks and tasks in\nlanguages other than English. This underscores the need for careful evaluation\nof LLMs within each legal system before application. Here, we introduce KBL, a\nbenchmark for assessing the Korean legal language understanding of LLMs,\nconsisting of (1) 7 legal knowledge tasks (510 examples), (2) 4 legal reasoning\ntasks (288 examples), and (3) the Korean bar exam (4 domains, 53 tasks, 2,510\nexamples). First two datasets were developed in close collaboration with\nlawyers to evaluate LLMs in practical scenarios in a certified manner.\nFurthermore, considering legal practitioners' frequent use of extensive legal\ndocuments for research, we assess LLMs in both a closed book setting, where\nthey rely solely on internal knowledge, and a retrieval-augmented generation\n(RAG) setting, using a corpus of Korean statutes and precedents. The results\nindicate substantial room and opportunities for improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance in the\nlegal domain, with GPT-4 even passing the Uniform Bar Exam in the U.S. However\ntheir efficacy remains limited for non-standardized tasks and tasks in\nlanguages other than English. This underscores the need for careful evaluation\nof LLMs within each legal system before application. Here, we introduce KBL, a\nbenchmark for assessing the Korean legal language understanding of LLMs,\nconsisting of (1) 7 legal knowledge tasks (510 examples), (2) 4 legal reasoning\ntasks (288 examples), and (3) the Korean bar exam (4 domains, 53 tasks, 2,510\nexamples). First two datasets were developed in close collaboration with\nlawyers to evaluate LLMs in practical scenarios in a certified manner.\nFurthermore, considering legal practitioners' frequent use of extensive legal\ndocuments for research, we assess LLMs in both a closed book setting, where\nthey rely solely on internal knowledge, and a retrieval-augmented generation\n(RAG) setting, using a corpus of Korean statutes and precedents. The results\nindicate substantial room and opportunities for improvement."
                },
                "authors": [
                    {
                        "name": "Yeeun Kim"
                    },
                    {
                        "name": "Young Rok Choi"
                    },
                    {
                        "name": "Eunkyung Choi"
                    },
                    {
                        "name": "Jinhwan Choi"
                    },
                    {
                        "name": "Hai Jin Park"
                    },
                    {
                        "name": "Wonseok Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Wonseok Hwang"
                },
                "author": "Wonseok Hwang",
                "arxiv_comment": "EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20222v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20222v2",
                "updated": "2024-10-11T11:32:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    11,
                    32,
                    16,
                    4,
                    285,
                    0
                ],
                "published": "2024-09-30T12:01:29Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    12,
                    1,
                    29,
                    0,
                    274,
                    0
                ],
                "title": "Beyond Prompts: Dynamic Conversational Benchmarking of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Prompts: Dynamic Conversational Benchmarking of Large Language\n  Models"
                },
                "summary": "We introduce a dynamic benchmarking system for conversational agents that\nevaluates their performance through a single, simulated, and lengthy\nuser$\\leftrightarrow$agent interaction. The interaction is a conversation\nbetween the user and agent, where multiple tasks are introduced and then\nundertaken concurrently. We context switch regularly to interleave the tasks,\nwhich constructs a realistic testing scenario in which we assess the Long-Term\nMemory, Continual Learning, and Information Integration capabilities of the\nagents. Results from both proprietary and open-source Large-Language Models\nshow that LLMs in general perform well on single-task interactions, but they\nstruggle on the same tasks when they are interleaved. Notably, short-context\nLLMs supplemented with an LTM system perform as well as or better than those\nwith larger contexts. Our benchmark suggests that there are other challenges\nfor LLMs responding to more natural interactions that contemporary benchmarks\nhave heretofore not been able to capture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a dynamic benchmarking system for conversational agents that\nevaluates their performance through a single, simulated, and lengthy\nuser$\\leftrightarrow$agent interaction. The interaction is a conversation\nbetween the user and agent, where multiple tasks are introduced and then\nundertaken concurrently. We context switch regularly to interleave the tasks,\nwhich constructs a realistic testing scenario in which we assess the Long-Term\nMemory, Continual Learning, and Information Integration capabilities of the\nagents. Results from both proprietary and open-source Large-Language Models\nshow that LLMs in general perform well on single-task interactions, but they\nstruggle on the same tasks when they are interleaved. Notably, short-context\nLLMs supplemented with an LTM system perform as well as or better than those\nwith larger contexts. Our benchmark suggests that there are other challenges\nfor LLMs responding to more natural interactions that contemporary benchmarks\nhave heretofore not been able to capture."
                },
                "authors": [
                    {
                        "name": "David Castillo-Bolado"
                    },
                    {
                        "name": "Joseph Davidson"
                    },
                    {
                        "name": "Finlay Gray"
                    },
                    {
                        "name": "Marek Rosa"
                    }
                ],
                "author_detail": {
                    "name": "Marek Rosa"
                },
                "author": "Marek Rosa",
                "arxiv_comment": "Accepted as a poster at NeurIPS D&B Track 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20222v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20222v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08726v1",
                "updated": "2024-10-11T11:27:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    11,
                    27,
                    57,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T11:27:57Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    11,
                    27,
                    57,
                    4,
                    285,
                    0
                ],
                "title": "5G as Enabler for Industrie 4.0 Use Cases: Challenges and Concepts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "5G as Enabler for Industrie 4.0 Use Cases: Challenges and Concepts"
                },
                "summary": "The increasing demand for highly customized products, as well as flexible\nproduction lines, can be seen as trigger for the \"fourth industrial\nrevolution\", referred to as \"Industrie 4.0\". Current systems usually rely on\nwire-line technologies to connect sensors and actuators. To enable a higher\nflexibility such as moving robots or drones, these connections need to be\nreplaced by wireless technologies in the future. Furthermore, this facilitates\nthe renewal of brownfield deployments to address Industrie 4.0 requirements.\nThis paper proposes representative use cases, which have been examined in the\nGerman Tactile Internet 4.0 (TACNET 4.0) research project. In order to analyze\nthese use cases, this paper identifies the main challenges and requirements of\ncommunication networks in Industrie 4.0 and discusses the applicability of 5th\ngeneration wireless communication systems (5G).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for highly customized products, as well as flexible\nproduction lines, can be seen as trigger for the \"fourth industrial\nrevolution\", referred to as \"Industrie 4.0\". Current systems usually rely on\nwire-line technologies to connect sensors and actuators. To enable a higher\nflexibility such as moving robots or drones, these connections need to be\nreplaced by wireless technologies in the future. Furthermore, this facilitates\nthe renewal of brownfield deployments to address Industrie 4.0 requirements.\nThis paper proposes representative use cases, which have been examined in the\nGerman Tactile Internet 4.0 (TACNET 4.0) research project. In order to analyze\nthese use cases, this paper identifies the main challenges and requirements of\ncommunication networks in Industrie 4.0 and discusses the applicability of 5th\ngeneration wireless communication systems (5G)."
                },
                "authors": [
                    {
                        "name": "M. Gundall"
                    },
                    {
                        "name": "J. Schneider"
                    },
                    {
                        "name": "H. D. Schotten"
                    },
                    {
                        "name": "M. Aleksy"
                    },
                    {
                        "name": "D. Schulz"
                    },
                    {
                        "name": "N. Franchi"
                    },
                    {
                        "name": "N. Schwarzenberg"
                    },
                    {
                        "name": "C. Markwart"
                    },
                    {
                        "name": "R. Halfmann"
                    },
                    {
                        "name": "P. Rost"
                    },
                    {
                        "name": "D. Wübben"
                    },
                    {
                        "name": "A. Neumann"
                    },
                    {
                        "name": "M. Düngen"
                    },
                    {
                        "name": "T. Neugebauer"
                    },
                    {
                        "name": "R. Blunk"
                    },
                    {
                        "name": "M. Kus"
                    },
                    {
                        "name": "J. Grießbach"
                    }
                ],
                "author_detail": {
                    "name": "J. Grießbach"
                },
                "author": "J. Grießbach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08723v1",
                "updated": "2024-10-11T11:23:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    11,
                    23,
                    26,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T11:23:26Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    11,
                    23,
                    26,
                    4,
                    285,
                    0
                ],
                "title": "Investigating Human-Computer Interaction and Visual Comprehension in\n  Text Generation Process of Natural Language Generation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Human-Computer Interaction and Visual Comprehension in\n  Text Generation Process of Natural Language Generation Models"
                },
                "summary": "Natural language generation (NLG) models are becoming a highly sought-after\nresearch focus in the field of natural language processing (NLP), demonstrating\nstrong capabilities in text generation tasks such as writing and dialogue\ngeneration. Despite the impressive performance of NLG models, their complex\narchitecture and extensive model weights result in a lack of interpretability.\nThis limitation hampers their adoption in many critical decision-making\nscenarios. Fortunately, the intervention of human-computer interaction and\nvisual comprehension provides users with the possibility of opening the \"black\nbox\". In this paper, we conduct a investigation addressing the roles and\nlimitations of human-computer interactive and visual comprehension in text\ngeneration process of NLG models. We present a taxonomy of interaction methods\nand visualization techniques, providing a structured overview of the three main\nresearch subjects and their corresponding six tasks within the application\nprocess of large language models (LLMs). Finally, we summarize the shortcomings\nin the existing work and investigate the key challenges and emerging\nopportunities in the era of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language generation (NLG) models are becoming a highly sought-after\nresearch focus in the field of natural language processing (NLP), demonstrating\nstrong capabilities in text generation tasks such as writing and dialogue\ngeneration. Despite the impressive performance of NLG models, their complex\narchitecture and extensive model weights result in a lack of interpretability.\nThis limitation hampers their adoption in many critical decision-making\nscenarios. Fortunately, the intervention of human-computer interaction and\nvisual comprehension provides users with the possibility of opening the \"black\nbox\". In this paper, we conduct a investigation addressing the roles and\nlimitations of human-computer interactive and visual comprehension in text\ngeneration process of NLG models. We present a taxonomy of interaction methods\nand visualization techniques, providing a structured overview of the three main\nresearch subjects and their corresponding six tasks within the application\nprocess of large language models (LLMs). Finally, we summarize the shortcomings\nin the existing work and investigate the key challenges and emerging\nopportunities in the era of LLMs."
                },
                "authors": [
                    {
                        "name": "Yunchao Wang"
                    },
                    {
                        "name": "Zihang Fu"
                    },
                    {
                        "name": "Chaoqing Xu"
                    },
                    {
                        "name": "Guodao Sun"
                    },
                    {
                        "name": "Ronghua Liang"
                    }
                ],
                "author_detail": {
                    "name": "Ronghua Liang"
                },
                "author": "Ronghua Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.06468v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.06468v4",
                "updated": "2024-10-11T10:48:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    10,
                    48,
                    15,
                    4,
                    285,
                    0
                ],
                "published": "2024-01-12T09:29:13Z",
                "published_parsed": [
                    2024,
                    1,
                    12,
                    9,
                    29,
                    13,
                    4,
                    12,
                    0
                ],
                "title": "Adapting Large Language Models for Document-Level Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Large Language Models for Document-Level Machine Translation"
                },
                "summary": "Large language models (LLMs) have significantly advanced various natural\nlanguage processing (NLP) tasks. Recent research indicates that\nmoderately-sized LLMs often outperform larger ones after task-specific\nfine-tuning. This study focuses on adapting LLMs for document-level machine\ntranslation (DocMT) for specific language pairs. We first investigate the\nimpact of prompt strategies on translation performance and then conduct\nextensive experiments using two fine-tuning methods, three LLM backbones, and\n18 translation tasks across nine language pairs. Our results show that\nspecialized models can sometimes surpass GPT-4 in translation performance but\nstill face issues like off-target translation due to error propagation in\ndecoding. We provide an in-depth analysis of these LLMs tailored for DocMT,\nexamining translation errors, discourse phenomena, strategies for training and\ninference, the data efficiency of parallel documents, recent test set\nevaluations, and zero-shot crosslingual transfer. Our findings highlight the\nstrengths and limitations of LLM-based DocMT models and provide a foundation\nfor future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly advanced various natural\nlanguage processing (NLP) tasks. Recent research indicates that\nmoderately-sized LLMs often outperform larger ones after task-specific\nfine-tuning. This study focuses on adapting LLMs for document-level machine\ntranslation (DocMT) for specific language pairs. We first investigate the\nimpact of prompt strategies on translation performance and then conduct\nextensive experiments using two fine-tuning methods, three LLM backbones, and\n18 translation tasks across nine language pairs. Our results show that\nspecialized models can sometimes surpass GPT-4 in translation performance but\nstill face issues like off-target translation due to error propagation in\ndecoding. We provide an in-depth analysis of these LLMs tailored for DocMT,\nexamining translation errors, discourse phenomena, strategies for training and\ninference, the data efficiency of parallel documents, recent test set\nevaluations, and zero-shot crosslingual transfer. Our findings highlight the\nstrengths and limitations of LLM-based DocMT models and provide a foundation\nfor future research."
                },
                "authors": [
                    {
                        "name": "Minghao Wu"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Lizhen Qu"
                    },
                    {
                        "name": "George Foster"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    }
                ],
                "author_detail": {
                    "name": "Gholamreza Haffari"
                },
                "author": "Gholamreza Haffari",
                "arxiv_comment": "25 pages, 18 tables, 7 figures; ARR Feb 2024, 4/3/2, meta 2, rejected\n  by ACL2024; ARR June 2024, 4.5/3/2, meta 3, rejected by EMNLP2024;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.06468v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.06468v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08698v1",
                "updated": "2024-10-11T10:35:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    10,
                    35,
                    58,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T10:35:58Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    10,
                    35,
                    58,
                    4,
                    285,
                    0
                ],
                "title": "SocialGaze: Improving the Integration of Human Social Norms in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SocialGaze: Improving the Integration of Human Social Norms in Large\n  Language Models"
                },
                "summary": "While much research has explored enhancing the reasoning capabilities of\nlarge language models (LLMs) in the last few years, there is a gap in\nunderstanding the alignment of these models with social values and norms. We\nintroduce the task of judging social acceptance. Social acceptance requires\nmodels to judge and rationalize the acceptability of people's actions in social\nsituations. For example, is it socially acceptable for a neighbor to ask others\nin the community to keep their pets indoors at night? We find that LLMs'\nunderstanding of social acceptance is often misaligned with human consensus. To\nalleviate this, we introduce SocialGaze, a multi-step prompting framework, in\nwhich a language model verbalizes a social situation from multiple perspectives\nbefore forming a judgment. Our experiments demonstrate that the SocialGaze\napproach improves the alignment with human judgments by up to 11 F1 points with\nthe GPT-3.5 model. We also identify biases and correlations in LLMs in\nassigning blame that is related to features such as the gender (males are\nsignificantly more likely to be judged unfairly) and age (LLMs are more aligned\nwith humans for older narrators).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While much research has explored enhancing the reasoning capabilities of\nlarge language models (LLMs) in the last few years, there is a gap in\nunderstanding the alignment of these models with social values and norms. We\nintroduce the task of judging social acceptance. Social acceptance requires\nmodels to judge and rationalize the acceptability of people's actions in social\nsituations. For example, is it socially acceptable for a neighbor to ask others\nin the community to keep their pets indoors at night? We find that LLMs'\nunderstanding of social acceptance is often misaligned with human consensus. To\nalleviate this, we introduce SocialGaze, a multi-step prompting framework, in\nwhich a language model verbalizes a social situation from multiple perspectives\nbefore forming a judgment. Our experiments demonstrate that the SocialGaze\napproach improves the alignment with human judgments by up to 11 F1 points with\nthe GPT-3.5 model. We also identify biases and correlations in LLMs in\nassigning blame that is related to features such as the gender (males are\nsignificantly more likely to be judged unfairly) and age (LLMs are more aligned\nwith humans for older narrators)."
                },
                "authors": [
                    {
                        "name": "Anvesh Rao Vijjini"
                    },
                    {
                        "name": "Rakesh R. Menon"
                    },
                    {
                        "name": "Jiayi Fu"
                    },
                    {
                        "name": "Shashank Srivastava"
                    },
                    {
                        "name": "Snigdha Chaturvedi"
                    }
                ],
                "author_detail": {
                    "name": "Snigdha Chaturvedi"
                },
                "author": "Snigdha Chaturvedi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08696v1",
                "updated": "2024-10-11T10:34:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    10,
                    34,
                    28,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T10:34:28Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    10,
                    34,
                    28,
                    4,
                    285,
                    0
                ],
                "title": "AMPO: Automatic Multi-Branched Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AMPO: Automatic Multi-Branched Prompt Optimization"
                },
                "summary": "Prompt engineering is very important to enhance the performance of large\nlanguage models (LLMs). When dealing with complex issues, prompt engineers tend\nto distill multiple patterns from examples and inject relevant solutions to\noptimize the prompts, achieving satisfying results. However, existing automatic\nprompt optimization techniques are only limited to producing single flow\ninstructions, struggling with handling diverse patterns. In this paper, we\npresent AMPO, an automatic prompt optimization method that can iteratively\ndevelop a multi-branched prompt using failure cases as feedback. Our goal is to\nexplore a novel way of structuring prompts with multi-branches to better handle\nmultiple patterns in complex tasks, for which we introduce three modules:\nPattern Recognition, Branch Adjustment, and Branch Pruning. In experiments\nacross five tasks, AMPO consistently achieves the best results. Additionally,\nour approach demonstrates significant optimization efficiency due to our\nadoption of a minimal search strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt engineering is very important to enhance the performance of large\nlanguage models (LLMs). When dealing with complex issues, prompt engineers tend\nto distill multiple patterns from examples and inject relevant solutions to\noptimize the prompts, achieving satisfying results. However, existing automatic\nprompt optimization techniques are only limited to producing single flow\ninstructions, struggling with handling diverse patterns. In this paper, we\npresent AMPO, an automatic prompt optimization method that can iteratively\ndevelop a multi-branched prompt using failure cases as feedback. Our goal is to\nexplore a novel way of structuring prompts with multi-branches to better handle\nmultiple patterns in complex tasks, for which we introduce three modules:\nPattern Recognition, Branch Adjustment, and Branch Pruning. In experiments\nacross five tasks, AMPO consistently achieves the best results. Additionally,\nour approach demonstrates significant optimization efficiency due to our\nadoption of a minimal search strategy."
                },
                "authors": [
                    {
                        "name": "Sheng Yang"
                    },
                    {
                        "name": "Yurong Wu"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Zineng Zhou"
                    },
                    {
                        "name": "Bin Benjamin Zhu"
                    },
                    {
                        "name": "Xiaodi Sun"
                    },
                    {
                        "name": "Jian-Guang Lou"
                    },
                    {
                        "name": "Zhiming Ding"
                    },
                    {
                        "name": "Anbang Hu"
                    },
                    {
                        "name": "Yuan Fang"
                    },
                    {
                        "name": "Yunsong Li"
                    },
                    {
                        "name": "Junyan Chen"
                    },
                    {
                        "name": "Linjun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Linjun Yang"
                },
                "author": "Linjun Yang",
                "arxiv_comment": "13 pages, 7 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12902v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12902v2",
                "updated": "2024-10-11T10:27:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    10,
                    27,
                    16,
                    4,
                    285,
                    0
                ],
                "published": "2024-06-10T06:43:25Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    6,
                    43,
                    25,
                    0,
                    162,
                    0
                ],
                "title": "JavaBench: A Benchmark of Object-Oriented Code Generation for Evaluating\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JavaBench: A Benchmark of Object-Oriented Code Generation for Evaluating\n  Large Language Models"
                },
                "summary": "Code generation benchmarks such as HumanEval are widely adopted to evaluate\nLLMs' capabilities. However, after consolidating the latest 24 benchmarks, we\nnoticed three significant imbalances. First, imbalanced programming language.\n95.8% of benchmarks involve Python, while only 5 benchmarks involve Java.\nSecond, imbalanced code granularity. Function-/statement-level benchmarks\naccount for over 83.3% of benchmarks. Only a mere handful extends to\nclass-/project-levels, and all are limited to Python. Third, lacking advanced\nfeatures. Existing benchmarks primarily assess basic coding skills, while\noverlooking advanced Object-Oriented Programming (OOP) features (i.e.,\nencapsulation, inheritance, and polymorphism).\n  To fill these gaps, we propose JavaBench, a project-level Java benchmark that\nexercises OOP features. It comprises four Java projects with 389 methods in 106\nJava classes. The test coverage is up to 92%, and JavaBench is attested by 282\nundergraduate students, reaching a 90.93/100 average score (i.e., pass rate\nagainst the test suite), ensuring the quality of documentation, code skeleton,\nand tests. To better evaluate LLM's capability against JavaBench, we introduce\na systematic evaluation design covering three context settings and five\nsynthesis strategies at two granularities using three hierarchical metrics. Our\nextensive experiment yields several interesting findings. First, we noticed\nthat regarding project-level Java programming, LLMs are far behind\nundergraduate students (no project can be correctly completed by any studied\nLLMs, and at most 41.17% Pass@5 in a more relaxed evaluation). Second, using\nmethod signature as prompt context may strike an ideal balance for\nproject-level code generation. JavaBench is publicly available at\nhttps://github.com/java-bench/JavaBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation benchmarks such as HumanEval are widely adopted to evaluate\nLLMs' capabilities. However, after consolidating the latest 24 benchmarks, we\nnoticed three significant imbalances. First, imbalanced programming language.\n95.8% of benchmarks involve Python, while only 5 benchmarks involve Java.\nSecond, imbalanced code granularity. Function-/statement-level benchmarks\naccount for over 83.3% of benchmarks. Only a mere handful extends to\nclass-/project-levels, and all are limited to Python. Third, lacking advanced\nfeatures. Existing benchmarks primarily assess basic coding skills, while\noverlooking advanced Object-Oriented Programming (OOP) features (i.e.,\nencapsulation, inheritance, and polymorphism).\n  To fill these gaps, we propose JavaBench, a project-level Java benchmark that\nexercises OOP features. It comprises four Java projects with 389 methods in 106\nJava classes. The test coverage is up to 92%, and JavaBench is attested by 282\nundergraduate students, reaching a 90.93/100 average score (i.e., pass rate\nagainst the test suite), ensuring the quality of documentation, code skeleton,\nand tests. To better evaluate LLM's capability against JavaBench, we introduce\na systematic evaluation design covering three context settings and five\nsynthesis strategies at two granularities using three hierarchical metrics. Our\nextensive experiment yields several interesting findings. First, we noticed\nthat regarding project-level Java programming, LLMs are far behind\nundergraduate students (no project can be correctly completed by any studied\nLLMs, and at most 41.17% Pass@5 in a more relaxed evaluation). Second, using\nmethod signature as prompt context may strike an ideal balance for\nproject-level code generation. JavaBench is publicly available at\nhttps://github.com/java-bench/JavaBench."
                },
                "authors": [
                    {
                        "name": "Jialun Cao"
                    },
                    {
                        "name": "Zhiyong Chen"
                    },
                    {
                        "name": "Jiarong Wu"
                    },
                    {
                        "name": "Shing-chi Cheung"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "arxiv_comment": "Accepted by ASE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12902v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12902v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08688v1",
                "updated": "2024-10-11T10:21:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    10,
                    21,
                    42,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T10:21:42Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    10,
                    21,
                    42,
                    4,
                    285,
                    0
                ],
                "title": "Chain-of-Restoration: Multi-Task Image Restoration Models are Zero-Shot\n  Step-by-Step Universal Image Restorers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Restoration: Multi-Task Image Restoration Models are Zero-Shot\n  Step-by-Step Universal Image Restorers"
                },
                "summary": "Despite previous works typically targeting isolated degradation types, recent\nresearch has increasingly focused on addressing composite degradations which\ninvolve a complex interplay of multiple different isolated degradations.\nRecognizing the challenges posed by the exponential number of possible\ndegradation combinations, we propose Universal Image Restoration (UIR), a new\ntask setting that requires models to be trained on a set of degradation bases\nand then remove any degradation that these bases can potentially compose in a\nzero-shot manner. Inspired by the Chain-of-Thought which prompts LLMs to\naddress problems step-by-step, we propose the Chain-of-Restoration (CoR), which\ninstructs models to step-by-step remove unknown composite degradations. By\nintegrating a simple Degradation Discriminator into pre-trained multi-task\nmodels, CoR facilitates the process where models remove one degradation basis\nper step, continuing this process until the image is fully restored from the\nunknown composite degradation. Extensive experiments show that CoR\nsignificantly improves model performance in removing composite degradations,\nachieving results comparable to or surpassing those of State-of-The-Art (SoTA)\nmethods trained on all degradations. The code will be released at\nhttps://github.com/toummHus/Chain-of-Restoration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite previous works typically targeting isolated degradation types, recent\nresearch has increasingly focused on addressing composite degradations which\ninvolve a complex interplay of multiple different isolated degradations.\nRecognizing the challenges posed by the exponential number of possible\ndegradation combinations, we propose Universal Image Restoration (UIR), a new\ntask setting that requires models to be trained on a set of degradation bases\nand then remove any degradation that these bases can potentially compose in a\nzero-shot manner. Inspired by the Chain-of-Thought which prompts LLMs to\naddress problems step-by-step, we propose the Chain-of-Restoration (CoR), which\ninstructs models to step-by-step remove unknown composite degradations. By\nintegrating a simple Degradation Discriminator into pre-trained multi-task\nmodels, CoR facilitates the process where models remove one degradation basis\nper step, continuing this process until the image is fully restored from the\nunknown composite degradation. Extensive experiments show that CoR\nsignificantly improves model performance in removing composite degradations,\nachieving results comparable to or surpassing those of State-of-The-Art (SoTA)\nmethods trained on all degradations. The code will be released at\nhttps://github.com/toummHus/Chain-of-Restoration."
                },
                "authors": [
                    {
                        "name": "Jin Cao"
                    },
                    {
                        "name": "Deyu Meng"
                    },
                    {
                        "name": "Xiangyong Cao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyong Cao"
                },
                "author": "Xiangyong Cao",
                "arxiv_comment": "11 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08666v1",
                "updated": "2024-10-11T09:44:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    9,
                    44,
                    16,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T09:44:16Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    9,
                    44,
                    16,
                    4,
                    285,
                    0
                ],
                "title": "DeltaDQ: Ultra-High Delta Compression for Fine-Tuned LLMs via Group-wise\n  Dropout and Separate Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeltaDQ: Ultra-High Delta Compression for Fine-Tuned LLMs via Group-wise\n  Dropout and Separate Quantization"
                },
                "summary": "Large language models achieve exceptional performance on various downstream\ntasks through supervised fine-tuning. However, the diversity of downstream\ntasks and practical requirements makes deploying multiple full-parameter\nfine-tuned models challenging. Current methods that compress the delta weight\nstruggle to achieve ultra-high compression, failing to minimize the deployment\noverhead. To address the above issue, we propose a novel distribution-driven\ndelta compression framework DeltaDQ, which utilizes Group-wise Dropout and\nSeparate Quantization to achieve ultra-high compression for the delta weight.\nWe have observed that the matrix-computed intermediate results for the delta\nweight exhibit extremely small variance and min-max range characteristics,\nreferred to as Balanced Intermediate Results. Exploiting this phenomenon, we\nintroduce Group-wise Dropout to perform dropout on the delta weight using an\noptimal group size. Furthermore, using Separate Quantization, sparse weights\nare quantized and decomposed to achieve a lower bit. Experimental results show\nthat DeltaDQ achieves 16x compression with improved accuracy compared to\nbaselines for WizardMath and WizardCoder models across different parameter\nscales. Moreover, DeltaDQ demonstrates the ability for ultra-high compression\nratio, achieving 128x compression for the WizardMath-7B model and 512x\ncompression for the WizardMath-70B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models achieve exceptional performance on various downstream\ntasks through supervised fine-tuning. However, the diversity of downstream\ntasks and practical requirements makes deploying multiple full-parameter\nfine-tuned models challenging. Current methods that compress the delta weight\nstruggle to achieve ultra-high compression, failing to minimize the deployment\noverhead. To address the above issue, we propose a novel distribution-driven\ndelta compression framework DeltaDQ, which utilizes Group-wise Dropout and\nSeparate Quantization to achieve ultra-high compression for the delta weight.\nWe have observed that the matrix-computed intermediate results for the delta\nweight exhibit extremely small variance and min-max range characteristics,\nreferred to as Balanced Intermediate Results. Exploiting this phenomenon, we\nintroduce Group-wise Dropout to perform dropout on the delta weight using an\noptimal group size. Furthermore, using Separate Quantization, sparse weights\nare quantized and decomposed to achieve a lower bit. Experimental results show\nthat DeltaDQ achieves 16x compression with improved accuracy compared to\nbaselines for WizardMath and WizardCoder models across different parameter\nscales. Moreover, DeltaDQ demonstrates the ability for ultra-high compression\nratio, achieving 128x compression for the WizardMath-7B model and 512x\ncompression for the WizardMath-70B model."
                },
                "authors": [
                    {
                        "name": "Yanfeng Jiang"
                    },
                    {
                        "name": "Zelan Yang"
                    },
                    {
                        "name": "Bohua Chen"
                    },
                    {
                        "name": "Shen Li"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Tao Li"
                    }
                ],
                "author_detail": {
                    "name": "Tao Li"
                },
                "author": "Tao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.03853v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.03853v3",
                "updated": "2024-10-11T09:43:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    9,
                    43,
                    32,
                    4,
                    285,
                    0
                ],
                "published": "2024-03-06T17:04:18Z",
                "published_parsed": [
                    2024,
                    3,
                    6,
                    17,
                    4,
                    18,
                    2,
                    66,
                    0
                ],
                "title": "ShortGPT: Layers in Large Language Models are More Redundant Than You\n  Expect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShortGPT: Layers in Large Language Models are More Redundant Than You\n  Expect"
                },
                "summary": "As Large Language Models (LLMs) continue to advance in performance, their\nsize has escalated significantly, with current LLMs containing billions or even\ntrillions of parameters. However, in this study, we discovered that many layers\nof LLMs exhibit high similarity, and some layers play a negligible role in\nnetwork functionality. Based on this observation, we define a metric called\nBlock Influence (BI) to gauge the significance of each layer in LLMs. We then\npropose a straightforward pruning approach: layer removal, in which we directly\ndelete the redundant layers in LLMs based on their BI scores. Experiments\ndemonstrate that our method, which we call ShortGPT, significantly outperforms\nprevious state-of-the-art (SOTA) methods in model pruning. Moreover, ShortGPT\nis orthogonal to quantization-like methods, enabling further reduction in\nparameters and computation. The ability to achieve better results through\nsimple layer removal, as opposed to more complex pruning techniques, suggests a\nhigh degree of redundancy in the model architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to advance in performance, their\nsize has escalated significantly, with current LLMs containing billions or even\ntrillions of parameters. However, in this study, we discovered that many layers\nof LLMs exhibit high similarity, and some layers play a negligible role in\nnetwork functionality. Based on this observation, we define a metric called\nBlock Influence (BI) to gauge the significance of each layer in LLMs. We then\npropose a straightforward pruning approach: layer removal, in which we directly\ndelete the redundant layers in LLMs based on their BI scores. Experiments\ndemonstrate that our method, which we call ShortGPT, significantly outperforms\nprevious state-of-the-art (SOTA) methods in model pruning. Moreover, ShortGPT\nis orthogonal to quantization-like methods, enabling further reduction in\nparameters and computation. The ability to achieve better results through\nsimple layer removal, as opposed to more complex pruning techniques, suggests a\nhigh degree of redundancy in the model architecture."
                },
                "authors": [
                    {
                        "name": "Xin Men"
                    },
                    {
                        "name": "Mingyu Xu"
                    },
                    {
                        "name": "Qingyu Zhang"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.03853v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.03853v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16692v2",
                "updated": "2024-10-11T09:42:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    9,
                    42,
                    34,
                    4,
                    285,
                    0
                ],
                "published": "2024-04-25T15:53:00Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    15,
                    53,
                    0,
                    3,
                    116,
                    0
                ],
                "title": "Influence of Solution Efficiency and Valence of Instruction on Additive\n  and Subtractive Solution Strategies in Humans and GPT-4",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influence of Solution Efficiency and Valence of Instruction on Additive\n  and Subtractive Solution Strategies in Humans and GPT-4"
                },
                "summary": "Generative artificial intelligences, especially large language models (LLMs),\nare increasingly being used, necessitating transparency about their\ncapabilities. While prior studies have shown addition biases in humans (Adams\net al., 2021) and OpenAI's GPT-3 (Winter et al., 2023), this study extends the\nresearch by comparing human and GPT-4 problem-solving across both spatial and\nlinguistic tasks, with variations in solution efficiency and valence of task\ninstruction. Four preregistered experiments with 588 participants from the U.S.\nand 680 GPT-4 iterations revealed a stronger tendency towards additive\ntransformations in GPT-4 than in humans. Human participants were less likely to\nuse additive strategies when subtraction was relatively more efficient than\nwhen addition and subtraction were equally efficient. GPT-4 exhibited the\nopposite behavior, with a strong addition bias when subtraction was more\nefficient. In terms of valence of task instruction, GPT-4's use of additive\nstrategies increased when instructed to \"improve\" (positive valence) rather\nthan \"edit\" (neutral valence). These findings demonstrate that biases in human\nproblem-solving are amplified in the outputs of GPT-4, and that LLM's solution\nstrategies differ from human efficiency-based strategies. This highlights the\nevolving limitations of LLMs and the need for caution when using them in\nreal-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative artificial intelligences, especially large language models (LLMs),\nare increasingly being used, necessitating transparency about their\ncapabilities. While prior studies have shown addition biases in humans (Adams\net al., 2021) and OpenAI's GPT-3 (Winter et al., 2023), this study extends the\nresearch by comparing human and GPT-4 problem-solving across both spatial and\nlinguistic tasks, with variations in solution efficiency and valence of task\ninstruction. Four preregistered experiments with 588 participants from the U.S.\nand 680 GPT-4 iterations revealed a stronger tendency towards additive\ntransformations in GPT-4 than in humans. Human participants were less likely to\nuse additive strategies when subtraction was relatively more efficient than\nwhen addition and subtraction were equally efficient. GPT-4 exhibited the\nopposite behavior, with a strong addition bias when subtraction was more\nefficient. In terms of valence of task instruction, GPT-4's use of additive\nstrategies increased when instructed to \"improve\" (positive valence) rather\nthan \"edit\" (neutral valence). These findings demonstrate that biases in human\nproblem-solving are amplified in the outputs of GPT-4, and that LLM's solution\nstrategies differ from human efficiency-based strategies. This highlights the\nevolving limitations of LLMs and the need for caution when using them in\nreal-world applications."
                },
                "authors": [
                    {
                        "name": "Lydia Uhler"
                    },
                    {
                        "name": "Verena Jordan"
                    },
                    {
                        "name": "Jürgen Buder"
                    },
                    {
                        "name": "Markus Huff"
                    },
                    {
                        "name": "Frank Papenmeier"
                    }
                ],
                "author_detail": {
                    "name": "Frank Papenmeier"
                },
                "author": "Frank Papenmeier",
                "arxiv_comment": "52 pages, 4 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08661v1",
                "updated": "2024-10-11T09:39:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    9,
                    39,
                    33,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T09:39:33Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    9,
                    39,
                    33,
                    4,
                    285,
                    0
                ],
                "title": "QEFT: Quantization for Efficient Fine-Tuning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QEFT: Quantization for Efficient Fine-Tuning of LLMs"
                },
                "summary": "With the rapid growth in the use of fine-tuning for large language models\n(LLMs), optimizing fine-tuning while keeping inference efficient has become\nhighly important. However, this is a challenging task as it requires\nimprovements in all aspects, including inference speed, fine-tuning speed,\nmemory consumption, and, most importantly, model quality. Previous studies have\nattempted to achieve this by combining quantization with fine-tuning, but they\nhave failed to enhance all four aspects simultaneously. In this study, we\npropose a new lightweight technique called Quantization for Efficient\nFine-Tuning (QEFT). QEFT accelerates both inference and fine-tuning, is\nsupported by robust theoretical foundations, offers high flexibility, and\nmaintains good hardware compatibility. Our extensive experiments demonstrate\nthat QEFT matches the quality and versatility of full-precision\nparameter-efficient fine-tuning, while using fewer resources. Our code is\navailable at https://github.com/xvyaward/qeft.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth in the use of fine-tuning for large language models\n(LLMs), optimizing fine-tuning while keeping inference efficient has become\nhighly important. However, this is a challenging task as it requires\nimprovements in all aspects, including inference speed, fine-tuning speed,\nmemory consumption, and, most importantly, model quality. Previous studies have\nattempted to achieve this by combining quantization with fine-tuning, but they\nhave failed to enhance all four aspects simultaneously. In this study, we\npropose a new lightweight technique called Quantization for Efficient\nFine-Tuning (QEFT). QEFT accelerates both inference and fine-tuning, is\nsupported by robust theoretical foundations, offers high flexibility, and\nmaintains good hardware compatibility. Our extensive experiments demonstrate\nthat QEFT matches the quality and versatility of full-precision\nparameter-efficient fine-tuning, while using fewer resources. Our code is\navailable at https://github.com/xvyaward/qeft."
                },
                "authors": [
                    {
                        "name": "Changhun Lee"
                    },
                    {
                        "name": "Jun-gyu Jin"
                    },
                    {
                        "name": "Younghyun Cho"
                    },
                    {
                        "name": "Eunhyeok Park"
                    }
                ],
                "author_detail": {
                    "name": "Eunhyeok Park"
                },
                "author": "Eunhyeok Park",
                "arxiv_comment": "Accepted at Findings of EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08660v1",
                "updated": "2024-10-11T09:39:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    9,
                    39,
                    11,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T09:39:11Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    9,
                    39,
                    11,
                    4,
                    285,
                    0
                ],
                "title": "RePD: Defending Jailbreak Attack through a Retrieval-based Prompt\n  Decomposition Process",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RePD: Defending Jailbreak Attack through a Retrieval-based Prompt\n  Decomposition Process"
                },
                "summary": "In this study, we introduce RePD, an innovative attack Retrieval-based Prompt\nDecomposition framework designed to mitigate the risk of jailbreak attacks on\nlarge language models (LLMs). Despite rigorous pretraining and finetuning\nfocused on ethical alignment, LLMs are still susceptible to jailbreak exploits.\nRePD operates on a one-shot learning model, wherein it accesses a database of\npre-collected jailbreak prompt templates to identify and decompose harmful\ninquiries embedded within user prompts. This process involves integrating the\ndecomposition of the jailbreak prompt into the user's original query into a\none-shot learning example to effectively teach the LLM to discern and separate\nmalicious components. Consequently, the LLM is equipped to first neutralize any\npotentially harmful elements before addressing the user's prompt in a manner\nthat aligns with its ethical guidelines. RePD is versatile and compatible with\na variety of open-source LLMs acting as agents. Through comprehensive\nexperimentation with both harmful and benign prompts, we have demonstrated the\nefficacy of our proposed RePD in enhancing the resilience of LLMs against\njailbreak attacks, without compromising their performance in responding to\ntypical user requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we introduce RePD, an innovative attack Retrieval-based Prompt\nDecomposition framework designed to mitigate the risk of jailbreak attacks on\nlarge language models (LLMs). Despite rigorous pretraining and finetuning\nfocused on ethical alignment, LLMs are still susceptible to jailbreak exploits.\nRePD operates on a one-shot learning model, wherein it accesses a database of\npre-collected jailbreak prompt templates to identify and decompose harmful\ninquiries embedded within user prompts. This process involves integrating the\ndecomposition of the jailbreak prompt into the user's original query into a\none-shot learning example to effectively teach the LLM to discern and separate\nmalicious components. Consequently, the LLM is equipped to first neutralize any\npotentially harmful elements before addressing the user's prompt in a manner\nthat aligns with its ethical guidelines. RePD is versatile and compatible with\na variety of open-source LLMs acting as agents. Through comprehensive\nexperimentation with both harmful and benign prompts, we have demonstrated the\nefficacy of our proposed RePD in enhancing the resilience of LLMs against\njailbreak attacks, without compromising their performance in responding to\ntypical user requests."
                },
                "authors": [
                    {
                        "name": "Peiran Wang"
                    },
                    {
                        "name": "Xiaogeng Liu"
                    },
                    {
                        "name": "Chaowei Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Chaowei Xiao"
                },
                "author": "Chaowei Xiao",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2403.04783 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05120v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05120v2",
                "updated": "2024-10-11T09:38:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    9,
                    38,
                    40,
                    4,
                    285,
                    0
                ],
                "published": "2024-02-03T05:55:24Z",
                "published_parsed": [
                    2024,
                    2,
                    3,
                    5,
                    55,
                    24,
                    5,
                    34,
                    0
                ],
                "title": "More Agents Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Agents Is All You Need"
                },
                "summary": "We find that, simply via a sampling-and-voting method, the performance of\nlarge language models (LLMs) scales with the number of agents instantiated.\nAlso, this method, termed as Agent Forest, is orthogonal to existing\ncomplicated methods to further enhance LLMs, while the degree of enhancement is\ncorrelated to the task difficulty. We conduct comprehensive experiments on a\nwide range of LLM benchmarks to verify the presence of our finding, and to\nstudy the properties that can facilitate its occurrence. Our code is publicly\navailable at: https://github.com/MoreAgentsIsAllYouNeed/AgentForest",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We find that, simply via a sampling-and-voting method, the performance of\nlarge language models (LLMs) scales with the number of agents instantiated.\nAlso, this method, termed as Agent Forest, is orthogonal to existing\ncomplicated methods to further enhance LLMs, while the degree of enhancement is\ncorrelated to the task difficulty. We conduct comprehensive experiments on a\nwide range of LLM benchmarks to verify the presence of our finding, and to\nstudy the properties that can facilitate its occurrence. Our code is publicly\navailable at: https://github.com/MoreAgentsIsAllYouNeed/AgentForest"
                },
                "authors": [
                    {
                        "name": "Junyou Li"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Yangbin Yu"
                    },
                    {
                        "name": "Qiang Fu"
                    },
                    {
                        "name": "Deheng Ye"
                    }
                ],
                "author_detail": {
                    "name": "Deheng Ye"
                },
                "author": "Deheng Ye",
                "arxiv_comment": "Published at Transactions on Machine Learning Research (TMLR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05120v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05120v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11591v3",
                "updated": "2024-10-11T09:23:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    9,
                    23,
                    43,
                    4,
                    285,
                    0
                ],
                "published": "2024-07-16T10:50:39Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    10,
                    50,
                    39,
                    1,
                    198,
                    0
                ],
                "title": "AdaptEval: Evaluating Large Language Models on Domain Adaptation for\n  Text Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptEval: Evaluating Large Language Models on Domain Adaptation for\n  Text Summarization"
                },
                "summary": "Despite the advances in the abstractive summarization task using Large\nLanguage Models (LLM), there is a lack of research that asses their abilities\nto easily adapt to different domains. We evaluate the domain adaptation\nabilities of a wide range of LLMs on the summarization task across various\ndomains in both fine-tuning and in-context learning settings. We also present\nAdaptEval, the first domain adaptation evaluation suite. AdaptEval includes a\ndomain benchmark and a set of metrics to facilitate the analysis of domain\nadaptation. Our results demonstrate that LLMs exhibit comparable performance in\nthe in-context learning setting, regardless of their parameter scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the advances in the abstractive summarization task using Large\nLanguage Models (LLM), there is a lack of research that asses their abilities\nto easily adapt to different domains. We evaluate the domain adaptation\nabilities of a wide range of LLMs on the summarization task across various\ndomains in both fine-tuning and in-context learning settings. We also present\nAdaptEval, the first domain adaptation evaluation suite. AdaptEval includes a\ndomain benchmark and a set of metrics to facilitate the analysis of domain\nadaptation. Our results demonstrate that LLMs exhibit comparable performance in\nthe in-context learning setting, regardless of their parameter scale."
                },
                "authors": [
                    {
                        "name": "Anum Afzal"
                    },
                    {
                        "name": "Ribin Chalumattu"
                    },
                    {
                        "name": "Florian Matthes"
                    },
                    {
                        "name": "Laura Mascarell"
                    }
                ],
                "author_detail": {
                    "name": "Laura Mascarell"
                },
                "author": "Laura Mascarell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17019v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17019v3",
                "updated": "2024-10-11T09:07:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    9,
                    7,
                    22,
                    4,
                    285,
                    0
                ],
                "published": "2024-01-30T13:52:47Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    13,
                    52,
                    47,
                    1,
                    30,
                    0
                ],
                "title": "Towards Generating Executable Metamorphic Relations Using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Generating Executable Metamorphic Relations Using Large Language\n  Models"
                },
                "summary": "Metamorphic testing (MT) has proven to be a successful solution to automating\ntesting and addressing the oracle problem. However, it entails manually\nderiving metamorphic relations (MRs) and converting them into an executable\nform; these steps are time-consuming and may prevent the adoption of MT. In\nthis paper, we propose an approach for automatically deriving executable MRs\n(EMRs) from requirements using large language models (LLMs). Instead of merely\nasking the LLM to produce EMRs, our approach relies on a few-shot prompting\nstrategy to instruct the LLM to perform activities in the MT process, by\nproviding requirements and API specifications, as one would do with software\nengineers. To assess the feasibility of our approach, we conducted a\nquestionnaire-based survey in collaboration with Siemens Industry Software, a\nworldwide leader in providing industry software and services, focusing on four\nof their software applications. Additionally, we evaluated the accuracy of the\ngenerated EMRs for a Web application. The outcomes of our study are highly\npromising, as they demonstrate the capability of our approach to generate MRs\nand EMRs that are both comprehensible and pertinent for testing purposes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metamorphic testing (MT) has proven to be a successful solution to automating\ntesting and addressing the oracle problem. However, it entails manually\nderiving metamorphic relations (MRs) and converting them into an executable\nform; these steps are time-consuming and may prevent the adoption of MT. In\nthis paper, we propose an approach for automatically deriving executable MRs\n(EMRs) from requirements using large language models (LLMs). Instead of merely\nasking the LLM to produce EMRs, our approach relies on a few-shot prompting\nstrategy to instruct the LLM to perform activities in the MT process, by\nproviding requirements and API specifications, as one would do with software\nengineers. To assess the feasibility of our approach, we conducted a\nquestionnaire-based survey in collaboration with Siemens Industry Software, a\nworldwide leader in providing industry software and services, focusing on four\nof their software applications. Additionally, we evaluated the accuracy of the\ngenerated EMRs for a Web application. The outcomes of our study are highly\npromising, as they demonstrate the capability of our approach to generate MRs\nand EMRs that are both comprehensible and pertinent for testing purposes."
                },
                "authors": [
                    {
                        "name": "Seung Yeob Shin"
                    },
                    {
                        "name": "Fabrizio Pastore"
                    },
                    {
                        "name": "Domenico Bianculli"
                    },
                    {
                        "name": "Alexandra Baicoianu"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Baicoianu"
                },
                "author": "Alexandra Baicoianu",
                "arxiv_comment": "This preprint has not undergone peer review (when applicable) or any\n  post-submission improvements or corrections. The Version of Record of this\n  contribution is published in Communications in Computer and Information\n  Science (CCIS, volume 2178), and is available online at\n  https://doi.org/10.1007/978-3-031-70245-7_9",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17019v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17019v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13505v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13505v2",
                "updated": "2024-10-11T08:58:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    58,
                    20,
                    4,
                    285,
                    0
                ],
                "published": "2024-07-18T13:38:21Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    13,
                    38,
                    21,
                    3,
                    200,
                    0
                ],
                "title": "Robots Can Multitask Too: Integrating a Memory Architecture and LLMs for\n  Enhanced Cross-Task Robot Action Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robots Can Multitask Too: Integrating a Memory Architecture and LLMs for\n  Enhanced Cross-Task Robot Action Generation"
                },
                "summary": "Large Language Models (LLMs) have been recently used in robot applications\nfor grounding LLM common-sense reasoning with the robot's perception and\nphysical abilities. In humanoid robots, memory also plays a critical role in\nfostering real-world embodiment and facilitating long-term interactive\ncapabilities, especially in multi-task setups where the robot must remember\nprevious task states, environment states, and executed actions. In this paper,\nwe address incorporating memory processes with LLMs for generating cross-task\nrobot actions, while the robot effectively switches between tasks. Our proposed\ndual-layered architecture features two LLMs, utilizing their complementary\nskills of reasoning and following instructions, combined with a memory model\ninspired by human cognition. Our results show a significant improvement in\nperformance over a baseline of five robotic tasks, demonstrating the potential\nof integrating memory with LLMs for combining the robot's action and perception\nfor adaptive task execution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been recently used in robot applications\nfor grounding LLM common-sense reasoning with the robot's perception and\nphysical abilities. In humanoid robots, memory also plays a critical role in\nfostering real-world embodiment and facilitating long-term interactive\ncapabilities, especially in multi-task setups where the robot must remember\nprevious task states, environment states, and executed actions. In this paper,\nwe address incorporating memory processes with LLMs for generating cross-task\nrobot actions, while the robot effectively switches between tasks. Our proposed\ndual-layered architecture features two LLMs, utilizing their complementary\nskills of reasoning and following instructions, combined with a memory model\ninspired by human cognition. Our results show a significant improvement in\nperformance over a baseline of five robotic tasks, demonstrating the potential\nof integrating memory with LLMs for combining the robot's action and perception\nfor adaptive task execution."
                },
                "authors": [
                    {
                        "name": "Hassan Ali"
                    },
                    {
                        "name": "Philipp Allgeuer"
                    },
                    {
                        "name": "Carlo Mazzola"
                    },
                    {
                        "name": "Giulia Belgiovine"
                    },
                    {
                        "name": "Burak Can Kaplan"
                    },
                    {
                        "name": "Lukáš Gajdošech"
                    },
                    {
                        "name": "Stefan Wermter"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wermter"
                },
                "author": "Stefan Wermter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13505v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13505v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08632v1",
                "updated": "2024-10-11T08:54:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    54,
                    45,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T08:54:45Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    54,
                    45,
                    4,
                    285,
                    0
                ],
                "title": "Words as Beacons: Guiding RL Agents with High-Level Language Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Words as Beacons: Guiding RL Agents with High-Level Language Prompts"
                },
                "summary": "Sparse reward environments in reinforcement learning (RL) pose significant\nchallenges for exploration, often leading to inefficient or incomplete learning\nprocesses. To tackle this issue, this work proposes a teacher-student RL\nframework that leverages Large Language Models (LLMs) as \"teachers\" to guide\nthe agent's learning process by decomposing complex tasks into subgoals. Due to\ntheir inherent capability to understand RL environments based on a textual\ndescription of structure and purpose, LLMs can provide subgoals to accomplish\nthe task defined for the environment in a similar fashion to how a human would\ndo. In doing so, three types of subgoals are proposed: positional targets\nrelative to the agent, object representations, and language-based instructions\ngenerated directly by the LLM. More importantly, we show that it is possible to\nquery the LLM only during the training phase, enabling agents to operate within\nthe environment without any LLM intervention. We assess the performance of this\nproposed framework by evaluating three state-of-the-art open-source LLMs\n(Llama, DeepSeek, Qwen) eliciting subgoals across various procedurally\ngenerated environment of the MiniGrid benchmark. Experimental results\ndemonstrate that this curriculum-based approach accelerates learning and\nenhances exploration in complex tasks, achieving up to 30 to 200 times faster\nconvergence in training steps compared to recent baselines designed for sparse\nreward environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse reward environments in reinforcement learning (RL) pose significant\nchallenges for exploration, often leading to inefficient or incomplete learning\nprocesses. To tackle this issue, this work proposes a teacher-student RL\nframework that leverages Large Language Models (LLMs) as \"teachers\" to guide\nthe agent's learning process by decomposing complex tasks into subgoals. Due to\ntheir inherent capability to understand RL environments based on a textual\ndescription of structure and purpose, LLMs can provide subgoals to accomplish\nthe task defined for the environment in a similar fashion to how a human would\ndo. In doing so, three types of subgoals are proposed: positional targets\nrelative to the agent, object representations, and language-based instructions\ngenerated directly by the LLM. More importantly, we show that it is possible to\nquery the LLM only during the training phase, enabling agents to operate within\nthe environment without any LLM intervention. We assess the performance of this\nproposed framework by evaluating three state-of-the-art open-source LLMs\n(Llama, DeepSeek, Qwen) eliciting subgoals across various procedurally\ngenerated environment of the MiniGrid benchmark. Experimental results\ndemonstrate that this curriculum-based approach accelerates learning and\nenhances exploration in complex tasks, achieving up to 30 to 200 times faster\nconvergence in training steps compared to recent baselines designed for sparse\nreward environments."
                },
                "authors": [
                    {
                        "name": "Unai Ruiz-Gonzalez"
                    },
                    {
                        "name": "Alain Andres"
                    },
                    {
                        "name": "Pedro G. Bascoy"
                    },
                    {
                        "name": "Javier Del Ser"
                    }
                ],
                "author_detail": {
                    "name": "Javier Del Ser"
                },
                "author": "Javier Del Ser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11242v2",
                "updated": "2024-10-11T08:53:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    53,
                    31,
                    4,
                    285,
                    0
                ],
                "published": "2024-09-17T14:47:33Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    47,
                    33,
                    1,
                    261,
                    0
                ],
                "title": "Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded\n  Attributions and Learning to Refuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded\n  Attributions and Learning to Refuse"
                },
                "summary": "LLMs are an integral component of retrieval-augmented generation (RAG)\nsystems. While many studies focus on evaluating the overall quality of\nend-to-end RAG systems, there is a gap in understanding the appropriateness of\nLLMs for the RAG task. To address this, we introduce Trust-Score, a holistic\nmetric that evaluates the trustworthiness of LLMs within the RAG framework. Our\nresults show that various prompting methods, such as in-context learning, fail\nto effectively adapt LLMs to the RAG task as measured by Trust-Score.\nConsequently, we propose Trust-Align, a method to align LLMs for improved\nTrust-Score performance. The LLaMA-3 family, aligned using our method,\nsignificantly outperforms open-source LLMs of similar sizes on ASQA (up 14.0),\nQAMPARI (up 28.9), and ELI5 (up 13.7). We also demonstrate the effectiveness of\nTrust-Align across different open-weight models, including the LLaMA series (1b\nto 8b), Qwen-2.5 series (0.5b to 7b), and Phi3.5 (3.8b). We release our code at\n\\url{https://anonymous.4open.science/r/trust-align}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are an integral component of retrieval-augmented generation (RAG)\nsystems. While many studies focus on evaluating the overall quality of\nend-to-end RAG systems, there is a gap in understanding the appropriateness of\nLLMs for the RAG task. To address this, we introduce Trust-Score, a holistic\nmetric that evaluates the trustworthiness of LLMs within the RAG framework. Our\nresults show that various prompting methods, such as in-context learning, fail\nto effectively adapt LLMs to the RAG task as measured by Trust-Score.\nConsequently, we propose Trust-Align, a method to align LLMs for improved\nTrust-Score performance. The LLaMA-3 family, aligned using our method,\nsignificantly outperforms open-source LLMs of similar sizes on ASQA (up 14.0),\nQAMPARI (up 28.9), and ELI5 (up 13.7). We also demonstrate the effectiveness of\nTrust-Align across different open-weight models, including the LLaMA series (1b\nto 8b), Qwen-2.5 series (0.5b to 7b), and Phi3.5 (3.8b). We release our code at\n\\url{https://anonymous.4open.science/r/trust-align}"
                },
                "authors": [
                    {
                        "name": "Maojia Song"
                    },
                    {
                        "name": "Shang Hong Sim"
                    },
                    {
                        "name": "Rishabh Bhardwaj"
                    },
                    {
                        "name": "Hai Leong Chieu"
                    },
                    {
                        "name": "Navonil Majumder"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00159v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00159v2",
                "updated": "2024-10-11T08:29:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    29,
                    7,
                    4,
                    285,
                    0
                ],
                "published": "2024-08-30T15:04:11Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    15,
                    4,
                    11,
                    4,
                    243,
                    0
                ],
                "title": "LLMs hallucinate graphs too: a structural perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs hallucinate graphs too: a structural perspective"
                },
                "summary": "It is known that LLMs do hallucinate, that is, they return incorrect\ninformation as facts. In this paper, we introduce the possibility to study\nthese hallucinations under a structured form: graphs. Hallucinations in this\ncontext are incorrect outputs when prompted for well known graphs from the\nliterature (e.g. Karate club, Les Mis\\'erables, graph atlas). These\nhallucinated graphs have the advantage of being much richer than the factual\naccuracy -- or not -- of a statement; this paper thus argues that such rich\nhallucinations can be used to characterize the outputs of LLMs. Our first\ncontribution observes the diversity of topological hallucinations from major\nmodern LLMs. Our second contribution is the proposal of a metric for the\namplitude of such hallucinations: the Graph Atlas Distance, that is the average\ngraph edit distance from several graphs in the graph atlas set. We compare this\nmetric to the Hallucination Leaderboard, a hallucination rank that leverages\n10,000 times more prompts to obtain its ranking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is known that LLMs do hallucinate, that is, they return incorrect\ninformation as facts. In this paper, we introduce the possibility to study\nthese hallucinations under a structured form: graphs. Hallucinations in this\ncontext are incorrect outputs when prompted for well known graphs from the\nliterature (e.g. Karate club, Les Mis\\'erables, graph atlas). These\nhallucinated graphs have the advantage of being much richer than the factual\naccuracy -- or not -- of a statement; this paper thus argues that such rich\nhallucinations can be used to characterize the outputs of LLMs. Our first\ncontribution observes the diversity of topological hallucinations from major\nmodern LLMs. Our second contribution is the proposal of a metric for the\namplitude of such hallucinations: the Graph Atlas Distance, that is the average\ngraph edit distance from several graphs in the graph atlas set. We compare this\nmetric to the Hallucination Leaderboard, a hallucination rank that leverages\n10,000 times more prompts to obtain its ranking."
                },
                "authors": [
                    {
                        "name": "Erwan Le Merrer"
                    },
                    {
                        "name": "Gilles Tredan"
                    }
                ],
                "author_detail": {
                    "name": "Gilles Tredan"
                },
                "author": "Gilles Tredan",
                "arxiv_journal_ref": "COMPLEX NETWORKS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00159v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00159v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19085v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19085v3",
                "updated": "2024-10-11T08:21:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    21,
                    50,
                    4,
                    285,
                    0
                ],
                "published": "2024-02-29T12:12:30Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    12,
                    12,
                    30,
                    3,
                    60,
                    0
                ],
                "title": "Controllable Preference Optimization: Toward Controllable\n  Multi-Objective Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllable Preference Optimization: Toward Controllable\n  Multi-Objective Alignment"
                },
                "summary": "Alignment in artificial intelligence pursues the consistency between model\nresponses and human preferences as well as values. In practice, the\nmultifaceted nature of human preferences inadvertently introduces what is known\nas the \"alignment tax\" -a compromise where enhancements in alignment within one\nobjective (e.g.,harmlessness) can diminish performance in others\n(e.g.,helpfulness). However, existing alignment techniques are mostly\nunidirectional, leading to suboptimal trade-offs and poor flexibility over\nvarious objectives. To navigate this challenge, we argue the prominence of\ngrounding LLMs with evident preferences. We introduce controllable preference\noptimization (CPO), which explicitly specifies preference scores for different\nobjectives, thereby guiding the model to generate responses that meet the\nrequirements. Our experimental analysis reveals that the aligned models can\nprovide responses that match various preferences among the \"3H\" (helpfulness,\nhonesty, harmlessness) desiderata. Furthermore, by introducing diverse data and\nalignment goals, we surpass baseline methods in aligning with single\nobjectives, hence mitigating the impact of the alignment tax and achieving\nimprovements in multi-objective alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment in artificial intelligence pursues the consistency between model\nresponses and human preferences as well as values. In practice, the\nmultifaceted nature of human preferences inadvertently introduces what is known\nas the \"alignment tax\" -a compromise where enhancements in alignment within one\nobjective (e.g.,harmlessness) can diminish performance in others\n(e.g.,helpfulness). However, existing alignment techniques are mostly\nunidirectional, leading to suboptimal trade-offs and poor flexibility over\nvarious objectives. To navigate this challenge, we argue the prominence of\ngrounding LLMs with evident preferences. We introduce controllable preference\noptimization (CPO), which explicitly specifies preference scores for different\nobjectives, thereby guiding the model to generate responses that meet the\nrequirements. Our experimental analysis reveals that the aligned models can\nprovide responses that match various preferences among the \"3H\" (helpfulness,\nhonesty, harmlessness) desiderata. Furthermore, by introducing diverse data and\nalignment goals, we surpass baseline methods in aligning with single\nobjectives, hence mitigating the impact of the alignment tax and achieving\nimprovements in multi-objective alignment."
                },
                "authors": [
                    {
                        "name": "Yiju Guo"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Lifan Yuan"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Zexu Sun"
                    },
                    {
                        "name": "Bowen Sun"
                    },
                    {
                        "name": "Huimin Chen"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "EMNLP 2024 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19085v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19085v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.08295v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.08295v3",
                "updated": "2024-10-11T08:11:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    11,
                    10,
                    4,
                    285,
                    0
                ],
                "published": "2023-08-16T11:50:38Z",
                "published_parsed": [
                    2023,
                    8,
                    16,
                    11,
                    50,
                    38,
                    2,
                    228,
                    0
                ],
                "title": "CMD: a framework for Context-aware Model self-Detoxification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMD: a framework for Context-aware Model self-Detoxification"
                },
                "summary": "Text detoxification aims to minimize the risk of language models producing\ntoxic content. Existing detoxification methods of directly constraining the\nmodel output or further training the model on the non-toxic corpus fail to\nachieve a decent balance between detoxification effectiveness and generation\nquality. This issue stems from the neglect of constrain imposed by the context\nsince language models are designed to generate output that closely matches the\ncontext while detoxification methods endeavor to ensure the safety of the\noutput even if it semantically deviates from the context. In view of this, we\nintroduce a Context-aware Model self-Detoxification~(CMD) framework that pays\nattention to both the context and the detoxification process, i.e., first\ndetoxifying the context and then making the language model generate along the\nsafe context. Specifically, CMD framework involves two phases: utilizing\nlanguage models to synthesize data and applying these data for training. We\nalso introduce a toxic contrastive loss that encourages the model generation\naway from the negative toxic samples. Experiments on various LLMs have verified\nthe effectiveness of our MSD framework, which can yield the best performance\ncompared to baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text detoxification aims to minimize the risk of language models producing\ntoxic content. Existing detoxification methods of directly constraining the\nmodel output or further training the model on the non-toxic corpus fail to\nachieve a decent balance between detoxification effectiveness and generation\nquality. This issue stems from the neglect of constrain imposed by the context\nsince language models are designed to generate output that closely matches the\ncontext while detoxification methods endeavor to ensure the safety of the\noutput even if it semantically deviates from the context. In view of this, we\nintroduce a Context-aware Model self-Detoxification~(CMD) framework that pays\nattention to both the context and the detoxification process, i.e., first\ndetoxifying the context and then making the language model generate along the\nsafe context. Specifically, CMD framework involves two phases: utilizing\nlanguage models to synthesize data and applying these data for training. We\nalso introduce a toxic contrastive loss that encourages the model generation\naway from the negative toxic samples. Experiments on various LLMs have verified\nthe effectiveness of our MSD framework, which can yield the best performance\ncompared to baselines."
                },
                "authors": [
                    {
                        "name": "Zecheng Tang"
                    },
                    {
                        "name": "Keyan Zhou"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Yuyang Ding"
                    },
                    {
                        "name": "Pinzheng Wang"
                    },
                    {
                        "name": "Bowen Yan"
                    },
                    {
                        "name": "Rejie Hua"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.08295v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.08295v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19119v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19119v2",
                "updated": "2024-10-11T08:03:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    3,
                    4,
                    4,
                    285,
                    0
                ],
                "published": "2024-05-29T14:26:24Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    14,
                    26,
                    24,
                    2,
                    150,
                    0
                ],
                "title": "Can Graph Learning Improve Planning in LLM-based Agents?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Graph Learning Improve Planning in LLM-based Agents?"
                },
                "summary": "Task planning in language agents is emerging as an important research topic\nalongside the development of large language models (LLMs). It aims to break\ndown complex user requests in natural language into solvable sub-tasks, thereby\nfulfilling the original requests. In this context, the sub-tasks can be\nnaturally viewed as a graph, where the nodes represent the sub-tasks, and the\nedges denote the dependencies among them. Consequently, task planning is a\ndecision-making problem that involves selecting a connected path or subgraph\nwithin the corresponding graph and invoking it. In this paper, we explore graph\nlearning-based methods for task planning, a direction that is orthogonal to the\nprevalent focus on prompt design. Our interest in graph learning stems from a\ntheoretical discovery: the biases of attention and auto-regressive loss impede\nLLMs' ability to effectively navigate decision-making on graphs, which is\nadeptly addressed by graph neural networks (GNNs). This theoretical insight led\nus to integrate GNNs with LLMs to enhance overall performance. Extensive\nexperiments demonstrate that GNN-based methods surpass existing solutions even\nwithout training, and minimal training can further enhance their performance.\nThe performance gain increases with a larger task graph size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task planning in language agents is emerging as an important research topic\nalongside the development of large language models (LLMs). It aims to break\ndown complex user requests in natural language into solvable sub-tasks, thereby\nfulfilling the original requests. In this context, the sub-tasks can be\nnaturally viewed as a graph, where the nodes represent the sub-tasks, and the\nedges denote the dependencies among them. Consequently, task planning is a\ndecision-making problem that involves selecting a connected path or subgraph\nwithin the corresponding graph and invoking it. In this paper, we explore graph\nlearning-based methods for task planning, a direction that is orthogonal to the\nprevalent focus on prompt design. Our interest in graph learning stems from a\ntheoretical discovery: the biases of attention and auto-regressive loss impede\nLLMs' ability to effectively navigate decision-making on graphs, which is\nadeptly addressed by graph neural networks (GNNs). This theoretical insight led\nus to integrate GNNs with LLMs to enhance overall performance. Extensive\nexperiments demonstrate that GNN-based methods surpass existing solutions even\nwithout training, and minimal training can further enhance their performance.\nThe performance gain increases with a larger task graph size."
                },
                "authors": [
                    {
                        "name": "Xixi Wu"
                    },
                    {
                        "name": "Yifei Shen"
                    },
                    {
                        "name": "Caihua Shan"
                    },
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Bohang Zhang"
                    },
                    {
                        "name": "Jiarui Feng"
                    },
                    {
                        "name": "Hong Cheng"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Yun Xiong"
                    },
                    {
                        "name": "Dongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Dongsheng Li"
                },
                "author": "Dongsheng Li",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19119v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19119v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08604v1",
                "updated": "2024-10-11T08:00:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    0,
                    49,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T08:00:49Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    0,
                    49,
                    4,
                    285,
                    0
                ],
                "title": "MergePrint: Robust Fingerprinting against Merging Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MergePrint: Robust Fingerprinting against Merging Large Language Models"
                },
                "summary": "As the cost of training large language models (LLMs) rises, protecting their\nintellectual property has become increasingly critical. Model merging, which\nintegrates multiple expert models into a single model capable of performing\nmultiple tasks, presents a growing risk of unauthorized and malicious usage.\nWhile fingerprinting techniques have been studied for asserting model\nownership, existing methods have primarily focused on fine-tuning, leaving\nmodel merging underexplored. To address this gap, we propose a novel\nfingerprinting method MergePrint that embeds robust fingerprints designed to\npreserve ownership claims even after model merging. By optimizing against a\npseudo-merged model, which simulates post-merged model weights, MergePrint\ngenerates fingerprints that remain detectable after merging. Additionally, we\noptimize the fingerprint inputs to minimize performance degradation, enabling\nverification through specific outputs from targeted inputs. This approach\nprovides a practical fingerprinting strategy for asserting ownership in cases\nof misappropriation through model merging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the cost of training large language models (LLMs) rises, protecting their\nintellectual property has become increasingly critical. Model merging, which\nintegrates multiple expert models into a single model capable of performing\nmultiple tasks, presents a growing risk of unauthorized and malicious usage.\nWhile fingerprinting techniques have been studied for asserting model\nownership, existing methods have primarily focused on fine-tuning, leaving\nmodel merging underexplored. To address this gap, we propose a novel\nfingerprinting method MergePrint that embeds robust fingerprints designed to\npreserve ownership claims even after model merging. By optimizing against a\npseudo-merged model, which simulates post-merged model weights, MergePrint\ngenerates fingerprints that remain detectable after merging. Additionally, we\noptimize the fingerprint inputs to minimize performance degradation, enabling\nverification through specific outputs from targeted inputs. This approach\nprovides a practical fingerprinting strategy for asserting ownership in cases\nof misappropriation through model merging."
                },
                "authors": [
                    {
                        "name": "Shojiro Yamabe"
                    },
                    {
                        "name": "Tsubasa Takahashi"
                    },
                    {
                        "name": "Futa Waseda"
                    },
                    {
                        "name": "Koki Wataoka"
                    }
                ],
                "author_detail": {
                    "name": "Koki Wataoka"
                },
                "author": "Koki Wataoka",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08601v1",
                "updated": "2024-10-11T07:55:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    55,
                    42,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T07:55:42Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    55,
                    42,
                    4,
                    285,
                    0
                ],
                "title": "StraGo: Harnessing Strategic Guidance for Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StraGo: Harnessing Strategic Guidance for Prompt Optimization"
                },
                "summary": "Prompt engineering is pivotal for harnessing the capabilities of large\nlanguage models (LLMs) across diverse applications. While existing prompt\noptimization methods improve prompt effectiveness, they often lead to prompt\ndrifting, where newly generated prompts can adversely impact previously\nsuccessful cases while addressing failures. Furthermore, these methods tend to\nrely heavily on LLMs' intrinsic capabilities for prompt optimization tasks. In\nthis paper, we introduce StraGo (Strategic-Guided Optimization), a novel\napproach designed to mitigate prompt drifting by leveraging insights from both\nsuccessful and failed cases to identify critical factors for achieving\noptimization objectives. StraGo employs a how-to-do methodology, integrating\nin-context learning to formulate specific, actionable strategies that provide\ndetailed, step-by-step guidance for prompt optimization. Extensive experiments\nconducted across a range of tasks, including reasoning, natural language\nunderstanding, domain-specific knowledge, and industrial applications,\ndemonstrate StraGo's superior performance. It establishes a new\nstate-of-the-art in prompt optimization, showcasing its ability to deliver\nstable and effective prompt improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt engineering is pivotal for harnessing the capabilities of large\nlanguage models (LLMs) across diverse applications. While existing prompt\noptimization methods improve prompt effectiveness, they often lead to prompt\ndrifting, where newly generated prompts can adversely impact previously\nsuccessful cases while addressing failures. Furthermore, these methods tend to\nrely heavily on LLMs' intrinsic capabilities for prompt optimization tasks. In\nthis paper, we introduce StraGo (Strategic-Guided Optimization), a novel\napproach designed to mitigate prompt drifting by leveraging insights from both\nsuccessful and failed cases to identify critical factors for achieving\noptimization objectives. StraGo employs a how-to-do methodology, integrating\nin-context learning to formulate specific, actionable strategies that provide\ndetailed, step-by-step guidance for prompt optimization. Extensive experiments\nconducted across a range of tasks, including reasoning, natural language\nunderstanding, domain-specific knowledge, and industrial applications,\ndemonstrate StraGo's superior performance. It establishes a new\nstate-of-the-art in prompt optimization, showcasing its ability to deliver\nstable and effective prompt improvements."
                },
                "authors": [
                    {
                        "name": "Yurong Wu"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Bin Benjamin Zhu"
                    },
                    {
                        "name": "Zineng Zhou"
                    },
                    {
                        "name": "Xiaodi Sun"
                    },
                    {
                        "name": "Sheng Yang"
                    },
                    {
                        "name": "Jian-Guang Lou"
                    },
                    {
                        "name": "Zhiming Ding"
                    },
                    {
                        "name": "Linjun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Linjun Yang"
                },
                "author": "Linjun Yang",
                "arxiv_comment": "19 pages, 3 figures, 20 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08598v1",
                "updated": "2024-10-11T07:55:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    55,
                    9,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T07:55:09Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    55,
                    9,
                    4,
                    285,
                    0
                ],
                "title": "Parameter-Efficient Fine-Tuning of Large Language Models using Semantic\n  Knowledge Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-Efficient Fine-Tuning of Large Language Models using Semantic\n  Knowledge Tuning"
                },
                "summary": "Large Language Models (LLMs) are gaining significant popularity in recent\nyears for specialized tasks using prompts due to their low computational cost.\nStandard methods like prefix tuning utilize special, modifiable tokens that\nlack semantic meaning and require extensive training for best performance,\noften falling short. In this context, we propose a novel method called Semantic\nKnowledge Tuning (SK-Tuning) for prompt and prefix tuning that employs\nmeaningful words instead of random tokens. This method involves using a fixed\nLLM to understand and process the semantic content of the prompt through\nzero-shot capabilities. Following this, it integrates the processed prompt with\nthe input text to improve the model's performance on particular tasks. Our\nexperimental results show that SK-Tuning exhibits faster training times, fewer\nparameters, and superior performance on tasks such as text classification and\nunderstanding compared to other tuning methods. This approach offers a\npromising method for optimizing the efficiency and effectiveness of LLMs in\nprocessing language tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are gaining significant popularity in recent\nyears for specialized tasks using prompts due to their low computational cost.\nStandard methods like prefix tuning utilize special, modifiable tokens that\nlack semantic meaning and require extensive training for best performance,\noften falling short. In this context, we propose a novel method called Semantic\nKnowledge Tuning (SK-Tuning) for prompt and prefix tuning that employs\nmeaningful words instead of random tokens. This method involves using a fixed\nLLM to understand and process the semantic content of the prompt through\nzero-shot capabilities. Following this, it integrates the processed prompt with\nthe input text to improve the model's performance on particular tasks. Our\nexperimental results show that SK-Tuning exhibits faster training times, fewer\nparameters, and superior performance on tasks such as text classification and\nunderstanding compared to other tuning methods. This approach offers a\npromising method for optimizing the efficiency and effectiveness of LLMs in\nprocessing language tasks."
                },
                "authors": [
                    {
                        "name": "Nusrat Jahan Prottasha"
                    },
                    {
                        "name": "Asif Mahmud"
                    },
                    {
                        "name": "Md. Shohanur Islam Sobuj"
                    },
                    {
                        "name": "Prakash Bhat"
                    },
                    {
                        "name": "Md Kowsher"
                    },
                    {
                        "name": "Niloofar Yousefi"
                    },
                    {
                        "name": "Ozlem Ozmen Garibay"
                    }
                ],
                "author_detail": {
                    "name": "Ozlem Ozmen Garibay"
                },
                "author": "Ozlem Ozmen Garibay",
                "arxiv_comment": "Accepted in Nature Scientific Reports",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08593v1",
                "updated": "2024-10-11T07:42:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    42,
                    36,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T07:42:36Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    42,
                    36,
                    4,
                    285,
                    0
                ],
                "title": "VERIFIED: A Video Corpus Moment Retrieval Benchmark for Fine-Grained\n  Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VERIFIED: A Video Corpus Moment Retrieval Benchmark for Fine-Grained\n  Video Understanding"
                },
                "summary": "Existing Video Corpus Moment Retrieval (VCMR) is limited to coarse-grained\nunderstanding, which hinders precise video moment localization when given\nfine-grained queries. In this paper, we propose a more challenging fine-grained\nVCMR benchmark requiring methods to localize the best-matched moment from the\ncorpus with other partially matched candidates. To improve the dataset\nconstruction efficiency and guarantee high-quality data annotations, we propose\nVERIFIED, an automatic \\underline{V}id\\underline{E}o-text annotation pipeline\nto generate captions with \\underline{R}el\\underline{I}able\n\\underline{FI}n\\underline{E}-grained statics and \\underline{D}ynamics.\nSpecifically, we resort to large language models (LLM) and large multimodal\nmodels (LMM) with our proposed Statics and Dynamics Enhanced Captioning modules\nto generate diverse fine-grained captions for each video. To filter out the\ninaccurate annotations caused by the LLM hallucination, we propose a\nFine-Granularity Aware Noise Evaluator where we fine-tune a video foundation\nmodel with disturbed hard-negatives augmented contrastive and matching losses.\nWith VERIFIED, we construct a more challenging fine-grained VCMR benchmark\ncontaining Charades-FIG, DiDeMo-FIG, and ActivityNet-FIG which demonstrate a\nhigh level of annotation quality. We evaluate several state-of-the-art VCMR\nmodels on the proposed dataset, revealing that there is still significant scope\nfor fine-grained video understanding in VCMR. Code and Datasets are in\n\\href{https://github.com/hlchen23/VERIFIED}{https://github.com/hlchen23/VERIFIED}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing Video Corpus Moment Retrieval (VCMR) is limited to coarse-grained\nunderstanding, which hinders precise video moment localization when given\nfine-grained queries. In this paper, we propose a more challenging fine-grained\nVCMR benchmark requiring methods to localize the best-matched moment from the\ncorpus with other partially matched candidates. To improve the dataset\nconstruction efficiency and guarantee high-quality data annotations, we propose\nVERIFIED, an automatic \\underline{V}id\\underline{E}o-text annotation pipeline\nto generate captions with \\underline{R}el\\underline{I}able\n\\underline{FI}n\\underline{E}-grained statics and \\underline{D}ynamics.\nSpecifically, we resort to large language models (LLM) and large multimodal\nmodels (LMM) with our proposed Statics and Dynamics Enhanced Captioning modules\nto generate diverse fine-grained captions for each video. To filter out the\ninaccurate annotations caused by the LLM hallucination, we propose a\nFine-Granularity Aware Noise Evaluator where we fine-tune a video foundation\nmodel with disturbed hard-negatives augmented contrastive and matching losses.\nWith VERIFIED, we construct a more challenging fine-grained VCMR benchmark\ncontaining Charades-FIG, DiDeMo-FIG, and ActivityNet-FIG which demonstrate a\nhigh level of annotation quality. We evaluate several state-of-the-art VCMR\nmodels on the proposed dataset, revealing that there is still significant scope\nfor fine-grained video understanding in VCMR. Code and Datasets are in\n\\href{https://github.com/hlchen23/VERIFIED}{https://github.com/hlchen23/VERIFIED}."
                },
                "authors": [
                    {
                        "name": "Houlun Chen"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Zeyang Zhang"
                    },
                    {
                        "name": "Wei Feng"
                    },
                    {
                        "name": "Bin Huang"
                    },
                    {
                        "name": "Jia Jia"
                    },
                    {
                        "name": "Wenwu Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Wenwu Zhu"
                },
                "author": "Wenwu Zhu",
                "arxiv_comment": "Accepted by 38th NeurIPS Datasets & Benchmarks Track (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08589v1",
                "updated": "2024-10-11T07:36:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    36,
                    14,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T07:36:14Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    36,
                    14,
                    4,
                    285,
                    0
                ],
                "title": "Retraining-Free Merging of Sparse Mixture-of-Experts via Hierarchical\n  Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retraining-Free Merging of Sparse Mixture-of-Experts via Hierarchical\n  Clustering"
                },
                "summary": "Sparse Mixture-of-Experts (SMoE) models represent a significant breakthrough\nin large language model development. These models enable performance\nimprovements without a proportional increase in inference costs. By selectively\nactivating a small set of parameters during task execution, SMoEs enhance model\ncapacity. However, their deployment remains challenging due to the substantial\nmemory footprint required to accommodate the growing number of experts. This\nconstraint renders them less feasible in environments with limited hardware\nresources. To address this challenge, we propose Hierarchical Clustering for\nSparsely activated Mixture of Experts (HC-SMoE), a task-agnostic expert merging\nframework that reduces SMoE model parameters without retraining. Unlike\nprevious methods, HC-SMoE employs hierarchical clustering based on expert\noutputs. This approach ensures that the merging process remains unaffected by\nrouting decisions. The output-based clustering strategy captures functional\nsimilarities between experts, offering an adaptable solution for models with\nnumerous experts. We validate our approach through extensive experiments on\neight zero-shot language tasks and demonstrate its effectiveness in large-scale\nSMoE models such as Qwen and Mixtral. Our comprehensive results demonstrate\nthat HC-SMoE consistently achieves strong performance, which highlights its\npotential for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture-of-Experts (SMoE) models represent a significant breakthrough\nin large language model development. These models enable performance\nimprovements without a proportional increase in inference costs. By selectively\nactivating a small set of parameters during task execution, SMoEs enhance model\ncapacity. However, their deployment remains challenging due to the substantial\nmemory footprint required to accommodate the growing number of experts. This\nconstraint renders them less feasible in environments with limited hardware\nresources. To address this challenge, we propose Hierarchical Clustering for\nSparsely activated Mixture of Experts (HC-SMoE), a task-agnostic expert merging\nframework that reduces SMoE model parameters without retraining. Unlike\nprevious methods, HC-SMoE employs hierarchical clustering based on expert\noutputs. This approach ensures that the merging process remains unaffected by\nrouting decisions. The output-based clustering strategy captures functional\nsimilarities between experts, offering an adaptable solution for models with\nnumerous experts. We validate our approach through extensive experiments on\neight zero-shot language tasks and demonstrate its effectiveness in large-scale\nSMoE models such as Qwen and Mixtral. Our comprehensive results demonstrate\nthat HC-SMoE consistently achieves strong performance, which highlights its\npotential for real-world deployment."
                },
                "authors": [
                    {
                        "name": "I-Chun Chen"
                    },
                    {
                        "name": "Hsu-Shen Liu"
                    },
                    {
                        "name": "Wei-Fang Sun"
                    },
                    {
                        "name": "Chen-Hao Chao"
                    },
                    {
                        "name": "Yen-Chang Hsu"
                    },
                    {
                        "name": "Chun-Yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Chun-Yi Lee"
                },
                "author": "Chun-Yi Lee",
                "arxiv_comment": "Code: https://github.com/wazenmai/HC-SMoE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12739v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12739v2",
                "updated": "2024-10-11T06:18:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    6,
                    18,
                    30,
                    4,
                    285,
                    0
                ],
                "published": "2024-05-21T12:47:17Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    47,
                    17,
                    1,
                    142,
                    0
                ],
                "title": "SPO: Multi-Dimensional Preference Sequential Alignment With Implicit\n  Reward Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPO: Multi-Dimensional Preference Sequential Alignment With Implicit\n  Reward Modeling"
                },
                "summary": "Human preference alignment is critical in building powerful and reliable\nlarge language models (LLMs). However, current methods either ignore the\nmulti-dimensionality of human preferences (e.g. helpfulness and harmlessness)\nor struggle with the complexity of managing multiple reward models. To address\nthese issues, we propose Sequential Preference Optimization (SPO), a method\nthat sequentially fine-tunes LLMs to align with multiple dimensions of human\npreferences. SPO avoids explicit reward modeling, directly optimizing the\nmodels to align with nuanced human preferences. We theoretically derive\nclosed-form optimal SPO policy and loss function. Gradient analysis is\nconducted to show how SPO manages to fine-tune the LLMs while maintaining\nalignment on previously optimized dimensions. Empirical results on LLMs of\ndifferent size and multiple evaluation datasets demonstrate that SPO\nsuccessfully aligns LLMs across multiple dimensions of human preferences and\nsignificantly outperforms the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human preference alignment is critical in building powerful and reliable\nlarge language models (LLMs). However, current methods either ignore the\nmulti-dimensionality of human preferences (e.g. helpfulness and harmlessness)\nor struggle with the complexity of managing multiple reward models. To address\nthese issues, we propose Sequential Preference Optimization (SPO), a method\nthat sequentially fine-tunes LLMs to align with multiple dimensions of human\npreferences. SPO avoids explicit reward modeling, directly optimizing the\nmodels to align with nuanced human preferences. We theoretically derive\nclosed-form optimal SPO policy and loss function. Gradient analysis is\nconducted to show how SPO manages to fine-tune the LLMs while maintaining\nalignment on previously optimized dimensions. Empirical results on LLMs of\ndifferent size and multiple evaluation datasets demonstrate that SPO\nsuccessfully aligns LLMs across multiple dimensions of human preferences and\nsignificantly outperforms the baselines."
                },
                "authors": [
                    {
                        "name": "Xingzhou Lou"
                    },
                    {
                        "name": "Junge Zhang"
                    },
                    {
                        "name": "Jian Xie"
                    },
                    {
                        "name": "Lifeng Liu"
                    },
                    {
                        "name": "Dong Yan"
                    },
                    {
                        "name": "Kaiqi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaiqi Huang"
                },
                "author": "Kaiqi Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12739v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12739v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08545v1",
                "updated": "2024-10-11T05:53:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    5,
                    53,
                    11,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T05:53:11Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    5,
                    53,
                    11,
                    4,
                    285,
                    0
                ],
                "title": "Humanity in AI: Detecting the Personality of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humanity in AI: Detecting the Personality of Large Language Models"
                },
                "summary": "Questionnaires are a common method for detecting the personality of Large\nLanguage Models (LLMs). However, their reliability is often compromised by two\nmain issues: hallucinations (where LLMs produce inaccurate or irrelevant\nresponses) and the sensitivity of responses to the order of the presented\noptions. To address these issues, we propose combining text mining with\nquestionnaires method. Text mining can extract psychological features from the\nLLMs' responses without being affected by the order of options. Furthermore,\nbecause this method does not rely on specific answers, it reduces the influence\nof hallucinations. By normalizing the scores from both methods and calculating\nthe root mean square error, our experiment results confirm the effectiveness of\nthis approach. To further investigate the origins of personality traits in\nLLMs, we conduct experiments on both pre-trained language models (PLMs), such\nas BERT and GPT, as well as conversational models (ChatLLMs), such as ChatGPT.\nThe results show that LLMs do contain certain personalities, for example,\nChatGPT and ChatGLM exhibit the personality traits of 'Conscientiousness'.\nAdditionally, we find that the personalities of LLMs are derived from their\npre-trained data. The instruction data used to train ChatLLMs can enhance the\ngeneration of data containing personalities and expose their hidden\npersonality. We compare the results with the human average personality score,\nand we find that the personality of FLAN-T5 in PLMs and ChatGPT in ChatLLMs is\nmore similar to that of a human, with score differences of 0.34 and 0.22,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Questionnaires are a common method for detecting the personality of Large\nLanguage Models (LLMs). However, their reliability is often compromised by two\nmain issues: hallucinations (where LLMs produce inaccurate or irrelevant\nresponses) and the sensitivity of responses to the order of the presented\noptions. To address these issues, we propose combining text mining with\nquestionnaires method. Text mining can extract psychological features from the\nLLMs' responses without being affected by the order of options. Furthermore,\nbecause this method does not rely on specific answers, it reduces the influence\nof hallucinations. By normalizing the scores from both methods and calculating\nthe root mean square error, our experiment results confirm the effectiveness of\nthis approach. To further investigate the origins of personality traits in\nLLMs, we conduct experiments on both pre-trained language models (PLMs), such\nas BERT and GPT, as well as conversational models (ChatLLMs), such as ChatGPT.\nThe results show that LLMs do contain certain personalities, for example,\nChatGPT and ChatGLM exhibit the personality traits of 'Conscientiousness'.\nAdditionally, we find that the personalities of LLMs are derived from their\npre-trained data. The instruction data used to train ChatLLMs can enhance the\ngeneration of data containing personalities and expose their hidden\npersonality. We compare the results with the human average personality score,\nand we find that the personality of FLAN-T5 in PLMs and ChatGPT in ChatLLMs is\nmore similar to that of a human, with score differences of 0.34 and 0.22,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Baohua Zhan"
                    },
                    {
                        "name": "Yongyi Huang"
                    },
                    {
                        "name": "Wenyao Cui"
                    },
                    {
                        "name": "Huaping Zhang"
                    },
                    {
                        "name": "Jianyun Shang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyun Shang"
                },
                "author": "Jianyun Shang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21054v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21054v3",
                "updated": "2024-10-11T05:43:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    5,
                    43,
                    19,
                    4,
                    285,
                    0
                ],
                "published": "2024-07-24T12:07:54Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    12,
                    7,
                    54,
                    2,
                    206,
                    0
                ],
                "title": "Sentiment Reasoning for Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentiment Reasoning for Healthcare"
                },
                "summary": "Transparency in AI healthcare decision-making is crucial for building trust\namong AI and users. Incorporating reasoning capabilities enables Large Language\nModels (LLMs) to understand emotions in context, handle nuanced language, and\ninfer unstated sentiments. In this work, we introduce a new task -- Sentiment\nReasoning -- for both speech and text modalities, along with our proposed\nmultimodal multitask framework and dataset. Sentiment Reasoning is an auxiliary\ntask in sentiment analysis where the model predicts both the sentiment label\nand generates the rationale behind it based on the input transcript. Our study\nconducted on both human transcripts and Automatic Speech Recognition (ASR)\ntranscripts shows that Sentiment Reasoning helps improve model transparency by\nproviding rationale for model prediction with quality semantically comparable\nto humans while also improving model performance (1% increase in both accuracy\nand macro-F1) via rationale-augmented fine-tuning. Also, no significant\ndifference in the semantic quality of generated rationales between human and\nASR transcripts. All code, data (English-translated and Vietnamese) and models\nare published online: https://github.com/leduckhai/MultiMed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transparency in AI healthcare decision-making is crucial for building trust\namong AI and users. Incorporating reasoning capabilities enables Large Language\nModels (LLMs) to understand emotions in context, handle nuanced language, and\ninfer unstated sentiments. In this work, we introduce a new task -- Sentiment\nReasoning -- for both speech and text modalities, along with our proposed\nmultimodal multitask framework and dataset. Sentiment Reasoning is an auxiliary\ntask in sentiment analysis where the model predicts both the sentiment label\nand generates the rationale behind it based on the input transcript. Our study\nconducted on both human transcripts and Automatic Speech Recognition (ASR)\ntranscripts shows that Sentiment Reasoning helps improve model transparency by\nproviding rationale for model prediction with quality semantically comparable\nto humans while also improving model performance (1% increase in both accuracy\nand macro-F1) via rationale-augmented fine-tuning. Also, no significant\ndifference in the semantic quality of generated rationales between human and\nASR transcripts. All code, data (English-translated and Vietnamese) and models\nare published online: https://github.com/leduckhai/MultiMed."
                },
                "authors": [
                    {
                        "name": "Khai-Nguyen Nguyen"
                    },
                    {
                        "name": "Khai Le-Duc"
                    },
                    {
                        "name": "Bach Phan Tat"
                    },
                    {
                        "name": "Duy Le"
                    },
                    {
                        "name": "Long Vo-Dang"
                    },
                    {
                        "name": "Truong-Son Hy"
                    }
                ],
                "author_detail": {
                    "name": "Truong-Son Hy"
                },
                "author": "Truong-Son Hy",
                "arxiv_comment": "NeurIPS AIM-FM Workshop, 20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21054v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21054v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15992v2",
                "updated": "2024-10-11T05:42:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    5,
                    42,
                    1,
                    4,
                    285,
                    0
                ],
                "published": "2024-06-23T02:59:15Z",
                "published_parsed": [
                    2024,
                    6,
                    23,
                    2,
                    59,
                    15,
                    6,
                    175,
                    0
                ],
                "title": "Can LLM Graph Reasoning Generalize beyond Pattern Memorization?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLM Graph Reasoning Generalize beyond Pattern Memorization?"
                },
                "summary": "Large language models (LLMs) demonstrate great potential for problems with\nimplicit graphical structures, while recent works seek to enhance the graph\nreasoning capabilities of LLMs through specialized instruction tuning. The\nresulting 'graph LLMs' are evaluated with in-distribution settings only, thus\nit remains underexplored whether LLMs are learning generalizable graph\nreasoning skills or merely memorizing patterns in the synthetic training data.\nTo this end, we propose the NLGift benchmark, an evaluation suite of LLM graph\nreasoning generalization: whether LLMs could go beyond semantic, numeric,\nstructural, reasoning patterns in the synthetic training data and improve\nutility on real-world graph-based tasks. Extensive experiments with two LLMs\nacross four graph reasoning tasks demonstrate that while generalization on\nsimple patterns (semantic, numeric) is somewhat satisfactory, LLMs struggle to\ngeneralize across reasoning and real-world patterns, casting doubt on the\nbenefit of synthetic graph tuning for real-world tasks with underlying network\nstructures. We explore three strategies to improve LLM graph reasoning\ngeneralization, and we find that while post-training alignment is most\npromising for real-world tasks, empowering LLM graph reasoning to go beyond\npattern memorization remains an open research question.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate great potential for problems with\nimplicit graphical structures, while recent works seek to enhance the graph\nreasoning capabilities of LLMs through specialized instruction tuning. The\nresulting 'graph LLMs' are evaluated with in-distribution settings only, thus\nit remains underexplored whether LLMs are learning generalizable graph\nreasoning skills or merely memorizing patterns in the synthetic training data.\nTo this end, we propose the NLGift benchmark, an evaluation suite of LLM graph\nreasoning generalization: whether LLMs could go beyond semantic, numeric,\nstructural, reasoning patterns in the synthetic training data and improve\nutility on real-world graph-based tasks. Extensive experiments with two LLMs\nacross four graph reasoning tasks demonstrate that while generalization on\nsimple patterns (semantic, numeric) is somewhat satisfactory, LLMs struggle to\ngeneralize across reasoning and real-world patterns, casting doubt on the\nbenefit of synthetic graph tuning for real-world tasks with underlying network\nstructures. We explore three strategies to improve LLM graph reasoning\ngeneralization, and we find that while post-training alignment is most\npromising for real-world tasks, empowering LLM graph reasoning to go beyond\npattern memorization remains an open research question."
                },
                "authors": [
                    {
                        "name": "Yizhuo Zhang"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Shangbin Feng"
                    },
                    {
                        "name": "Zhaoxuan Tan"
                    },
                    {
                        "name": "Xiaochuang Han"
                    },
                    {
                        "name": "Tianxing He"
                    },
                    {
                        "name": "Yulia Tsvetkov"
                    }
                ],
                "author_detail": {
                    "name": "Yulia Tsvetkov"
                },
                "author": "Yulia Tsvetkov",
                "arxiv_comment": "17 pages, 6 figures. EMNLP 2024 Findings. Code and data is publicly\n  available at https://github.com/MatthewYZhang/NLGift",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08001v2",
                "updated": "2024-10-11T05:38:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    5,
                    38,
                    13,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-10T14:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    57,
                    51,
                    3,
                    284,
                    0
                ],
                "title": "Towards Synergistic, Generalized, and Efficient Dual-System for Robotic\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Synergistic, Generalized, and Efficient Dual-System for Robotic\n  Manipulation"
                },
                "summary": "The increasing demand for versatile robotic systems to operate in diverse and\ndynamic environments has emphasized the importance of a generalist policy,\nwhich leverages a large cross-embodiment data corpus to facilitate broad\nadaptability and high-level reasoning. However, the generalist would struggle\nwith inefficient inference and cost-expensive training. The specialist policy,\ninstead, is curated for specific domain data and excels at task-level precision\nwith efficiency. Yet, it lacks the generalization capacity for a wide range of\napplications. Inspired by these observations, we introduce RoboDual, a\nsynergistic dual-system that supplements the merits of both generalist and\nspecialist policy. A diffusion transformer-based specialist is devised for\nmulti-step action rollouts, exquisitely conditioned on the high-level task\nunderstanding and discretized action output of a vision-language-action (VLA)\nbased generalist. Compared to OpenVLA, RoboDual achieves 26.7% improvement in\nreal-world setting and 12% gain on CALVIN by introducing a specialist policy\nwith merely 20M trainable parameters. It maintains strong performance with 5%\nof demonstration data only, and enables a 3.8 times higher control frequency in\nreal-world deployment. Code would be made publicly available. Our project page\nis hosted at: https://opendrivelab.com/RoboDual/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for versatile robotic systems to operate in diverse and\ndynamic environments has emphasized the importance of a generalist policy,\nwhich leverages a large cross-embodiment data corpus to facilitate broad\nadaptability and high-level reasoning. However, the generalist would struggle\nwith inefficient inference and cost-expensive training. The specialist policy,\ninstead, is curated for specific domain data and excels at task-level precision\nwith efficiency. Yet, it lacks the generalization capacity for a wide range of\napplications. Inspired by these observations, we introduce RoboDual, a\nsynergistic dual-system that supplements the merits of both generalist and\nspecialist policy. A diffusion transformer-based specialist is devised for\nmulti-step action rollouts, exquisitely conditioned on the high-level task\nunderstanding and discretized action output of a vision-language-action (VLA)\nbased generalist. Compared to OpenVLA, RoboDual achieves 26.7% improvement in\nreal-world setting and 12% gain on CALVIN by introducing a specialist policy\nwith merely 20M trainable parameters. It maintains strong performance with 5%\nof demonstration data only, and enables a 3.8 times higher control frequency in\nreal-world deployment. Code would be made publicly available. Our project page\nis hosted at: https://opendrivelab.com/RoboDual/"
                },
                "authors": [
                    {
                        "name": "Qingwen Bu"
                    },
                    {
                        "name": "Hongyang Li"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Jisong Cai"
                    },
                    {
                        "name": "Jia Zeng"
                    },
                    {
                        "name": "Heming Cui"
                    },
                    {
                        "name": "Maoqing Yao"
                    },
                    {
                        "name": "Yu Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Yu Qiao"
                },
                "author": "Yu Qiao",
                "arxiv_comment": "Project page: https://opendrivelab.com/RoboDual/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08027v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08027v2",
                "updated": "2024-10-11T05:31:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    5,
                    31,
                    50,
                    4,
                    285,
                    0
                ],
                "published": "2024-08-15T08:50:58Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    8,
                    50,
                    58,
                    3,
                    228,
                    0
                ],
                "title": "Enhancing Large Language Model-based Speech Recognition by\n  Contextualization for Rare and Ambiguous Words",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Language Model-based Speech Recognition by\n  Contextualization for Rare and Ambiguous Words"
                },
                "summary": "We develop a large language model (LLM) based automatic speech recognition\n(ASR) system that can be contextualized by providing keywords as prior\ninformation in text prompts. We adopt decoder-only architecture and use our\nin-house LLM, PLaMo-100B, pre-trained from scratch using datasets dominated by\nJapanese and English texts as the decoder. We adopt a pre-trained Whisper\nencoder as an audio encoder, and the audio embeddings from the audio encoder\nare projected to the text embedding space by an adapter layer and concatenated\nwith text embeddings converted from text prompts to form inputs to the decoder.\nBy providing keywords as prior information in the text prompts, we can\ncontextualize our LLM-based ASR system without modifying the model architecture\nto transcribe ambiguous words in the input audio accurately. Experimental\nresults demonstrate that providing keywords to the decoder can significantly\nimprove the recognition performance of rare and ambiguous words.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a large language model (LLM) based automatic speech recognition\n(ASR) system that can be contextualized by providing keywords as prior\ninformation in text prompts. We adopt decoder-only architecture and use our\nin-house LLM, PLaMo-100B, pre-trained from scratch using datasets dominated by\nJapanese and English texts as the decoder. We adopt a pre-trained Whisper\nencoder as an audio encoder, and the audio embeddings from the audio encoder\nare projected to the text embedding space by an adapter layer and concatenated\nwith text embeddings converted from text prompts to form inputs to the decoder.\nBy providing keywords as prior information in the text prompts, we can\ncontextualize our LLM-based ASR system without modifying the model architecture\nto transcribe ambiguous words in the input audio accurately. Experimental\nresults demonstrate that providing keywords to the decoder can significantly\nimprove the recognition performance of rare and ambiguous words."
                },
                "authors": [
                    {
                        "name": "Kento Nozawa"
                    },
                    {
                        "name": "Takashi Masuko"
                    },
                    {
                        "name": "Toru Taniguchi"
                    }
                ],
                "author_detail": {
                    "name": "Toru Taniguchi"
                },
                "author": "Toru Taniguchi",
                "arxiv_comment": "13 pages, 1 figure, and 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08027v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08027v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12624v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12624v3",
                "updated": "2024-10-11T05:29:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    5,
                    29,
                    18,
                    4,
                    285,
                    0
                ],
                "published": "2024-06-18T13:49:54Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    13,
                    49,
                    54,
                    1,
                    170,
                    0
                ],
                "title": "Judging the Judges: Evaluating Alignment and Vulnerabilities in\n  LLMs-as-Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judging the Judges: Evaluating Alignment and Vulnerabilities in\n  LLMs-as-Judges"
                },
                "summary": "Offering a promising solution to the scalability challenges associated with\nhuman evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an\napproach to evaluating large language models (LLMs). However, there are still\nmany open questions about the strengths and weaknesses of this paradigm, and\nwhat potential biases it may hold. In this paper, we present a comprehensive\nstudy of the performance of various LLMs acting as judges, focusing on a clean\nscenario in which inter-human agreement is high. Investigating thirteen judge\nmodels of different model sizes and families, judging answers of nine different\n'examtaker models' - both base and instruction-tuned - we find that only the\nbest (and largest) models achieve reasonable alignment with humans. However,\nthey are still quite far behind inter-human agreement and their assigned scores\nmay still differ with up to 5 points from human-assigned scores. In terms of\ntheir ranking of the nine exam-taker models, instead, also smaller models and\neven the lexical metric contains may provide a reasonable signal. Through error\nanalysis and other studies, we identify vulnerabilities in judge models, such\nas their sensitivity to prompt complexity and length, and a tendency toward\nleniency. The fact that even the best judges differ from humans in this\ncomparatively simple setup suggest that caution may be wise when using judges\nin more complex setups. Lastly, our research rediscovers the importance of\nusing alignment metrics beyond simple percent alignment, showing that judges\nwith high percent agreement can still assign vastly different scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offering a promising solution to the scalability challenges associated with\nhuman evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an\napproach to evaluating large language models (LLMs). However, there are still\nmany open questions about the strengths and weaknesses of this paradigm, and\nwhat potential biases it may hold. In this paper, we present a comprehensive\nstudy of the performance of various LLMs acting as judges, focusing on a clean\nscenario in which inter-human agreement is high. Investigating thirteen judge\nmodels of different model sizes and families, judging answers of nine different\n'examtaker models' - both base and instruction-tuned - we find that only the\nbest (and largest) models achieve reasonable alignment with humans. However,\nthey are still quite far behind inter-human agreement and their assigned scores\nmay still differ with up to 5 points from human-assigned scores. In terms of\ntheir ranking of the nine exam-taker models, instead, also smaller models and\neven the lexical metric contains may provide a reasonable signal. Through error\nanalysis and other studies, we identify vulnerabilities in judge models, such\nas their sensitivity to prompt complexity and length, and a tendency toward\nleniency. The fact that even the best judges differ from humans in this\ncomparatively simple setup suggest that caution may be wise when using judges\nin more complex setups. Lastly, our research rediscovers the importance of\nusing alignment metrics beyond simple percent alignment, showing that judges\nwith high percent agreement can still assign vastly different scores."
                },
                "authors": [
                    {
                        "name": "Aman Singh Thakur"
                    },
                    {
                        "name": "Kartik Choudhary"
                    },
                    {
                        "name": "Venkat Srinik Ramayapally"
                    },
                    {
                        "name": "Sankaran Vaidyanathan"
                    },
                    {
                        "name": "Dieuwke Hupkes"
                    }
                ],
                "author_detail": {
                    "name": "Dieuwke Hupkes"
                },
                "author": "Dieuwke Hupkes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12624v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12624v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06541v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06541v2",
                "updated": "2024-10-11T05:20:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    5,
                    20,
                    19,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-09T04:35:22Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    4,
                    35,
                    22,
                    2,
                    283,
                    0
                ],
                "title": "Chip-Tuning: Classify Before Language Models Say",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chip-Tuning: Classify Before Language Models Say"
                },
                "summary": "The rapid development in the performance of large language models (LLMs) is\naccompanied by the escalation of model size, leading to the increasing cost of\nmodel training and inference. Previous research has discovered that certain\nlayers in LLMs exhibit redundancy, and removing these layers brings only\nmarginal loss in model performance. In this paper, we adopt the probing\ntechnique to explain the layer redundancy in LLMs and demonstrate that language\nmodels can be effectively pruned with probing classifiers. We propose\nchip-tuning, a simple and effective structured pruning framework specialized\nfor classification problems. Chip-tuning attaches tiny probing classifiers\nnamed chips to different layers of LLMs, and trains chips with the backbone\nmodel frozen. After selecting a chip for classification, all layers subsequent\nto the attached layer could be removed with marginal performance loss.\nExperimental results on various LLMs and datasets demonstrate that chip-tuning\nsignificantly outperforms previous state-of-the-art baselines in both accuracy\nand pruning ratio, achieving a pruning ratio of up to 50%. We also find that\nchip-tuning could be applied on multimodal models, and could be combined with\nmodel finetuning, proving its excellent compatibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development in the performance of large language models (LLMs) is\naccompanied by the escalation of model size, leading to the increasing cost of\nmodel training and inference. Previous research has discovered that certain\nlayers in LLMs exhibit redundancy, and removing these layers brings only\nmarginal loss in model performance. In this paper, we adopt the probing\ntechnique to explain the layer redundancy in LLMs and demonstrate that language\nmodels can be effectively pruned with probing classifiers. We propose\nchip-tuning, a simple and effective structured pruning framework specialized\nfor classification problems. Chip-tuning attaches tiny probing classifiers\nnamed chips to different layers of LLMs, and trains chips with the backbone\nmodel frozen. After selecting a chip for classification, all layers subsequent\nto the attached layer could be removed with marginal performance loss.\nExperimental results on various LLMs and datasets demonstrate that chip-tuning\nsignificantly outperforms previous state-of-the-art baselines in both accuracy\nand pruning ratio, achieving a pruning ratio of up to 50%. We also find that\nchip-tuning could be applied on multimodal models, and could be combined with\nmodel finetuning, proving its excellent compatibility."
                },
                "authors": [
                    {
                        "name": "Fangwei Zhu"
                    },
                    {
                        "name": "Dian Li"
                    },
                    {
                        "name": "Jiajun Huang"
                    },
                    {
                        "name": "Gang Liu"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Zhifang Sui"
                    }
                ],
                "author_detail": {
                    "name": "Zhifang Sui"
                },
                "author": "Zhifang Sui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06541v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06541v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]